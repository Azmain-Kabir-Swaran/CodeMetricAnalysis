{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PBImageXmlWriter.java",
  "functionName": "dumpINodeFile",
  "functionId": "dumpINodeFile___f-INodeSection.INodeFile",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
  "functionStartLine": 520,
  "functionEndLine": 560,
  "numCommitsSeen": 27,
  "timeTaken": 4927,
  "changeHistory": [
    "46d37a65cf09c2714b4c0c4ec0399031d60027a5",
    "5ca6ef0c268b1acb3abf12505b9ead6fe7e38a23",
    "55c07bbed2f475f7b584a86112ee1b6fe0221e98",
    "a2a5d7b5bca715835d92816e7b267b59f7270708",
    "680716f31e120f4d3ee70b095e4db46c05b891d9",
    "700b0e4019cf483f7532609711812150b8c44742",
    "a24c6e84205c684ef864b0fc5301dc07b3578351",
    "cc432885adb0182c2c5b3bf92edde12231fd567c",
    "bb84f1fccb18c6c7373851e05d2451d55e908242",
    "042b33f20b01aadb5cd03da731ae7a3d94026aac",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305"
  ],
  "changeHistoryShort": {
    "46d37a65cf09c2714b4c0c4ec0399031d60027a5": "Ybodychange",
    "5ca6ef0c268b1acb3abf12505b9ead6fe7e38a23": "Ybodychange",
    "55c07bbed2f475f7b584a86112ee1b6fe0221e98": "Ybodychange",
    "a2a5d7b5bca715835d92816e7b267b59f7270708": "Ybodychange",
    "680716f31e120f4d3ee70b095e4db46c05b891d9": "Ybodychange",
    "700b0e4019cf483f7532609711812150b8c44742": "Ybodychange",
    "a24c6e84205c684ef864b0fc5301dc07b3578351": "Ybodychange",
    "cc432885adb0182c2c5b3bf92edde12231fd567c": "Ybodychange",
    "bb84f1fccb18c6c7373851e05d2451d55e908242": "Ybodychange",
    "042b33f20b01aadb5cd03da731ae7a3d94026aac": "Ybodychange",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Yintroduced"
  },
  "changeHistoryDetails": {
    "46d37a65cf09c2714b4c0c4ec0399031d60027a5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10506. Addendum patch to include missing changes. Contributed by Akira Ajisaka.\n",
      "commitDate": "27/03/17 5:23 AM",
      "commitName": "46d37a65cf09c2714b4c0c4ec0399031d60027a5",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "08/03/17 3:36 PM",
      "commitNameOld": "5ca6ef0c268b1acb3abf12505b9ead6fe7e38a23",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 18.53,
      "commitsBetweenForRepo": 101,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   private void dumpINodeFile(INodeSection.INodeFile f) {\n     if (f.hasErasureCodingPolicyID()) {\n       o(SECTION_REPLICATION, INodeFile.DEFAULT_REPL_FOR_STRIPED_BLOCKS);\n     } else {\n       o(SECTION_REPLICATION, f.getReplication());\n     }\n     o(INODE_SECTION_MTIME, f.getModificationTime())\n         .o(INODE_SECTION_ATIME, f.getAccessTime())\n         .o(INODE_SECTION_PREFERRED_BLOCK_SIZE, f.getPreferredBlockSize())\n         .o(INODE_SECTION_PERMISSION, dumpPermission(f.getPermission()));\n     if (f.hasXAttrs()) {\n       dumpXattrs(f.getXAttrs());\n     }\n     dumpAcls(f.getAcl());\n     if (f.getBlocksCount() \u003e 0) {\n       out.print(\"\u003c\" + INODE_SECTION_BLOCKS + \"\u003e\");\n       for (BlockProto b : f.getBlocksList()) {\n         out.print(\"\u003c\" + INODE_SECTION_BLOCK + \"\u003e\");\n         o(SECTION_ID, b.getBlockId())\n-            .o(INODE_SECTION_GEMSTAMP, b.getGenStamp())\n+            .o(INODE_SECTION_GENSTAMP, b.getGenStamp())\n             .o(INODE_SECTION_NUM_BYTES, b.getNumBytes());\n         out.print(\"\u003c/\" + INODE_SECTION_BLOCK + \"\u003e\\n\");\n       }\n       out.print(\"\u003c/\" + INODE_SECTION_BLOCKS + \"\u003e\\n\");\n     }\n     if (f.hasStoragePolicyID()) {\n       o(INODE_SECTION_STORAGE_POLICY_ID, f.getStoragePolicyID());\n     }\n     if (f.hasErasureCodingPolicyID()) {\n       o(INODE_SECTION_BLOCK_TYPE, f.getBlockType().name());\n       o(INODE_SECTION_EC_POLICY_ID, f.getErasureCodingPolicyID());\n     }\n \n     if (f.hasFileUC()) {\n       INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n       out.print(\"\u003c\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\");\n       o(INODE_SECTION_CLIENT_NAME, u.getClientName())\n           .o(INODE_SECTION_CLIENT_MACHINE, u.getClientMachine());\n       out.print(\"\u003c/\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\\n\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeFile(INodeSection.INodeFile f) {\n    if (f.hasErasureCodingPolicyID()) {\n      o(SECTION_REPLICATION, INodeFile.DEFAULT_REPL_FOR_STRIPED_BLOCKS);\n    } else {\n      o(SECTION_REPLICATION, f.getReplication());\n    }\n    o(INODE_SECTION_MTIME, f.getModificationTime())\n        .o(INODE_SECTION_ATIME, f.getAccessTime())\n        .o(INODE_SECTION_PREFERRED_BLOCK_SIZE, f.getPreferredBlockSize())\n        .o(INODE_SECTION_PERMISSION, dumpPermission(f.getPermission()));\n    if (f.hasXAttrs()) {\n      dumpXattrs(f.getXAttrs());\n    }\n    dumpAcls(f.getAcl());\n    if (f.getBlocksCount() \u003e 0) {\n      out.print(\"\u003c\" + INODE_SECTION_BLOCKS + \"\u003e\");\n      for (BlockProto b : f.getBlocksList()) {\n        out.print(\"\u003c\" + INODE_SECTION_BLOCK + \"\u003e\");\n        o(SECTION_ID, b.getBlockId())\n            .o(INODE_SECTION_GENSTAMP, b.getGenStamp())\n            .o(INODE_SECTION_NUM_BYTES, b.getNumBytes());\n        out.print(\"\u003c/\" + INODE_SECTION_BLOCK + \"\u003e\\n\");\n      }\n      out.print(\"\u003c/\" + INODE_SECTION_BLOCKS + \"\u003e\\n\");\n    }\n    if (f.hasStoragePolicyID()) {\n      o(INODE_SECTION_STORAGE_POLICY_ID, f.getStoragePolicyID());\n    }\n    if (f.hasErasureCodingPolicyID()) {\n      o(INODE_SECTION_BLOCK_TYPE, f.getBlockType().name());\n      o(INODE_SECTION_EC_POLICY_ID, f.getErasureCodingPolicyID());\n    }\n\n    if (f.hasFileUC()) {\n      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n      out.print(\"\u003c\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\");\n      o(INODE_SECTION_CLIENT_NAME, u.getClientName())\n          .o(INODE_SECTION_CLIENT_MACHINE, u.getClientMachine());\n      out.print(\"\u003c/\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\\n\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "5ca6ef0c268b1acb3abf12505b9ead6fe7e38a23": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10983. OIV tool should make an EC file explicit. Contributed by Manoj Govindassamy.\n",
      "commitDate": "08/03/17 3:36 PM",
      "commitName": "5ca6ef0c268b1acb3abf12505b9ead6fe7e38a23",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/02/17 5:07 PM",
      "commitNameOld": "55c07bbed2f475f7b584a86112ee1b6fe0221e98",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 8.94,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,41 @@\n   private void dumpINodeFile(INodeSection.INodeFile f) {\n-    o(SECTION_REPLICATION, f.getReplication())\n-        .o(INODE_SECTION_MTIME, f.getModificationTime())\n+    if (f.hasErasureCodingPolicyID()) {\n+      o(SECTION_REPLICATION, INodeFile.DEFAULT_REPL_FOR_STRIPED_BLOCKS);\n+    } else {\n+      o(SECTION_REPLICATION, f.getReplication());\n+    }\n+    o(INODE_SECTION_MTIME, f.getModificationTime())\n         .o(INODE_SECTION_ATIME, f.getAccessTime())\n         .o(INODE_SECTION_PREFERRED_BLOCK_SIZE, f.getPreferredBlockSize())\n         .o(INODE_SECTION_PERMISSION, dumpPermission(f.getPermission()));\n     if (f.hasXAttrs()) {\n       dumpXattrs(f.getXAttrs());\n     }\n     dumpAcls(f.getAcl());\n     if (f.getBlocksCount() \u003e 0) {\n       out.print(\"\u003c\" + INODE_SECTION_BLOCKS + \"\u003e\");\n       for (BlockProto b : f.getBlocksList()) {\n         out.print(\"\u003c\" + INODE_SECTION_BLOCK + \"\u003e\");\n         o(SECTION_ID, b.getBlockId())\n             .o(INODE_SECTION_GEMSTAMP, b.getGenStamp())\n             .o(INODE_SECTION_NUM_BYTES, b.getNumBytes());\n         out.print(\"\u003c/\" + INODE_SECTION_BLOCK + \"\u003e\\n\");\n       }\n       out.print(\"\u003c/\" + INODE_SECTION_BLOCKS + \"\u003e\\n\");\n     }\n     if (f.hasStoragePolicyID()) {\n       o(INODE_SECTION_STORAGE_POLICY_ID, f.getStoragePolicyID());\n     }\n-    if (f.getBlockType() !\u003d BlockTypeProto.CONTIGUOUS) {\n+    if (f.hasErasureCodingPolicyID()) {\n       o(INODE_SECTION_BLOCK_TYPE, f.getBlockType().name());\n+      o(INODE_SECTION_EC_POLICY_ID, f.getErasureCodingPolicyID());\n     }\n \n     if (f.hasFileUC()) {\n       INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n       out.print(\"\u003c\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\");\n       o(INODE_SECTION_CLIENT_NAME, u.getClientName())\n           .o(INODE_SECTION_CLIENT_MACHINE, u.getClientMachine());\n       out.print(\"\u003c/\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\\n\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeFile(INodeSection.INodeFile f) {\n    if (f.hasErasureCodingPolicyID()) {\n      o(SECTION_REPLICATION, INodeFile.DEFAULT_REPL_FOR_STRIPED_BLOCKS);\n    } else {\n      o(SECTION_REPLICATION, f.getReplication());\n    }\n    o(INODE_SECTION_MTIME, f.getModificationTime())\n        .o(INODE_SECTION_ATIME, f.getAccessTime())\n        .o(INODE_SECTION_PREFERRED_BLOCK_SIZE, f.getPreferredBlockSize())\n        .o(INODE_SECTION_PERMISSION, dumpPermission(f.getPermission()));\n    if (f.hasXAttrs()) {\n      dumpXattrs(f.getXAttrs());\n    }\n    dumpAcls(f.getAcl());\n    if (f.getBlocksCount() \u003e 0) {\n      out.print(\"\u003c\" + INODE_SECTION_BLOCKS + \"\u003e\");\n      for (BlockProto b : f.getBlocksList()) {\n        out.print(\"\u003c\" + INODE_SECTION_BLOCK + \"\u003e\");\n        o(SECTION_ID, b.getBlockId())\n            .o(INODE_SECTION_GEMSTAMP, b.getGenStamp())\n            .o(INODE_SECTION_NUM_BYTES, b.getNumBytes());\n        out.print(\"\u003c/\" + INODE_SECTION_BLOCK + \"\u003e\\n\");\n      }\n      out.print(\"\u003c/\" + INODE_SECTION_BLOCKS + \"\u003e\\n\");\n    }\n    if (f.hasStoragePolicyID()) {\n      o(INODE_SECTION_STORAGE_POLICY_ID, f.getStoragePolicyID());\n    }\n    if (f.hasErasureCodingPolicyID()) {\n      o(INODE_SECTION_BLOCK_TYPE, f.getBlockType().name());\n      o(INODE_SECTION_EC_POLICY_ID, f.getErasureCodingPolicyID());\n    }\n\n    if (f.hasFileUC()) {\n      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n      out.print(\"\u003c\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\");\n      o(INODE_SECTION_CLIENT_NAME, u.getClientName())\n          .o(INODE_SECTION_CLIENT_MACHINE, u.getClientMachine());\n      out.print(\"\u003c/\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\\n\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "55c07bbed2f475f7b584a86112ee1b6fe0221e98": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11382. Persist Erasure Coding Policy ID in a new optional field in INodeFile in FSImage. Contributed by Manoj Govindassamy.\n",
      "commitDate": "27/02/17 5:07 PM",
      "commitName": "55c07bbed2f475f7b584a86112ee1b6fe0221e98",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "25/02/17 2:38 PM",
      "commitNameOld": "05391c1845639d4f01da8e5df966e2dc2682f2ca",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 2.1,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,36 @@\n   private void dumpINodeFile(INodeSection.INodeFile f) {\n     o(SECTION_REPLICATION, f.getReplication())\n         .o(INODE_SECTION_MTIME, f.getModificationTime())\n         .o(INODE_SECTION_ATIME, f.getAccessTime())\n         .o(INODE_SECTION_PREFERRED_BLOCK_SIZE, f.getPreferredBlockSize())\n         .o(INODE_SECTION_PERMISSION, dumpPermission(f.getPermission()));\n     if (f.hasXAttrs()) {\n       dumpXattrs(f.getXAttrs());\n     }\n     dumpAcls(f.getAcl());\n     if (f.getBlocksCount() \u003e 0) {\n       out.print(\"\u003c\" + INODE_SECTION_BLOCKS + \"\u003e\");\n       for (BlockProto b : f.getBlocksList()) {\n         out.print(\"\u003c\" + INODE_SECTION_BLOCK + \"\u003e\");\n         o(SECTION_ID, b.getBlockId())\n             .o(INODE_SECTION_GEMSTAMP, b.getGenStamp())\n             .o(INODE_SECTION_NUM_BYTES, b.getNumBytes());\n         out.print(\"\u003c/\" + INODE_SECTION_BLOCK + \"\u003e\\n\");\n       }\n       out.print(\"\u003c/\" + INODE_SECTION_BLOCKS + \"\u003e\\n\");\n     }\n     if (f.hasStoragePolicyID()) {\n       o(INODE_SECTION_STORAGE_POLICY_ID, f.getStoragePolicyID());\n     }\n     if (f.getBlockType() !\u003d BlockTypeProto.CONTIGUOUS) {\n-      out.print(\"\u003c\" + INODE_SECTION_BLOCK_TYPE + \"\u003e\");\n-      o(SECTION_NAME, f.getBlockType().name());\n-      out.print(\"\u003c/\" + INODE_SECTION_BLOCK_TYPE + \"\u003e\\n\");\n+      o(INODE_SECTION_BLOCK_TYPE, f.getBlockType().name());\n     }\n \n     if (f.hasFileUC()) {\n       INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n       out.print(\"\u003c\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\");\n       o(INODE_SECTION_CLIENT_NAME, u.getClientName())\n           .o(INODE_SECTION_CLIENT_MACHINE, u.getClientMachine());\n       out.print(\"\u003c/\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\\n\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeFile(INodeSection.INodeFile f) {\n    o(SECTION_REPLICATION, f.getReplication())\n        .o(INODE_SECTION_MTIME, f.getModificationTime())\n        .o(INODE_SECTION_ATIME, f.getAccessTime())\n        .o(INODE_SECTION_PREFERRED_BLOCK_SIZE, f.getPreferredBlockSize())\n        .o(INODE_SECTION_PERMISSION, dumpPermission(f.getPermission()));\n    if (f.hasXAttrs()) {\n      dumpXattrs(f.getXAttrs());\n    }\n    dumpAcls(f.getAcl());\n    if (f.getBlocksCount() \u003e 0) {\n      out.print(\"\u003c\" + INODE_SECTION_BLOCKS + \"\u003e\");\n      for (BlockProto b : f.getBlocksList()) {\n        out.print(\"\u003c\" + INODE_SECTION_BLOCK + \"\u003e\");\n        o(SECTION_ID, b.getBlockId())\n            .o(INODE_SECTION_GEMSTAMP, b.getGenStamp())\n            .o(INODE_SECTION_NUM_BYTES, b.getNumBytes());\n        out.print(\"\u003c/\" + INODE_SECTION_BLOCK + \"\u003e\\n\");\n      }\n      out.print(\"\u003c/\" + INODE_SECTION_BLOCKS + \"\u003e\\n\");\n    }\n    if (f.hasStoragePolicyID()) {\n      o(INODE_SECTION_STORAGE_POLICY_ID, f.getStoragePolicyID());\n    }\n    if (f.getBlockType() !\u003d BlockTypeProto.CONTIGUOUS) {\n      o(INODE_SECTION_BLOCK_TYPE, f.getBlockType().name());\n    }\n\n    if (f.hasFileUC()) {\n      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n      out.print(\"\u003c\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\");\n      o(INODE_SECTION_CLIENT_NAME, u.getClientName())\n          .o(INODE_SECTION_CLIENT_MACHINE, u.getClientMachine());\n      out.print(\"\u003c/\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\\n\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "a2a5d7b5bca715835d92816e7b267b59f7270708": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10759. Change fsimage bool isStriped from boolean to an enum. Contributed by Ewan Higgs.\n",
      "commitDate": "18/01/17 1:31 PM",
      "commitName": "a2a5d7b5bca715835d92816e7b267b59f7270708",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "21/03/16 11:40 AM",
      "commitNameOld": "680716f31e120f4d3ee70b095e4db46c05b891d9",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 303.12,
      "commitsBetweenForRepo": 2118,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,38 @@\n   private void dumpINodeFile(INodeSection.INodeFile f) {\n     o(SECTION_REPLICATION, f.getReplication())\n         .o(INODE_SECTION_MTIME, f.getModificationTime())\n         .o(INODE_SECTION_ATIME, f.getAccessTime())\n         .o(INODE_SECTION_PREFERRED_BLOCK_SIZE, f.getPreferredBlockSize())\n         .o(INODE_SECTION_PERMISSION, dumpPermission(f.getPermission()));\n     if (f.hasXAttrs()) {\n       dumpXattrs(f.getXAttrs());\n     }\n     dumpAcls(f.getAcl());\n     if (f.getBlocksCount() \u003e 0) {\n       out.print(\"\u003c\" + INODE_SECTION_BLOCKS + \"\u003e\");\n       for (BlockProto b : f.getBlocksList()) {\n         out.print(\"\u003c\" + INODE_SECTION_BLOCK + \"\u003e\");\n         o(SECTION_ID, b.getBlockId())\n             .o(INODE_SECTION_GEMSTAMP, b.getGenStamp())\n             .o(INODE_SECTION_NUM_BYTES, b.getNumBytes());\n         out.print(\"\u003c/\" + INODE_SECTION_BLOCK + \"\u003e\\n\");\n       }\n       out.print(\"\u003c/\" + INODE_SECTION_BLOCKS + \"\u003e\\n\");\n     }\n     if (f.hasStoragePolicyID()) {\n       o(INODE_SECTION_STORAGE_POLICY_ID, f.getStoragePolicyID());\n     }\n-    if (f.getIsStriped()) {\n-      out.print(\"\u003c\" + INODE_SECTION_IS_STRIPED + \"/\u003e\");\n+    if (f.getBlockType() !\u003d BlockTypeProto.CONTIGUOUS) {\n+      out.print(\"\u003c\" + INODE_SECTION_BLOCK_TYPE + \"\u003e\");\n+      o(SECTION_NAME, f.getBlockType().name());\n+      out.print(\"\u003c/\" + INODE_SECTION_BLOCK_TYPE + \"\u003e\\n\");\n     }\n \n     if (f.hasFileUC()) {\n       INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n       out.print(\"\u003c\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\");\n       o(INODE_SECTION_CLIENT_NAME, u.getClientName())\n           .o(INODE_SECTION_CLIENT_MACHINE, u.getClientMachine());\n       out.print(\"\u003c/\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\\n\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeFile(INodeSection.INodeFile f) {\n    o(SECTION_REPLICATION, f.getReplication())\n        .o(INODE_SECTION_MTIME, f.getModificationTime())\n        .o(INODE_SECTION_ATIME, f.getAccessTime())\n        .o(INODE_SECTION_PREFERRED_BLOCK_SIZE, f.getPreferredBlockSize())\n        .o(INODE_SECTION_PERMISSION, dumpPermission(f.getPermission()));\n    if (f.hasXAttrs()) {\n      dumpXattrs(f.getXAttrs());\n    }\n    dumpAcls(f.getAcl());\n    if (f.getBlocksCount() \u003e 0) {\n      out.print(\"\u003c\" + INODE_SECTION_BLOCKS + \"\u003e\");\n      for (BlockProto b : f.getBlocksList()) {\n        out.print(\"\u003c\" + INODE_SECTION_BLOCK + \"\u003e\");\n        o(SECTION_ID, b.getBlockId())\n            .o(INODE_SECTION_GEMSTAMP, b.getGenStamp())\n            .o(INODE_SECTION_NUM_BYTES, b.getNumBytes());\n        out.print(\"\u003c/\" + INODE_SECTION_BLOCK + \"\u003e\\n\");\n      }\n      out.print(\"\u003c/\" + INODE_SECTION_BLOCKS + \"\u003e\\n\");\n    }\n    if (f.hasStoragePolicyID()) {\n      o(INODE_SECTION_STORAGE_POLICY_ID, f.getStoragePolicyID());\n    }\n    if (f.getBlockType() !\u003d BlockTypeProto.CONTIGUOUS) {\n      out.print(\"\u003c\" + INODE_SECTION_BLOCK_TYPE + \"\u003e\");\n      o(SECTION_NAME, f.getBlockType().name());\n      out.print(\"\u003c/\" + INODE_SECTION_BLOCK_TYPE + \"\u003e\\n\");\n    }\n\n    if (f.hasFileUC()) {\n      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n      out.print(\"\u003c\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\");\n      o(INODE_SECTION_CLIENT_NAME, u.getClientName())\n          .o(INODE_SECTION_CLIENT_MACHINE, u.getClientMachine());\n      out.print(\"\u003c/\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\\n\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "680716f31e120f4d3ee70b095e4db46c05b891d9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9951. Use string constants for XML tags in OfflineImageReconstructor (Lin Yiqun via cmccabe)\n",
      "commitDate": "21/03/16 11:40 AM",
      "commitName": "680716f31e120f4d3ee70b095e4db46c05b891d9",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "02/03/16 5:56 PM",
      "commitNameOld": "700b0e4019cf483f7532609711812150b8c44742",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 18.7,
      "commitsBetweenForRepo": 94,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,36 @@\n   private void dumpINodeFile(INodeSection.INodeFile f) {\n-    o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n-        .o(\"atime\", f.getAccessTime())\n-        .o(\"preferredBlockSize\", f.getPreferredBlockSize())\n-        .o(\"permission\", dumpPermission(f.getPermission()));\n+    o(SECTION_REPLICATION, f.getReplication())\n+        .o(INODE_SECTION_MTIME, f.getModificationTime())\n+        .o(INODE_SECTION_ATIME, f.getAccessTime())\n+        .o(INODE_SECTION_PREFERRED_BLOCK_SIZE, f.getPreferredBlockSize())\n+        .o(INODE_SECTION_PERMISSION, dumpPermission(f.getPermission()));\n     if (f.hasXAttrs()) {\n       dumpXattrs(f.getXAttrs());\n     }\n     dumpAcls(f.getAcl());\n     if (f.getBlocksCount() \u003e 0) {\n-      out.print(\"\u003cblocks\u003e\");\n+      out.print(\"\u003c\" + INODE_SECTION_BLOCKS + \"\u003e\");\n       for (BlockProto b : f.getBlocksList()) {\n-        out.print(\"\u003cblock\u003e\");\n-        o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n-            b.getNumBytes());\n-        out.print(\"\u003c/block\u003e\\n\");\n+        out.print(\"\u003c\" + INODE_SECTION_BLOCK + \"\u003e\");\n+        o(SECTION_ID, b.getBlockId())\n+            .o(INODE_SECTION_GEMSTAMP, b.getGenStamp())\n+            .o(INODE_SECTION_NUM_BYTES, b.getNumBytes());\n+        out.print(\"\u003c/\" + INODE_SECTION_BLOCK + \"\u003e\\n\");\n       }\n-      out.print(\"\u003c/blocks\u003e\\n\");\n+      out.print(\"\u003c/\" + INODE_SECTION_BLOCKS + \"\u003e\\n\");\n     }\n     if (f.hasStoragePolicyID()) {\n-      o(\"storagePolicyId\", f.getStoragePolicyID());\n+      o(INODE_SECTION_STORAGE_POLICY_ID, f.getStoragePolicyID());\n     }\n     if (f.getIsStriped()) {\n-      out.print(\"\u003cisStriped/\u003e\");\n+      out.print(\"\u003c\" + INODE_SECTION_IS_STRIPED + \"/\u003e\");\n     }\n \n     if (f.hasFileUC()) {\n       INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n-      out.print(\"\u003cfile-under-construction\u003e\");\n-      o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n-          u.getClientMachine());\n-      out.print(\"\u003c/file-under-construction\u003e\\n\");\n+      out.print(\"\u003c\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\");\n+      o(INODE_SECTION_CLIENT_NAME, u.getClientName())\n+          .o(INODE_SECTION_CLIENT_MACHINE, u.getClientMachine());\n+      out.print(\"\u003c/\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\\n\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeFile(INodeSection.INodeFile f) {\n    o(SECTION_REPLICATION, f.getReplication())\n        .o(INODE_SECTION_MTIME, f.getModificationTime())\n        .o(INODE_SECTION_ATIME, f.getAccessTime())\n        .o(INODE_SECTION_PREFERRED_BLOCK_SIZE, f.getPreferredBlockSize())\n        .o(INODE_SECTION_PERMISSION, dumpPermission(f.getPermission()));\n    if (f.hasXAttrs()) {\n      dumpXattrs(f.getXAttrs());\n    }\n    dumpAcls(f.getAcl());\n    if (f.getBlocksCount() \u003e 0) {\n      out.print(\"\u003c\" + INODE_SECTION_BLOCKS + \"\u003e\");\n      for (BlockProto b : f.getBlocksList()) {\n        out.print(\"\u003c\" + INODE_SECTION_BLOCK + \"\u003e\");\n        o(SECTION_ID, b.getBlockId())\n            .o(INODE_SECTION_GEMSTAMP, b.getGenStamp())\n            .o(INODE_SECTION_NUM_BYTES, b.getNumBytes());\n        out.print(\"\u003c/\" + INODE_SECTION_BLOCK + \"\u003e\\n\");\n      }\n      out.print(\"\u003c/\" + INODE_SECTION_BLOCKS + \"\u003e\\n\");\n    }\n    if (f.hasStoragePolicyID()) {\n      o(INODE_SECTION_STORAGE_POLICY_ID, f.getStoragePolicyID());\n    }\n    if (f.getIsStriped()) {\n      out.print(\"\u003c\" + INODE_SECTION_IS_STRIPED + \"/\u003e\");\n    }\n\n    if (f.hasFileUC()) {\n      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n      out.print(\"\u003c\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\");\n      o(INODE_SECTION_CLIENT_NAME, u.getClientName())\n          .o(INODE_SECTION_CLIENT_MACHINE, u.getClientMachine());\n      out.print(\"\u003c/\" + INODE_SECTION_FILE_UNDER_CONSTRUCTION + \"\u003e\\n\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "700b0e4019cf483f7532609711812150b8c44742": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9835. OIV: add ReverseXML processor which reconstructs an fsimage from an XML file (cmccabe)\n",
      "commitDate": "02/03/16 5:56 PM",
      "commitName": "700b0e4019cf483f7532609711812150b8c44742",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "21/10/15 2:58 PM",
      "commitNameOld": "a24c6e84205c684ef864b0fc5301dc07b3578351",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 133.17,
      "commitsBetweenForRepo": 905,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,34 @@\n   private void dumpINodeFile(INodeSection.INodeFile f) {\n     o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n         .o(\"atime\", f.getAccessTime())\n         .o(\"preferredBlockSize\", f.getPreferredBlockSize())\n         .o(\"permission\", dumpPermission(f.getPermission()));\n+    if (f.hasXAttrs()) {\n+      dumpXattrs(f.getXAttrs());\n+    }\n     dumpAcls(f.getAcl());\n     if (f.getBlocksCount() \u003e 0) {\n       out.print(\"\u003cblocks\u003e\");\n       for (BlockProto b : f.getBlocksList()) {\n         out.print(\"\u003cblock\u003e\");\n         o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n             b.getNumBytes());\n         out.print(\"\u003c/block\u003e\\n\");\n       }\n       out.print(\"\u003c/blocks\u003e\\n\");\n     }\n+    if (f.hasStoragePolicyID()) {\n+      o(\"storagePolicyId\", f.getStoragePolicyID());\n+    }\n+    if (f.getIsStriped()) {\n+      out.print(\"\u003cisStriped/\u003e\");\n+    }\n \n     if (f.hasFileUC()) {\n       INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n       out.print(\"\u003cfile-under-construction\u003e\");\n       o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n           u.getClientMachine());\n       out.print(\"\u003c/file-under-construction\u003e\\n\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeFile(INodeSection.INodeFile f) {\n    o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n        .o(\"atime\", f.getAccessTime())\n        .o(\"preferredBlockSize\", f.getPreferredBlockSize())\n        .o(\"permission\", dumpPermission(f.getPermission()));\n    if (f.hasXAttrs()) {\n      dumpXattrs(f.getXAttrs());\n    }\n    dumpAcls(f.getAcl());\n    if (f.getBlocksCount() \u003e 0) {\n      out.print(\"\u003cblocks\u003e\");\n      for (BlockProto b : f.getBlocksList()) {\n        out.print(\"\u003cblock\u003e\");\n        o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n            b.getNumBytes());\n        out.print(\"\u003c/block\u003e\\n\");\n      }\n      out.print(\"\u003c/blocks\u003e\\n\");\n    }\n    if (f.hasStoragePolicyID()) {\n      o(\"storagePolicyId\", f.getStoragePolicyID());\n    }\n    if (f.getIsStriped()) {\n      out.print(\"\u003cisStriped/\u003e\");\n    }\n\n    if (f.hasFileUC()) {\n      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n      out.print(\"\u003cfile-under-construction\u003e\");\n      o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n          u.getClientMachine());\n      out.print(\"\u003c/file-under-construction\u003e\\n\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "a24c6e84205c684ef864b0fc5301dc07b3578351": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9278. Fix preferredBlockSize typo in OIV XML output. Contributed by Nicole Pazmany.\n",
      "commitDate": "21/10/15 2:58 PM",
      "commitName": "a24c6e84205c684ef864b0fc5301dc07b3578351",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "17/06/15 5:41 PM",
      "commitNameOld": "cc432885adb0182c2c5b3bf92edde12231fd567c",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 125.89,
      "commitsBetweenForRepo": 847,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   private void dumpINodeFile(INodeSection.INodeFile f) {\n     o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n         .o(\"atime\", f.getAccessTime())\n-        .o(\"perferredBlockSize\", f.getPreferredBlockSize())\n+        .o(\"preferredBlockSize\", f.getPreferredBlockSize())\n         .o(\"permission\", dumpPermission(f.getPermission()));\n     dumpAcls(f.getAcl());\n     if (f.getBlocksCount() \u003e 0) {\n       out.print(\"\u003cblocks\u003e\");\n       for (BlockProto b : f.getBlocksList()) {\n         out.print(\"\u003cblock\u003e\");\n         o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n             b.getNumBytes());\n         out.print(\"\u003c/block\u003e\\n\");\n       }\n       out.print(\"\u003c/blocks\u003e\\n\");\n     }\n \n     if (f.hasFileUC()) {\n       INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n       out.print(\"\u003cfile-under-construction\u003e\");\n       o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n           u.getClientMachine());\n       out.print(\"\u003c/file-under-construction\u003e\\n\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeFile(INodeSection.INodeFile f) {\n    o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n        .o(\"atime\", f.getAccessTime())\n        .o(\"preferredBlockSize\", f.getPreferredBlockSize())\n        .o(\"permission\", dumpPermission(f.getPermission()));\n    dumpAcls(f.getAcl());\n    if (f.getBlocksCount() \u003e 0) {\n      out.print(\"\u003cblocks\u003e\");\n      for (BlockProto b : f.getBlocksList()) {\n        out.print(\"\u003cblock\u003e\");\n        o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n            b.getNumBytes());\n        out.print(\"\u003c/block\u003e\\n\");\n      }\n      out.print(\"\u003c/blocks\u003e\\n\");\n    }\n\n    if (f.hasFileUC()) {\n      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n      out.print(\"\u003cfile-under-construction\u003e\");\n      o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n          u.getClientMachine());\n      out.print(\"\u003c/file-under-construction\u003e\\n\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "cc432885adb0182c2c5b3bf92edde12231fd567c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6249. Output AclEntry in PBImageXmlWriter. Contributed by surendra singh lilhore.\n",
      "commitDate": "17/06/15 5:41 PM",
      "commitName": "cc432885adb0182c2c5b3bf92edde12231fd567c",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "11/12/14 12:36 PM",
      "commitNameOld": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 188.17,
      "commitsBetweenForRepo": 1579,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   private void dumpINodeFile(INodeSection.INodeFile f) {\n     o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n         .o(\"atime\", f.getAccessTime())\n         .o(\"perferredBlockSize\", f.getPreferredBlockSize())\n         .o(\"permission\", dumpPermission(f.getPermission()));\n-\n+    dumpAcls(f.getAcl());\n     if (f.getBlocksCount() \u003e 0) {\n       out.print(\"\u003cblocks\u003e\");\n       for (BlockProto b : f.getBlocksList()) {\n         out.print(\"\u003cblock\u003e\");\n         o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n             b.getNumBytes());\n         out.print(\"\u003c/block\u003e\\n\");\n       }\n       out.print(\"\u003c/blocks\u003e\\n\");\n     }\n \n     if (f.hasFileUC()) {\n       INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n       out.print(\"\u003cfile-under-construction\u003e\");\n       o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n           u.getClientMachine());\n       out.print(\"\u003c/file-under-construction\u003e\\n\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeFile(INodeSection.INodeFile f) {\n    o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n        .o(\"atime\", f.getAccessTime())\n        .o(\"perferredBlockSize\", f.getPreferredBlockSize())\n        .o(\"permission\", dumpPermission(f.getPermission()));\n    dumpAcls(f.getAcl());\n    if (f.getBlocksCount() \u003e 0) {\n      out.print(\"\u003cblocks\u003e\");\n      for (BlockProto b : f.getBlocksList()) {\n        out.print(\"\u003cblock\u003e\");\n        o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n            b.getNumBytes());\n        out.print(\"\u003c/block\u003e\\n\");\n      }\n      out.print(\"\u003c/blocks\u003e\\n\");\n    }\n\n    if (f.hasFileUC()) {\n      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n      out.print(\"\u003cfile-under-construction\u003e\");\n      o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n          u.getClientMachine());\n      out.print(\"\u003c/file-under-construction\u003e\\n\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "bb84f1fccb18c6c7373851e05d2451d55e908242": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7159. Use block storage policy to set lazy persist preference. (Arpit Agarwal)\n",
      "commitDate": "29/09/14 10:27 PM",
      "commitName": "bb84f1fccb18c6c7373851e05d2451d55e908242",
      "commitAuthor": "arp",
      "commitDateOld": "27/08/14 9:47 PM",
      "commitNameOld": "042b33f20b01aadb5cd03da731ae7a3d94026aac",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 33.03,
      "commitsBetweenForRepo": 374,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,25 @@\n   private void dumpINodeFile(INodeSection.INodeFile f) {\n     o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n         .o(\"atime\", f.getAccessTime())\n         .o(\"perferredBlockSize\", f.getPreferredBlockSize())\n         .o(\"permission\", dumpPermission(f.getPermission()));\n \n-    if (f.hasIsLazyPersist()) {\n-      o(\"lazyPersist\", f.getIsLazyPersist());\n-    }\n-\n     if (f.getBlocksCount() \u003e 0) {\n       out.print(\"\u003cblocks\u003e\");\n       for (BlockProto b : f.getBlocksList()) {\n         out.print(\"\u003cblock\u003e\");\n         o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n             b.getNumBytes());\n         out.print(\"\u003c/block\u003e\\n\");\n       }\n       out.print(\"\u003c/blocks\u003e\\n\");\n     }\n \n     if (f.hasFileUC()) {\n       INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n       out.print(\"\u003cfile-under-construction\u003e\");\n       o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n           u.getClientMachine());\n       out.print(\"\u003c/file-under-construction\u003e\\n\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeFile(INodeSection.INodeFile f) {\n    o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n        .o(\"atime\", f.getAccessTime())\n        .o(\"perferredBlockSize\", f.getPreferredBlockSize())\n        .o(\"permission\", dumpPermission(f.getPermission()));\n\n    if (f.getBlocksCount() \u003e 0) {\n      out.print(\"\u003cblocks\u003e\");\n      for (BlockProto b : f.getBlocksList()) {\n        out.print(\"\u003cblock\u003e\");\n        o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n            b.getNumBytes());\n        out.print(\"\u003c/block\u003e\\n\");\n      }\n      out.print(\"\u003c/blocks\u003e\\n\");\n    }\n\n    if (f.hasFileUC()) {\n      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n      out.print(\"\u003cfile-under-construction\u003e\");\n      o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n          u.getClientMachine());\n      out.print(\"\u003c/file-under-construction\u003e\\n\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "042b33f20b01aadb5cd03da731ae7a3d94026aac": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6922. Add LazyPersist flag to INodeFile, save it in FsImage and edit logs. (Arpit Agarwal)\n",
      "commitDate": "27/08/14 9:47 PM",
      "commitName": "042b33f20b01aadb5cd03da731ae7a3d94026aac",
      "commitAuthor": "arp",
      "commitDateOld": "28/02/14 11:45 AM",
      "commitNameOld": "8e809b8c814834a01e7fdeff08ce64ef69f31817",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 180.38,
      "commitsBetweenForRepo": 1281,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,29 @@\n   private void dumpINodeFile(INodeSection.INodeFile f) {\n     o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n         .o(\"atime\", f.getAccessTime())\n         .o(\"perferredBlockSize\", f.getPreferredBlockSize())\n         .o(\"permission\", dumpPermission(f.getPermission()));\n \n+    if (f.hasIsLazyPersist()) {\n+      o(\"lazyPersist\", f.getIsLazyPersist());\n+    }\n+\n     if (f.getBlocksCount() \u003e 0) {\n       out.print(\"\u003cblocks\u003e\");\n       for (BlockProto b : f.getBlocksList()) {\n         out.print(\"\u003cblock\u003e\");\n         o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n             b.getNumBytes());\n         out.print(\"\u003c/block\u003e\\n\");\n       }\n       out.print(\"\u003c/blocks\u003e\\n\");\n     }\n \n     if (f.hasFileUC()) {\n       INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n       out.print(\"\u003cfile-under-construction\u003e\");\n       o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n           u.getClientMachine());\n       out.print(\"\u003c/file-under-construction\u003e\\n\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeFile(INodeSection.INodeFile f) {\n    o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n        .o(\"atime\", f.getAccessTime())\n        .o(\"perferredBlockSize\", f.getPreferredBlockSize())\n        .o(\"permission\", dumpPermission(f.getPermission()));\n\n    if (f.hasIsLazyPersist()) {\n      o(\"lazyPersist\", f.getIsLazyPersist());\n    }\n\n    if (f.getBlocksCount() \u003e 0) {\n      out.print(\"\u003cblocks\u003e\");\n      for (BlockProto b : f.getBlocksList()) {\n        out.print(\"\u003cblock\u003e\");\n        o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n            b.getNumBytes());\n        out.print(\"\u003c/block\u003e\\n\");\n      }\n      out.print(\"\u003c/blocks\u003e\\n\");\n    }\n\n    if (f.hasFileUC()) {\n      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n      out.print(\"\u003cfile-under-construction\u003e\");\n      o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n          u.getClientMachine());\n      out.print(\"\u003c/file-under-construction\u003e\\n\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,25 @@\n+  private void dumpINodeFile(INodeSection.INodeFile f) {\n+    o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n+        .o(\"atime\", f.getAccessTime())\n+        .o(\"perferredBlockSize\", f.getPreferredBlockSize())\n+        .o(\"permission\", dumpPermission(f.getPermission()));\n+\n+    if (f.getBlocksCount() \u003e 0) {\n+      out.print(\"\u003cblocks\u003e\");\n+      for (BlockProto b : f.getBlocksList()) {\n+        out.print(\"\u003cblock\u003e\");\n+        o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n+            b.getNumBytes());\n+        out.print(\"\u003c/block\u003e\\n\");\n+      }\n+      out.print(\"\u003c/blocks\u003e\\n\");\n+    }\n+\n+    if (f.hasFileUC()) {\n+      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n+      out.print(\"\u003cfile-under-construction\u003e\");\n+      o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n+          u.getClientMachine());\n+      out.print(\"\u003c/file-under-construction\u003e\\n\");\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeFile(INodeSection.INodeFile f) {\n    o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n        .o(\"atime\", f.getAccessTime())\n        .o(\"perferredBlockSize\", f.getPreferredBlockSize())\n        .o(\"permission\", dumpPermission(f.getPermission()));\n\n    if (f.getBlocksCount() \u003e 0) {\n      out.print(\"\u003cblocks\u003e\");\n      for (BlockProto b : f.getBlocksList()) {\n        out.print(\"\u003cblock\u003e\");\n        o(\"id\", b.getBlockId()).o(\"genstamp\", b.getGenStamp()).o(\"numBytes\",\n            b.getNumBytes());\n        out.print(\"\u003c/block\u003e\\n\");\n      }\n      out.print(\"\u003c/blocks\u003e\\n\");\n    }\n\n    if (f.hasFileUC()) {\n      INodeSection.FileUnderConstructionFeature u \u003d f.getFileUC();\n      out.print(\"\u003cfile-under-construction\u003e\");\n      o(\"clientName\", u.getClientName()).o(\"clientMachine\",\n          u.getClientMachine());\n      out.print(\"\u003c/file-under-construction\u003e\\n\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java"
    }
  }
}