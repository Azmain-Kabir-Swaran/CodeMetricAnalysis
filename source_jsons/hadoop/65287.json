{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "S3ABlockOutputStream.java",
  "functionName": "maybeRethrowUploadFailure",
  "functionId": "maybeRethrowUploadFailure",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
  "functionStartLine": 600,
  "functionEndLine": 604,
  "numCommitsSeen": 10,
  "timeTaken": 1208,
  "changeHistory": [
    "29b19cd59245c8809b697b3d7d7445813a685aad"
  ],
  "changeHistoryShort": {
    "29b19cd59245c8809b697b3d7d7445813a685aad": "Yintroduced"
  },
  "changeHistoryDetails": {
    "29b19cd59245c8809b697b3d7d7445813a685aad": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-16900. Very large files can be truncated when written through the S3A FileSystem.\n\nContributed by Mukund Thakur and Steve Loughran.\n\nThis patch ensures that writes to S3A fail when more than 10,000 blocks are\nwritten. That upper bound still exists. To write massive files, make sure\nthat the value of fs.s3a.multipart.size is set to a size which is large\nenough to upload the files in fewer than 10,000 blocks.\n\nChange-Id: Icec604e2a357ffd38d7ae7bc3f887ff55f2d721a\n",
      "commitDate": "20/05/20 5:42 AM",
      "commitName": "29b19cd59245c8809b697b3d7d7445813a685aad",
      "commitAuthor": "Mukund Thakur",
      "diff": "@@ -0,0 +1,5 @@\n+    public void maybeRethrowUploadFailure() throws IOException {\n+      if (blockUploadFailure !\u003d null) {\n+        throw blockUploadFailure;\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public void maybeRethrowUploadFailure() throws IOException {\n      if (blockUploadFailure !\u003d null) {\n        throw blockUploadFailure;\n      }\n    }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java"
    }
  }
}