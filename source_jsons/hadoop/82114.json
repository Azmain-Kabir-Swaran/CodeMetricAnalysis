{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "IdentityHashStore.java",
  "functionName": "realloc",
  "functionId": "realloc___newCapacity-int",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/IdentityHashStore.java",
  "functionStartLine": 76,
  "functionEndLine": 92,
  "numCommitsSeen": 2,
  "timeTaken": 865,
  "changeHistory": [
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036"
  ],
  "changeHistoryShort": {
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": "Yintroduced"
  },
  "changeHistoryDetails": {
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5260. Merge zero-copy memory-mapped HDFS client reads to trunk and branch-2. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1527113 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 3:51 PM",
      "commitName": "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
      "commitAuthor": "Chris Nauroth",
      "diff": "@@ -0,0 +1,17 @@\n+  private void realloc(int newCapacity) {\n+    Preconditions.checkArgument(newCapacity \u003e 0);\n+    Object prevBuffer[] \u003d buffer;\n+    this.capacity \u003d newCapacity;\n+    // Each element takes two array slots -- one for the key, \n+    // and another for the value.  We also want a load factor \n+    // of 0.50.  Combine those together and you get 4 * newCapacity.\n+    this.buffer \u003d new Object[4 * newCapacity];\n+    this.numInserted \u003d 0;\n+    if (prevBuffer !\u003d null) {\n+      for (int i \u003d 0; i \u003c prevBuffer.length; i +\u003d 2) {\n+        if (prevBuffer[i] !\u003d null) {\n+          putInternal(prevBuffer[i], prevBuffer[i + 1]);\n+        }\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void realloc(int newCapacity) {\n    Preconditions.checkArgument(newCapacity \u003e 0);\n    Object prevBuffer[] \u003d buffer;\n    this.capacity \u003d newCapacity;\n    // Each element takes two array slots -- one for the key, \n    // and another for the value.  We also want a load factor \n    // of 0.50.  Combine those together and you get 4 * newCapacity.\n    this.buffer \u003d new Object[4 * newCapacity];\n    this.numInserted \u003d 0;\n    if (prevBuffer !\u003d null) {\n      for (int i \u003d 0; i \u003c prevBuffer.length; i +\u003d 2) {\n        if (prevBuffer[i] !\u003d null) {\n          putInternal(prevBuffer[i], prevBuffer[i + 1]);\n        }\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/IdentityHashStore.java"
    }
  }
}