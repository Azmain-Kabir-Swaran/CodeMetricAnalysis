{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockSender.java",
  "functionName": "close",
  "functionId": "close",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
  "functionStartLine": 476,
  "functionEndLine": 497,
  "numCommitsSeen": 70,
  "timeTaken": 7695,
  "changeHistory": [
    "df983b524ab68ea0c70cee9033bfff2d28052cbf",
    "dcedb72af468128458e597f08d22f5c34b744ae5",
    "aeecfa24f4fb6af289920cbf8830c394e66bd78e",
    "21d10ccc6e463cf250414264c78acb4a6e7c83e3",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c",
    "efea68dc3538de9aafae206d64903506e41fc9e1",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
    "638801cce16fc1dc3259c541dc30a599faaddda1",
    "f7d20b2198e47926f5a1203cad3655a4afdfe7be",
    "6b0963c53be360b710614b9f44a29c4171af6b83",
    "e90a5b40430cc1fbce075d34b31e3cc05fd9831f",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "df983b524ab68ea0c70cee9033bfff2d28052cbf": "Ybodychange",
    "dcedb72af468128458e597f08d22f5c34b744ae5": "Ybodychange",
    "aeecfa24f4fb6af289920cbf8830c394e66bd78e": "Ybodychange",
    "21d10ccc6e463cf250414264c78acb4a6e7c83e3": "Ybodychange",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": "Ybodychange",
    "efea68dc3538de9aafae206d64903506e41fc9e1": "Ybodychange",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": "Ybodychange",
    "638801cce16fc1dc3259c541dc30a599faaddda1": "Ybodychange",
    "f7d20b2198e47926f5a1203cad3655a4afdfe7be": "Ybodychange",
    "6b0963c53be360b710614b9f44a29c4171af6b83": "Ybodychange",
    "e90a5b40430cc1fbce075d34b31e3cc05fd9831f": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "df983b524ab68ea0c70cee9033bfff2d28052cbf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10930. Refactor: Wrap Datanode IO related operations. Contributed by Xiaoyu Yao.\n",
      "commitDate": "06/12/16 11:05 AM",
      "commitName": "df983b524ab68ea0c70cee9033bfff2d28052cbf",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "05/12/16 12:44 PM",
      "commitNameOld": "dcedb72af468128458e597f08d22f5c34b744ae5",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 0.93,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,22 @@\n   public void close() throws IOException {\n-    if (blockInFd !\u003d null \u0026\u0026\n+    if (ris.getDataInFd() !\u003d null \u0026\u0026\n         ((dropCacheBehindAllReads) ||\n          (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n       try {\n-        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n-            block.getBlockName(), blockInFd, lastCacheDropOffset,\n+        ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset,\n             offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n       } catch (Exception e) {\n         LOG.warn(\"Unable to drop cache on file close\", e);\n       }\n     }\n     if (curReadahead !\u003d null) {\n       curReadahead.cancel();\n     }\n-    \n-    IOException ioe \u003d null;\n-    if(checksumIn!\u003dnull) {\n-      try {\n-        checksumIn.close(); // close checksum file\n-      } catch (IOException e) {\n-        ioe \u003d e;\n-      }\n-      checksumIn \u003d null;\n-    }   \n-    if(blockIn!\u003dnull) {\n-      try {\n-        blockIn.close(); // close data file\n-      } catch (IOException e) {\n-        ioe \u003d e;\n-      }\n-      blockIn \u003d null;\n-      blockInFd \u003d null;\n-    }\n-    if (volumeRef !\u003d null) {\n-      IOUtils.cleanup(null, volumeRef);\n-      volumeRef \u003d null;\n-    }\n-    // throw IOException if there is any\n-    if(ioe!\u003d null) {\n-      throw ioe;\n+\n+    try {\n+      ris.closeStreams();\n+    } finally {\n+      IOUtils.closeStream(ris);\n+      ris \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (ris.getDataInFd() !\u003d null \u0026\u0026\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n      try {\n        ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset,\n            offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead !\u003d null) {\n      curReadahead.cancel();\n    }\n\n    try {\n      ris.closeStreams();\n    } finally {\n      IOUtils.closeStream(ris);\n      ris \u003d null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "dcedb72af468128458e597f08d22f5c34b744ae5": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HADOOP-10930. Refactor: Wrap Datanode IO related operations. Contributed by Xiaoyu Yao.\"\n\nThis reverts commit aeecfa24f4fb6af289920cbf8830c394e66bd78e.\n",
      "commitDate": "05/12/16 12:44 PM",
      "commitName": "dcedb72af468128458e597f08d22f5c34b744ae5",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "29/11/16 8:52 PM",
      "commitNameOld": "aeecfa24f4fb6af289920cbf8830c394e66bd78e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 5.66,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,43 @@\n   public void close() throws IOException {\n-    if (ris.getDataInFd() !\u003d null \u0026\u0026\n+    if (blockInFd !\u003d null \u0026\u0026\n         ((dropCacheBehindAllReads) ||\n          (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n       try {\n-        ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset,\n+        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n+            block.getBlockName(), blockInFd, lastCacheDropOffset,\n             offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n       } catch (Exception e) {\n         LOG.warn(\"Unable to drop cache on file close\", e);\n       }\n     }\n     if (curReadahead !\u003d null) {\n       curReadahead.cancel();\n     }\n-\n-    try {\n-      ris.closeStreams();\n-    } finally {\n-      IOUtils.closeStream(ris);\n-      ris \u003d null;\n+    \n+    IOException ioe \u003d null;\n+    if(checksumIn!\u003dnull) {\n+      try {\n+        checksumIn.close(); // close checksum file\n+      } catch (IOException e) {\n+        ioe \u003d e;\n+      }\n+      checksumIn \u003d null;\n+    }   \n+    if(blockIn!\u003dnull) {\n+      try {\n+        blockIn.close(); // close data file\n+      } catch (IOException e) {\n+        ioe \u003d e;\n+      }\n+      blockIn \u003d null;\n+      blockInFd \u003d null;\n+    }\n+    if (volumeRef !\u003d null) {\n+      IOUtils.cleanup(null, volumeRef);\n+      volumeRef \u003d null;\n+    }\n+    // throw IOException if there is any\n+    if(ioe!\u003d null) {\n+      throw ioe;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (blockInFd !\u003d null \u0026\u0026\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n      try {\n        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n            block.getBlockName(), blockInFd, lastCacheDropOffset,\n            offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead !\u003d null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe \u003d null;\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }   \n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n      blockInFd \u003d null;\n    }\n    if (volumeRef !\u003d null) {\n      IOUtils.cleanup(null, volumeRef);\n      volumeRef \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "aeecfa24f4fb6af289920cbf8830c394e66bd78e": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10930. Refactor: Wrap Datanode IO related operations. Contributed by Xiaoyu Yao.\n",
      "commitDate": "29/11/16 8:52 PM",
      "commitName": "aeecfa24f4fb6af289920cbf8830c394e66bd78e",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "27/10/16 4:14 AM",
      "commitNameOld": "1cf6ec4ad4e1f4ea71f912923b5e8627b61ef482",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 33.74,
      "commitsBetweenForRepo": 277,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,22 @@\n   public void close() throws IOException {\n-    if (blockInFd !\u003d null \u0026\u0026\n+    if (ris.getDataInFd() !\u003d null \u0026\u0026\n         ((dropCacheBehindAllReads) ||\n          (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n       try {\n-        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n-            block.getBlockName(), blockInFd, lastCacheDropOffset,\n+        ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset,\n             offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n       } catch (Exception e) {\n         LOG.warn(\"Unable to drop cache on file close\", e);\n       }\n     }\n     if (curReadahead !\u003d null) {\n       curReadahead.cancel();\n     }\n-    \n-    IOException ioe \u003d null;\n-    if(checksumIn!\u003dnull) {\n-      try {\n-        checksumIn.close(); // close checksum file\n-      } catch (IOException e) {\n-        ioe \u003d e;\n-      }\n-      checksumIn \u003d null;\n-    }   \n-    if(blockIn!\u003dnull) {\n-      try {\n-        blockIn.close(); // close data file\n-      } catch (IOException e) {\n-        ioe \u003d e;\n-      }\n-      blockIn \u003d null;\n-      blockInFd \u003d null;\n-    }\n-    if (volumeRef !\u003d null) {\n-      IOUtils.cleanup(null, volumeRef);\n-      volumeRef \u003d null;\n-    }\n-    // throw IOException if there is any\n-    if(ioe!\u003d null) {\n-      throw ioe;\n+\n+    try {\n+      ris.closeStreams();\n+    } finally {\n+      IOUtils.closeStream(ris);\n+      ris \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (ris.getDataInFd() !\u003d null \u0026\u0026\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n      try {\n        ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset,\n            offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead !\u003d null) {\n      curReadahead.cancel();\n    }\n\n    try {\n      ris.closeStreams();\n    } finally {\n      IOUtils.closeStream(ris);\n      ris \u003d null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "21d10ccc6e463cf250414264c78acb4a6e7c83e3": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-7824. NativeIO.java flags and identifiers must be set correctly for each platform, not hardcoded to their Linux values (Martin Walsh via Colin P. McCabe)\n",
      "commitDate": "31/07/15 3:01 PM",
      "commitName": "21d10ccc6e463cf250414264c78acb4a6e7c83e3",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "05/05/15 3:41 PM",
      "commitNameOld": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 86.97,
      "commitsBetweenForRepo": 681,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,43 @@\n   public void close() throws IOException {\n     if (blockInFd !\u003d null \u0026\u0026\n         ((dropCacheBehindAllReads) ||\n          (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n       try {\n         NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n             block.getBlockName(), blockInFd, lastCacheDropOffset,\n-            offset - lastCacheDropOffset,\n-            NativeIO.POSIX.POSIX_FADV_DONTNEED);\n+            offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n       } catch (Exception e) {\n         LOG.warn(\"Unable to drop cache on file close\", e);\n       }\n     }\n     if (curReadahead !\u003d null) {\n       curReadahead.cancel();\n     }\n     \n     IOException ioe \u003d null;\n     if(checksumIn!\u003dnull) {\n       try {\n         checksumIn.close(); // close checksum file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       checksumIn \u003d null;\n     }   \n     if(blockIn!\u003dnull) {\n       try {\n         blockIn.close(); // close data file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       blockIn \u003d null;\n       blockInFd \u003d null;\n     }\n     if (volumeRef !\u003d null) {\n       IOUtils.cleanup(null, volumeRef);\n       volumeRef \u003d null;\n     }\n     // throw IOException if there is any\n     if(ioe!\u003d null) {\n       throw ioe;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (blockInFd !\u003d null \u0026\u0026\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n      try {\n        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n            block.getBlockName(), blockInFd, lastCacheDropOffset,\n            offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead !\u003d null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe \u003d null;\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }   \n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n      blockInFd \u003d null;\n    }\n    if (volumeRef !\u003d null) {\n      IOUtils.cleanup(null, volumeRef);\n      volumeRef \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)\n",
      "commitDate": "20/01/15 7:05 PM",
      "commitName": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "30/10/14 11:04 AM",
      "commitNameOld": "c2866ac34d063a4d2e150fe35632111b37a1514d",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 82.38,
      "commitsBetweenForRepo": 551,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,44 @@\n   public void close() throws IOException {\n     if (blockInFd !\u003d null \u0026\u0026\n         ((dropCacheBehindAllReads) ||\n          (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n       try {\n         NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n             block.getBlockName(), blockInFd, lastCacheDropOffset,\n             offset - lastCacheDropOffset,\n             NativeIO.POSIX.POSIX_FADV_DONTNEED);\n       } catch (Exception e) {\n         LOG.warn(\"Unable to drop cache on file close\", e);\n       }\n     }\n     if (curReadahead !\u003d null) {\n       curReadahead.cancel();\n     }\n     \n     IOException ioe \u003d null;\n     if(checksumIn!\u003dnull) {\n       try {\n         checksumIn.close(); // close checksum file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       checksumIn \u003d null;\n     }   \n     if(blockIn!\u003dnull) {\n       try {\n         blockIn.close(); // close data file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       blockIn \u003d null;\n       blockInFd \u003d null;\n     }\n+    if (volumeRef !\u003d null) {\n+      IOUtils.cleanup(null, volumeRef);\n+      volumeRef \u003d null;\n+    }\n     // throw IOException if there is any\n     if(ioe!\u003d null) {\n       throw ioe;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (blockInFd !\u003d null \u0026\u0026\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n      try {\n        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n            block.getBlockName(), blockInFd, lastCacheDropOffset,\n            offset - lastCacheDropOffset,\n            NativeIO.POSIX.POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead !\u003d null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe \u003d null;\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }   \n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n      blockInFd \u003d null;\n    }\n    if (volumeRef !\u003d null) {\n      IOUtils.cleanup(null, volumeRef);\n      volumeRef \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "efea68dc3538de9aafae206d64903506e41fc9e1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5511. improve CacheManipulator interface to allow better unit testing (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543676 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/11/13 4:48 PM",
      "commitName": "efea68dc3538de9aafae206d64903506e41fc9e1",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "22/07/13 11:15 AM",
      "commitNameOld": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 120.27,
      "commitsBetweenForRepo": 765,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,40 @@\n   public void close() throws IOException {\n     if (blockInFd !\u003d null \u0026\u0026\n         ((dropCacheBehindAllReads) ||\n          (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n       try {\n-        NativeIO.POSIX.posixFadviseIfPossible(block.getBlockName(),\n-            blockInFd, lastCacheDropOffset, offset - lastCacheDropOffset,\n+        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n+            block.getBlockName(), blockInFd, lastCacheDropOffset,\n+            offset - lastCacheDropOffset,\n             NativeIO.POSIX.POSIX_FADV_DONTNEED);\n       } catch (Exception e) {\n         LOG.warn(\"Unable to drop cache on file close\", e);\n       }\n     }\n     if (curReadahead !\u003d null) {\n       curReadahead.cancel();\n     }\n     \n     IOException ioe \u003d null;\n     if(checksumIn!\u003dnull) {\n       try {\n         checksumIn.close(); // close checksum file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       checksumIn \u003d null;\n     }   \n     if(blockIn!\u003dnull) {\n       try {\n         blockIn.close(); // close data file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       blockIn \u003d null;\n       blockInFd \u003d null;\n     }\n     // throw IOException if there is any\n     if(ioe!\u003d null) {\n       throw ioe;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (blockInFd !\u003d null \u0026\u0026\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n      try {\n        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n            block.getBlockName(), blockInFd, lastCacheDropOffset,\n            offset - lastCacheDropOffset,\n            NativeIO.POSIX.POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead !\u003d null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe \u003d null;\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }   \n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n      blockInFd \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4817.  Make HDFS advisory caching configurable on a per-file basis.  (Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505753 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/13 11:15 AM",
      "commitName": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "06/03/13 11:15 AM",
      "commitNameOld": "638801cce16fc1dc3259c541dc30a599faaddda1",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 137.96,
      "commitsBetweenForRepo": 857,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,39 @@\n   public void close() throws IOException {\n-    if (blockInFd !\u003d null \u0026\u0026 shouldDropCacheBehindRead \u0026\u0026 isLongRead()) {\n-      // drop the last few MB of the file from cache\n+    if (blockInFd !\u003d null \u0026\u0026\n+        ((dropCacheBehindAllReads) ||\n+         (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n       try {\n-        NativeIO.POSIX.posixFadviseIfPossible(\n+        NativeIO.POSIX.posixFadviseIfPossible(block.getBlockName(),\n             blockInFd, lastCacheDropOffset, offset - lastCacheDropOffset,\n             NativeIO.POSIX.POSIX_FADV_DONTNEED);\n       } catch (Exception e) {\n         LOG.warn(\"Unable to drop cache on file close\", e);\n       }\n     }\n     if (curReadahead !\u003d null) {\n       curReadahead.cancel();\n     }\n     \n     IOException ioe \u003d null;\n     if(checksumIn!\u003dnull) {\n       try {\n         checksumIn.close(); // close checksum file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       checksumIn \u003d null;\n     }   \n     if(blockIn!\u003dnull) {\n       try {\n         blockIn.close(); // close data file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       blockIn \u003d null;\n       blockInFd \u003d null;\n     }\n     // throw IOException if there is any\n     if(ioe!\u003d null) {\n       throw ioe;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (blockInFd !\u003d null \u0026\u0026\n        ((dropCacheBehindAllReads) ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead()))) {\n      try {\n        NativeIO.POSIX.posixFadviseIfPossible(block.getBlockName(),\n            blockInFd, lastCacheDropOffset, offset - lastCacheDropOffset,\n            NativeIO.POSIX.POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead !\u003d null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe \u003d null;\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }   \n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n      blockInFd \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "638801cce16fc1dc3259c541dc30a599faaddda1": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-8952. Enhancements to support Hadoop on Windows Server and Windows Azure environments. Contributed by Ivan Mitic, Chuan Liu, Ramya Sunil, Bikas Saha, Kanna Karanam, John Gordon, Brandon Li, Chris Nauroth, David Lao, Sumadhur Reddy Bolli, Arpit Agarwal, Ahmed El Baz, Mike Liddell, Jing Zhao, Thejas Nair, Steve Maine, Ganeshan Iyer, Raja Aluri, Giridharan Kesavan, Ramya Bharathi Nimmagadda.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1453486 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/03/13 11:15 AM",
      "commitName": "638801cce16fc1dc3259c541dc30a599faaddda1",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/01/13 12:47 PM",
      "commitNameOld": "3052ad1f0069af5caee621374b29d17d7f12ab51",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 50.94,
      "commitsBetweenForRepo": 214,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   public void close() throws IOException {\n     if (blockInFd !\u003d null \u0026\u0026 shouldDropCacheBehindRead \u0026\u0026 isLongRead()) {\n       // drop the last few MB of the file from cache\n       try {\n-        NativeIO.posixFadviseIfPossible(\n+        NativeIO.POSIX.posixFadviseIfPossible(\n             blockInFd, lastCacheDropOffset, offset - lastCacheDropOffset,\n-            NativeIO.POSIX_FADV_DONTNEED);\n+            NativeIO.POSIX.POSIX_FADV_DONTNEED);\n       } catch (Exception e) {\n         LOG.warn(\"Unable to drop cache on file close\", e);\n       }\n     }\n     if (curReadahead !\u003d null) {\n       curReadahead.cancel();\n     }\n     \n     IOException ioe \u003d null;\n     if(checksumIn!\u003dnull) {\n       try {\n         checksumIn.close(); // close checksum file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       checksumIn \u003d null;\n     }   \n     if(blockIn!\u003dnull) {\n       try {\n         blockIn.close(); // close data file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       blockIn \u003d null;\n       blockInFd \u003d null;\n     }\n     // throw IOException if there is any\n     if(ioe!\u003d null) {\n       throw ioe;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (blockInFd !\u003d null \u0026\u0026 shouldDropCacheBehindRead \u0026\u0026 isLongRead()) {\n      // drop the last few MB of the file from cache\n      try {\n        NativeIO.POSIX.posixFadviseIfPossible(\n            blockInFd, lastCacheDropOffset, offset - lastCacheDropOffset,\n            NativeIO.POSIX.POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead !\u003d null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe \u003d null;\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }   \n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n      blockInFd \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "f7d20b2198e47926f5a1203cad3655a4afdfe7be": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2751. Datanode may incorrectly drop OS cache behind reads even for short reads. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1233796 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/01/12 11:32 PM",
      "commitName": "f7d20b2198e47926f5a1203cad3655a4afdfe7be",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "18/11/11 1:04 AM",
      "commitNameOld": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 62.94,
      "commitsBetweenForRepo": 312,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   public void close() throws IOException {\n-    if (blockInFd !\u003d null \u0026\u0026 shouldDropCacheBehindRead) {\n+    if (blockInFd !\u003d null \u0026\u0026 shouldDropCacheBehindRead \u0026\u0026 isLongRead()) {\n       // drop the last few MB of the file from cache\n       try {\n         NativeIO.posixFadviseIfPossible(\n             blockInFd, lastCacheDropOffset, offset - lastCacheDropOffset,\n             NativeIO.POSIX_FADV_DONTNEED);\n       } catch (Exception e) {\n         LOG.warn(\"Unable to drop cache on file close\", e);\n       }\n     }\n     if (curReadahead !\u003d null) {\n       curReadahead.cancel();\n     }\n     \n     IOException ioe \u003d null;\n     if(checksumIn!\u003dnull) {\n       try {\n         checksumIn.close(); // close checksum file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       checksumIn \u003d null;\n     }   \n     if(blockIn!\u003dnull) {\n       try {\n         blockIn.close(); // close data file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       blockIn \u003d null;\n       blockInFd \u003d null;\n     }\n     // throw IOException if there is any\n     if(ioe!\u003d null) {\n       throw ioe;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (blockInFd !\u003d null \u0026\u0026 shouldDropCacheBehindRead \u0026\u0026 isLongRead()) {\n      // drop the last few MB of the file from cache\n      try {\n        NativeIO.posixFadviseIfPossible(\n            blockInFd, lastCacheDropOffset, offset - lastCacheDropOffset,\n            NativeIO.POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead !\u003d null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe \u003d null;\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }   \n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n      blockInFd \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "6b0963c53be360b710614b9f44a29c4171af6b83": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2465. Add HDFS support for fadvise readahead and drop-behind. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190626 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/10/11 3:18 PM",
      "commitName": "6b0963c53be360b710614b9f44a29c4171af6b83",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "28/09/11 9:40 PM",
      "commitNameOld": "e90a5b40430cc1fbce075d34b31e3cc05fd9831f",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 29.74,
      "commitsBetweenForRepo": 266,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,38 @@\n   public void close() throws IOException {\n+    if (blockInFd !\u003d null \u0026\u0026 shouldDropCacheBehindRead) {\n+      // drop the last few MB of the file from cache\n+      try {\n+        NativeIO.posixFadviseIfPossible(\n+            blockInFd, lastCacheDropOffset, offset - lastCacheDropOffset,\n+            NativeIO.POSIX_FADV_DONTNEED);\n+      } catch (Exception e) {\n+        LOG.warn(\"Unable to drop cache on file close\", e);\n+      }\n+    }\n+    if (curReadahead !\u003d null) {\n+      curReadahead.cancel();\n+    }\n+    \n     IOException ioe \u003d null;\n     if(checksumIn!\u003dnull) {\n       try {\n         checksumIn.close(); // close checksum file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       checksumIn \u003d null;\n     }   \n     if(blockIn!\u003dnull) {\n       try {\n         blockIn.close(); // close data file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       blockIn \u003d null;\n+      blockInFd \u003d null;\n     }\n     // throw IOException if there is any\n     if(ioe!\u003d null) {\n       throw ioe;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (blockInFd !\u003d null \u0026\u0026 shouldDropCacheBehindRead) {\n      // drop the last few MB of the file from cache\n      try {\n        NativeIO.posixFadviseIfPossible(\n            blockInFd, lastCacheDropOffset, offset - lastCacheDropOffset,\n            NativeIO.POSIX_FADV_DONTNEED);\n      } catch (Exception e) {\n        LOG.warn(\"Unable to drop cache on file close\", e);\n      }\n    }\n    if (curReadahead !\u003d null) {\n      curReadahead.cancel();\n    }\n    \n    IOException ioe \u003d null;\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }   \n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n      blockInFd \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "e90a5b40430cc1fbce075d34b31e3cc05fd9831f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2371. Refactor BlockSender.java for better readability. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1177161 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/09/11 9:40 PM",
      "commitName": "e90a5b40430cc1fbce075d34b31e3cc05fd9831f",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "04/09/11 12:30 PM",
      "commitNameOld": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 24.38,
      "commitsBetweenForRepo": 173,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,23 @@\n   public void close() throws IOException {\n     IOException ioe \u003d null;\n-    // close checksum file\n     if(checksumIn!\u003dnull) {\n       try {\n-        checksumIn.close();\n+        checksumIn.close(); // close checksum file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       checksumIn \u003d null;\n-    }\n-    // close data file\n+    }   \n     if(blockIn!\u003dnull) {\n       try {\n-        blockIn.close();\n+        blockIn.close(); // close data file\n       } catch (IOException e) {\n         ioe \u003d e;\n       }\n       blockIn \u003d null;\n     }\n     // throw IOException if there is any\n     if(ioe!\u003d null) {\n       throw ioe;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    IOException ioe \u003d null;\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close(); // close checksum file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }   \n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close(); // close data file\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void close() throws IOException {\n    IOException ioe \u003d null;\n    // close checksum file\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close();\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }\n    // close data file\n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close();\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void close() throws IOException {\n    IOException ioe \u003d null;\n    // close checksum file\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close();\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }\n    // close data file\n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close();\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,25 @@\n+  public void close() throws IOException {\n+    IOException ioe \u003d null;\n+    // close checksum file\n+    if(checksumIn!\u003dnull) {\n+      try {\n+        checksumIn.close();\n+      } catch (IOException e) {\n+        ioe \u003d e;\n+      }\n+      checksumIn \u003d null;\n+    }\n+    // close data file\n+    if(blockIn!\u003dnull) {\n+      try {\n+        blockIn.close();\n+      } catch (IOException e) {\n+        ioe \u003d e;\n+      }\n+      blockIn \u003d null;\n+    }\n+    // throw IOException if there is any\n+    if(ioe!\u003d null) {\n+      throw ioe;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    IOException ioe \u003d null;\n    // close checksum file\n    if(checksumIn!\u003dnull) {\n      try {\n        checksumIn.close();\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      checksumIn \u003d null;\n    }\n    // close data file\n    if(blockIn!\u003dnull) {\n      try {\n        blockIn.close();\n      } catch (IOException e) {\n        ioe \u003d e;\n      }\n      blockIn \u003d null;\n    }\n    // throw IOException if there is any\n    if(ioe!\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java"
    }
  }
}