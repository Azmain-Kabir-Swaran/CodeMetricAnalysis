{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Mover.java",
  "functionName": "run",
  "functionId": "run___namenodes-Map__URI,List__Path______conf-Configuration",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
  "functionStartLine": 629,
  "functionEndLine": 688,
  "numCommitsSeen": 59,
  "timeTaken": 8043,
  "changeHistory": [
    "66357574ae1da09ced735da36bf7d80a40c3fa1b",
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
    "39ed3a66dbb01383ed16b141183fc48bfd2e613d",
    "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
    "5845c36c16c423107183287cce3be9357dad7564",
    "681d2804c95e5a569ffb8d9ceafaf5a4f8be2b88",
    "cd5262aba00aa51b905aaac95e201d4d48f2480d",
    "e24a923db50879f7dbe5d2afac0e6757089fb07d",
    "de4894936a5b581572f35fa5b8979d9f23da0891",
    "d37dc5d1b8e022a7085118a2e7066623483c293f",
    "e806db719053a5b2a7b14f47e6f2962e70008d25",
    "0faa4efa3dd74de9cc39584bf6e88cfbf3e9a045",
    "cdec12d1b84d444e13bf997c817643ec24aaa832",
    "b94c1117a28e996adee68fe0e181eb6f536289f4",
    "2689b6ca727fff8a13347b811eb4cf79b9d30f48",
    "0d85f7e59146cc3e9a040c2203995f3efd8ed4eb",
    "2b5c528a7331a00cfc67e64cd10342650948d686",
    "a26aa6bd0716da89853566961390d711511084e3",
    "5d5aae0694bc27df5b9fa50819854cd3050a8658"
  ],
  "changeHistoryShort": {
    "66357574ae1da09ced735da36bf7d80a40c3fa1b": "Ybodychange",
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": "Ybodychange",
    "39ed3a66dbb01383ed16b141183fc48bfd2e613d": "Ybodychange",
    "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52": "Ybodychange",
    "5845c36c16c423107183287cce3be9357dad7564": "Ybodychange",
    "681d2804c95e5a569ffb8d9ceafaf5a4f8be2b88": "Ybodychange",
    "cd5262aba00aa51b905aaac95e201d4d48f2480d": "Ybodychange",
    "e24a923db50879f7dbe5d2afac0e6757089fb07d": "Ybodychange",
    "de4894936a5b581572f35fa5b8979d9f23da0891": "Ybodychange",
    "d37dc5d1b8e022a7085118a2e7066623483c293f": "Ybodychange",
    "e806db719053a5b2a7b14f47e6f2962e70008d25": "Ybodychange",
    "0faa4efa3dd74de9cc39584bf6e88cfbf3e9a045": "Ybodychange",
    "cdec12d1b84d444e13bf997c817643ec24aaa832": "Ybodychange",
    "b94c1117a28e996adee68fe0e181eb6f536289f4": "Ybodychange",
    "2689b6ca727fff8a13347b811eb4cf79b9d30f48": "Ybodychange",
    "0d85f7e59146cc3e9a040c2203995f3efd8ed4eb": "Ybodychange",
    "2b5c528a7331a00cfc67e64cd10342650948d686": "Yparameterchange",
    "a26aa6bd0716da89853566961390d711511084e3": "Ybodychange",
    "5d5aae0694bc27df5b9fa50819854cd3050a8658": "Yintroduced"
  },
  "changeHistoryDetails": {
    "66357574ae1da09ced735da36bf7d80a40c3fa1b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14346. Add better time precision to Configuration#getTimeDuration, allowing return unit and default unit to be specified independently. Contributed by Chao Sun.\n",
      "commitDate": "13/03/19 1:15 PM",
      "commitName": "66357574ae1da09ced735da36bf7d80a40c3fa1b",
      "commitAuthor": "Erik Krogen",
      "commitDateOld": "06/09/18 2:48 PM",
      "commitNameOld": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 187.94,
      "commitsBetweenForRepo": 1538,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n-            TimeUnit.SECONDS) * 2000 +\n+            TimeUnit.SECONDS, TimeUnit.MILLISECONDS) * 2 +\n         conf.getTimeDuration(\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n-            TimeUnit.SECONDS) * 1000;\n+            TimeUnit.SECONDS, TimeUnit.MILLISECONDS);\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n     // TODO: Need to limit the size of the pinned blocks to limit memory usage\n     Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n     LOG.info(\"namenodes \u003d \" + namenodes);\n \n     checkKeytabAndInit(conf);\n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf, retryCount,\n               excludedPinnedBlocks);\n \n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanupWithLogger(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n               System.err.println(\"Failed to move some blocks after \"\n                   + m.retryMaxAttempts + \" retries. Exiting...\");\n             } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n               System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n             } else {\n               System.err.println(\"Mover failed. Exiting with status \" + r\n                   + \"... \");\n             }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       System.out.println(\"Mover Successful: all blocks satisfy\"\n           + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanupWithLogger(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n            TimeUnit.SECONDS, TimeUnit.MILLISECONDS) * 2 +\n        conf.getTimeDuration(\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n            TimeUnit.SECONDS, TimeUnit.MILLISECONDS);\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    // TODO: Need to limit the size of the pinned blocks to limit memory usage\n    Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n    LOG.info(\"namenodes \u003d \" + namenodes);\n\n    checkKeytabAndInit(conf);\n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount,\n              excludedPinnedBlocks);\n\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanupWithLogger(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanupWithLogger(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13695. Move logging to slf4j in HDFS package. Contributed by Ian Pickering.\n",
      "commitDate": "06/09/18 2:48 PM",
      "commitName": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "39ed3a66dbb01383ed16b141183fc48bfd2e613d",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 25.49,
      "commitsBetweenForRepo": 158,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n             TimeUnit.SECONDS) * 2000 +\n         conf.getTimeDuration(\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n             TimeUnit.SECONDS) * 1000;\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n     // TODO: Need to limit the size of the pinned blocks to limit memory usage\n     Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n     LOG.info(\"namenodes \u003d \" + namenodes);\n \n     checkKeytabAndInit(conf);\n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf, retryCount,\n               excludedPinnedBlocks);\n \n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n-            IOUtils.cleanup(LOG, nnc);\n+            IOUtils.cleanupWithLogger(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n               System.err.println(\"Failed to move some blocks after \"\n                   + m.retryMaxAttempts + \" retries. Exiting...\");\n             } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n               System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n             } else {\n               System.err.println(\"Mover failed. Exiting with status \" + r\n                   + \"... \");\n             }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       System.out.println(\"Mover Successful: all blocks satisfy\"\n           + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n-        IOUtils.cleanup(LOG, nnc);\n+        IOUtils.cleanupWithLogger(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n            TimeUnit.SECONDS) * 2000 +\n        conf.getTimeDuration(\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n            TimeUnit.SECONDS) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    // TODO: Need to limit the size of the pinned blocks to limit memory usage\n    Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n    LOG.info(\"namenodes \u003d \" + namenodes);\n\n    checkKeytabAndInit(conf);\n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount,\n              excludedPinnedBlocks);\n\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanupWithLogger(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanupWithLogger(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "39ed3a66dbb01383ed16b141183fc48bfd2e613d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13076: [SPS]: Cleanup work for HDFS-10285 merge. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "39ed3a66dbb01383ed16b141183fc48bfd2e613d",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,79 +1,60 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n             TimeUnit.SECONDS) * 2000 +\n         conf.getTimeDuration(\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n             TimeUnit.SECONDS) * 1000;\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n     // TODO: Need to limit the size of the pinned blocks to limit memory usage\n     Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n     LOG.info(\"namenodes \u003d \" + namenodes);\n \n     checkKeytabAndInit(conf);\n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf, retryCount,\n               excludedPinnedBlocks);\n \n-          boolean spsRunning;\n-          try {\n-            spsRunning \u003d nnc.getDistributedFileSystem().getClient()\n-                .isInternalSatisfierRunning();\n-          } catch (RemoteException e) {\n-            IOException cause \u003d e.unwrapRemoteException();\n-            if (cause instanceof StandbyException) {\n-              System.err.println(\"Skip Standby Namenode. \" + nnc.toString());\n-              continue;\n-            }\n-            throw e;\n-          }\n-          if (spsRunning) {\n-            System.err.println(\"Mover failed due to StoragePolicySatisfier\"\n-                + \" service running inside namenode. Exiting with status \"\n-                + ExitStatus.SKIPPED_DUE_TO_SPS + \"... \");\n-            return ExitStatus.SKIPPED_DUE_TO_SPS.getExitCode();\n-          }\n-\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n               System.err.println(\"Failed to move some blocks after \"\n                   + m.retryMaxAttempts + \" retries. Exiting...\");\n             } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n               System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n             } else {\n               System.err.println(\"Mover failed. Exiting with status \" + r\n                   + \"... \");\n             }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       System.out.println(\"Mover Successful: all blocks satisfy\"\n           + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n            TimeUnit.SECONDS) * 2000 +\n        conf.getTimeDuration(\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n            TimeUnit.SECONDS) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    // TODO: Need to limit the size of the pinned blocks to limit memory usage\n    Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n    LOG.info(\"namenodes \u003d \" + namenodes);\n\n    checkKeytabAndInit(conf);\n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount,\n              excludedPinnedBlocks);\n\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13097: [SPS]: Fix the branch review comments(Part1). Contributed by Surendra Singh.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "5845c36c16c423107183287cce3be9357dad7564",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,79 +1,79 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n             TimeUnit.SECONDS) * 2000 +\n         conf.getTimeDuration(\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n             TimeUnit.SECONDS) * 1000;\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n     // TODO: Need to limit the size of the pinned blocks to limit memory usage\n     Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n     LOG.info(\"namenodes \u003d \" + namenodes);\n \n     checkKeytabAndInit(conf);\n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf, retryCount,\n               excludedPinnedBlocks);\n \n           boolean spsRunning;\n           try {\n             spsRunning \u003d nnc.getDistributedFileSystem().getClient()\n-                .isStoragePolicySatisfierRunning();\n+                .isInternalSatisfierRunning();\n           } catch (RemoteException e) {\n             IOException cause \u003d e.unwrapRemoteException();\n             if (cause instanceof StandbyException) {\n               System.err.println(\"Skip Standby Namenode. \" + nnc.toString());\n               continue;\n             }\n             throw e;\n           }\n           if (spsRunning) {\n             System.err.println(\"Mover failed due to StoragePolicySatisfier\"\n                 + \" service running inside namenode. Exiting with status \"\n                 + ExitStatus.SKIPPED_DUE_TO_SPS + \"... \");\n             return ExitStatus.SKIPPED_DUE_TO_SPS.getExitCode();\n           }\n \n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n               System.err.println(\"Failed to move some blocks after \"\n                   + m.retryMaxAttempts + \" retries. Exiting...\");\n             } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n               System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n             } else {\n               System.err.println(\"Mover failed. Exiting with status \" + r\n                   + \"... \");\n             }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       System.out.println(\"Mover Successful: all blocks satisfy\"\n           + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n            TimeUnit.SECONDS) * 2000 +\n        conf.getTimeDuration(\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n            TimeUnit.SECONDS) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    // TODO: Need to limit the size of the pinned blocks to limit memory usage\n    Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n    LOG.info(\"namenodes \u003d \" + namenodes);\n\n    checkKeytabAndInit(conf);\n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount,\n              excludedPinnedBlocks);\n\n          boolean spsRunning;\n          try {\n            spsRunning \u003d nnc.getDistributedFileSystem().getClient()\n                .isInternalSatisfierRunning();\n          } catch (RemoteException e) {\n            IOException cause \u003d e.unwrapRemoteException();\n            if (cause instanceof StandbyException) {\n              System.err.println(\"Skip Standby Namenode. \" + nnc.toString());\n              continue;\n            }\n            throw e;\n          }\n          if (spsRunning) {\n            System.err.println(\"Mover failed due to StoragePolicySatisfier\"\n                + \" service running inside namenode. Exiting with status \"\n                + ExitStatus.SKIPPED_DUE_TO_SPS + \"... \");\n            return ExitStatus.SKIPPED_DUE_TO_SPS.getExitCode();\n          }\n\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "5845c36c16c423107183287cce3be9357dad7564": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13050: [SPS]: Create start/stop script to start external SPS process. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "5845c36c16c423107183287cce3be9357dad7564",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "12/08/18 3:05 AM",
      "commitNameOld": "681d2804c95e5a569ffb8d9ceafaf5a4f8be2b88",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,79 +1,79 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n             TimeUnit.SECONDS) * 2000 +\n         conf.getTimeDuration(\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n             TimeUnit.SECONDS) * 1000;\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n     // TODO: Need to limit the size of the pinned blocks to limit memory usage\n     Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n     LOG.info(\"namenodes \u003d \" + namenodes);\n \n     checkKeytabAndInit(conf);\n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf, retryCount,\n               excludedPinnedBlocks);\n \n           boolean spsRunning;\n           try {\n             spsRunning \u003d nnc.getDistributedFileSystem().getClient()\n                 .isStoragePolicySatisfierRunning();\n           } catch (RemoteException e) {\n             IOException cause \u003d e.unwrapRemoteException();\n             if (cause instanceof StandbyException) {\n               System.err.println(\"Skip Standby Namenode. \" + nnc.toString());\n               continue;\n             }\n             throw e;\n           }\n           if (spsRunning) {\n             System.err.println(\"Mover failed due to StoragePolicySatisfier\"\n-                + \" is running. Exiting with status \"\n+                + \" service running inside namenode. Exiting with status \"\n                 + ExitStatus.SKIPPED_DUE_TO_SPS + \"... \");\n             return ExitStatus.SKIPPED_DUE_TO_SPS.getExitCode();\n           }\n \n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n               System.err.println(\"Failed to move some blocks after \"\n                   + m.retryMaxAttempts + \" retries. Exiting...\");\n             } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n               System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n             } else {\n               System.err.println(\"Mover failed. Exiting with status \" + r\n                   + \"... \");\n             }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       System.out.println(\"Mover Successful: all blocks satisfy\"\n           + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n            TimeUnit.SECONDS) * 2000 +\n        conf.getTimeDuration(\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n            TimeUnit.SECONDS) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    // TODO: Need to limit the size of the pinned blocks to limit memory usage\n    Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n    LOG.info(\"namenodes \u003d \" + namenodes);\n\n    checkKeytabAndInit(conf);\n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount,\n              excludedPinnedBlocks);\n\n          boolean spsRunning;\n          try {\n            spsRunning \u003d nnc.getDistributedFileSystem().getClient()\n                .isStoragePolicySatisfierRunning();\n          } catch (RemoteException e) {\n            IOException cause \u003d e.unwrapRemoteException();\n            if (cause instanceof StandbyException) {\n              System.err.println(\"Skip Standby Namenode. \" + nnc.toString());\n              continue;\n            }\n            throw e;\n          }\n          if (spsRunning) {\n            System.err.println(\"Mover failed due to StoragePolicySatisfier\"\n                + \" service running inside namenode. Exiting with status \"\n                + ExitStatus.SKIPPED_DUE_TO_SPS + \"... \");\n            return ExitStatus.SKIPPED_DUE_TO_SPS.getExitCode();\n          }\n\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "681d2804c95e5a569ffb8d9ceafaf5a4f8be2b88": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11186. [SPS]: Daemon thread of SPS should start only in Active NN. Contributed by Wei Zhou\n",
      "commitDate": "12/08/18 3:05 AM",
      "commitName": "681d2804c95e5a569ffb8d9ceafaf5a4f8be2b88",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "12/08/18 3:05 AM",
      "commitNameOld": "cd5262aba00aa51b905aaac95e201d4d48f2480d",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,79 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n             TimeUnit.SECONDS) * 2000 +\n         conf.getTimeDuration(\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n             TimeUnit.SECONDS) * 1000;\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n     // TODO: Need to limit the size of the pinned blocks to limit memory usage\n     Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n     LOG.info(\"namenodes \u003d \" + namenodes);\n \n     checkKeytabAndInit(conf);\n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf, retryCount,\n               excludedPinnedBlocks);\n \n           boolean spsRunning;\n           try {\n             spsRunning \u003d nnc.getDistributedFileSystem().getClient()\n                 .isStoragePolicySatisfierRunning();\n-          } catch (StandbyException e) {\n-            System.err.println(\"Skip Standby Namenode. \" + nnc.toString());\n-            continue;\n+          } catch (RemoteException e) {\n+            IOException cause \u003d e.unwrapRemoteException();\n+            if (cause instanceof StandbyException) {\n+              System.err.println(\"Skip Standby Namenode. \" + nnc.toString());\n+              continue;\n+            }\n+            throw e;\n           }\n           if (spsRunning) {\n             System.err.println(\"Mover failed due to StoragePolicySatisfier\"\n                 + \" is running. Exiting with status \"\n                 + ExitStatus.SKIPPED_DUE_TO_SPS + \"... \");\n             return ExitStatus.SKIPPED_DUE_TO_SPS.getExitCode();\n           }\n \n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n               System.err.println(\"Failed to move some blocks after \"\n                   + m.retryMaxAttempts + \" retries. Exiting...\");\n             } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n               System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n             } else {\n               System.err.println(\"Mover failed. Exiting with status \" + r\n                   + \"... \");\n             }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       System.out.println(\"Mover Successful: all blocks satisfy\"\n           + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n            TimeUnit.SECONDS) * 2000 +\n        conf.getTimeDuration(\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n            TimeUnit.SECONDS) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    // TODO: Need to limit the size of the pinned blocks to limit memory usage\n    Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n    LOG.info(\"namenodes \u003d \" + namenodes);\n\n    checkKeytabAndInit(conf);\n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount,\n              excludedPinnedBlocks);\n\n          boolean spsRunning;\n          try {\n            spsRunning \u003d nnc.getDistributedFileSystem().getClient()\n                .isStoragePolicySatisfierRunning();\n          } catch (RemoteException e) {\n            IOException cause \u003d e.unwrapRemoteException();\n            if (cause instanceof StandbyException) {\n              System.err.println(\"Skip Standby Namenode. \" + nnc.toString());\n              continue;\n            }\n            throw e;\n          }\n          if (spsRunning) {\n            System.err.println(\"Mover failed due to StoragePolicySatisfier\"\n                + \" is running. Exiting with status \"\n                + ExitStatus.SKIPPED_DUE_TO_SPS + \"... \");\n            return ExitStatus.SKIPPED_DUE_TO_SPS.getExitCode();\n          }\n\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "cd5262aba00aa51b905aaac95e201d4d48f2480d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10885. [SPS]: Mover tool should not be allowed to run when Storage Policy Satisfier is on. Contributed by Wei Zhou\n",
      "commitDate": "12/08/18 3:05 AM",
      "commitName": "cd5262aba00aa51b905aaac95e201d4d48f2480d",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "b668eb91556b8c85c2b4925808ccb1f769031c20",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 239.34,
      "commitsBetweenForRepo": 2213,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,75 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n             TimeUnit.SECONDS) * 2000 +\n         conf.getTimeDuration(\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n             TimeUnit.SECONDS) * 1000;\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n     // TODO: Need to limit the size of the pinned blocks to limit memory usage\n     Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n     LOG.info(\"namenodes \u003d \" + namenodes);\n \n     checkKeytabAndInit(conf);\n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n-          Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n+          Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf, retryCount,\n               excludedPinnedBlocks);\n+\n+          boolean spsRunning;\n+          try {\n+            spsRunning \u003d nnc.getDistributedFileSystem().getClient()\n+                .isStoragePolicySatisfierRunning();\n+          } catch (StandbyException e) {\n+            System.err.println(\"Skip Standby Namenode. \" + nnc.toString());\n+            continue;\n+          }\n+          if (spsRunning) {\n+            System.err.println(\"Mover failed due to StoragePolicySatisfier\"\n+                + \" is running. Exiting with status \"\n+                + ExitStatus.SKIPPED_DUE_TO_SPS + \"... \");\n+            return ExitStatus.SKIPPED_DUE_TO_SPS.getExitCode();\n+          }\n+\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n               System.err.println(\"Failed to move some blocks after \"\n                   + m.retryMaxAttempts + \" retries. Exiting...\");\n             } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n               System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n             } else {\n               System.err.println(\"Mover failed. Exiting with status \" + r\n                   + \"... \");\n             }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       System.out.println(\"Mover Successful: all blocks satisfy\"\n           + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n            TimeUnit.SECONDS) * 2000 +\n        conf.getTimeDuration(\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n            TimeUnit.SECONDS) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    // TODO: Need to limit the size of the pinned blocks to limit memory usage\n    Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n    LOG.info(\"namenodes \u003d \" + namenodes);\n\n    checkKeytabAndInit(conf);\n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), HdfsServerConstants.MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount,\n              excludedPinnedBlocks);\n\n          boolean spsRunning;\n          try {\n            spsRunning \u003d nnc.getDistributedFileSystem().getClient()\n                .isStoragePolicySatisfierRunning();\n          } catch (StandbyException e) {\n            System.err.println(\"Skip Standby Namenode. \" + nnc.toString());\n            continue;\n          }\n          if (spsRunning) {\n            System.err.println(\"Mover failed due to StoragePolicySatisfier\"\n                + \" is running. Exiting with status \"\n                + ExitStatus.SKIPPED_DUE_TO_SPS + \"... \");\n            return ExitStatus.SKIPPED_DUE_TO_SPS.getExitCode();\n          }\n\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "e24a923db50879f7dbe5d2afac0e6757089fb07d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11164: Mover should avoid unnecessary retries if the block is pinned. Contributed by Rakesh R\n",
      "commitDate": "13/12/16 5:09 PM",
      "commitName": "e24a923db50879f7dbe5d2afac0e6757089fb07d",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "23/11/16 4:42 PM",
      "commitNameOld": "de4894936a5b581572f35fa5b8979d9f23da0891",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 20.02,
      "commitsBetweenForRepo": 123,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,59 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n             TimeUnit.SECONDS) * 2000 +\n         conf.getTimeDuration(\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n             TimeUnit.SECONDS) * 1000;\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n+    // TODO: Need to limit the size of the pinned blocks to limit memory usage\n+    Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n     LOG.info(\"namenodes \u003d \" + namenodes);\n \n     checkKeytabAndInit(conf);\n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n-          final Mover m \u003d new Mover(nnc, conf, retryCount);\n+          final Mover m \u003d new Mover(nnc, conf, retryCount,\n+              excludedPinnedBlocks);\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n               System.err.println(\"Failed to move some blocks after \"\n                   + m.retryMaxAttempts + \" retries. Exiting...\");\n             } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n               System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n             } else {\n               System.err.println(\"Mover failed. Exiting with status \" + r\n                   + \"... \");\n             }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       System.out.println(\"Mover Successful: all blocks satisfy\"\n           + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n            TimeUnit.SECONDS) * 2000 +\n        conf.getTimeDuration(\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n            TimeUnit.SECONDS) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    // TODO: Need to limit the size of the pinned blocks to limit memory usage\n    Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks \u003d new HashMap\u003c\u003e();\n    LOG.info(\"namenodes \u003d \" + namenodes);\n\n    checkKeytabAndInit(conf);\n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount,\n              excludedPinnedBlocks);\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "de4894936a5b581572f35fa5b8979d9f23da0891": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11368. Erasure Coding: Deprecate replication-related config keys. Contributed by Rakesh R.\n",
      "commitDate": "23/11/16 4:42 PM",
      "commitName": "de4894936a5b581572f35fa5b8979d9f23da0891",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "21/11/16 8:13 AM",
      "commitNameOld": "49a09179e3fadae090126261be0a7fe0aa48798e",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 2.35,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n             TimeUnit.SECONDS) * 2000 +\n         conf.getTimeDuration(\n-            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n-            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT,\n+            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n+            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n             TimeUnit.SECONDS) * 1000;\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n     LOG.info(\"namenodes \u003d \" + namenodes);\n \n     checkKeytabAndInit(conf);\n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf, retryCount);\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n               System.err.println(\"Failed to move some blocks after \"\n                   + m.retryMaxAttempts + \" retries. Exiting...\");\n             } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n               System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n             } else {\n               System.err.println(\"Mover failed. Exiting with status \" + r\n                   + \"... \");\n             }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       System.out.println(\"Mover Successful: all blocks satisfy\"\n           + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n            TimeUnit.SECONDS) * 2000 +\n        conf.getTimeDuration(\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,\n            TimeUnit.SECONDS) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    LOG.info(\"namenodes \u003d \" + namenodes);\n\n    checkKeytabAndInit(conf);\n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount);\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "d37dc5d1b8e022a7085118a2e7066623483c293f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9847. HDFS configuration should accept time units. Contributed by Yiqun Lin\n",
      "commitDate": "06/09/16 10:38 AM",
      "commitName": "d37dc5d1b8e022a7085118a2e7066623483c293f",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "26/08/16 4:43 PM",
      "commitNameOld": "e806db719053a5b2a7b14f47e6f2962e70008d25",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 10.75,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,56 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n-        conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n-            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n-        conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n-            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n+        conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n+            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n+            TimeUnit.SECONDS) * 2000 +\n+        conf.getTimeDuration(\n+            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n+            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT,\n+            TimeUnit.SECONDS) * 1000;\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n     LOG.info(\"namenodes \u003d \" + namenodes);\n \n     checkKeytabAndInit(conf);\n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf, retryCount);\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n               System.err.println(\"Failed to move some blocks after \"\n                   + m.retryMaxAttempts + \" retries. Exiting...\");\n             } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n               System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n             } else {\n               System.err.println(\"Mover failed. Exiting with status \" + r\n                   + \"... \");\n             }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       System.out.println(\"Mover Successful: all blocks satisfy\"\n           + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,\n            TimeUnit.SECONDS) * 2000 +\n        conf.getTimeDuration(\n            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT,\n            TimeUnit.SECONDS) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    LOG.info(\"namenodes \u003d \" + namenodes);\n\n    checkKeytabAndInit(conf);\n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount);\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "e806db719053a5b2a7b14f47e6f2962e70008d25": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10584. Allow long-running Mover tool to login with keytab. Contributed by Rakesh R.\n",
      "commitDate": "26/08/16 4:43 PM",
      "commitName": "e806db719053a5b2a7b14f47e6f2962e70008d25",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "22/06/16 11:17 AM",
      "commitNameOld": "17eae9ebb30a3b106c4f6ae0c5374a3ab83abd8a",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 65.23,
      "commitsBetweenForRepo": 588,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,53 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n         conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n     LOG.info(\"namenodes \u003d \" + namenodes);\n-    \n+\n+    checkKeytabAndInit(conf);\n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf, retryCount);\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n               System.err.println(\"Failed to move some blocks after \"\n                   + m.retryMaxAttempts + \" retries. Exiting...\");\n             } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n               System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n             } else {\n               System.err.println(\"Mover failed. Exiting with status \" + r\n                   + \"... \");\n             }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       System.out.println(\"Mover Successful: all blocks satisfy\"\n           + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n        conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    LOG.info(\"namenodes \u003d \" + namenodes);\n\n    checkKeytabAndInit(conf);\n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount);\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "0faa4efa3dd74de9cc39584bf6e88cfbf3e9a045": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9151. Mover should print the exit status/reason on console like balancer tool. (Contributed by Surendra singh lilhore)\n",
      "commitDate": "05/10/15 12:52 AM",
      "commitName": "0faa4efa3dd74de9cc39584bf6e88cfbf3e9a045",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "23/09/15 1:34 PM",
      "commitNameOld": "c09dc258a8f64fab852bf6f26187163480dbee3c",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 11.47,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,52 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n         conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n     AtomicInteger retryCount \u003d new AtomicInteger(0);\n     LOG.info(\"namenodes \u003d \" + namenodes);\n     \n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf, retryCount);\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n+            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n+              System.err.println(\"Failed to move some blocks after \"\n+                  + m.retryMaxAttempts + \" retries. Exiting...\");\n+            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n+              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n+            } else {\n+              System.err.println(\"Mover failed. Exiting with status \" + r\n+                  + \"... \");\n+            }\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n+      System.out.println(\"Mover Successful: all blocks satisfy\"\n+          + \" the specified storage policy. Exiting...\");\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n        conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    LOG.info(\"namenodes \u003d \" + namenodes);\n    \n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount);\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            if (r \u003d\u003d ExitStatus.NO_MOVE_PROGRESS) {\n              System.err.println(\"Failed to move some blocks after \"\n                  + m.retryMaxAttempts + \" retries. Exiting...\");\n            } else if (r \u003d\u003d ExitStatus.NO_MOVE_BLOCK) {\n              System.err.println(\"Some blocks can\u0027t be moved. Exiting...\");\n            } else {\n              System.err.println(\"Mover failed. Exiting with status \" + r\n                  + \"... \");\n            }\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      System.out.println(\"Mover Successful: all blocks satisfy\"\n          + \" the specified storage policy. Exiting...\");\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "cdec12d1b84d444e13bf997c817643ec24aaa832": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8143. Mover should exit after some retry when failed to move blocks.  Contributed by surendra singh lilhore\n",
      "commitDate": "13/05/15 11:57 AM",
      "commitName": "cdec12d1b84d444e13bf997c817643ec24aaa832",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 11.08,
      "commitsBetweenForRepo": 166,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,41 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n         conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n+    AtomicInteger retryCount \u003d new AtomicInteger(0);\n     LOG.info(\"namenodes \u003d \" + namenodes);\n     \n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n           Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n           NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n-          final Mover m \u003d new Mover(nnc, conf);\n+          final Mover m \u003d new Mover(nnc, conf, retryCount);\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n        conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n    AtomicInteger retryCount \u003d new AtomicInteger(0);\n    LOG.info(\"namenodes \u003d \" + namenodes);\n    \n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf, retryCount);\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "b94c1117a28e996adee68fe0e181eb6f536289f4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-316. Balancer should run for a configurable # of iterations (Xiaoyu Yao via aw)\n",
      "commitDate": "11/02/15 8:10 AM",
      "commitName": "b94c1117a28e996adee68fe0e181eb6f536289f4",
      "commitAuthor": "Allen Wittenauer",
      "commitDateOld": "11/12/14 12:36 PM",
      "commitNameOld": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 61.82,
      "commitsBetweenForRepo": 416,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,40 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n         conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n     LOG.info(\"namenodes \u003d \" + namenodes);\n     \n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n-            Mover.class.getSimpleName(), MOVER_ID_PATH, conf);\n+          Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n+          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf);\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n        conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n    LOG.info(\"namenodes \u003d \" + namenodes);\n    \n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n          Mover.class.getSimpleName(), MOVER_ID_PATH, conf,\n          NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf);\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "2689b6ca727fff8a13347b811eb4cf79b9d30f48": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7062. Archival Storage: skip under construction block for migration. Contributed by Jing Zhao.\n",
      "commitDate": "15/09/14 10:16 AM",
      "commitName": "2689b6ca727fff8a13347b811eb4cf79b9d30f48",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "11/09/14 1:00 PM",
      "commitNameOld": "0d85f7e59146cc3e9a040c2203995f3efd8ed4eb",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 3.89,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,39 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d\n         conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n             DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n         conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n             DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n     LOG.info(\"namenodes \u003d \" + namenodes);\n     \n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n             Mover.class.getSimpleName(), MOVER_ID_PATH, conf);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf);\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n+            IOUtils.cleanup(LOG, nnc);\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n        conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n    LOG.info(\"namenodes \u003d \" + namenodes);\n    \n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n            Mover.class.getSimpleName(), MOVER_ID_PATH, conf);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf);\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            IOUtils.cleanup(LOG, nnc);\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "0d85f7e59146cc3e9a040c2203995f3efd8ed4eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7034. Archival Storage: Fix TestBlockPlacement and TestStorageMover. Contributed by Jing Zhao.\n",
      "commitDate": "11/09/14 1:00 PM",
      "commitName": "0d85f7e59146cc3e9a040c2203995f3efd8ed4eb",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "08/09/14 2:21 PM",
      "commitNameOld": "74a7e227c822f3a43354f52fcf60635310af093a",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.94,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,38 @@\n   static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n-    final long sleeptime \u003d 2000 * conf.getLong(\n-        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n-        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT);\n+    final long sleeptime \u003d\n+        conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n+            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n+        conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n+            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n     LOG.info(\"namenodes \u003d \" + namenodes);\n     \n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n             Mover.class.getSimpleName(), MOVER_ID_PATH, conf);\n \n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf);\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d\n        conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n            DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 +\n        conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,\n            DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;\n    LOG.info(\"namenodes \u003d \" + namenodes);\n    \n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n            Mover.class.getSimpleName(), MOVER_ID_PATH, conf);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf);\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "2b5c528a7331a00cfc67e64cd10342650948d686": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-6875. Archival Storage: support migration for a list of specified paths. Contributed by Jing Zhao.\n",
      "commitDate": "08/09/14 2:10 PM",
      "commitName": "2b5c528a7331a00cfc67e64cd10342650948d686",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "06/09/14 4:44 PM",
      "commitNameOld": "22a41dce4af4d5b533ba875b322551db1c152878",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 1.89,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n-  static int run(Collection\u003cURI\u003e namenodes, Configuration conf)\n+  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n     final long sleeptime \u003d 2000 * conf.getLong(\n         DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n         DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT);\n     LOG.info(\"namenodes \u003d \" + namenodes);\n     \n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n       connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n             Mover.class.getSimpleName(), MOVER_ID_PATH, conf);\n-    \n+\n       while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n         Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n         while (iter.hasNext()) {\n           NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf);\n           final ExitStatus r \u003d m.run();\n \n           if (r \u003d\u003d ExitStatus.SUCCESS) {\n             iter.remove();\n           } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n             // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n         Thread.sleep(sleeptime);\n       }\n       return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n       for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Map\u003cURI, List\u003cPath\u003e\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d 2000 * conf.getLong(\n        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT);\n    LOG.info(\"namenodes \u003d \" + namenodes);\n    \n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n            Mover.class.getSimpleName(), MOVER_ID_PATH, conf);\n\n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf);\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {
        "oldValue": "[namenodes-Collection\u003cURI\u003e, conf-Configuration]",
        "newValue": "[namenodes-Map\u003cURI,List\u003cPath\u003e\u003e, conf-Configuration]"
      }
    },
    "a26aa6bd0716da89853566961390d711511084e3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6944. Archival Storage: add retry and termination logic for Mover. Contributed by Jing Zhao.\n",
      "commitDate": "27/08/14 2:20 PM",
      "commitName": "a26aa6bd0716da89853566961390d711511084e3",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/08/14 10:38 AM",
      "commitNameOld": "8ea20b53a861a2771c206afaacf8e7783568c4b1",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.15,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,36 @@\n   static int run(Collection\u003cURI\u003e namenodes, Configuration conf)\n       throws IOException, InterruptedException {\n-    final long sleeptime \u003d 2000*conf.getLong(\n+    final long sleeptime \u003d 2000 * conf.getLong(\n         DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n         DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT);\n     LOG.info(\"namenodes \u003d \" + namenodes);\n     \n     List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n     try {\n-      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes, \n+      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n             Mover.class.getSimpleName(), MOVER_ID_PATH, conf);\n     \n-      while (true) {\n+      while (connectors.size() \u003e 0) {\n         Collections.shuffle(connectors);\n-        for(NameNodeConnector nnc : connectors) {\n+        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n+        while (iter.hasNext()) {\n+          NameNodeConnector nnc \u003d iter.next();\n           final Mover m \u003d new Mover(nnc, conf);\n           final ExitStatus r \u003d m.run();\n \n-          if (r !\u003d ExitStatus.IN_PROGRESS) {\n-            //must be an error statue, return.\n+          if (r \u003d\u003d ExitStatus.SUCCESS) {\n+            iter.remove();\n+          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n+            // must be an error statue, return\n             return r.getExitCode();\n           }\n         }\n-\n         Thread.sleep(sleeptime);\n       }\n+      return ExitStatus.SUCCESS.getExitCode();\n     } finally {\n-      for(NameNodeConnector nnc : connectors) {\n+      for (NameNodeConnector nnc : connectors) {\n         IOUtils.cleanup(LOG, nnc);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Collection\u003cURI\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d 2000 * conf.getLong(\n        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT);\n    LOG.info(\"namenodes \u003d \" + namenodes);\n    \n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes,\n            Mover.class.getSimpleName(), MOVER_ID_PATH, conf);\n    \n      while (connectors.size() \u003e 0) {\n        Collections.shuffle(connectors);\n        Iterator\u003cNameNodeConnector\u003e iter \u003d connectors.iterator();\n        while (iter.hasNext()) {\n          NameNodeConnector nnc \u003d iter.next();\n          final Mover m \u003d new Mover(nnc, conf);\n          final ExitStatus r \u003d m.run();\n\n          if (r \u003d\u003d ExitStatus.SUCCESS) {\n            iter.remove();\n          } else if (r !\u003d ExitStatus.IN_PROGRESS) {\n            // must be an error statue, return\n            return r.getExitCode();\n          }\n        }\n        Thread.sleep(sleeptime);\n      }\n      return ExitStatus.SUCCESS.getExitCode();\n    } finally {\n      for (NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
      "extendedDetails": {}
    },
    "5d5aae0694bc27df5b9fa50819854cd3050a8658": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-6801. Archival Storage: Add a new data migration tool. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-6584@1618675 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/14 10:51 AM",
      "commitName": "5d5aae0694bc27df5b9fa50819854cd3050a8658",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,32 @@\n+  static int run(Collection\u003cURI\u003e namenodes, Configuration conf)\n+      throws IOException, InterruptedException {\n+    final long sleeptime \u003d 2000*conf.getLong(\n+        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n+        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT);\n+    LOG.info(\"namenodes \u003d \" + namenodes);\n+    \n+    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n+    try {\n+      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes, \n+            Mover.class.getSimpleName(), MOVER_ID_PATH, conf);\n+    \n+      while (true) {\n+        Collections.shuffle(connectors);\n+        for(NameNodeConnector nnc : connectors) {\n+          final Mover m \u003d new Mover(nnc, conf);\n+          final ExitStatus r \u003d m.run();\n+\n+          if (r !\u003d ExitStatus.IN_PROGRESS) {\n+            //must be an error statue, return.\n+            return r.getExitCode();\n+          }\n+        }\n+\n+        Thread.sleep(sleeptime);\n+      }\n+    } finally {\n+      for(NameNodeConnector nnc : connectors) {\n+        IOUtils.cleanup(LOG, nnc);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  static int run(Collection\u003cURI\u003e namenodes, Configuration conf)\n      throws IOException, InterruptedException {\n    final long sleeptime \u003d 2000*conf.getLong(\n        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,\n        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT);\n    LOG.info(\"namenodes \u003d \" + namenodes);\n    \n    List\u003cNameNodeConnector\u003e connectors \u003d Collections.emptyList();\n    try {\n      connectors \u003d NameNodeConnector.newNameNodeConnectors(namenodes, \n            Mover.class.getSimpleName(), MOVER_ID_PATH, conf);\n    \n      while (true) {\n        Collections.shuffle(connectors);\n        for(NameNodeConnector nnc : connectors) {\n          final Mover m \u003d new Mover(nnc, conf);\n          final ExitStatus r \u003d m.run();\n\n          if (r !\u003d ExitStatus.IN_PROGRESS) {\n            //must be an error statue, return.\n            return r.getExitCode();\n          }\n        }\n\n        Thread.sleep(sleeptime);\n      }\n    } finally {\n      for(NameNodeConnector nnc : connectors) {\n        IOUtils.cleanup(LOG, nnc);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java"
    }
  }
}