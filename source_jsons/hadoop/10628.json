{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockChecksumHelper.java",
  "functionName": "crcPartialBlock",
  "functionId": "crcPartialBlock",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
  "functionStartLine": 259,
  "functionEndLine": 278,
  "numCommitsSeen": 8,
  "timeTaken": 2940,
  "changeHistory": [
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
    "a337ceb74e984991dbf976236d2e785cf5921b16",
    "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6"
  ],
  "changeHistoryShort": {
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720": "Ymultichange(Ymodifierchange,Ybodychange)",
    "a337ceb74e984991dbf976236d2e785cf5921b16": "Ymultichange(Ymodifierchange,Ybodychange)",
    "e5ff0ea7ba087984262f1f27200ae5bb40d9b838": "Ymultichange(Ymodifierchange,Ybodychange)",
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
      "commitDate": "26/03/16 7:58 PM",
      "commitName": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
      "commitAuthor": "Uma Maheswara Rao G",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
          "commitDate": "26/03/16 7:58 PM",
          "commitName": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "26/03/16 9:20 AM",
          "commitNameOld": "a337ceb74e984991dbf976236d2e785cf5921b16",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.44,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-    protected byte[] crcPartialBlock() throws IOException {\n-      int partialLength \u003d (int) (requestLength % bytesPerCRC);\n+    byte[] crcPartialBlock() throws IOException {\n+      int partialLength \u003d (int) (requestLength % getBytesPerCRC());\n       if (partialLength \u003e 0) {\n         byte[] buf \u003d new byte[partialLength];\n-        final InputStream blockIn \u003d datanode.data.getBlockInputStream(block,\n+        final InputStream blockIn \u003d getBlockInputStream(block,\n             requestLength - partialLength);\n         try {\n           // Get the CRC of the partialLength.\n           IOUtils.readFully(blockIn, buf, 0, partialLength);\n         } finally {\n           IOUtils.closeStream(blockIn);\n         }\n         checksum.update(buf, 0, partialLength);\n-        byte[] partialCrc \u003d new byte[checksumSize];\n+        byte[] partialCrc \u003d new byte[getChecksumSize()];\n         checksum.writeValue(partialCrc, 0, true);\n         return partialCrc;\n       }\n \n       return null;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    byte[] crcPartialBlock() throws IOException {\n      int partialLength \u003d (int) (requestLength % getBytesPerCRC());\n      if (partialLength \u003e 0) {\n        byte[] buf \u003d new byte[partialLength];\n        final InputStream blockIn \u003d getBlockInputStream(block,\n            requestLength - partialLength);\n        try {\n          // Get the CRC of the partialLength.\n          IOUtils.readFully(blockIn, buf, 0, partialLength);\n        } finally {\n          IOUtils.closeStream(blockIn);\n        }\n        checksum.update(buf, 0, partialLength);\n        byte[] partialCrc \u003d new byte[getChecksumSize()];\n        checksum.writeValue(partialCrc, 0, true);\n        return partialCrc;\n      }\n\n      return null;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
          "extendedDetails": {
            "oldValue": "[protected]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
          "commitDate": "26/03/16 7:58 PM",
          "commitName": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "26/03/16 9:20 AM",
          "commitNameOld": "a337ceb74e984991dbf976236d2e785cf5921b16",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.44,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-    protected byte[] crcPartialBlock() throws IOException {\n-      int partialLength \u003d (int) (requestLength % bytesPerCRC);\n+    byte[] crcPartialBlock() throws IOException {\n+      int partialLength \u003d (int) (requestLength % getBytesPerCRC());\n       if (partialLength \u003e 0) {\n         byte[] buf \u003d new byte[partialLength];\n-        final InputStream blockIn \u003d datanode.data.getBlockInputStream(block,\n+        final InputStream blockIn \u003d getBlockInputStream(block,\n             requestLength - partialLength);\n         try {\n           // Get the CRC of the partialLength.\n           IOUtils.readFully(blockIn, buf, 0, partialLength);\n         } finally {\n           IOUtils.closeStream(blockIn);\n         }\n         checksum.update(buf, 0, partialLength);\n-        byte[] partialCrc \u003d new byte[checksumSize];\n+        byte[] partialCrc \u003d new byte[getChecksumSize()];\n         checksum.writeValue(partialCrc, 0, true);\n         return partialCrc;\n       }\n \n       return null;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    byte[] crcPartialBlock() throws IOException {\n      int partialLength \u003d (int) (requestLength % getBytesPerCRC());\n      if (partialLength \u003e 0) {\n        byte[] buf \u003d new byte[partialLength];\n        final InputStream blockIn \u003d getBlockInputStream(block,\n            requestLength - partialLength);\n        try {\n          // Get the CRC of the partialLength.\n          IOUtils.readFully(blockIn, buf, 0, partialLength);\n        } finally {\n          IOUtils.closeStream(blockIn);\n        }\n        checksum.update(buf, 0, partialLength);\n        byte[] partialCrc \u003d new byte[getChecksumSize()];\n        checksum.writeValue(partialCrc, 0, true);\n        return partialCrc;\n      }\n\n      return null;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "a337ceb74e984991dbf976236d2e785cf5921b16": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "Revert \"HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\"\n\nThis reverts commit e5ff0ea7ba087984262f1f27200ae5bb40d9b838.\n",
      "commitDate": "26/03/16 9:20 AM",
      "commitName": "a337ceb74e984991dbf976236d2e785cf5921b16",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "Revert \"HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\"\n\nThis reverts commit e5ff0ea7ba087984262f1f27200ae5bb40d9b838.\n",
          "commitDate": "26/03/16 9:20 AM",
          "commitName": "a337ceb74e984991dbf976236d2e785cf5921b16",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "26/03/16 12:52 AM",
          "commitNameOld": "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 0.35,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-    byte[] crcPartialBlock() throws IOException {\n-      int partialLength \u003d (int) (requestLength % getBytesPerCRC());\n+    protected byte[] crcPartialBlock() throws IOException {\n+      int partialLength \u003d (int) (requestLength % bytesPerCRC);\n       if (partialLength \u003e 0) {\n         byte[] buf \u003d new byte[partialLength];\n-        final InputStream blockIn \u003d getBlockInputStream(block,\n+        final InputStream blockIn \u003d datanode.data.getBlockInputStream(block,\n             requestLength - partialLength);\n         try {\n           // Get the CRC of the partialLength.\n           IOUtils.readFully(blockIn, buf, 0, partialLength);\n         } finally {\n           IOUtils.closeStream(blockIn);\n         }\n         checksum.update(buf, 0, partialLength);\n-        byte[] partialCrc \u003d new byte[getChecksumSize()];\n+        byte[] partialCrc \u003d new byte[checksumSize];\n         checksum.writeValue(partialCrc, 0, true);\n         return partialCrc;\n       }\n \n       return null;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    protected byte[] crcPartialBlock() throws IOException {\n      int partialLength \u003d (int) (requestLength % bytesPerCRC);\n      if (partialLength \u003e 0) {\n        byte[] buf \u003d new byte[partialLength];\n        final InputStream blockIn \u003d datanode.data.getBlockInputStream(block,\n            requestLength - partialLength);\n        try {\n          // Get the CRC of the partialLength.\n          IOUtils.readFully(blockIn, buf, 0, partialLength);\n        } finally {\n          IOUtils.closeStream(blockIn);\n        }\n        checksum.update(buf, 0, partialLength);\n        byte[] partialCrc \u003d new byte[checksumSize];\n        checksum.writeValue(partialCrc, 0, true);\n        return partialCrc;\n      }\n\n      return null;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[protected]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "Revert \"HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\"\n\nThis reverts commit e5ff0ea7ba087984262f1f27200ae5bb40d9b838.\n",
          "commitDate": "26/03/16 9:20 AM",
          "commitName": "a337ceb74e984991dbf976236d2e785cf5921b16",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "26/03/16 12:52 AM",
          "commitNameOld": "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 0.35,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-    byte[] crcPartialBlock() throws IOException {\n-      int partialLength \u003d (int) (requestLength % getBytesPerCRC());\n+    protected byte[] crcPartialBlock() throws IOException {\n+      int partialLength \u003d (int) (requestLength % bytesPerCRC);\n       if (partialLength \u003e 0) {\n         byte[] buf \u003d new byte[partialLength];\n-        final InputStream blockIn \u003d getBlockInputStream(block,\n+        final InputStream blockIn \u003d datanode.data.getBlockInputStream(block,\n             requestLength - partialLength);\n         try {\n           // Get the CRC of the partialLength.\n           IOUtils.readFully(blockIn, buf, 0, partialLength);\n         } finally {\n           IOUtils.closeStream(blockIn);\n         }\n         checksum.update(buf, 0, partialLength);\n-        byte[] partialCrc \u003d new byte[getChecksumSize()];\n+        byte[] partialCrc \u003d new byte[checksumSize];\n         checksum.writeValue(partialCrc, 0, true);\n         return partialCrc;\n       }\n \n       return null;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    protected byte[] crcPartialBlock() throws IOException {\n      int partialLength \u003d (int) (requestLength % bytesPerCRC);\n      if (partialLength \u003e 0) {\n        byte[] buf \u003d new byte[partialLength];\n        final InputStream blockIn \u003d datanode.data.getBlockInputStream(block,\n            requestLength - partialLength);\n        try {\n          // Get the CRC of the partialLength.\n          IOUtils.readFully(blockIn, buf, 0, partialLength);\n        } finally {\n          IOUtils.closeStream(blockIn);\n        }\n        checksum.update(buf, 0, partialLength);\n        byte[] partialCrc \u003d new byte[checksumSize];\n        checksum.writeValue(partialCrc, 0, true);\n        return partialCrc;\n      }\n\n      return null;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "e5ff0ea7ba087984262f1f27200ae5bb40d9b838": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
      "commitDate": "26/03/16 12:52 AM",
      "commitName": "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
      "commitAuthor": "Uma Maheswara Rao G",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
          "commitDate": "26/03/16 12:52 AM",
          "commitName": "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "29/02/16 9:52 PM",
          "commitNameOld": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 25.08,
          "commitsBetweenForRepo": 134,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-    protected byte[] crcPartialBlock() throws IOException {\n-      int partialLength \u003d (int) (requestLength % bytesPerCRC);\n+    byte[] crcPartialBlock() throws IOException {\n+      int partialLength \u003d (int) (requestLength % getBytesPerCRC());\n       if (partialLength \u003e 0) {\n         byte[] buf \u003d new byte[partialLength];\n-        final InputStream blockIn \u003d datanode.data.getBlockInputStream(block,\n+        final InputStream blockIn \u003d getBlockInputStream(block,\n             requestLength - partialLength);\n         try {\n           // Get the CRC of the partialLength.\n           IOUtils.readFully(blockIn, buf, 0, partialLength);\n         } finally {\n           IOUtils.closeStream(blockIn);\n         }\n         checksum.update(buf, 0, partialLength);\n-        byte[] partialCrc \u003d new byte[checksumSize];\n+        byte[] partialCrc \u003d new byte[getChecksumSize()];\n         checksum.writeValue(partialCrc, 0, true);\n         return partialCrc;\n       }\n \n       return null;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    byte[] crcPartialBlock() throws IOException {\n      int partialLength \u003d (int) (requestLength % getBytesPerCRC());\n      if (partialLength \u003e 0) {\n        byte[] buf \u003d new byte[partialLength];\n        final InputStream blockIn \u003d getBlockInputStream(block,\n            requestLength - partialLength);\n        try {\n          // Get the CRC of the partialLength.\n          IOUtils.readFully(blockIn, buf, 0, partialLength);\n        } finally {\n          IOUtils.closeStream(blockIn);\n        }\n        checksum.update(buf, 0, partialLength);\n        byte[] partialCrc \u003d new byte[getChecksumSize()];\n        checksum.writeValue(partialCrc, 0, true);\n        return partialCrc;\n      }\n\n      return null;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
          "extendedDetails": {
            "oldValue": "[protected]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
          "commitDate": "26/03/16 12:52 AM",
          "commitName": "e5ff0ea7ba087984262f1f27200ae5bb40d9b838",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "29/02/16 9:52 PM",
          "commitNameOld": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 25.08,
          "commitsBetweenForRepo": 134,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-    protected byte[] crcPartialBlock() throws IOException {\n-      int partialLength \u003d (int) (requestLength % bytesPerCRC);\n+    byte[] crcPartialBlock() throws IOException {\n+      int partialLength \u003d (int) (requestLength % getBytesPerCRC());\n       if (partialLength \u003e 0) {\n         byte[] buf \u003d new byte[partialLength];\n-        final InputStream blockIn \u003d datanode.data.getBlockInputStream(block,\n+        final InputStream blockIn \u003d getBlockInputStream(block,\n             requestLength - partialLength);\n         try {\n           // Get the CRC of the partialLength.\n           IOUtils.readFully(blockIn, buf, 0, partialLength);\n         } finally {\n           IOUtils.closeStream(blockIn);\n         }\n         checksum.update(buf, 0, partialLength);\n-        byte[] partialCrc \u003d new byte[checksumSize];\n+        byte[] partialCrc \u003d new byte[getChecksumSize()];\n         checksum.writeValue(partialCrc, 0, true);\n         return partialCrc;\n       }\n \n       return null;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    byte[] crcPartialBlock() throws IOException {\n      int partialLength \u003d (int) (requestLength % getBytesPerCRC());\n      if (partialLength \u003e 0) {\n        byte[] buf \u003d new byte[partialLength];\n        final InputStream blockIn \u003d getBlockInputStream(block,\n            requestLength - partialLength);\n        try {\n          // Get the CRC of the partialLength.\n          IOUtils.readFully(blockIn, buf, 0, partialLength);\n        } finally {\n          IOUtils.closeStream(blockIn);\n        }\n        checksum.update(buf, 0, partialLength);\n        byte[] partialCrc \u003d new byte[getChecksumSize()];\n        checksum.writeValue(partialCrc, 0, true);\n        return partialCrc;\n      }\n\n      return null;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9733. Refactor DFSClient#getFileChecksum and DataXceiver#blockChecksum. Contributed by Kai Zheng\n",
      "commitDate": "29/02/16 9:52 PM",
      "commitName": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,20 @@\n+    protected byte[] crcPartialBlock() throws IOException {\n+      int partialLength \u003d (int) (requestLength % bytesPerCRC);\n+      if (partialLength \u003e 0) {\n+        byte[] buf \u003d new byte[partialLength];\n+        final InputStream blockIn \u003d datanode.data.getBlockInputStream(block,\n+            requestLength - partialLength);\n+        try {\n+          // Get the CRC of the partialLength.\n+          IOUtils.readFully(blockIn, buf, 0, partialLength);\n+        } finally {\n+          IOUtils.closeStream(blockIn);\n+        }\n+        checksum.update(buf, 0, partialLength);\n+        byte[] partialCrc \u003d new byte[checksumSize];\n+        checksum.writeValue(partialCrc, 0, true);\n+        return partialCrc;\n+      }\n+\n+      return null;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    protected byte[] crcPartialBlock() throws IOException {\n      int partialLength \u003d (int) (requestLength % bytesPerCRC);\n      if (partialLength \u003e 0) {\n        byte[] buf \u003d new byte[partialLength];\n        final InputStream blockIn \u003d datanode.data.getBlockInputStream(block,\n            requestLength - partialLength);\n        try {\n          // Get the CRC of the partialLength.\n          IOUtils.readFully(blockIn, buf, 0, partialLength);\n        } finally {\n          IOUtils.closeStream(blockIn);\n        }\n        checksum.update(buf, 0, partialLength);\n        byte[] partialCrc \u003d new byte[checksumSize];\n        checksum.writeValue(partialCrc, 0, true);\n        return partialCrc;\n      }\n\n      return null;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java"
    }
  }
}