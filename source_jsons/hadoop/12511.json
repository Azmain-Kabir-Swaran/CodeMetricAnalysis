{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Dispatcher.java",
  "functionName": "checkForBlockPinningFailures",
  "functionId": "checkForBlockPinningFailures___excludedPinnedBlocks-Map__Long,Set__DatanodeInfo______targets-Iterable__? extends StorageGroup__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
  "functionStartLine": 1250,
  "functionEndLine": 1271,
  "numCommitsSeen": 50,
  "timeTaken": 1339,
  "changeHistory": [
    "e24a923db50879f7dbe5d2afac0e6757089fb07d"
  ],
  "changeHistoryShort": {
    "e24a923db50879f7dbe5d2afac0e6757089fb07d": "Yintroduced"
  },
  "changeHistoryDetails": {
    "e24a923db50879f7dbe5d2afac0e6757089fb07d": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11164: Mover should avoid unnecessary retries if the block is pinned. Contributed by Rakesh R\n",
      "commitDate": "13/12/16 5:09 PM",
      "commitName": "e24a923db50879f7dbe5d2afac0e6757089fb07d",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,22 @@\n+  public static void checkForBlockPinningFailures(\n+      Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks,\n+      Iterable\u003c? extends StorageGroup\u003e targets) {\n+    for (StorageGroup t : targets) {\n+      Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e blockPinningFailureList \u003d t.getDDatanode()\n+          .getBlockPinningFailureList();\n+      Set\u003cEntry\u003cLong, Set\u003cDatanodeInfo\u003e\u003e\u003e entrySet \u003d blockPinningFailureList\n+          .entrySet();\n+      for (Entry\u003cLong, Set\u003cDatanodeInfo\u003e\u003e entry : entrySet) {\n+        Long blockId \u003d entry.getKey();\n+        Set\u003cDatanodeInfo\u003e locs \u003d excludedPinnedBlocks.get(blockId);\n+        if (locs \u003d\u003d null) {\n+          // blockId doesn\u0027t exists in the excluded list.\n+          locs \u003d entry.getValue();\n+          excludedPinnedBlocks.put(blockId, locs);\n+        } else {\n+          // blockId already exists in the excluded list, add the pinned node.\n+          locs.addAll(entry.getValue());\n+        }\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static void checkForBlockPinningFailures(\n      Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e excludedPinnedBlocks,\n      Iterable\u003c? extends StorageGroup\u003e targets) {\n    for (StorageGroup t : targets) {\n      Map\u003cLong, Set\u003cDatanodeInfo\u003e\u003e blockPinningFailureList \u003d t.getDDatanode()\n          .getBlockPinningFailureList();\n      Set\u003cEntry\u003cLong, Set\u003cDatanodeInfo\u003e\u003e\u003e entrySet \u003d blockPinningFailureList\n          .entrySet();\n      for (Entry\u003cLong, Set\u003cDatanodeInfo\u003e\u003e entry : entrySet) {\n        Long blockId \u003d entry.getKey();\n        Set\u003cDatanodeInfo\u003e locs \u003d excludedPinnedBlocks.get(blockId);\n        if (locs \u003d\u003d null) {\n          // blockId doesn\u0027t exists in the excluded list.\n          locs \u003d entry.getValue();\n          excludedPinnedBlocks.put(blockId, locs);\n        } else {\n          // blockId already exists in the excluded list, add the pinned node.\n          locs.addAll(entry.getValue());\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java"
    }
  }
}