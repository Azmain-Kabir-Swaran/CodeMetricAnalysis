{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FileInputFormat.java",
  "functionName": "getSplits",
  "functionId": "getSplits___job-JobConf__numSplits-int",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
  "functionStartLine": 331,
  "functionEndLine": 413,
  "numCommitsSeen": 23,
  "timeTaken": 6097,
  "changeHistory": [
    "68c6ec719da8e79ada31c8f3a82124f90b9a71fd",
    "6c154abd33279475315b5f7f78dc47f1b0aa7028",
    "a6ed4894b518351bf1b3290e725a475570a21296",
    "988639640024933448f822b48119dc832ebd2af3",
    "2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b",
    "bd23a2ff22dba8a5203e8e498244f985e728da51",
    "396c6c63a26b098fd0221e830f79be13b7e97432",
    "ec18984252731089ab5af12b3603dcfc3d4f4593",
    "dc615c312b81d2bff17821b59fba1b76aa24f585",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "68c6ec719da8e79ada31c8f3a82124f90b9a71fd": "Ybodychange",
    "6c154abd33279475315b5f7f78dc47f1b0aa7028": "Ybodychange",
    "a6ed4894b518351bf1b3290e725a475570a21296": "Ybodychange",
    "988639640024933448f822b48119dc832ebd2af3": "Ybodychange",
    "2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b": "Ybodychange",
    "bd23a2ff22dba8a5203e8e498244f985e728da51": "Ybodychange",
    "396c6c63a26b098fd0221e830f79be13b7e97432": "Ybodychange",
    "ec18984252731089ab5af12b3603dcfc3d4f4593": "Ybodychange",
    "dc615c312b81d2bff17821b59fba1b76aa24f585": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "68c6ec719da8e79ada31c8f3a82124f90b9a71fd": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-7086. Add config to allow FileInputFormat to ignore directories when recursive\u003dfalse. Contributed by Sergey Shelukhin\n",
      "commitDate": "01/05/18 2:19 PM",
      "commitName": "68c6ec719da8e79ada31c8f3a82124f90b9a71fd",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "02/11/17 1:43 AM",
      "commitNameOld": "178751ed8c9d47038acf8616c226f1f52e884feb",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 180.53,
      "commitsBetweenForRepo": 1832,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,83 @@\n   public InputSplit[] getSplits(JobConf job, int numSplits)\n     throws IOException {\n     StopWatch sw \u003d new StopWatch().start();\n-    FileStatus[] files \u003d listStatus(job);\n-    \n+    FileStatus[] stats \u003d listStatus(job);\n+\n     // Save the number of input files for metrics/loadgen\n-    job.setLong(NUM_INPUT_FILES, files.length);\n+    job.setLong(NUM_INPUT_FILES, stats.length);\n     long totalSize \u003d 0;                           // compute total size\n-    for (FileStatus file: files) {                // check we have valid files\n+    boolean ignoreDirs \u003d !job.getBoolean(INPUT_DIR_RECURSIVE, false)\n+      \u0026\u0026 job.getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, false);\n+\n+    List\u003cFileStatus\u003e files \u003d new ArrayList\u003c\u003e(stats.length);\n+    for (FileStatus file: stats) {                // check we have valid files\n       if (file.isDirectory()) {\n-        throw new IOException(\"Not a file: \"+ file.getPath());\n+        if (!ignoreDirs) {\n+          throw new IOException(\"Not a file: \"+ file.getPath());\n+        }\n+      } else {\n+        files.add(file);\n+        totalSize +\u003d file.getLen();\n       }\n-      totalSize +\u003d file.getLen();\n     }\n \n     long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n     long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n       FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n \n     // generate splits\n     ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n     NetworkTopology clusterMap \u003d new NetworkTopology();\n     for (FileStatus file: files) {\n       Path path \u003d file.getPath();\n       long length \u003d file.getLen();\n       if (length !\u003d 0) {\n         FileSystem fs \u003d path.getFileSystem(job);\n         BlockLocation[] blkLocations;\n         if (file instanceof LocatedFileStatus) {\n           blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n         } else {\n           blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n         }\n         if (isSplitable(fs, path)) {\n           long blockSize \u003d file.getBlockSize();\n           long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n \n           long bytesRemaining \u003d length;\n           while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n             String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                 length-bytesRemaining, splitSize, clusterMap);\n             splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                 splitHosts[0], splitHosts[1]));\n             bytesRemaining -\u003d splitSize;\n           }\n \n           if (bytesRemaining !\u003d 0) {\n             String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                 - bytesRemaining, bytesRemaining, clusterMap);\n             splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                 splitHosts[0], splitHosts[1]));\n           }\n         } else {\n           if (LOG.isDebugEnabled()) {\n             // Log only if the file is big enough to be splitted\n             if (length \u003e Math.min(file.getBlockSize(), minSize)) {\n               LOG.debug(\"File is not splittable so no parallelization \"\n                   + \"is possible: \" + file.getPath());\n             }\n           }\n           String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n           splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n         }\n       } else { \n         //Create empty hosts array for zero length files\n         splits.add(makeSplit(path, 0, length, new String[0]));\n       }\n     }\n     sw.stop();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n           + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\n     }\n     return splits.toArray(new FileSplit[splits.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    StopWatch sw \u003d new StopWatch().start();\n    FileStatus[] stats \u003d listStatus(job);\n\n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, stats.length);\n    long totalSize \u003d 0;                           // compute total size\n    boolean ignoreDirs \u003d !job.getBoolean(INPUT_DIR_RECURSIVE, false)\n      \u0026\u0026 job.getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, false);\n\n    List\u003cFileStatus\u003e files \u003d new ArrayList\u003c\u003e(stats.length);\n    for (FileStatus file: stats) {                // check we have valid files\n      if (file.isDirectory()) {\n        if (!ignoreDirs) {\n          throw new IOException(\"Not a file: \"+ file.getPath());\n        }\n      } else {\n        files.add(file);\n        totalSize +\u003d file.getLen();\n      }\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      long length \u003d file.getLen();\n      if (length !\u003d 0) {\n        FileSystem fs \u003d path.getFileSystem(job);\n        BlockLocation[] blkLocations;\n        if (file instanceof LocatedFileStatus) {\n          blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n        } else {\n          blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n        }\n        if (isSplitable(fs, path)) {\n          long blockSize \u003d file.getBlockSize();\n          long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n          long bytesRemaining \u003d length;\n          while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                length-bytesRemaining, splitSize, clusterMap);\n            splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                splitHosts[0], splitHosts[1]));\n            bytesRemaining -\u003d splitSize;\n          }\n\n          if (bytesRemaining !\u003d 0) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                - bytesRemaining, bytesRemaining, clusterMap);\n            splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                splitHosts[0], splitHosts[1]));\n          }\n        } else {\n          if (LOG.isDebugEnabled()) {\n            // Log only if the file is big enough to be splitted\n            if (length \u003e Math.min(file.getBlockSize(), minSize)) {\n              LOG.debug(\"File is not splittable so no parallelization \"\n                  + \"is possible: \" + file.getPath());\n            }\n          }\n          String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n          splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n        }\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n          + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\n    }\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
      "extendedDetails": {}
    },
    "6c154abd33279475315b5f7f78dc47f1b0aa7028": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6751. Add debug log message when splitting is not possible due to unsplittable compression. (Peter Vary via rchiang)\n",
      "commitDate": "16/08/16 12:13 PM",
      "commitName": "6c154abd33279475315b5f7f78dc47f1b0aa7028",
      "commitAuthor": "Ray Chiang",
      "commitDateOld": "17/06/15 11:12 PM",
      "commitNameOld": "1babe50a2cbaae3c8165229347e743d0dc94e979",
      "commitAuthorOld": "Devaraj K",
      "daysBetweenCommits": 425.54,
      "commitsBetweenForRepo": 2948,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,75 @@\n   public InputSplit[] getSplits(JobConf job, int numSplits)\n     throws IOException {\n     StopWatch sw \u003d new StopWatch().start();\n     FileStatus[] files \u003d listStatus(job);\n     \n     // Save the number of input files for metrics/loadgen\n     job.setLong(NUM_INPUT_FILES, files.length);\n     long totalSize \u003d 0;                           // compute total size\n     for (FileStatus file: files) {                // check we have valid files\n       if (file.isDirectory()) {\n         throw new IOException(\"Not a file: \"+ file.getPath());\n       }\n       totalSize +\u003d file.getLen();\n     }\n \n     long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n     long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n       FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n \n     // generate splits\n     ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n     NetworkTopology clusterMap \u003d new NetworkTopology();\n     for (FileStatus file: files) {\n       Path path \u003d file.getPath();\n       long length \u003d file.getLen();\n       if (length !\u003d 0) {\n         FileSystem fs \u003d path.getFileSystem(job);\n         BlockLocation[] blkLocations;\n         if (file instanceof LocatedFileStatus) {\n           blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n         } else {\n           blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n         }\n         if (isSplitable(fs, path)) {\n           long blockSize \u003d file.getBlockSize();\n           long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n \n           long bytesRemaining \u003d length;\n           while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n             String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                 length-bytesRemaining, splitSize, clusterMap);\n             splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                 splitHosts[0], splitHosts[1]));\n             bytesRemaining -\u003d splitSize;\n           }\n \n           if (bytesRemaining !\u003d 0) {\n             String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                 - bytesRemaining, bytesRemaining, clusterMap);\n             splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                 splitHosts[0], splitHosts[1]));\n           }\n         } else {\n+          if (LOG.isDebugEnabled()) {\n+            // Log only if the file is big enough to be splitted\n+            if (length \u003e Math.min(file.getBlockSize(), minSize)) {\n+              LOG.debug(\"File is not splittable so no parallelization \"\n+                  + \"is possible: \" + file.getPath());\n+            }\n+          }\n           String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n           splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n         }\n       } else { \n         //Create empty hosts array for zero length files\n         splits.add(makeSplit(path, 0, length, new String[0]));\n       }\n     }\n     sw.stop();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n           + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\n     }\n     return splits.toArray(new FileSplit[splits.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    StopWatch sw \u003d new StopWatch().start();\n    FileStatus[] files \u003d listStatus(job);\n    \n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, files.length);\n    long totalSize \u003d 0;                           // compute total size\n    for (FileStatus file: files) {                // check we have valid files\n      if (file.isDirectory()) {\n        throw new IOException(\"Not a file: \"+ file.getPath());\n      }\n      totalSize +\u003d file.getLen();\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      long length \u003d file.getLen();\n      if (length !\u003d 0) {\n        FileSystem fs \u003d path.getFileSystem(job);\n        BlockLocation[] blkLocations;\n        if (file instanceof LocatedFileStatus) {\n          blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n        } else {\n          blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n        }\n        if (isSplitable(fs, path)) {\n          long blockSize \u003d file.getBlockSize();\n          long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n          long bytesRemaining \u003d length;\n          while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                length-bytesRemaining, splitSize, clusterMap);\n            splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                splitHosts[0], splitHosts[1]));\n            bytesRemaining -\u003d splitSize;\n          }\n\n          if (bytesRemaining !\u003d 0) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                - bytesRemaining, bytesRemaining, clusterMap);\n            splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                splitHosts[0], splitHosts[1]));\n          }\n        } else {\n          if (LOG.isDebugEnabled()) {\n            // Log only if the file is big enough to be splitted\n            if (length \u003e Math.min(file.getBlockSize(), minSize)) {\n              LOG.debug(\"File is not splittable so no parallelization \"\n                  + \"is possible: \" + file.getPath());\n            }\n          }\n          String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n          splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n        }\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n          + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\n    }\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
      "extendedDetails": {}
    },
    "a6ed4894b518351bf1b3290e725a475570a21296": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11032. Replace use of Guava\u0027s Stopwatch with Hadoop\u0027s StopWatch. (ozawa)\n",
      "commitDate": "07/01/15 9:51 PM",
      "commitName": "a6ed4894b518351bf1b3290e725a475570a21296",
      "commitAuthor": "Tsuyoshi Ozawa",
      "commitDateOld": "07/01/15 9:35 PM",
      "commitNameOld": "988639640024933448f822b48119dc832ebd2af3",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,68 @@\n   public InputSplit[] getSplits(JobConf job, int numSplits)\n     throws IOException {\n-    Stopwatch sw \u003d new Stopwatch().start();\n+    StopWatch sw \u003d new StopWatch().start();\n     FileStatus[] files \u003d listStatus(job);\n     \n     // Save the number of input files for metrics/loadgen\n     job.setLong(NUM_INPUT_FILES, files.length);\n     long totalSize \u003d 0;                           // compute total size\n     for (FileStatus file: files) {                // check we have valid files\n       if (file.isDirectory()) {\n         throw new IOException(\"Not a file: \"+ file.getPath());\n       }\n       totalSize +\u003d file.getLen();\n     }\n \n     long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n     long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n       FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n \n     // generate splits\n     ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n     NetworkTopology clusterMap \u003d new NetworkTopology();\n     for (FileStatus file: files) {\n       Path path \u003d file.getPath();\n       long length \u003d file.getLen();\n       if (length !\u003d 0) {\n         FileSystem fs \u003d path.getFileSystem(job);\n         BlockLocation[] blkLocations;\n         if (file instanceof LocatedFileStatus) {\n           blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n         } else {\n           blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n         }\n         if (isSplitable(fs, path)) {\n           long blockSize \u003d file.getBlockSize();\n           long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n \n           long bytesRemaining \u003d length;\n           while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n             String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                 length-bytesRemaining, splitSize, clusterMap);\n             splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                 splitHosts[0], splitHosts[1]));\n             bytesRemaining -\u003d splitSize;\n           }\n \n           if (bytesRemaining !\u003d 0) {\n             String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                 - bytesRemaining, bytesRemaining, clusterMap);\n             splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                 splitHosts[0], splitHosts[1]));\n           }\n         } else {\n           String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n           splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n         }\n       } else { \n         //Create empty hosts array for zero length files\n         splits.add(makeSplit(path, 0, length, new String[0]));\n       }\n     }\n     sw.stop();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n-          + \", TimeTaken: \" + sw.elapsedMillis());\n+          + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\n     }\n     return splits.toArray(new FileSplit[splits.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    StopWatch sw \u003d new StopWatch().start();\n    FileStatus[] files \u003d listStatus(job);\n    \n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, files.length);\n    long totalSize \u003d 0;                           // compute total size\n    for (FileStatus file: files) {                // check we have valid files\n      if (file.isDirectory()) {\n        throw new IOException(\"Not a file: \"+ file.getPath());\n      }\n      totalSize +\u003d file.getLen();\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      long length \u003d file.getLen();\n      if (length !\u003d 0) {\n        FileSystem fs \u003d path.getFileSystem(job);\n        BlockLocation[] blkLocations;\n        if (file instanceof LocatedFileStatus) {\n          blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n        } else {\n          blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n        }\n        if (isSplitable(fs, path)) {\n          long blockSize \u003d file.getBlockSize();\n          long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n          long bytesRemaining \u003d length;\n          while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                length-bytesRemaining, splitSize, clusterMap);\n            splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                splitHosts[0], splitHosts[1]));\n            bytesRemaining -\u003d splitSize;\n          }\n\n          if (bytesRemaining !\u003d 0) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                - bytesRemaining, bytesRemaining, clusterMap);\n            splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                splitHosts[0], splitHosts[1]));\n          }\n        } else {\n          String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n          splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n        }\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n          + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\n    }\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
      "extendedDetails": {}
    },
    "988639640024933448f822b48119dc832ebd2af3": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"Replace use of Guava\u0027s Stopwatch with Hadoop\u0027s StopWatch. (ozawa)\" because of missing JIRA\u0027s number.\n\nThis reverts commit 2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b.\n",
      "commitDate": "07/01/15 9:35 PM",
      "commitName": "988639640024933448f822b48119dc832ebd2af3",
      "commitAuthor": "Tsuyoshi Ozawa",
      "commitDateOld": "07/01/15 9:21 PM",
      "commitNameOld": "2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,68 @@\n   public InputSplit[] getSplits(JobConf job, int numSplits)\n     throws IOException {\n-    StopWatch sw \u003d new StopWatch().start();\n+    Stopwatch sw \u003d new Stopwatch().start();\n     FileStatus[] files \u003d listStatus(job);\n     \n     // Save the number of input files for metrics/loadgen\n     job.setLong(NUM_INPUT_FILES, files.length);\n     long totalSize \u003d 0;                           // compute total size\n     for (FileStatus file: files) {                // check we have valid files\n       if (file.isDirectory()) {\n         throw new IOException(\"Not a file: \"+ file.getPath());\n       }\n       totalSize +\u003d file.getLen();\n     }\n \n     long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n     long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n       FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n \n     // generate splits\n     ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n     NetworkTopology clusterMap \u003d new NetworkTopology();\n     for (FileStatus file: files) {\n       Path path \u003d file.getPath();\n       long length \u003d file.getLen();\n       if (length !\u003d 0) {\n         FileSystem fs \u003d path.getFileSystem(job);\n         BlockLocation[] blkLocations;\n         if (file instanceof LocatedFileStatus) {\n           blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n         } else {\n           blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n         }\n         if (isSplitable(fs, path)) {\n           long blockSize \u003d file.getBlockSize();\n           long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n \n           long bytesRemaining \u003d length;\n           while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n             String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                 length-bytesRemaining, splitSize, clusterMap);\n             splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                 splitHosts[0], splitHosts[1]));\n             bytesRemaining -\u003d splitSize;\n           }\n \n           if (bytesRemaining !\u003d 0) {\n             String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                 - bytesRemaining, bytesRemaining, clusterMap);\n             splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                 splitHosts[0], splitHosts[1]));\n           }\n         } else {\n           String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n           splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n         }\n       } else { \n         //Create empty hosts array for zero length files\n         splits.add(makeSplit(path, 0, length, new String[0]));\n       }\n     }\n     sw.stop();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n-          + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\n+          + \", TimeTaken: \" + sw.elapsedMillis());\n     }\n     return splits.toArray(new FileSplit[splits.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    Stopwatch sw \u003d new Stopwatch().start();\n    FileStatus[] files \u003d listStatus(job);\n    \n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, files.length);\n    long totalSize \u003d 0;                           // compute total size\n    for (FileStatus file: files) {                // check we have valid files\n      if (file.isDirectory()) {\n        throw new IOException(\"Not a file: \"+ file.getPath());\n      }\n      totalSize +\u003d file.getLen();\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      long length \u003d file.getLen();\n      if (length !\u003d 0) {\n        FileSystem fs \u003d path.getFileSystem(job);\n        BlockLocation[] blkLocations;\n        if (file instanceof LocatedFileStatus) {\n          blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n        } else {\n          blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n        }\n        if (isSplitable(fs, path)) {\n          long blockSize \u003d file.getBlockSize();\n          long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n          long bytesRemaining \u003d length;\n          while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                length-bytesRemaining, splitSize, clusterMap);\n            splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                splitHosts[0], splitHosts[1]));\n            bytesRemaining -\u003d splitSize;\n          }\n\n          if (bytesRemaining !\u003d 0) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                - bytesRemaining, bytesRemaining, clusterMap);\n            splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                splitHosts[0], splitHosts[1]));\n          }\n        } else {\n          String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n          splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n        }\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n          + \", TimeTaken: \" + sw.elapsedMillis());\n    }\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
      "extendedDetails": {}
    },
    "2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b": {
      "type": "Ybodychange",
      "commitMessage": "Replace use of Guava\u0027s Stopwatch with Hadoop\u0027s StopWatch. (ozawa)\n",
      "commitDate": "07/01/15 9:21 PM",
      "commitName": "2eba7eb9aff5f7a1bf63ff1ebbe28d21fd37065b",
      "commitAuthor": "Tsuyoshi Ozawa",
      "commitDateOld": "18/06/14 4:28 PM",
      "commitNameOld": "bd23a2ff22dba8a5203e8e498244f985e728da51",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 203.24,
      "commitsBetweenForRepo": 1722,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,68 @@\n   public InputSplit[] getSplits(JobConf job, int numSplits)\n     throws IOException {\n-    Stopwatch sw \u003d new Stopwatch().start();\n+    StopWatch sw \u003d new StopWatch().start();\n     FileStatus[] files \u003d listStatus(job);\n     \n     // Save the number of input files for metrics/loadgen\n     job.setLong(NUM_INPUT_FILES, files.length);\n     long totalSize \u003d 0;                           // compute total size\n     for (FileStatus file: files) {                // check we have valid files\n       if (file.isDirectory()) {\n         throw new IOException(\"Not a file: \"+ file.getPath());\n       }\n       totalSize +\u003d file.getLen();\n     }\n \n     long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n     long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n       FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n \n     // generate splits\n     ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n     NetworkTopology clusterMap \u003d new NetworkTopology();\n     for (FileStatus file: files) {\n       Path path \u003d file.getPath();\n       long length \u003d file.getLen();\n       if (length !\u003d 0) {\n         FileSystem fs \u003d path.getFileSystem(job);\n         BlockLocation[] blkLocations;\n         if (file instanceof LocatedFileStatus) {\n           blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n         } else {\n           blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n         }\n         if (isSplitable(fs, path)) {\n           long blockSize \u003d file.getBlockSize();\n           long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n \n           long bytesRemaining \u003d length;\n           while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n             String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                 length-bytesRemaining, splitSize, clusterMap);\n             splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                 splitHosts[0], splitHosts[1]));\n             bytesRemaining -\u003d splitSize;\n           }\n \n           if (bytesRemaining !\u003d 0) {\n             String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                 - bytesRemaining, bytesRemaining, clusterMap);\n             splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                 splitHosts[0], splitHosts[1]));\n           }\n         } else {\n           String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n           splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n         }\n       } else { \n         //Create empty hosts array for zero length files\n         splits.add(makeSplit(path, 0, length, new String[0]));\n       }\n     }\n     sw.stop();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n-          + \", TimeTaken: \" + sw.elapsedMillis());\n+          + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\n     }\n     return splits.toArray(new FileSplit[splits.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    StopWatch sw \u003d new StopWatch().start();\n    FileStatus[] files \u003d listStatus(job);\n    \n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, files.length);\n    long totalSize \u003d 0;                           // compute total size\n    for (FileStatus file: files) {                // check we have valid files\n      if (file.isDirectory()) {\n        throw new IOException(\"Not a file: \"+ file.getPath());\n      }\n      totalSize +\u003d file.getLen();\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      long length \u003d file.getLen();\n      if (length !\u003d 0) {\n        FileSystem fs \u003d path.getFileSystem(job);\n        BlockLocation[] blkLocations;\n        if (file instanceof LocatedFileStatus) {\n          blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n        } else {\n          blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n        }\n        if (isSplitable(fs, path)) {\n          long blockSize \u003d file.getBlockSize();\n          long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n          long bytesRemaining \u003d length;\n          while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                length-bytesRemaining, splitSize, clusterMap);\n            splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                splitHosts[0], splitHosts[1]));\n            bytesRemaining -\u003d splitSize;\n          }\n\n          if (bytesRemaining !\u003d 0) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                - bytesRemaining, bytesRemaining, clusterMap);\n            splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                splitHosts[0], splitHosts[1]));\n          }\n        } else {\n          String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n          splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n        }\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n          + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\n    }\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
      "extendedDetails": {}
    },
    "bd23a2ff22dba8a5203e8e498244f985e728da51": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5896. InputSplits should indicate which locations have the block cached in memory. (Sandy Ryza via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603670 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/06/14 4:28 PM",
      "commitName": "bd23a2ff22dba8a5203e8e498244f985e728da51",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "19/03/14 7:46 PM",
      "commitNameOld": "396c6c63a26b098fd0221e830f79be13b7e97432",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 90.86,
      "commitsBetweenForRepo": 549,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,68 @@\n   public InputSplit[] getSplits(JobConf job, int numSplits)\n     throws IOException {\n     Stopwatch sw \u003d new Stopwatch().start();\n     FileStatus[] files \u003d listStatus(job);\n     \n     // Save the number of input files for metrics/loadgen\n     job.setLong(NUM_INPUT_FILES, files.length);\n     long totalSize \u003d 0;                           // compute total size\n     for (FileStatus file: files) {                // check we have valid files\n       if (file.isDirectory()) {\n         throw new IOException(\"Not a file: \"+ file.getPath());\n       }\n       totalSize +\u003d file.getLen();\n     }\n \n     long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n     long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n       FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n \n     // generate splits\n     ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n     NetworkTopology clusterMap \u003d new NetworkTopology();\n     for (FileStatus file: files) {\n       Path path \u003d file.getPath();\n       long length \u003d file.getLen();\n       if (length !\u003d 0) {\n         FileSystem fs \u003d path.getFileSystem(job);\n         BlockLocation[] blkLocations;\n         if (file instanceof LocatedFileStatus) {\n           blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n         } else {\n           blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n         }\n         if (isSplitable(fs, path)) {\n           long blockSize \u003d file.getBlockSize();\n           long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n \n           long bytesRemaining \u003d length;\n           while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n-            String[] splitHosts \u003d getSplitHosts(blkLocations,\n+            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                 length-bytesRemaining, splitSize, clusterMap);\n             splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n-                splitHosts));\n+                splitHosts[0], splitHosts[1]));\n             bytesRemaining -\u003d splitSize;\n           }\n \n           if (bytesRemaining !\u003d 0) {\n-            String[] splitHosts \u003d getSplitHosts(blkLocations, length\n+            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                 - bytesRemaining, bytesRemaining, clusterMap);\n             splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n-                splitHosts));\n+                splitHosts[0], splitHosts[1]));\n           }\n         } else {\n-          String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n-          splits.add(makeSplit(path, 0, length, splitHosts));\n+          String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n+          splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n         }\n       } else { \n         //Create empty hosts array for zero length files\n         splits.add(makeSplit(path, 0, length, new String[0]));\n       }\n     }\n     sw.stop();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n           + \", TimeTaken: \" + sw.elapsedMillis());\n     }\n     return splits.toArray(new FileSplit[splits.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    Stopwatch sw \u003d new Stopwatch().start();\n    FileStatus[] files \u003d listStatus(job);\n    \n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, files.length);\n    long totalSize \u003d 0;                           // compute total size\n    for (FileStatus file: files) {                // check we have valid files\n      if (file.isDirectory()) {\n        throw new IOException(\"Not a file: \"+ file.getPath());\n      }\n      totalSize +\u003d file.getLen();\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      long length \u003d file.getLen();\n      if (length !\u003d 0) {\n        FileSystem fs \u003d path.getFileSystem(job);\n        BlockLocation[] blkLocations;\n        if (file instanceof LocatedFileStatus) {\n          blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n        } else {\n          blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n        }\n        if (isSplitable(fs, path)) {\n          long blockSize \u003d file.getBlockSize();\n          long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n          long bytesRemaining \u003d length;\n          while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,\n                length-bytesRemaining, splitSize, clusterMap);\n            splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                splitHosts[0], splitHosts[1]));\n            bytesRemaining -\u003d splitSize;\n          }\n\n          if (bytesRemaining !\u003d 0) {\n            String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations, length\n                - bytesRemaining, bytesRemaining, clusterMap);\n            splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                splitHosts[0], splitHosts[1]));\n          }\n        } else {\n          String[][] splitHosts \u003d getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);\n          splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\n        }\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n          + \", TimeTaken: \" + sw.elapsedMillis());\n    }\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
      "extendedDetails": {}
    },
    "396c6c63a26b098fd0221e830f79be13b7e97432": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2349. Modified FileInputFormat to be able to issue file and block location calls in parallel. Contributed by Siddharth Seth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1579515 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/03/14 7:46 PM",
      "commitName": "396c6c63a26b098fd0221e830f79be13b7e97432",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "26/07/13 11:16 AM",
      "commitNameOld": "ec18984252731089ab5af12b3603dcfc3d4f4593",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 236.35,
      "commitsBetweenForRepo": 1629,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,68 @@\n   public InputSplit[] getSplits(JobConf job, int numSplits)\n     throws IOException {\n+    Stopwatch sw \u003d new Stopwatch().start();\n     FileStatus[] files \u003d listStatus(job);\n     \n     // Save the number of input files for metrics/loadgen\n     job.setLong(NUM_INPUT_FILES, files.length);\n     long totalSize \u003d 0;                           // compute total size\n     for (FileStatus file: files) {                // check we have valid files\n       if (file.isDirectory()) {\n         throw new IOException(\"Not a file: \"+ file.getPath());\n       }\n       totalSize +\u003d file.getLen();\n     }\n \n     long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n     long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n       FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n \n     // generate splits\n     ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n     NetworkTopology clusterMap \u003d new NetworkTopology();\n     for (FileStatus file: files) {\n       Path path \u003d file.getPath();\n       long length \u003d file.getLen();\n       if (length !\u003d 0) {\n         FileSystem fs \u003d path.getFileSystem(job);\n         BlockLocation[] blkLocations;\n         if (file instanceof LocatedFileStatus) {\n           blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n         } else {\n           blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n         }\n         if (isSplitable(fs, path)) {\n           long blockSize \u003d file.getBlockSize();\n           long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n \n           long bytesRemaining \u003d length;\n           while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n             String[] splitHosts \u003d getSplitHosts(blkLocations,\n                 length-bytesRemaining, splitSize, clusterMap);\n             splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                 splitHosts));\n             bytesRemaining -\u003d splitSize;\n           }\n \n           if (bytesRemaining !\u003d 0) {\n             String[] splitHosts \u003d getSplitHosts(blkLocations, length\n                 - bytesRemaining, bytesRemaining, clusterMap);\n             splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                 splitHosts));\n           }\n         } else {\n           String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n           splits.add(makeSplit(path, 0, length, splitHosts));\n         }\n       } else { \n         //Create empty hosts array for zero length files\n         splits.add(makeSplit(path, 0, length, new String[0]));\n       }\n     }\n-    LOG.debug(\"Total # of splits: \" + splits.size());\n+    sw.stop();\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n+          + \", TimeTaken: \" + sw.elapsedMillis());\n+    }\n     return splits.toArray(new FileSplit[splits.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    Stopwatch sw \u003d new Stopwatch().start();\n    FileStatus[] files \u003d listStatus(job);\n    \n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, files.length);\n    long totalSize \u003d 0;                           // compute total size\n    for (FileStatus file: files) {                // check we have valid files\n      if (file.isDirectory()) {\n        throw new IOException(\"Not a file: \"+ file.getPath());\n      }\n      totalSize +\u003d file.getLen();\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      long length \u003d file.getLen();\n      if (length !\u003d 0) {\n        FileSystem fs \u003d path.getFileSystem(job);\n        BlockLocation[] blkLocations;\n        if (file instanceof LocatedFileStatus) {\n          blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n        } else {\n          blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n        }\n        if (isSplitable(fs, path)) {\n          long blockSize \u003d file.getBlockSize();\n          long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n          long bytesRemaining \u003d length;\n          while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n            String[] splitHosts \u003d getSplitHosts(blkLocations,\n                length-bytesRemaining, splitSize, clusterMap);\n            splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                splitHosts));\n            bytesRemaining -\u003d splitSize;\n          }\n\n          if (bytesRemaining !\u003d 0) {\n            String[] splitHosts \u003d getSplitHosts(blkLocations, length\n                - bytesRemaining, bytesRemaining, clusterMap);\n            splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                splitHosts));\n          }\n        } else {\n          String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n          splits.add(makeSplit(path, 0, length, splitHosts));\n        }\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n          + \", TimeTaken: \" + sw.elapsedMillis());\n    }\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
      "extendedDetails": {}
    },
    "ec18984252731089ab5af12b3603dcfc3d4f4593": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-1981. Improve getSplits performance by using listLocatedStatus. Contributed by Hairong Kuang and Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1507385 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/13 11:16 AM",
      "commitName": "ec18984252731089ab5af12b3603dcfc3d4f4593",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "02/07/13 2:47 PM",
      "commitNameOld": "e846c98397518838bba22f24df0f0d9ef4f3e1ad",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 23.85,
      "commitsBetweenForRepo": 137,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,63 @@\n   public InputSplit[] getSplits(JobConf job, int numSplits)\n     throws IOException {\n     FileStatus[] files \u003d listStatus(job);\n     \n     // Save the number of input files for metrics/loadgen\n     job.setLong(NUM_INPUT_FILES, files.length);\n     long totalSize \u003d 0;                           // compute total size\n     for (FileStatus file: files) {                // check we have valid files\n       if (file.isDirectory()) {\n         throw new IOException(\"Not a file: \"+ file.getPath());\n       }\n       totalSize +\u003d file.getLen();\n     }\n \n     long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n     long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n       FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n \n     // generate splits\n     ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n     NetworkTopology clusterMap \u003d new NetworkTopology();\n     for (FileStatus file: files) {\n       Path path \u003d file.getPath();\n-      FileSystem fs \u003d path.getFileSystem(job);\n       long length \u003d file.getLen();\n-      BlockLocation[] blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n-      if ((length !\u003d 0) \u0026\u0026 isSplitable(fs, path)) { \n-        long blockSize \u003d file.getBlockSize();\n-        long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n+      if (length !\u003d 0) {\n+        FileSystem fs \u003d path.getFileSystem(job);\n+        BlockLocation[] blkLocations;\n+        if (file instanceof LocatedFileStatus) {\n+          blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n+        } else {\n+          blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n+        }\n+        if (isSplitable(fs, path)) {\n+          long blockSize \u003d file.getBlockSize();\n+          long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n \n-        long bytesRemaining \u003d length;\n-        while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n-          String[] splitHosts \u003d getSplitHosts(blkLocations, \n-              length-bytesRemaining, splitSize, clusterMap);\n-          splits.add(makeSplit(path, length-bytesRemaining, splitSize, \n-                               splitHosts));\n-          bytesRemaining -\u003d splitSize;\n+          long bytesRemaining \u003d length;\n+          while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n+            String[] splitHosts \u003d getSplitHosts(blkLocations,\n+                length-bytesRemaining, splitSize, clusterMap);\n+            splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n+                splitHosts));\n+            bytesRemaining -\u003d splitSize;\n+          }\n+\n+          if (bytesRemaining !\u003d 0) {\n+            String[] splitHosts \u003d getSplitHosts(blkLocations, length\n+                - bytesRemaining, bytesRemaining, clusterMap);\n+            splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n+                splitHosts));\n+          }\n+        } else {\n+          String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n+          splits.add(makeSplit(path, 0, length, splitHosts));\n         }\n-        \n-        if (bytesRemaining !\u003d 0) {\n-          String[] splitHosts \u003d getSplitHosts(blkLocations, length\n-              - bytesRemaining, bytesRemaining, clusterMap);\n-          splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n-              splitHosts));\n-        }\n-      } else if (length !\u003d 0) {\n-        String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n-        splits.add(makeSplit(path, 0, length, splitHosts));\n       } else { \n         //Create empty hosts array for zero length files\n         splits.add(makeSplit(path, 0, length, new String[0]));\n       }\n     }\n     LOG.debug(\"Total # of splits: \" + splits.size());\n     return splits.toArray(new FileSplit[splits.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    FileStatus[] files \u003d listStatus(job);\n    \n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, files.length);\n    long totalSize \u003d 0;                           // compute total size\n    for (FileStatus file: files) {                // check we have valid files\n      if (file.isDirectory()) {\n        throw new IOException(\"Not a file: \"+ file.getPath());\n      }\n      totalSize +\u003d file.getLen();\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      long length \u003d file.getLen();\n      if (length !\u003d 0) {\n        FileSystem fs \u003d path.getFileSystem(job);\n        BlockLocation[] blkLocations;\n        if (file instanceof LocatedFileStatus) {\n          blkLocations \u003d ((LocatedFileStatus) file).getBlockLocations();\n        } else {\n          blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n        }\n        if (isSplitable(fs, path)) {\n          long blockSize \u003d file.getBlockSize();\n          long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n          long bytesRemaining \u003d length;\n          while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n            String[] splitHosts \u003d getSplitHosts(blkLocations,\n                length-bytesRemaining, splitSize, clusterMap);\n            splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                splitHosts));\n            bytesRemaining -\u003d splitSize;\n          }\n\n          if (bytesRemaining !\u003d 0) {\n            String[] splitHosts \u003d getSplitHosts(blkLocations, length\n                - bytesRemaining, bytesRemaining, clusterMap);\n            splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n                splitHosts));\n          }\n        } else {\n          String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n          splits.add(makeSplit(path, 0, length, splitHosts));\n        }\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    LOG.debug(\"Total # of splits: \" + splits.size());\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
      "extendedDetails": {}
    },
    "dc615c312b81d2bff17821b59fba1b76aa24f585": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3710. Improved FileInputFormat to return better locality for the last split. Contributed by Siddarth Seth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1235510 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/01/12 1:29 PM",
      "commitName": "dc615c312b81d2bff17821b59fba1b76aa24f585",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 152.89,
      "commitsBetweenForRepo": 934,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,56 @@\n   public InputSplit[] getSplits(JobConf job, int numSplits)\n     throws IOException {\n     FileStatus[] files \u003d listStatus(job);\n     \n     // Save the number of input files for metrics/loadgen\n     job.setLong(NUM_INPUT_FILES, files.length);\n     long totalSize \u003d 0;                           // compute total size\n     for (FileStatus file: files) {                // check we have valid files\n       if (file.isDirectory()) {\n         throw new IOException(\"Not a file: \"+ file.getPath());\n       }\n       totalSize +\u003d file.getLen();\n     }\n \n     long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n     long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n       FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n \n     // generate splits\n     ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n     NetworkTopology clusterMap \u003d new NetworkTopology();\n     for (FileStatus file: files) {\n       Path path \u003d file.getPath();\n       FileSystem fs \u003d path.getFileSystem(job);\n       long length \u003d file.getLen();\n       BlockLocation[] blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n       if ((length !\u003d 0) \u0026\u0026 isSplitable(fs, path)) { \n         long blockSize \u003d file.getBlockSize();\n         long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n \n         long bytesRemaining \u003d length;\n         while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n           String[] splitHosts \u003d getSplitHosts(blkLocations, \n               length-bytesRemaining, splitSize, clusterMap);\n           splits.add(makeSplit(path, length-bytesRemaining, splitSize, \n                                splitHosts));\n           bytesRemaining -\u003d splitSize;\n         }\n         \n         if (bytesRemaining !\u003d 0) {\n-          splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining, \n-                     blkLocations[blkLocations.length-1].getHosts()));\n+          String[] splitHosts \u003d getSplitHosts(blkLocations, length\n+              - bytesRemaining, bytesRemaining, clusterMap);\n+          splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n+              splitHosts));\n         }\n       } else if (length !\u003d 0) {\n         String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n         splits.add(makeSplit(path, 0, length, splitHosts));\n       } else { \n         //Create empty hosts array for zero length files\n         splits.add(makeSplit(path, 0, length, new String[0]));\n       }\n     }\n     LOG.debug(\"Total # of splits: \" + splits.size());\n     return splits.toArray(new FileSplit[splits.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    FileStatus[] files \u003d listStatus(job);\n    \n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, files.length);\n    long totalSize \u003d 0;                           // compute total size\n    for (FileStatus file: files) {                // check we have valid files\n      if (file.isDirectory()) {\n        throw new IOException(\"Not a file: \"+ file.getPath());\n      }\n      totalSize +\u003d file.getLen();\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      FileSystem fs \u003d path.getFileSystem(job);\n      long length \u003d file.getLen();\n      BlockLocation[] blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n      if ((length !\u003d 0) \u0026\u0026 isSplitable(fs, path)) { \n        long blockSize \u003d file.getBlockSize();\n        long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n        long bytesRemaining \u003d length;\n        while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n          String[] splitHosts \u003d getSplitHosts(blkLocations, \n              length-bytesRemaining, splitSize, clusterMap);\n          splits.add(makeSplit(path, length-bytesRemaining, splitSize, \n                               splitHosts));\n          bytesRemaining -\u003d splitSize;\n        }\n        \n        if (bytesRemaining !\u003d 0) {\n          String[] splitHosts \u003d getSplitHosts(blkLocations, length\n              - bytesRemaining, bytesRemaining, clusterMap);\n          splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,\n              splitHosts));\n        }\n      } else if (length !\u003d 0) {\n        String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n        splits.add(makeSplit(path, 0, length, splitHosts));\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    LOG.debug(\"Total # of splits: \" + splits.size());\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    FileStatus[] files \u003d listStatus(job);\n    \n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, files.length);\n    long totalSize \u003d 0;                           // compute total size\n    for (FileStatus file: files) {                // check we have valid files\n      if (file.isDirectory()) {\n        throw new IOException(\"Not a file: \"+ file.getPath());\n      }\n      totalSize +\u003d file.getLen();\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      FileSystem fs \u003d path.getFileSystem(job);\n      long length \u003d file.getLen();\n      BlockLocation[] blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n      if ((length !\u003d 0) \u0026\u0026 isSplitable(fs, path)) { \n        long blockSize \u003d file.getBlockSize();\n        long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n        long bytesRemaining \u003d length;\n        while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n          String[] splitHosts \u003d getSplitHosts(blkLocations, \n              length-bytesRemaining, splitSize, clusterMap);\n          splits.add(makeSplit(path, length-bytesRemaining, splitSize, \n                               splitHosts));\n          bytesRemaining -\u003d splitSize;\n        }\n        \n        if (bytesRemaining !\u003d 0) {\n          splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining, \n                     blkLocations[blkLocations.length-1].getHosts()));\n        }\n      } else if (length !\u003d 0) {\n        String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n        splits.add(makeSplit(path, 0, length, splitHosts));\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    LOG.debug(\"Total # of splits: \" + splits.size());\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    FileStatus[] files \u003d listStatus(job);\n    \n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, files.length);\n    long totalSize \u003d 0;                           // compute total size\n    for (FileStatus file: files) {                // check we have valid files\n      if (file.isDirectory()) {\n        throw new IOException(\"Not a file: \"+ file.getPath());\n      }\n      totalSize +\u003d file.getLen();\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      FileSystem fs \u003d path.getFileSystem(job);\n      long length \u003d file.getLen();\n      BlockLocation[] blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n      if ((length !\u003d 0) \u0026\u0026 isSplitable(fs, path)) { \n        long blockSize \u003d file.getBlockSize();\n        long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n        long bytesRemaining \u003d length;\n        while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n          String[] splitHosts \u003d getSplitHosts(blkLocations, \n              length-bytesRemaining, splitSize, clusterMap);\n          splits.add(makeSplit(path, length-bytesRemaining, splitSize, \n                               splitHosts));\n          bytesRemaining -\u003d splitSize;\n        }\n        \n        if (bytesRemaining !\u003d 0) {\n          splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining, \n                     blkLocations[blkLocations.length-1].getHosts()));\n        }\n      } else if (length !\u003d 0) {\n        String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n        splits.add(makeSplit(path, 0, length, splitHosts));\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    LOG.debug(\"Total # of splits: \" + splits.size());\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/FileInputFormat.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,54 @@\n+  public InputSplit[] getSplits(JobConf job, int numSplits)\n+    throws IOException {\n+    FileStatus[] files \u003d listStatus(job);\n+    \n+    // Save the number of input files for metrics/loadgen\n+    job.setLong(NUM_INPUT_FILES, files.length);\n+    long totalSize \u003d 0;                           // compute total size\n+    for (FileStatus file: files) {                // check we have valid files\n+      if (file.isDirectory()) {\n+        throw new IOException(\"Not a file: \"+ file.getPath());\n+      }\n+      totalSize +\u003d file.getLen();\n+    }\n+\n+    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n+    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n+      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n+\n+    // generate splits\n+    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n+    NetworkTopology clusterMap \u003d new NetworkTopology();\n+    for (FileStatus file: files) {\n+      Path path \u003d file.getPath();\n+      FileSystem fs \u003d path.getFileSystem(job);\n+      long length \u003d file.getLen();\n+      BlockLocation[] blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n+      if ((length !\u003d 0) \u0026\u0026 isSplitable(fs, path)) { \n+        long blockSize \u003d file.getBlockSize();\n+        long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n+\n+        long bytesRemaining \u003d length;\n+        while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n+          String[] splitHosts \u003d getSplitHosts(blkLocations, \n+              length-bytesRemaining, splitSize, clusterMap);\n+          splits.add(makeSplit(path, length-bytesRemaining, splitSize, \n+                               splitHosts));\n+          bytesRemaining -\u003d splitSize;\n+        }\n+        \n+        if (bytesRemaining !\u003d 0) {\n+          splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining, \n+                     blkLocations[blkLocations.length-1].getHosts()));\n+        }\n+      } else if (length !\u003d 0) {\n+        String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n+        splits.add(makeSplit(path, 0, length, splitHosts));\n+      } else { \n+        //Create empty hosts array for zero length files\n+        splits.add(makeSplit(path, 0, length, new String[0]));\n+      }\n+    }\n+    LOG.debug(\"Total # of splits: \" + splits.size());\n+    return splits.toArray(new FileSplit[splits.size()]);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public InputSplit[] getSplits(JobConf job, int numSplits)\n    throws IOException {\n    FileStatus[] files \u003d listStatus(job);\n    \n    // Save the number of input files for metrics/loadgen\n    job.setLong(NUM_INPUT_FILES, files.length);\n    long totalSize \u003d 0;                           // compute total size\n    for (FileStatus file: files) {                // check we have valid files\n      if (file.isDirectory()) {\n        throw new IOException(\"Not a file: \"+ file.getPath());\n      }\n      totalSize +\u003d file.getLen();\n    }\n\n    long goalSize \u003d totalSize / (numSplits \u003d\u003d 0 ? 1 : numSplits);\n    long minSize \u003d Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.\n      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\n\n    // generate splits\n    ArrayList\u003cFileSplit\u003e splits \u003d new ArrayList\u003cFileSplit\u003e(numSplits);\n    NetworkTopology clusterMap \u003d new NetworkTopology();\n    for (FileStatus file: files) {\n      Path path \u003d file.getPath();\n      FileSystem fs \u003d path.getFileSystem(job);\n      long length \u003d file.getLen();\n      BlockLocation[] blkLocations \u003d fs.getFileBlockLocations(file, 0, length);\n      if ((length !\u003d 0) \u0026\u0026 isSplitable(fs, path)) { \n        long blockSize \u003d file.getBlockSize();\n        long splitSize \u003d computeSplitSize(goalSize, minSize, blockSize);\n\n        long bytesRemaining \u003d length;\n        while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n          String[] splitHosts \u003d getSplitHosts(blkLocations, \n              length-bytesRemaining, splitSize, clusterMap);\n          splits.add(makeSplit(path, length-bytesRemaining, splitSize, \n                               splitHosts));\n          bytesRemaining -\u003d splitSize;\n        }\n        \n        if (bytesRemaining !\u003d 0) {\n          splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining, \n                     blkLocations[blkLocations.length-1].getHosts()));\n        }\n      } else if (length !\u003d 0) {\n        String[] splitHosts \u003d getSplitHosts(blkLocations,0,length,clusterMap);\n        splits.add(makeSplit(path, 0, length, splitHosts));\n      } else { \n        //Create empty hosts array for zero length files\n        splits.add(makeSplit(path, 0, length, new String[0]));\n      }\n    }\n    LOG.debug(\"Total # of splits: \" + splits.size());\n    return splits.toArray(new FileSplit[splits.size()]);\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/FileInputFormat.java"
    }
  }
}