{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirStatAndListingOp.java",
  "functionName": "getSnapshotsListing",
  "functionId": "getSnapshotsListing___fsd-FSDirectory__iip-INodesInPath__startAfter-byte[]",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
  "functionStartLine": 285,
  "functionEndLine": 314,
  "numCommitsSeen": 306,
  "timeTaken": 11961,
  "changeHistory": [
    "693169ef34f856a27dc09d90a45fb4ec5b66ed2c",
    "a0730aa5ced7666a8c92f9fb830b615f5f9f477a",
    "ec252ce0fc0998ce13f31af3440c08a236328e5a",
    "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
    "22fc46d7659972ff016ccf1c6f781f0c160be26f",
    "9f4bf3bdf9e74800643477cfb18361e01cf6859c",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "c55d609053fe24b3a50fbe17dc1b47717b453ed6",
    "5c97db07fb306842f49d73a67a90cecec19a7833",
    "53a28afe293e5bf185c8d4f2c7aea212e66015c2",
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4",
    "1737950d0fc83c68f386881b843c41b0b1e342de",
    "9b250d74f029f8ccf3a613f9fb74f59838a66ec1",
    "407bb3d3e452c8277c498dd14e0cc5b7762a7091",
    "76a621ffd2d66bf012a554f4400091a92a5b473e",
    "70cff9e2f0c8f78c1dc54a064182971bb2106795"
  ],
  "changeHistoryShort": {
    "693169ef34f856a27dc09d90a45fb4ec5b66ed2c": "Ybodychange",
    "a0730aa5ced7666a8c92f9fb830b615f5f9f477a": "Ymultichange(Yparameterchange,Ybodychange)",
    "ec252ce0fc0998ce13f31af3440c08a236328e5a": "Ybodychange",
    "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88": "Ybodychange",
    "22fc46d7659972ff016ccf1c6f781f0c160be26f": "Ybodychange",
    "9f4bf3bdf9e74800643477cfb18361e01cf6859c": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "c55d609053fe24b3a50fbe17dc1b47717b453ed6": "Ybodychange",
    "5c97db07fb306842f49d73a67a90cecec19a7833": "Ybodychange",
    "53a28afe293e5bf185c8d4f2c7aea212e66015c2": "Ybodychange",
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2": "Ybodychange",
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1": "Ymultichange(Ymovefromfile,Ymodifierchange,Yexceptionschange,Ybodychange,Yparameterchange)",
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4": "Ybodychange",
    "1737950d0fc83c68f386881b843c41b0b1e342de": "Ybodychange",
    "9b250d74f029f8ccf3a613f9fb74f59838a66ec1": "Ybodychange",
    "407bb3d3e452c8277c498dd14e0cc5b7762a7091": "Ybodychange",
    "76a621ffd2d66bf012a554f4400091a92a5b473e": "Ybodychange",
    "70cff9e2f0c8f78c1dc54a064182971bb2106795": "Ybodychange"
  },
  "changeHistoryDetails": {
    "693169ef34f856a27dc09d90a45fb4ec5b66ed2c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12882. Support full open(PathHandle) contract in HDFS\n",
      "commitDate": "11/12/17 8:14 PM",
      "commitName": "693169ef34f856a27dc09d90a45fb4ec5b66ed2c",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "29/11/17 8:28 PM",
      "commitNameOld": "0e560f3b8d194c10dce06443979df4074e14b0db",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 11.99,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n   private static DirectoryListing getSnapshotsListing(\n       FSDirectory fsd, INodesInPath iip, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(iip.isDotSnapshotDir(),\n         \"%s does not end with %s\",\n         iip.getPath(), HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n     // drop off the null .snapshot component\n     iip \u003d iip.getParentINodesInPath();\n     final String dirPath \u003d iip.getPath();\n     final INode node \u003d iip.getLastINode();\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       listing[i] \u003d createFileStatus(fsd, iip, sRoot,\n-          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, false);\n+          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, false, false);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, INodesInPath iip, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(iip.isDotSnapshotDir(),\n        \"%s does not end with %s\",\n        iip.getPath(), HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n    // drop off the null .snapshot component\n    iip \u003d iip.getParentINodesInPath();\n    final String dirPath \u003d iip.getPath();\n    final INode node \u003d iip.getLastINode();\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(fsd, iip, sRoot,\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, false, false);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "a0730aa5ced7666a8c92f9fb830b615f5f9f477a": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10851. FSDirStatAndListingOp: stop passing path as string. Contributed by Daryn Sharp.\n",
      "commitDate": "30/09/16 11:03 AM",
      "commitName": "a0730aa5ced7666a8c92f9fb830b615f5f9f477a",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10851. FSDirStatAndListingOp: stop passing path as string. Contributed by Daryn Sharp.\n",
          "commitDate": "30/09/16 11:03 AM",
          "commitName": "a0730aa5ced7666a8c92f9fb830b615f5f9f477a",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "24/08/16 1:21 PM",
          "commitNameOld": "a1f3293762dddb0ca953d1145f5b53d9086b25b8",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 36.9,
          "commitsBetweenForRepo": 208,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,30 @@\n   private static DirectoryListing getSnapshotsListing(\n-      FSDirectory fsd, String src, byte[] startAfter)\n+      FSDirectory fsd, INodesInPath iip, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n-    Preconditions.checkArgument(\n-        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n-        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n-\n-    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n-        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n-\n-    final INode node \u003d fsd.getINode(dirPath);\n+    Preconditions.checkArgument(iip.isDotSnapshotDir(),\n+        \"%s does not end with %s\",\n+        iip.getPath(), HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n+    // drop off the null .snapshot component\n+    iip \u003d iip.getParentINodesInPath();\n+    final String dirPath \u003d iip.getPath();\n+    final INode node \u003d iip.getLastINode();\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n-      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n-          fsd, src, sRoot.getLocalNameBytes(),\n-          node, Snapshot.CURRENT_STATE_ID);\n-      listing[i] \u003d createFileStatus(\n-          fsd, sRoot.getLocalNameBytes(), nodeAttrs,\n-          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n-          INodesInPath.fromINode(sRoot));\n+      listing[i] \u003d createFileStatus(fsd, iip, sRoot,\n+          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, false);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, INodesInPath iip, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(iip.isDotSnapshotDir(),\n        \"%s does not end with %s\",\n        iip.getPath(), HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n    // drop off the null .snapshot component\n    iip \u003d iip.getParentINodesInPath();\n    final String dirPath \u003d iip.getPath();\n    final INode node \u003d iip.getLastINode();\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(fsd, iip, sRoot,\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, false);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, src-String, startAfter-byte[]]",
            "newValue": "[fsd-FSDirectory, iip-INodesInPath, startAfter-byte[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10851. FSDirStatAndListingOp: stop passing path as string. Contributed by Daryn Sharp.\n",
          "commitDate": "30/09/16 11:03 AM",
          "commitName": "a0730aa5ced7666a8c92f9fb830b615f5f9f477a",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "24/08/16 1:21 PM",
          "commitNameOld": "a1f3293762dddb0ca953d1145f5b53d9086b25b8",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 36.9,
          "commitsBetweenForRepo": 208,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,30 @@\n   private static DirectoryListing getSnapshotsListing(\n-      FSDirectory fsd, String src, byte[] startAfter)\n+      FSDirectory fsd, INodesInPath iip, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n-    Preconditions.checkArgument(\n-        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n-        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n-\n-    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n-        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n-\n-    final INode node \u003d fsd.getINode(dirPath);\n+    Preconditions.checkArgument(iip.isDotSnapshotDir(),\n+        \"%s does not end with %s\",\n+        iip.getPath(), HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n+    // drop off the null .snapshot component\n+    iip \u003d iip.getParentINodesInPath();\n+    final String dirPath \u003d iip.getPath();\n+    final INode node \u003d iip.getLastINode();\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n-      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n-          fsd, src, sRoot.getLocalNameBytes(),\n-          node, Snapshot.CURRENT_STATE_ID);\n-      listing[i] \u003d createFileStatus(\n-          fsd, sRoot.getLocalNameBytes(), nodeAttrs,\n-          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n-          INodesInPath.fromINode(sRoot));\n+      listing[i] \u003d createFileStatus(fsd, iip, sRoot,\n+          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, false);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, INodesInPath iip, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(iip.isDotSnapshotDir(),\n        \"%s does not end with %s\",\n        iip.getPath(), HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n    // drop off the null .snapshot component\n    iip \u003d iip.getParentINodesInPath();\n    final String dirPath \u003d iip.getPath();\n    final INode node \u003d iip.getLastINode();\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(fsd, iip, sRoot,\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, false);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "ec252ce0fc0998ce13f31af3440c08a236328e5a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10762. Pass IIP for file status related methods\n",
      "commitDate": "24/08/16 6:46 AM",
      "commitName": "ec252ce0fc0998ce13f31af3440c08a236328e5a",
      "commitAuthor": "Daryn Sharp",
      "commitDateOld": "22/08/16 2:57 PM",
      "commitNameOld": "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 1.66,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,36 @@\n   private static DirectoryListing getSnapshotsListing(\n       FSDirectory fsd, String src, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n \n     final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       INodeAttributes nodeAttrs \u003d getINodeAttributes(\n           fsd, src, sRoot.getLocalNameBytes(),\n           node, Snapshot.CURRENT_STATE_ID);\n       listing[i] \u003d createFileStatus(\n           fsd, sRoot.getLocalNameBytes(), nodeAttrs,\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n-          Snapshot.CURRENT_STATE_ID, false,\n           INodesInPath.fromINode(sRoot));\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n          fsd, src, sRoot.getLocalNameBytes(),\n          node, Snapshot.CURRENT_STATE_ID);\n      listing[i] \u003d createFileStatus(\n          fsd, sRoot.getLocalNameBytes(), nodeAttrs,\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n          INodesInPath.fromINode(sRoot));\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-10762. Pass IIP for file status related methods. Contributed by Daryn Sharp.\"\n\nThis reverts commit 22fc46d7659972ff016ccf1c6f781f0c160be26f.\n",
      "commitDate": "22/08/16 2:57 PM",
      "commitName": "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "22/08/16 1:37 PM",
      "commitNameOld": "22fc46d7659972ff016ccf1c6f781f0c160be26f",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,37 @@\n   private static DirectoryListing getSnapshotsListing(\n       FSDirectory fsd, String src, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n \n     final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       INodeAttributes nodeAttrs \u003d getINodeAttributes(\n           fsd, src, sRoot.getLocalNameBytes(),\n           node, Snapshot.CURRENT_STATE_ID);\n       listing[i] \u003d createFileStatus(\n           fsd, sRoot.getLocalNameBytes(), nodeAttrs,\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n+          Snapshot.CURRENT_STATE_ID, false,\n           INodesInPath.fromINode(sRoot));\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n          fsd, src, sRoot.getLocalNameBytes(),\n          node, Snapshot.CURRENT_STATE_ID);\n      listing[i] \u003d createFileStatus(\n          fsd, sRoot.getLocalNameBytes(), nodeAttrs,\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n          Snapshot.CURRENT_STATE_ID, false,\n          INodesInPath.fromINode(sRoot));\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "22fc46d7659972ff016ccf1c6f781f0c160be26f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10762. Pass IIP for file status related methods. Contributed by Daryn Sharp.\n",
      "commitDate": "22/08/16 1:37 PM",
      "commitName": "22fc46d7659972ff016ccf1c6f781f0c160be26f",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "17/08/16 1:53 PM",
      "commitNameOld": "869393643de23dcb010cc33091c8eb398de0fd6c",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 4.99,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,36 @@\n   private static DirectoryListing getSnapshotsListing(\n       FSDirectory fsd, String src, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n \n     final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       INodeAttributes nodeAttrs \u003d getINodeAttributes(\n           fsd, src, sRoot.getLocalNameBytes(),\n           node, Snapshot.CURRENT_STATE_ID);\n       listing[i] \u003d createFileStatus(\n           fsd, sRoot.getLocalNameBytes(), nodeAttrs,\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n-          Snapshot.CURRENT_STATE_ID, false,\n           INodesInPath.fromINode(sRoot));\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n          fsd, src, sRoot.getLocalNameBytes(),\n          node, Snapshot.CURRENT_STATE_ID);\n      listing[i] \u003d createFileStatus(\n          fsd, sRoot.getLocalNameBytes(), nodeAttrs,\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n          INodesInPath.fromINode(sRoot));\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "9f4bf3bdf9e74800643477cfb18361e01cf6859c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9621. getListing wrongly associates Erasure Coding policy to pre-existing replicated files under an EC directory. Contributed by Jing Zhao.\n",
      "commitDate": "11/01/16 11:31 AM",
      "commitName": "9f4bf3bdf9e74800643477cfb18361e01cf6859c",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/11/15 1:14 PM",
      "commitNameOld": "977e0b3c4ce76746a3d8590d2d790fdc96c86ca5",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 47.93,
      "commitsBetweenForRepo": 246,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,37 @@\n   private static DirectoryListing getSnapshotsListing(\n       FSDirectory fsd, String src, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n \n     final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       INodeAttributes nodeAttrs \u003d getINodeAttributes(\n           fsd, src, sRoot.getLocalNameBytes(),\n           node, Snapshot.CURRENT_STATE_ID);\n       listing[i] \u003d createFileStatus(\n-          fsd, sRoot.getLocalNameBytes(),\n-          sRoot, nodeAttrs,\n+          fsd, sRoot.getLocalNameBytes(), nodeAttrs,\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n           Snapshot.CURRENT_STATE_ID, false,\n           INodesInPath.fromINode(sRoot));\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n          fsd, src, sRoot.getLocalNameBytes(),\n          node, Snapshot.CURRENT_STATE_ID);\n      listing[i] \u003d createFileStatus(\n          fsd, sRoot.getLocalNameBytes(), nodeAttrs,\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n          Snapshot.CURRENT_STATE_ID, false,\n          INodesInPath.fromINode(sRoot));\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/04/15 1:41 PM",
      "commitNameOld": "c55d609053fe24b3a50fbe17dc1b47717b453ed6",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.85,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   private static DirectoryListing getSnapshotsListing(\n       FSDirectory fsd, String src, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n \n     final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       INodeAttributes nodeAttrs \u003d getINodeAttributes(\n           fsd, src, sRoot.getLocalNameBytes(),\n           node, Snapshot.CURRENT_STATE_ID);\n       listing[i] \u003d createFileStatus(\n           fsd, sRoot.getLocalNameBytes(),\n           sRoot, nodeAttrs,\n-          HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n+          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n           Snapshot.CURRENT_STATE_ID, false,\n           INodesInPath.fromINode(sRoot));\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n          fsd, src, sRoot.getLocalNameBytes(),\n          node, Snapshot.CURRENT_STATE_ID);\n      listing[i] \u003d createFileStatus(\n          fsd, sRoot.getLocalNameBytes(),\n          sRoot, nodeAttrs,\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n          Snapshot.CURRENT_STATE_ID, false,\n          INodesInPath.fromINode(sRoot));\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "c55d609053fe24b3a50fbe17dc1b47717b453ed6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8200. Refactor FSDirStatAndListingOp. Contributed by Haohui Mai.\n",
      "commitDate": "30/04/15 1:41 PM",
      "commitName": "c55d609053fe24b3a50fbe17dc1b47717b453ed6",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "20/04/15 12:36 AM",
      "commitNameOld": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 10.55,
      "commitsBetweenForRepo": 106,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,38 @@\n   private static DirectoryListing getSnapshotsListing(\n       FSDirectory fsd, String src, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n \n     final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n-      listing[i] \u003d createFileStatus(fsd, src, sRoot.getLocalNameBytes(), sRoot,\n-          HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n-          false, INodesInPath.fromINode(sRoot));\n+      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n+          fsd, src, sRoot.getLocalNameBytes(),\n+          node, Snapshot.CURRENT_STATE_ID);\n+      listing[i] \u003d createFileStatus(\n+          fsd, sRoot.getLocalNameBytes(),\n+          sRoot, nodeAttrs,\n+          HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n+          Snapshot.CURRENT_STATE_ID, false,\n+          INodesInPath.fromINode(sRoot));\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n          fsd, src, sRoot.getLocalNameBytes(),\n          node, Snapshot.CURRENT_STATE_ID);\n      listing[i] \u003d createFileStatus(\n          fsd, sRoot.getLocalNameBytes(),\n          sRoot, nodeAttrs,\n          HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED,\n          Snapshot.CURRENT_STATE_ID, false,\n          INodesInPath.fromINode(sRoot));\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "5c97db07fb306842f49d73a67a90cecec19a7833": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8169. Move LocatedBlocks and related classes to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "20/04/15 12:36 AM",
      "commitName": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "08/04/15 1:38 PM",
      "commitNameOld": "285b31e75e51ec8e3a796c2cb0208739368ca9b8",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 11.46,
      "commitsBetweenForRepo": 85,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,32 @@\n   private static DirectoryListing getSnapshotsListing(\n       FSDirectory fsd, String src, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n \n     final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       listing[i] \u003d createFileStatus(fsd, src, sRoot.getLocalNameBytes(), sRoot,\n-          BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n+          HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n           false, INodesInPath.fromINode(sRoot));\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(fsd, src, sRoot.getLocalNameBytes(), sRoot,\n          HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n          false, INodesInPath.fromINode(sRoot));\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "53a28afe293e5bf185c8d4f2c7aea212e66015c2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6826. Plugin interface to enable delegation of HDFS authorization assertions. Contributed by Arun Suresh.\n",
      "commitDate": "24/03/15 4:02 PM",
      "commitName": "53a28afe293e5bf185c8d4f2c7aea212e66015c2",
      "commitAuthor": "Jitendra Pandey",
      "commitDateOld": "13/01/15 12:24 AM",
      "commitNameOld": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 70.61,
      "commitsBetweenForRepo": 649,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,32 @@\n   private static DirectoryListing getSnapshotsListing(\n       FSDirectory fsd, String src, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n \n     final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n-      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n+      listing[i] \u003d createFileStatus(fsd, src, sRoot.getLocalNameBytes(), sRoot,\n           BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n           false, INodesInPath.fromINode(sRoot));\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(fsd, src, sRoot.getLocalNameBytes(), sRoot,\n          BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n          false, INodesInPath.fromINode(sRoot));\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7474. Avoid resolving path in FSPermissionChecker. Contributed by Jing Zhao.\n",
      "commitDate": "05/12/14 2:17 PM",
      "commitName": "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "01/12/14 9:36 PM",
      "commitNameOld": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.7,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,32 @@\n   private static DirectoryListing getSnapshotsListing(\n       FSDirectory fsd, String src, byte[] startAfter)\n       throws IOException {\n     Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n \n     final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n           BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n-          false, null);\n+          false, INodesInPath.fromINode(sRoot));\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n          BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n          false, INodesInPath.fromINode(sRoot));\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Yexceptionschange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "01/12/14 9:36 PM",
      "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,32 @@\n-  private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n-      throws UnresolvedLinkException, IOException {\n-    Preconditions.checkState(hasReadLock());\n+  private static DirectoryListing getSnapshotsListing(\n+      FSDirectory fsd, String src, byte[] startAfter)\n+      throws IOException {\n+    Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n-    final String dirPath \u003d normalizePath(src.substring(0,\n+    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n-    \n-    final INode node \u003d this.getINode(dirPath);\n+\n+    final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n-    int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n+    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n-      Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n-      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n+      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n+      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n           BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n           false, null);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n          BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n          false, null);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
            "oldMethodName": "getSnapshotsListing",
            "newMethodName": "getSnapshotsListing"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,32 @@\n-  private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n-      throws UnresolvedLinkException, IOException {\n-    Preconditions.checkState(hasReadLock());\n+  private static DirectoryListing getSnapshotsListing(\n+      FSDirectory fsd, String src, byte[] startAfter)\n+      throws IOException {\n+    Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n-    final String dirPath \u003d normalizePath(src.substring(0,\n+    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n-    \n-    final INode node \u003d this.getINode(dirPath);\n+\n+    final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n-    int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n+    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n-      Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n-      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n+      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n+      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n           BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n           false, null);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n          BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n          false, null);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[private, static]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,32 @@\n-  private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n-      throws UnresolvedLinkException, IOException {\n-    Preconditions.checkState(hasReadLock());\n+  private static DirectoryListing getSnapshotsListing(\n+      FSDirectory fsd, String src, byte[] startAfter)\n+      throws IOException {\n+    Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n-    final String dirPath \u003d normalizePath(src.substring(0,\n+    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n-    \n-    final INode node \u003d this.getINode(dirPath);\n+\n+    final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n-    int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n+    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n-      Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n-      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n+      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n+      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n           BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n           false, null);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n          BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n          false, null);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[UnresolvedLinkException, IOException]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,32 @@\n-  private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n-      throws UnresolvedLinkException, IOException {\n-    Preconditions.checkState(hasReadLock());\n+  private static DirectoryListing getSnapshotsListing(\n+      FSDirectory fsd, String src, byte[] startAfter)\n+      throws IOException {\n+    Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n-    final String dirPath \u003d normalizePath(src.substring(0,\n+    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n-    \n-    final INode node \u003d this.getINode(dirPath);\n+\n+    final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n-    int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n+    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n-      Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n-      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n+      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n+      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n           BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n           false, null);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n          BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n          false, null);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,32 @@\n-  private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n-      throws UnresolvedLinkException, IOException {\n-    Preconditions.checkState(hasReadLock());\n+  private static DirectoryListing getSnapshotsListing(\n+      FSDirectory fsd, String src, byte[] startAfter)\n+      throws IOException {\n+    Preconditions.checkState(fsd.hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n-    final String dirPath \u003d normalizePath(src.substring(0,\n+    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n-    \n-    final INode node \u003d this.getINode(dirPath);\n+\n+    final INode node \u003d fsd.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n-    int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n+    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n-      Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n-      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n+      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n+      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n           BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n           false, null);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getSnapshotsListing(\n      FSDirectory fsd, String src, byte[] startAfter)\n      throws IOException {\n    Preconditions.checkState(fsd.hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d FSDirectory.normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n\n    final INode node \u003d fsd.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, fsd.getLsLimit());\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Snapshot.Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(fsd, sRoot.getLocalNameBytes(), sRoot,\n          BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n          false, null);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[src-String, startAfter-byte[]]",
            "newValue": "[fsd-FSDirectory, src-String, startAfter-byte[]]"
          }
        }
      ]
    },
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7081. Add new DistributedFileSystem API for getting all the existing storage policies. Contributed by Jing Zhao.\n",
      "commitDate": "24/09/14 10:05 AM",
      "commitName": "073bbd805c6680f47bbfcc6e8efd708ad729bca4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "21/09/14 9:29 PM",
      "commitNameOld": "1737950d0fc83c68f386881b843c41b0b1e342de",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.52,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n       throws UnresolvedLinkException, IOException {\n     Preconditions.checkState(hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n     \n     final INode node \u003d this.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n-          BlockStoragePolicy.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n+          BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n           false, null);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n      throws UnresolvedLinkException, IOException {\n    Preconditions.checkState(hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n    \n    final INode node \u003d this.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n          BlockStoragePolicySuite.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n          false, null);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "1737950d0fc83c68f386881b843c41b0b1e342de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6987. Move CipherSuite xattr information up to the encryption zone root. Contributed by Zhe Zhang.\n",
      "commitDate": "21/09/14 9:29 PM",
      "commitName": "1737950d0fc83c68f386881b843c41b0b1e342de",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "17/09/14 10:00 PM",
      "commitNameOld": "2d2b0009e662db75cf22e2ce8d618ed0a8e61c2f",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.98,
      "commitsBetweenForRepo": 42,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,31 @@\n   private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n       throws UnresolvedLinkException, IOException {\n     Preconditions.checkState(hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n     \n     final INode node \u003d this.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n-          BlockStoragePolicy.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID, false);\n+          BlockStoragePolicy.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n+          false, null);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n      throws UnresolvedLinkException, IOException {\n    Preconditions.checkState(hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n    \n    final INode node \u003d this.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n          BlockStoragePolicy.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID,\n          false, null);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "9b250d74f029f8ccf3a613f9fb74f59838a66ec1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6847. Support storage policy on directories and include storage policy in HdfsFileStatus.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-6584@1618416 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/08/14 1:58 PM",
      "commitName": "9b250d74f029f8ccf3a613f9fb74f59838a66ec1",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "12/08/14 12:32 PM",
      "commitNameOld": "37207b75d4b83f7c032dc446d5c7e578f5b7e93a",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 4.06,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n   private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n       throws UnresolvedLinkException, IOException {\n     Preconditions.checkState(hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n     \n     final INode node \u003d this.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n-          Snapshot.CURRENT_STATE_ID);\n+          BlockStoragePolicy.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n      throws UnresolvedLinkException, IOException {\n    Preconditions.checkState(hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n    \n    final INode node \u003d this.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n          BlockStoragePolicy.ID_UNSPECIFIED, Snapshot.CURRENT_STATE_ID);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "407bb3d3e452c8277c498dd14e0cc5b7762a7091": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6509. Create a special /.reserved/raw directory for raw access to encrypted data. Contributed by Charles Lamb.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1614490 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/07/14 2:11 PM",
      "commitName": "407bb3d3e452c8277c498dd14e0cc5b7762a7091",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "22/07/14 10:57 AM",
      "commitNameOld": "69b75fca7aec5f5cbf79bc7db3915119cef69e65",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 7.13,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n   private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n       throws UnresolvedLinkException, IOException {\n     Preconditions.checkState(hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n \n     final String dirPath \u003d normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n     \n     final INode node \u003d this.getINode(dirPath);\n     final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n     final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n     if (sf \u003d\u003d null) {\n       throw new SnapshotException(\n           \"Directory is not a snapshottable directory: \" + dirPath);\n     }\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n-          Snapshot.CURRENT_STATE_ID);\n+          Snapshot.CURRENT_STATE_ID, false);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n      throws UnresolvedLinkException, IOException {\n    Preconditions.checkState(hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n    \n    final INode node \u003d this.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n          Snapshot.CURRENT_STATE_ID, false);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "76a621ffd2d66bf012a554f4400091a92a5b473e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6609. Use DirectorySnapshottableFeature to represent a snapshottable directory. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1608631 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/07/14 5:08 PM",
      "commitName": "76a621ffd2d66bf012a554f4400091a92a5b473e",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "23/06/14 11:56 PM",
      "commitNameOld": "08986fdbed5a15bcdc57d142922911759b97e9d1",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 13.72,
      "commitsBetweenForRepo": 88,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,30 @@\n   private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n       throws UnresolvedLinkException, IOException {\n     Preconditions.checkState(hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n-    \n+\n     final String dirPath \u003d normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n     \n     final INode node \u003d this.getINode(dirPath);\n-    final INodeDirectorySnapshottable dirNode \u003d INodeDirectorySnapshottable\n-        .valueOf(node, dirPath);\n-    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d dirNode.getSnapshotList();\n+    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n+    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n+    if (sf \u003d\u003d null) {\n+      throw new SnapshotException(\n+          \"Directory is not a snapshottable directory: \" + dirPath);\n+    }\n+    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n       listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n           Snapshot.CURRENT_STATE_ID);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n      throws UnresolvedLinkException, IOException {\n    Preconditions.checkState(hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR),\n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n\n    final String dirPath \u003d normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n    \n    final INode node \u003d this.getINode(dirPath);\n    final INodeDirectory dirNode \u003d INodeDirectory.valueOf(node, dirPath);\n    final DirectorySnapshottableFeature sf \u003d dirNode.getDirectorySnapshottableFeature();\n    if (sf \u003d\u003d null) {\n      throw new SnapshotException(\n          \"Directory is not a snapshottable directory: \" + dirPath);\n    }\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d sf.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n          Snapshot.CURRENT_STATE_ID);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "70cff9e2f0c8f78c1dc54a064182971bb2106795": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5715. Use Snapshot ID to indicate the corresponding Snapshot for a FileDiff/DirectoryDiff. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556353 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/01/14 12:52 PM",
      "commitName": "70cff9e2f0c8f78c1dc54a064182971bb2106795",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "14/12/13 2:13 AM",
      "commitNameOld": "44a6560b5da3f79d2299579a36e7a2a60a91f823",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 24.44,
      "commitsBetweenForRepo": 98,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,26 @@\n   private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n       throws UnresolvedLinkException, IOException {\n     Preconditions.checkState(hasReadLock());\n     Preconditions.checkArgument(\n         src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR), \n         \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n     \n     final String dirPath \u003d normalizePath(src.substring(0,\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n     \n     final INode node \u003d this.getINode(dirPath);\n     final INodeDirectorySnapshottable dirNode \u003d INodeDirectorySnapshottable\n         .valueOf(node, dirPath);\n     final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d dirNode.getSnapshotList();\n     int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n     skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n     int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n     final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n     for (int i \u003d 0; i \u003c numOfListing; i++) {\n       Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n-      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot, null);\n+      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n+          Snapshot.CURRENT_STATE_ID);\n     }\n     return new DirectoryListing(\n         listing, snapshots.size() - skipSize - numOfListing);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DirectoryListing getSnapshotsListing(String src, byte[] startAfter)\n      throws UnresolvedLinkException, IOException {\n    Preconditions.checkState(hasReadLock());\n    Preconditions.checkArgument(\n        src.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR), \n        \"%s does not end with %s\", src, HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR);\n    \n    final String dirPath \u003d normalizePath(src.substring(0,\n        src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n    \n    final INode node \u003d this.getINode(dirPath);\n    final INodeDirectorySnapshottable dirNode \u003d INodeDirectorySnapshottable\n        .valueOf(node, dirPath);\n    final ReadOnlyList\u003cSnapshot\u003e snapshots \u003d dirNode.getSnapshotList();\n    int skipSize \u003d ReadOnlyList.Util.binarySearch(snapshots, startAfter);\n    skipSize \u003d skipSize \u003c 0 ? -skipSize - 1 : skipSize + 1;\n    int numOfListing \u003d Math.min(snapshots.size() - skipSize, this.lsLimit);\n    final HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n    for (int i \u003d 0; i \u003c numOfListing; i++) {\n      Root sRoot \u003d snapshots.get(i + skipSize).getRoot();\n      listing[i] \u003d createFileStatus(sRoot.getLocalNameBytes(), sRoot,\n          Snapshot.CURRENT_STATE_ID);\n    }\n    return new DirectoryListing(\n        listing, snapshots.size() - skipSize - numOfListing);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    }
  }
}