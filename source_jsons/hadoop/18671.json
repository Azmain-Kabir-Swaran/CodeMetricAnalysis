{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "TeraInputFormat.java",
  "functionName": "writePartitionFile",
  "functionId": "writePartitionFile___job-JobContext(modifiers-final)__partFile-Path",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraInputFormat.java",
  "functionStartLine": 115,
  "functionEndLine": 190,
  "numCommitsSeen": 9,
  "timeTaken": 5167,
  "changeHistory": [
    "9d72f939759f407796ecb4715c2dc2f0d36d5578",
    "18ba71692288b585475595d21ed91d08d245009a",
    "1972a76e5a4483b2da431cc7532207c2e6274c6c",
    "26447229ba2c3d43db978c1b3ce95613669182ee",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "9d72f939759f407796ecb4715c2dc2f0d36d5578": "Ybodychange",
    "18ba71692288b585475595d21ed91d08d245009a": "Ybodychange",
    "1972a76e5a4483b2da431cc7532207c2e6274c6c": "Ymultichange(Yexceptionschange,Ybodychange)",
    "26447229ba2c3d43db978c1b3ce95613669182ee": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "9d72f939759f407796ecb4715c2dc2f0d36d5578": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5807. Print usage for TeraSort job. Contributed by Rohith.\n",
      "commitDate": "18/03/15 3:06 AM",
      "commitName": "9d72f939759f407796ecb4715c2dc2f0d36d5578",
      "commitAuthor": "Harsh J",
      "commitDateOld": "23/08/13 2:23 PM",
      "commitNameOld": "9ee38f3a841aa8c0ed68b54d8c0306c25e9c21bb",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 571.53,
      "commitsBetweenForRepo": 4329,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,76 @@\n   public static void writePartitionFile(final JobContext job, \n       Path partFile) throws Throwable  {\n     long t1 \u003d System.currentTimeMillis();\n     Configuration conf \u003d job.getConfiguration();\n     final TeraInputFormat inFormat \u003d new TeraInputFormat();\n     final TextSampler sampler \u003d new TextSampler();\n     int partitions \u003d job.getNumReduceTasks();\n-    long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n+    long sampleSize \u003d\n+        conf.getLong(TeraSortConfigKeys.SAMPLE_SIZE.key(),\n+            TeraSortConfigKeys.DEFAULT_SAMPLE_SIZE);\n     final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n     long t2 \u003d System.currentTimeMillis();\n     System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n-    int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n+    int samples \u003d\n+        Math.min(conf.getInt(TeraSortConfigKeys.NUM_PARTITIONS.key(),\n+                             TeraSortConfigKeys.DEFAULT_NUM_PARTITIONS),\n+            splits.size());\n     System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n     final long recordsPerSample \u003d sampleSize / samples;\n     final int sampleStep \u003d splits.size() / samples;\n     Thread[] samplerReader \u003d new Thread[samples];\n     SamplerThreadGroup threadGroup \u003d new SamplerThreadGroup(\"Sampler Reader Thread Group\");\n     // take N samples from different parts of the input\n     for(int i\u003d0; i \u003c samples; ++i) {\n       final int idx \u003d i;\n       samplerReader[i] \u003d \n         new Thread (threadGroup,\"Sampler Reader \" + idx) {\n         {\n           setDaemon(true);\n         }\n         public void run() {\n           long records \u003d 0;\n           try {\n             TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n               job.getConfiguration(), new TaskAttemptID());\n             RecordReader\u003cText, Text\u003e reader \u003d \n               inFormat.createRecordReader(splits.get(sampleStep * idx),\n               context);\n             reader.initialize(splits.get(sampleStep * idx), context);\n             while (reader.nextKeyValue()) {\n               sampler.addKey(new Text(reader.getCurrentKey()));\n               records +\u003d 1;\n               if (recordsPerSample \u003c\u003d records) {\n                 break;\n               }\n             }\n           } catch (IOException ie){\n             System.err.println(\"Got an exception while reading splits \" +\n                 StringUtils.stringifyException(ie));\n             throw new RuntimeException(ie);\n           } catch (InterruptedException e) {\n         \t  \n           }\n         }\n       };\n       samplerReader[i].start();\n     }\n     FileSystem outFs \u003d partFile.getFileSystem(conf);\n     DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n                                            outFs.getDefaultBlockSize(partFile));\n     for (int i \u003d 0; i \u003c samples; i++) {\n       try {\n         samplerReader[i].join();\n         if(threadGroup.getThrowable() !\u003d null){\n           throw threadGroup.getThrowable();\n         }\n       } catch (InterruptedException e) {\n       }\n     }\n     for(Text split : sampler.createPartitions(partitions)) {\n       split.write(writer);\n     }\n     writer.close();\n     long t3 \u003d System.currentTimeMillis();\n     System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static void writePartitionFile(final JobContext job, \n      Path partFile) throws Throwable  {\n    long t1 \u003d System.currentTimeMillis();\n    Configuration conf \u003d job.getConfiguration();\n    final TeraInputFormat inFormat \u003d new TeraInputFormat();\n    final TextSampler sampler \u003d new TextSampler();\n    int partitions \u003d job.getNumReduceTasks();\n    long sampleSize \u003d\n        conf.getLong(TeraSortConfigKeys.SAMPLE_SIZE.key(),\n            TeraSortConfigKeys.DEFAULT_SAMPLE_SIZE);\n    final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n    long t2 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n    int samples \u003d\n        Math.min(conf.getInt(TeraSortConfigKeys.NUM_PARTITIONS.key(),\n                             TeraSortConfigKeys.DEFAULT_NUM_PARTITIONS),\n            splits.size());\n    System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n    final long recordsPerSample \u003d sampleSize / samples;\n    final int sampleStep \u003d splits.size() / samples;\n    Thread[] samplerReader \u003d new Thread[samples];\n    SamplerThreadGroup threadGroup \u003d new SamplerThreadGroup(\"Sampler Reader Thread Group\");\n    // take N samples from different parts of the input\n    for(int i\u003d0; i \u003c samples; ++i) {\n      final int idx \u003d i;\n      samplerReader[i] \u003d \n        new Thread (threadGroup,\"Sampler Reader \" + idx) {\n        {\n          setDaemon(true);\n        }\n        public void run() {\n          long records \u003d 0;\n          try {\n            TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n              job.getConfiguration(), new TaskAttemptID());\n            RecordReader\u003cText, Text\u003e reader \u003d \n              inFormat.createRecordReader(splits.get(sampleStep * idx),\n              context);\n            reader.initialize(splits.get(sampleStep * idx), context);\n            while (reader.nextKeyValue()) {\n              sampler.addKey(new Text(reader.getCurrentKey()));\n              records +\u003d 1;\n              if (recordsPerSample \u003c\u003d records) {\n                break;\n              }\n            }\n          } catch (IOException ie){\n            System.err.println(\"Got an exception while reading splits \" +\n                StringUtils.stringifyException(ie));\n            throw new RuntimeException(ie);\n          } catch (InterruptedException e) {\n        \t  \n          }\n        }\n      };\n      samplerReader[i].start();\n    }\n    FileSystem outFs \u003d partFile.getFileSystem(conf);\n    DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n                                           outFs.getDefaultBlockSize(partFile));\n    for (int i \u003d 0; i \u003c samples; i++) {\n      try {\n        samplerReader[i].join();\n        if(threadGroup.getThrowable() !\u003d null){\n          throw threadGroup.getThrowable();\n        }\n      } catch (InterruptedException e) {\n      }\n    }\n    for(Text split : sampler.createPartitions(partitions)) {\n      split.write(writer);\n    }\n    writer.close();\n    long t3 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraInputFormat.java",
      "extendedDetails": {}
    },
    "18ba71692288b585475595d21ed91d08d245009a": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4307. TeraInputFormat calls FileSystem.getDefaultBlockSize() without a Path - Failure when using ViewFileSystem. Contributed by Ahmed Radwan\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1345546 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/06/12 11:47 AM",
      "commitName": "18ba71692288b585475595d21ed91d08d245009a",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "04/12/11 11:58 AM",
      "commitNameOld": "1972a76e5a4483b2da431cc7532207c2e6274c6c",
      "commitAuthorOld": "Mahadev Konar",
      "daysBetweenCommits": 180.95,
      "commitsBetweenForRepo": 1280,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,71 @@\n   public static void writePartitionFile(final JobContext job, \n       Path partFile) throws Throwable  {\n     long t1 \u003d System.currentTimeMillis();\n     Configuration conf \u003d job.getConfiguration();\n     final TeraInputFormat inFormat \u003d new TeraInputFormat();\n     final TextSampler sampler \u003d new TextSampler();\n     int partitions \u003d job.getNumReduceTasks();\n     long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n     final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n     long t2 \u003d System.currentTimeMillis();\n     System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n     int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n     System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n     final long recordsPerSample \u003d sampleSize / samples;\n     final int sampleStep \u003d splits.size() / samples;\n     Thread[] samplerReader \u003d new Thread[samples];\n     SamplerThreadGroup threadGroup \u003d new SamplerThreadGroup(\"Sampler Reader Thread Group\");\n     // take N samples from different parts of the input\n     for(int i\u003d0; i \u003c samples; ++i) {\n       final int idx \u003d i;\n       samplerReader[i] \u003d \n         new Thread (threadGroup,\"Sampler Reader \" + idx) {\n         {\n           setDaemon(true);\n         }\n         public void run() {\n           long records \u003d 0;\n           try {\n             TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n               job.getConfiguration(), new TaskAttemptID());\n             RecordReader\u003cText, Text\u003e reader \u003d \n               inFormat.createRecordReader(splits.get(sampleStep * idx),\n               context);\n             reader.initialize(splits.get(sampleStep * idx), context);\n             while (reader.nextKeyValue()) {\n               sampler.addKey(new Text(reader.getCurrentKey()));\n               records +\u003d 1;\n               if (recordsPerSample \u003c\u003d records) {\n                 break;\n               }\n             }\n           } catch (IOException ie){\n             System.err.println(\"Got an exception while reading splits \" +\n                 StringUtils.stringifyException(ie));\n             throw new RuntimeException(ie);\n           } catch (InterruptedException e) {\n         \t  \n           }\n         }\n       };\n       samplerReader[i].start();\n     }\n     FileSystem outFs \u003d partFile.getFileSystem(conf);\n     DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n-                                           outFs.getDefaultBlockSize());\n+                                           outFs.getDefaultBlockSize(partFile));\n     for (int i \u003d 0; i \u003c samples; i++) {\n       try {\n         samplerReader[i].join();\n         if(threadGroup.getThrowable() !\u003d null){\n           throw threadGroup.getThrowable();\n         }\n       } catch (InterruptedException e) {\n       }\n     }\n     for(Text split : sampler.createPartitions(partitions)) {\n       split.write(writer);\n     }\n     writer.close();\n     long t3 \u003d System.currentTimeMillis();\n     System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static void writePartitionFile(final JobContext job, \n      Path partFile) throws Throwable  {\n    long t1 \u003d System.currentTimeMillis();\n    Configuration conf \u003d job.getConfiguration();\n    final TeraInputFormat inFormat \u003d new TeraInputFormat();\n    final TextSampler sampler \u003d new TextSampler();\n    int partitions \u003d job.getNumReduceTasks();\n    long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n    final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n    long t2 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n    int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n    System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n    final long recordsPerSample \u003d sampleSize / samples;\n    final int sampleStep \u003d splits.size() / samples;\n    Thread[] samplerReader \u003d new Thread[samples];\n    SamplerThreadGroup threadGroup \u003d new SamplerThreadGroup(\"Sampler Reader Thread Group\");\n    // take N samples from different parts of the input\n    for(int i\u003d0; i \u003c samples; ++i) {\n      final int idx \u003d i;\n      samplerReader[i] \u003d \n        new Thread (threadGroup,\"Sampler Reader \" + idx) {\n        {\n          setDaemon(true);\n        }\n        public void run() {\n          long records \u003d 0;\n          try {\n            TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n              job.getConfiguration(), new TaskAttemptID());\n            RecordReader\u003cText, Text\u003e reader \u003d \n              inFormat.createRecordReader(splits.get(sampleStep * idx),\n              context);\n            reader.initialize(splits.get(sampleStep * idx), context);\n            while (reader.nextKeyValue()) {\n              sampler.addKey(new Text(reader.getCurrentKey()));\n              records +\u003d 1;\n              if (recordsPerSample \u003c\u003d records) {\n                break;\n              }\n            }\n          } catch (IOException ie){\n            System.err.println(\"Got an exception while reading splits \" +\n                StringUtils.stringifyException(ie));\n            throw new RuntimeException(ie);\n          } catch (InterruptedException e) {\n        \t  \n          }\n        }\n      };\n      samplerReader[i].start();\n    }\n    FileSystem outFs \u003d partFile.getFileSystem(conf);\n    DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n                                           outFs.getDefaultBlockSize(partFile));\n    for (int i \u003d 0; i \u003c samples; i++) {\n      try {\n        samplerReader[i].join();\n        if(threadGroup.getThrowable() !\u003d null){\n          throw threadGroup.getThrowable();\n        }\n      } catch (InterruptedException e) {\n      }\n    }\n    for(Text split : sampler.createPartitions(partitions)) {\n      split.write(writer);\n    }\n    writer.close();\n    long t3 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraInputFormat.java",
      "extendedDetails": {}
    },
    "1972a76e5a4483b2da431cc7532207c2e6274c6c": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "MAPREDUCE-3458. Fix findbugs warnings in hadoop-examples. (Devaraj K via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210190 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/12/11 11:58 AM",
      "commitName": "1972a76e5a4483b2da431cc7532207c2e6274c6c",
      "commitAuthor": "Mahadev Konar",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "MAPREDUCE-3458. Fix findbugs warnings in hadoop-examples. (Devaraj K via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210190 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/12/11 11:58 AM",
          "commitName": "1972a76e5a4483b2da431cc7532207c2e6274c6c",
          "commitAuthor": "Mahadev Konar",
          "commitDateOld": "18/11/11 5:24 PM",
          "commitNameOld": "26447229ba2c3d43db978c1b3ce95613669182ee",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 15.77,
          "commitsBetweenForRepo": 88,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,67 +1,71 @@\n   public static void writePartitionFile(final JobContext job, \n-      Path partFile) throws IOException, InterruptedException  {\n+      Path partFile) throws Throwable  {\n     long t1 \u003d System.currentTimeMillis();\n     Configuration conf \u003d job.getConfiguration();\n     final TeraInputFormat inFormat \u003d new TeraInputFormat();\n     final TextSampler sampler \u003d new TextSampler();\n     int partitions \u003d job.getNumReduceTasks();\n     long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n     final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n     long t2 \u003d System.currentTimeMillis();\n     System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n     int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n     System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n     final long recordsPerSample \u003d sampleSize / samples;\n     final int sampleStep \u003d splits.size() / samples;\n     Thread[] samplerReader \u003d new Thread[samples];\n+    SamplerThreadGroup threadGroup \u003d new SamplerThreadGroup(\"Sampler Reader Thread Group\");\n     // take N samples from different parts of the input\n     for(int i\u003d0; i \u003c samples; ++i) {\n       final int idx \u003d i;\n       samplerReader[i] \u003d \n-        new Thread (\"Sampler Reader \" + idx) {\n+        new Thread (threadGroup,\"Sampler Reader \" + idx) {\n         {\n           setDaemon(true);\n         }\n         public void run() {\n           long records \u003d 0;\n           try {\n             TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n               job.getConfiguration(), new TaskAttemptID());\n             RecordReader\u003cText, Text\u003e reader \u003d \n               inFormat.createRecordReader(splits.get(sampleStep * idx),\n               context);\n             reader.initialize(splits.get(sampleStep * idx), context);\n             while (reader.nextKeyValue()) {\n               sampler.addKey(new Text(reader.getCurrentKey()));\n               records +\u003d 1;\n               if (recordsPerSample \u003c\u003d records) {\n                 break;\n               }\n             }\n           } catch (IOException ie){\n             System.err.println(\"Got an exception while reading splits \" +\n                 StringUtils.stringifyException(ie));\n-            System.exit(-1);\n+            throw new RuntimeException(ie);\n           } catch (InterruptedException e) {\n         \t  \n           }\n         }\n       };\n       samplerReader[i].start();\n     }\n     FileSystem outFs \u003d partFile.getFileSystem(conf);\n     DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n                                            outFs.getDefaultBlockSize());\n     for (int i \u003d 0; i \u003c samples; i++) {\n       try {\n         samplerReader[i].join();\n+        if(threadGroup.getThrowable() !\u003d null){\n+          throw threadGroup.getThrowable();\n+        }\n       } catch (InterruptedException e) {\n       }\n     }\n     for(Text split : sampler.createPartitions(partitions)) {\n       split.write(writer);\n     }\n     writer.close();\n     long t3 \u003d System.currentTimeMillis();\n     System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void writePartitionFile(final JobContext job, \n      Path partFile) throws Throwable  {\n    long t1 \u003d System.currentTimeMillis();\n    Configuration conf \u003d job.getConfiguration();\n    final TeraInputFormat inFormat \u003d new TeraInputFormat();\n    final TextSampler sampler \u003d new TextSampler();\n    int partitions \u003d job.getNumReduceTasks();\n    long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n    final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n    long t2 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n    int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n    System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n    final long recordsPerSample \u003d sampleSize / samples;\n    final int sampleStep \u003d splits.size() / samples;\n    Thread[] samplerReader \u003d new Thread[samples];\n    SamplerThreadGroup threadGroup \u003d new SamplerThreadGroup(\"Sampler Reader Thread Group\");\n    // take N samples from different parts of the input\n    for(int i\u003d0; i \u003c samples; ++i) {\n      final int idx \u003d i;\n      samplerReader[i] \u003d \n        new Thread (threadGroup,\"Sampler Reader \" + idx) {\n        {\n          setDaemon(true);\n        }\n        public void run() {\n          long records \u003d 0;\n          try {\n            TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n              job.getConfiguration(), new TaskAttemptID());\n            RecordReader\u003cText, Text\u003e reader \u003d \n              inFormat.createRecordReader(splits.get(sampleStep * idx),\n              context);\n            reader.initialize(splits.get(sampleStep * idx), context);\n            while (reader.nextKeyValue()) {\n              sampler.addKey(new Text(reader.getCurrentKey()));\n              records +\u003d 1;\n              if (recordsPerSample \u003c\u003d records) {\n                break;\n              }\n            }\n          } catch (IOException ie){\n            System.err.println(\"Got an exception while reading splits \" +\n                StringUtils.stringifyException(ie));\n            throw new RuntimeException(ie);\n          } catch (InterruptedException e) {\n        \t  \n          }\n        }\n      };\n      samplerReader[i].start();\n    }\n    FileSystem outFs \u003d partFile.getFileSystem(conf);\n    DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n                                           outFs.getDefaultBlockSize());\n    for (int i \u003d 0; i \u003c samples; i++) {\n      try {\n        samplerReader[i].join();\n        if(threadGroup.getThrowable() !\u003d null){\n          throw threadGroup.getThrowable();\n        }\n      } catch (InterruptedException e) {\n      }\n    }\n    for(Text split : sampler.createPartitions(partitions)) {\n      split.write(writer);\n    }\n    writer.close();\n    long t3 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraInputFormat.java",
          "extendedDetails": {
            "oldValue": "[IOException, InterruptedException]",
            "newValue": "[Throwable]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-3458. Fix findbugs warnings in hadoop-examples. (Devaraj K via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210190 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/12/11 11:58 AM",
          "commitName": "1972a76e5a4483b2da431cc7532207c2e6274c6c",
          "commitAuthor": "Mahadev Konar",
          "commitDateOld": "18/11/11 5:24 PM",
          "commitNameOld": "26447229ba2c3d43db978c1b3ce95613669182ee",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 15.77,
          "commitsBetweenForRepo": 88,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,67 +1,71 @@\n   public static void writePartitionFile(final JobContext job, \n-      Path partFile) throws IOException, InterruptedException  {\n+      Path partFile) throws Throwable  {\n     long t1 \u003d System.currentTimeMillis();\n     Configuration conf \u003d job.getConfiguration();\n     final TeraInputFormat inFormat \u003d new TeraInputFormat();\n     final TextSampler sampler \u003d new TextSampler();\n     int partitions \u003d job.getNumReduceTasks();\n     long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n     final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n     long t2 \u003d System.currentTimeMillis();\n     System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n     int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n     System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n     final long recordsPerSample \u003d sampleSize / samples;\n     final int sampleStep \u003d splits.size() / samples;\n     Thread[] samplerReader \u003d new Thread[samples];\n+    SamplerThreadGroup threadGroup \u003d new SamplerThreadGroup(\"Sampler Reader Thread Group\");\n     // take N samples from different parts of the input\n     for(int i\u003d0; i \u003c samples; ++i) {\n       final int idx \u003d i;\n       samplerReader[i] \u003d \n-        new Thread (\"Sampler Reader \" + idx) {\n+        new Thread (threadGroup,\"Sampler Reader \" + idx) {\n         {\n           setDaemon(true);\n         }\n         public void run() {\n           long records \u003d 0;\n           try {\n             TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n               job.getConfiguration(), new TaskAttemptID());\n             RecordReader\u003cText, Text\u003e reader \u003d \n               inFormat.createRecordReader(splits.get(sampleStep * idx),\n               context);\n             reader.initialize(splits.get(sampleStep * idx), context);\n             while (reader.nextKeyValue()) {\n               sampler.addKey(new Text(reader.getCurrentKey()));\n               records +\u003d 1;\n               if (recordsPerSample \u003c\u003d records) {\n                 break;\n               }\n             }\n           } catch (IOException ie){\n             System.err.println(\"Got an exception while reading splits \" +\n                 StringUtils.stringifyException(ie));\n-            System.exit(-1);\n+            throw new RuntimeException(ie);\n           } catch (InterruptedException e) {\n         \t  \n           }\n         }\n       };\n       samplerReader[i].start();\n     }\n     FileSystem outFs \u003d partFile.getFileSystem(conf);\n     DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n                                            outFs.getDefaultBlockSize());\n     for (int i \u003d 0; i \u003c samples; i++) {\n       try {\n         samplerReader[i].join();\n+        if(threadGroup.getThrowable() !\u003d null){\n+          throw threadGroup.getThrowable();\n+        }\n       } catch (InterruptedException e) {\n       }\n     }\n     for(Text split : sampler.createPartitions(partitions)) {\n       split.write(writer);\n     }\n     writer.close();\n     long t3 \u003d System.currentTimeMillis();\n     System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void writePartitionFile(final JobContext job, \n      Path partFile) throws Throwable  {\n    long t1 \u003d System.currentTimeMillis();\n    Configuration conf \u003d job.getConfiguration();\n    final TeraInputFormat inFormat \u003d new TeraInputFormat();\n    final TextSampler sampler \u003d new TextSampler();\n    int partitions \u003d job.getNumReduceTasks();\n    long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n    final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n    long t2 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n    int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n    System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n    final long recordsPerSample \u003d sampleSize / samples;\n    final int sampleStep \u003d splits.size() / samples;\n    Thread[] samplerReader \u003d new Thread[samples];\n    SamplerThreadGroup threadGroup \u003d new SamplerThreadGroup(\"Sampler Reader Thread Group\");\n    // take N samples from different parts of the input\n    for(int i\u003d0; i \u003c samples; ++i) {\n      final int idx \u003d i;\n      samplerReader[i] \u003d \n        new Thread (threadGroup,\"Sampler Reader \" + idx) {\n        {\n          setDaemon(true);\n        }\n        public void run() {\n          long records \u003d 0;\n          try {\n            TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n              job.getConfiguration(), new TaskAttemptID());\n            RecordReader\u003cText, Text\u003e reader \u003d \n              inFormat.createRecordReader(splits.get(sampleStep * idx),\n              context);\n            reader.initialize(splits.get(sampleStep * idx), context);\n            while (reader.nextKeyValue()) {\n              sampler.addKey(new Text(reader.getCurrentKey()));\n              records +\u003d 1;\n              if (recordsPerSample \u003c\u003d records) {\n                break;\n              }\n            }\n          } catch (IOException ie){\n            System.err.println(\"Got an exception while reading splits \" +\n                StringUtils.stringifyException(ie));\n            throw new RuntimeException(ie);\n          } catch (InterruptedException e) {\n        \t  \n          }\n        }\n      };\n      samplerReader[i].start();\n    }\n    FileSystem outFs \u003d partFile.getFileSystem(conf);\n    DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n                                           outFs.getDefaultBlockSize());\n    for (int i \u003d 0; i \u003c samples; i++) {\n      try {\n        samplerReader[i].join();\n        if(threadGroup.getThrowable() !\u003d null){\n          throw threadGroup.getThrowable();\n        }\n      } catch (InterruptedException e) {\n      }\n    }\n    for(Text split : sampler.createPartitions(partitions)) {\n      split.write(writer);\n    }\n    writer.close();\n    long t3 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraInputFormat.java",
          "extendedDetails": {}
        }
      ]
    },
    "26447229ba2c3d43db978c1b3ce95613669182ee": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7590. Mavenize streaming and MR examples. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203941 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 5:24 PM",
      "commitName": "26447229ba2c3d43db978c1b3ce95613669182ee",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "18/11/11 1:04 AM",
      "commitNameOld": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.68,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static void writePartitionFile(final JobContext job, \n      Path partFile) throws IOException, InterruptedException  {\n    long t1 \u003d System.currentTimeMillis();\n    Configuration conf \u003d job.getConfiguration();\n    final TeraInputFormat inFormat \u003d new TeraInputFormat();\n    final TextSampler sampler \u003d new TextSampler();\n    int partitions \u003d job.getNumReduceTasks();\n    long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n    final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n    long t2 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n    int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n    System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n    final long recordsPerSample \u003d sampleSize / samples;\n    final int sampleStep \u003d splits.size() / samples;\n    Thread[] samplerReader \u003d new Thread[samples];\n    // take N samples from different parts of the input\n    for(int i\u003d0; i \u003c samples; ++i) {\n      final int idx \u003d i;\n      samplerReader[i] \u003d \n        new Thread (\"Sampler Reader \" + idx) {\n        {\n          setDaemon(true);\n        }\n        public void run() {\n          long records \u003d 0;\n          try {\n            TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n              job.getConfiguration(), new TaskAttemptID());\n            RecordReader\u003cText, Text\u003e reader \u003d \n              inFormat.createRecordReader(splits.get(sampleStep * idx),\n              context);\n            reader.initialize(splits.get(sampleStep * idx), context);\n            while (reader.nextKeyValue()) {\n              sampler.addKey(new Text(reader.getCurrentKey()));\n              records +\u003d 1;\n              if (recordsPerSample \u003c\u003d records) {\n                break;\n              }\n            }\n          } catch (IOException ie){\n            System.err.println(\"Got an exception while reading splits \" +\n                StringUtils.stringifyException(ie));\n            System.exit(-1);\n          } catch (InterruptedException e) {\n        \t  \n          }\n        }\n      };\n      samplerReader[i].start();\n    }\n    FileSystem outFs \u003d partFile.getFileSystem(conf);\n    DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n                                           outFs.getDefaultBlockSize());\n    for (int i \u003d 0; i \u003c samples; i++) {\n      try {\n        samplerReader[i].join();\n      } catch (InterruptedException e) {\n      }\n    }\n    for(Text split : sampler.createPartitions(partitions)) {\n      split.write(writer);\n    }\n    writer.close();\n    long t3 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraInputFormat.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/examples/org/apache/hadoop/examples/terasort/TeraInputFormat.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraInputFormat.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static void writePartitionFile(final JobContext job, \n      Path partFile) throws IOException, InterruptedException  {\n    long t1 \u003d System.currentTimeMillis();\n    Configuration conf \u003d job.getConfiguration();\n    final TeraInputFormat inFormat \u003d new TeraInputFormat();\n    final TextSampler sampler \u003d new TextSampler();\n    int partitions \u003d job.getNumReduceTasks();\n    long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n    final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n    long t2 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n    int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n    System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n    final long recordsPerSample \u003d sampleSize / samples;\n    final int sampleStep \u003d splits.size() / samples;\n    Thread[] samplerReader \u003d new Thread[samples];\n    // take N samples from different parts of the input\n    for(int i\u003d0; i \u003c samples; ++i) {\n      final int idx \u003d i;\n      samplerReader[i] \u003d \n        new Thread (\"Sampler Reader \" + idx) {\n        {\n          setDaemon(true);\n        }\n        public void run() {\n          long records \u003d 0;\n          try {\n            TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n              job.getConfiguration(), new TaskAttemptID());\n            RecordReader\u003cText, Text\u003e reader \u003d \n              inFormat.createRecordReader(splits.get(sampleStep * idx),\n              context);\n            reader.initialize(splits.get(sampleStep * idx), context);\n            while (reader.nextKeyValue()) {\n              sampler.addKey(new Text(reader.getCurrentKey()));\n              records +\u003d 1;\n              if (recordsPerSample \u003c\u003d records) {\n                break;\n              }\n            }\n          } catch (IOException ie){\n            System.err.println(\"Got an exception while reading splits \" +\n                StringUtils.stringifyException(ie));\n            System.exit(-1);\n          } catch (InterruptedException e) {\n        \t  \n          }\n        }\n      };\n      samplerReader[i].start();\n    }\n    FileSystem outFs \u003d partFile.getFileSystem(conf);\n    DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n                                           outFs.getDefaultBlockSize());\n    for (int i \u003d 0; i \u003c samples; i++) {\n      try {\n        samplerReader[i].join();\n      } catch (InterruptedException e) {\n      }\n    }\n    for(Text split : sampler.createPartitions(partitions)) {\n      split.write(writer);\n    }\n    writer.close();\n    long t3 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n  }",
      "path": "hadoop-mapreduce-project/src/examples/org/apache/hadoop/examples/terasort/TeraInputFormat.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/examples/org/apache/hadoop/examples/terasort/TeraInputFormat.java",
        "newPath": "hadoop-mapreduce-project/src/examples/org/apache/hadoop/examples/terasort/TeraInputFormat.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static void writePartitionFile(final JobContext job, \n      Path partFile) throws IOException, InterruptedException  {\n    long t1 \u003d System.currentTimeMillis();\n    Configuration conf \u003d job.getConfiguration();\n    final TeraInputFormat inFormat \u003d new TeraInputFormat();\n    final TextSampler sampler \u003d new TextSampler();\n    int partitions \u003d job.getNumReduceTasks();\n    long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n    final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n    long t2 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n    int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n    System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n    final long recordsPerSample \u003d sampleSize / samples;\n    final int sampleStep \u003d splits.size() / samples;\n    Thread[] samplerReader \u003d new Thread[samples];\n    // take N samples from different parts of the input\n    for(int i\u003d0; i \u003c samples; ++i) {\n      final int idx \u003d i;\n      samplerReader[i] \u003d \n        new Thread (\"Sampler Reader \" + idx) {\n        {\n          setDaemon(true);\n        }\n        public void run() {\n          long records \u003d 0;\n          try {\n            TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n              job.getConfiguration(), new TaskAttemptID());\n            RecordReader\u003cText, Text\u003e reader \u003d \n              inFormat.createRecordReader(splits.get(sampleStep * idx),\n              context);\n            reader.initialize(splits.get(sampleStep * idx), context);\n            while (reader.nextKeyValue()) {\n              sampler.addKey(new Text(reader.getCurrentKey()));\n              records +\u003d 1;\n              if (recordsPerSample \u003c\u003d records) {\n                break;\n              }\n            }\n          } catch (IOException ie){\n            System.err.println(\"Got an exception while reading splits \" +\n                StringUtils.stringifyException(ie));\n            System.exit(-1);\n          } catch (InterruptedException e) {\n        \t  \n          }\n        }\n      };\n      samplerReader[i].start();\n    }\n    FileSystem outFs \u003d partFile.getFileSystem(conf);\n    DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n                                           outFs.getDefaultBlockSize());\n    for (int i \u003d 0; i \u003c samples; i++) {\n      try {\n        samplerReader[i].join();\n      } catch (InterruptedException e) {\n      }\n    }\n    for(Text split : sampler.createPartitions(partitions)) {\n      split.write(writer);\n    }\n    writer.close();\n    long t3 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n  }",
      "path": "hadoop-mapreduce/src/examples/org/apache/hadoop/examples/terasort/TeraInputFormat.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/examples/org/apache/hadoop/examples/terasort/TeraInputFormat.java",
        "newPath": "hadoop-mapreduce/src/examples/org/apache/hadoop/examples/terasort/TeraInputFormat.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,67 @@\n+  public static void writePartitionFile(final JobContext job, \n+      Path partFile) throws IOException, InterruptedException  {\n+    long t1 \u003d System.currentTimeMillis();\n+    Configuration conf \u003d job.getConfiguration();\n+    final TeraInputFormat inFormat \u003d new TeraInputFormat();\n+    final TextSampler sampler \u003d new TextSampler();\n+    int partitions \u003d job.getNumReduceTasks();\n+    long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n+    final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n+    long t2 \u003d System.currentTimeMillis();\n+    System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n+    int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n+    System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n+    final long recordsPerSample \u003d sampleSize / samples;\n+    final int sampleStep \u003d splits.size() / samples;\n+    Thread[] samplerReader \u003d new Thread[samples];\n+    // take N samples from different parts of the input\n+    for(int i\u003d0; i \u003c samples; ++i) {\n+      final int idx \u003d i;\n+      samplerReader[i] \u003d \n+        new Thread (\"Sampler Reader \" + idx) {\n+        {\n+          setDaemon(true);\n+        }\n+        public void run() {\n+          long records \u003d 0;\n+          try {\n+            TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n+              job.getConfiguration(), new TaskAttemptID());\n+            RecordReader\u003cText, Text\u003e reader \u003d \n+              inFormat.createRecordReader(splits.get(sampleStep * idx),\n+              context);\n+            reader.initialize(splits.get(sampleStep * idx), context);\n+            while (reader.nextKeyValue()) {\n+              sampler.addKey(new Text(reader.getCurrentKey()));\n+              records +\u003d 1;\n+              if (recordsPerSample \u003c\u003d records) {\n+                break;\n+              }\n+            }\n+          } catch (IOException ie){\n+            System.err.println(\"Got an exception while reading splits \" +\n+                StringUtils.stringifyException(ie));\n+            System.exit(-1);\n+          } catch (InterruptedException e) {\n+        \t  \n+          }\n+        }\n+      };\n+      samplerReader[i].start();\n+    }\n+    FileSystem outFs \u003d partFile.getFileSystem(conf);\n+    DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n+                                           outFs.getDefaultBlockSize());\n+    for (int i \u003d 0; i \u003c samples; i++) {\n+      try {\n+        samplerReader[i].join();\n+      } catch (InterruptedException e) {\n+      }\n+    }\n+    for(Text split : sampler.createPartitions(partitions)) {\n+      split.write(writer);\n+    }\n+    writer.close();\n+    long t3 \u003d System.currentTimeMillis();\n+    System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static void writePartitionFile(final JobContext job, \n      Path partFile) throws IOException, InterruptedException  {\n    long t1 \u003d System.currentTimeMillis();\n    Configuration conf \u003d job.getConfiguration();\n    final TeraInputFormat inFormat \u003d new TeraInputFormat();\n    final TextSampler sampler \u003d new TextSampler();\n    int partitions \u003d job.getNumReduceTasks();\n    long sampleSize \u003d conf.getLong(SAMPLE_SIZE, 100000);\n    final List\u003cInputSplit\u003e splits \u003d inFormat.getSplits(job);\n    long t2 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\n    int samples \u003d Math.min(conf.getInt(NUM_PARTITIONS, 10), splits.size());\n    System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\n    final long recordsPerSample \u003d sampleSize / samples;\n    final int sampleStep \u003d splits.size() / samples;\n    Thread[] samplerReader \u003d new Thread[samples];\n    // take N samples from different parts of the input\n    for(int i\u003d0; i \u003c samples; ++i) {\n      final int idx \u003d i;\n      samplerReader[i] \u003d \n        new Thread (\"Sampler Reader \" + idx) {\n        {\n          setDaemon(true);\n        }\n        public void run() {\n          long records \u003d 0;\n          try {\n            TaskAttemptContext context \u003d new TaskAttemptContextImpl(\n              job.getConfiguration(), new TaskAttemptID());\n            RecordReader\u003cText, Text\u003e reader \u003d \n              inFormat.createRecordReader(splits.get(sampleStep * idx),\n              context);\n            reader.initialize(splits.get(sampleStep * idx), context);\n            while (reader.nextKeyValue()) {\n              sampler.addKey(new Text(reader.getCurrentKey()));\n              records +\u003d 1;\n              if (recordsPerSample \u003c\u003d records) {\n                break;\n              }\n            }\n          } catch (IOException ie){\n            System.err.println(\"Got an exception while reading splits \" +\n                StringUtils.stringifyException(ie));\n            System.exit(-1);\n          } catch (InterruptedException e) {\n        \t  \n          }\n        }\n      };\n      samplerReader[i].start();\n    }\n    FileSystem outFs \u003d partFile.getFileSystem(conf);\n    DataOutputStream writer \u003d outFs.create(partFile, true, 64*1024, (short) 10, \n                                           outFs.getDefaultBlockSize());\n    for (int i \u003d 0; i \u003c samples; i++) {\n      try {\n        samplerReader[i].join();\n      } catch (InterruptedException e) {\n      }\n    }\n    for(Text split : sampler.createPartitions(partitions)) {\n      split.write(writer);\n    }\n    writer.close();\n    long t3 \u003d System.currentTimeMillis();\n    System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\n  }",
      "path": "mapreduce/src/examples/org/apache/hadoop/examples/terasort/TeraInputFormat.java"
    }
  }
}