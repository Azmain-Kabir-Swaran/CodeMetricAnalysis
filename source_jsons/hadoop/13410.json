{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "InvalidateBlocks.java",
  "functionName": "add",
  "functionId": "add___block-Block(modifiers-final)__datanode-DatanodeInfo(modifiers-final)__log-boolean(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
  "functionStartLine": 168,
  "functionEndLine": 186,
  "numCommitsSeen": 25,
  "timeTaken": 3471,
  "changeHistory": [
    "d737bf99d44ce34cd01baad716d23df269267c95",
    "4e50dc976a92a9560630c87cfc4e4513916e5735",
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
    "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e",
    "4551da302d94cffea0313eac79479ab6f9b7cb34",
    "db71de2e11cfa56a254ef4c92fea5ef4f8c19100",
    "be7dd8333a7e56e732171db0781786987de03195",
    "9a3f147fdd5421460889b266ead3a2300323cda2",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "513f17d115564e49124bb744cecf36d16a144ffc"
  ],
  "changeHistoryShort": {
    "d737bf99d44ce34cd01baad716d23df269267c95": "Ybodychange",
    "4e50dc976a92a9560630c87cfc4e4513916e5735": "Ybodychange",
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e": "Ybodychange",
    "d311a38a6b32bbb210bd8748cfb65463e9c0740e": "Ybodychange",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": "Ybodychange",
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e": "Ybodychange",
    "4551da302d94cffea0313eac79479ab6f9b7cb34": "Ybodychange",
    "db71de2e11cfa56a254ef4c92fea5ef4f8c19100": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "9a3f147fdd5421460889b266ead3a2300323cda2": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "513f17d115564e49124bb744cecf36d16a144ffc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d737bf99d44ce34cd01baad716d23df269267c95": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13350. Negative legacy block ID will confuse Erasure Coding to be considered as striped block. (Contributed by Lei (Eddy) Xu).\n",
      "commitDate": "05/04/18 9:59 AM",
      "commitName": "d737bf99d44ce34cd01baad716d23df269267c95",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "11/12/17 4:43 PM",
      "commitNameOld": "55fc2d6485702a99c6d4bb261a720d1f0498af2b",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 114.68,
      "commitsBetweenForRepo": 833,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   synchronized void add(final Block block, final DatanodeInfo datanode,\n       final boolean log) {\n     LightWeightHashSet\u003cBlock\u003e set \u003d getBlocksSet(datanode, block);\n     if (set \u003d\u003d null) {\n       set \u003d new LightWeightHashSet\u003c\u003e();\n       putBlocksSet(datanode, block, set);\n     }\n     if (set.add(block)) {\n-      if (BlockIdManager.isStripedBlockID(block.getBlockId())) {\n+      if (blockIdManager.isStripedBlock(block)) {\n         numECBlocks.increment();\n       } else {\n         numBlocks.increment();\n       }\n       if (log) {\n         NameNode.blockStateChangeLog.debug(\"BLOCK* {}: add {} to {}\",\n             getClass().getSimpleName(), block, datanode);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    LightWeightHashSet\u003cBlock\u003e set \u003d getBlocksSet(datanode, block);\n    if (set \u003d\u003d null) {\n      set \u003d new LightWeightHashSet\u003c\u003e();\n      putBlocksSet(datanode, block, set);\n    }\n    if (set.add(block)) {\n      if (blockIdManager.isStripedBlock(block)) {\n        numECBlocks.increment();\n      } else {\n        numBlocks.increment();\n      }\n      if (log) {\n        NameNode.blockStateChangeLog.debug(\"BLOCK* {}: add {} to {}\",\n            getClass().getSimpleName(), block, datanode);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "4e50dc976a92a9560630c87cfc4e4513916e5735": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12218. Addendum. Rename split EC / replicated block metrics in BlockManager.\n",
      "commitDate": "07/09/17 4:57 PM",
      "commitName": "4e50dc976a92a9560630c87cfc4e4513916e5735",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "14/06/17 10:44 AM",
      "commitNameOld": "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 85.26,
      "commitsBetweenForRepo": 589,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   synchronized void add(final Block block, final DatanodeInfo datanode,\n       final boolean log) {\n     LightWeightHashSet\u003cBlock\u003e set \u003d getBlocksSet(datanode, block);\n     if (set \u003d\u003d null) {\n       set \u003d new LightWeightHashSet\u003c\u003e();\n       putBlocksSet(datanode, block, set);\n     }\n     if (set.add(block)) {\n       if (BlockIdManager.isStripedBlockID(block.getBlockId())) {\n-        numECBlockGroups.increment();\n+        numECBlocks.increment();\n       } else {\n         numBlocks.increment();\n       }\n       if (log) {\n         NameNode.blockStateChangeLog.debug(\"BLOCK* {}: add {} to {}\",\n             getClass().getSimpleName(), block, datanode);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    LightWeightHashSet\u003cBlock\u003e set \u003d getBlocksSet(datanode, block);\n    if (set \u003d\u003d null) {\n      set \u003d new LightWeightHashSet\u003c\u003e();\n      putBlocksSet(datanode, block, set);\n    }\n    if (set.add(block)) {\n      if (BlockIdManager.isStripedBlockID(block.getBlockId())) {\n        numECBlocks.increment();\n      } else {\n        numBlocks.increment();\n      }\n      if (log) {\n        NameNode.blockStateChangeLog.debug(\"BLOCK* {}: add {} to {}\",\n            getClass().getSimpleName(), block, datanode);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10999. Introduce separate stats for Replicated and Erasure Coded Blocks apart from the current Aggregated stats. (Manoj Govindassamy via lei)\n",
      "commitDate": "14/06/17 10:44 AM",
      "commitName": "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "29/05/17 1:30 AM",
      "commitNameOld": "a7f085d6bf499edf23e650a4f7211c53a442da0e",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 16.38,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,19 @@\n   synchronized void add(final Block block, final DatanodeInfo datanode,\n       final boolean log) {\n-    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode);\n+    LightWeightHashSet\u003cBlock\u003e set \u003d getBlocksSet(datanode, block);\n     if (set \u003d\u003d null) {\n-      set \u003d new LightWeightHashSet\u003cBlock\u003e();\n-      node2blocks.put(datanode, set);\n+      set \u003d new LightWeightHashSet\u003c\u003e();\n+      putBlocksSet(datanode, block, set);\n     }\n     if (set.add(block)) {\n-      numBlocks++;\n+      if (BlockIdManager.isStripedBlockID(block.getBlockId())) {\n+        numECBlockGroups.increment();\n+      } else {\n+        numBlocks.increment();\n+      }\n       if (log) {\n         NameNode.blockStateChangeLog.debug(\"BLOCK* {}: add {} to {}\",\n             getClass().getSimpleName(), block, datanode);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    LightWeightHashSet\u003cBlock\u003e set \u003d getBlocksSet(datanode, block);\n    if (set \u003d\u003d null) {\n      set \u003d new LightWeightHashSet\u003c\u003e();\n      putBlocksSet(datanode, block, set);\n    }\n    if (set.add(block)) {\n      if (BlockIdManager.isStripedBlockID(block.getBlockId())) {\n        numECBlockGroups.increment();\n      } else {\n        numBlocks.increment();\n      }\n      if (log) {\n        NameNode.blockStateChangeLog.debug(\"BLOCK* {}: add {} to {}\",\n            getClass().getSimpleName(), block, datanode);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "d311a38a6b32bbb210bd8748cfb65463e9c0740e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6860. BlockStateChange logs are too noisy. Contributed by Chang Li and Xiaoyu Yao.\n",
      "commitDate": "31/07/15 4:15 PM",
      "commitName": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "22/07/15 12:16 AM",
      "commitNameOld": "4025326288c0167ff300d4f7ecc96f84ed141912",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 9.67,
      "commitsBetweenForRepo": 66,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   synchronized void add(final Block block, final DatanodeInfo datanode,\n       final boolean log) {\n     LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode);\n     if (set \u003d\u003d null) {\n       set \u003d new LightWeightHashSet\u003cBlock\u003e();\n       node2blocks.put(datanode, set);\n     }\n     if (set.add(block)) {\n       numBlocks++;\n       if (log) {\n-        NameNode.blockStateChangeLog.info(\"BLOCK* {}: add {} to {}\",\n+        NameNode.blockStateChangeLog.debug(\"BLOCK* {}: add {} to {}\",\n             getClass().getSimpleName(), block, datanode);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode);\n    if (set \u003d\u003d null) {\n      set \u003d new LightWeightHashSet\u003cBlock\u003e();\n      node2blocks.put(datanode, set);\n    }\n    if (set.add(block)) {\n      numBlocks++;\n      if (log) {\n        NameNode.blockStateChangeLog.debug(\"BLOCK* {}: add {} to {}\",\n            getClass().getSimpleName(), block, datanode);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7712. Switch blockStateChangeLog to use slf4j.\n",
      "commitDate": "03/02/15 3:01 PM",
      "commitName": "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/01/15 11:33 AM",
      "commitNameOld": "951b3608a8cb1d9063b9be9c740b524c137b816f",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.14,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   synchronized void add(final Block block, final DatanodeInfo datanode,\n       final boolean log) {\n     LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode);\n     if (set \u003d\u003d null) {\n       set \u003d new LightWeightHashSet\u003cBlock\u003e();\n       node2blocks.put(datanode, set);\n     }\n     if (set.add(block)) {\n       numBlocks++;\n       if (log) {\n-        NameNode.blockStateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n-            + \": add \" + block + \" to \" + datanode);\n+        NameNode.blockStateChangeLog.info(\"BLOCK* {}: add {} to {}\",\n+            getClass().getSimpleName(), block, datanode);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode);\n    if (set \u003d\u003d null) {\n      set \u003d new LightWeightHashSet\u003cBlock\u003e();\n      node2blocks.put(datanode, set);\n    }\n    if (set.add(block)) {\n      numBlocks++;\n      if (log) {\n        NameNode.blockStateChangeLog.info(\"BLOCK* {}: add {} to {}\",\n            getClass().getSimpleName(), block, datanode);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6362. InvalidateBlocks is inconsistent in usage of DatanodeUuid and StorageID. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595056 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/05/14 2:30 PM",
      "commitName": "9a0fcae5bc9e481201e101c3c98e23b6e827774e",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "13/05/14 11:22 AM",
      "commitNameOld": "8e5b5165c14486af6d5d73e7b4e591d4787ad8f2",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.13,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   synchronized void add(final Block block, final DatanodeInfo datanode,\n       final boolean log) {\n-    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode.getDatanodeUuid());\n+    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode);\n     if (set \u003d\u003d null) {\n       set \u003d new LightWeightHashSet\u003cBlock\u003e();\n-      node2blocks.put(datanode.getDatanodeUuid(), set);\n+      node2blocks.put(datanode, set);\n     }\n     if (set.add(block)) {\n       numBlocks++;\n       if (log) {\n         NameNode.blockStateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n             + \": add \" + block + \" to \" + datanode);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode);\n    if (set \u003d\u003d null) {\n      set \u003d new LightWeightHashSet\u003cBlock\u003e();\n      node2blocks.put(datanode, set);\n    }\n    if (set.add(block)) {\n      numBlocks++;\n      if (log) {\n        NameNode.blockStateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n            + \": add \" + block + \" to \" + datanode);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "4551da302d94cffea0313eac79479ab6f9b7cb34": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5233. Use Datanode UUID to identify Datanodes.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1525407 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/09/13 11:03 AM",
      "commitName": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "09/11/12 10:07 AM",
      "commitNameOld": "db71de2e11cfa56a254ef4c92fea5ef4f8c19100",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 317.0,
      "commitsBetweenForRepo": 1771,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   synchronized void add(final Block block, final DatanodeInfo datanode,\n       final boolean log) {\n-    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode.getStorageID());\n+    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode.getDatanodeUuid());\n     if (set \u003d\u003d null) {\n       set \u003d new LightWeightHashSet\u003cBlock\u003e();\n-      node2blocks.put(datanode.getStorageID(), set);\n+      node2blocks.put(datanode.getDatanodeUuid(), set);\n     }\n     if (set.add(block)) {\n       numBlocks++;\n       if (log) {\n         NameNode.blockStateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n             + \": add \" + block + \" to \" + datanode);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode.getDatanodeUuid());\n    if (set \u003d\u003d null) {\n      set \u003d new LightWeightHashSet\u003cBlock\u003e();\n      node2blocks.put(datanode.getDatanodeUuid(), set);\n    }\n    if (set.add(block)) {\n      numBlocks++;\n      if (log) {\n        NameNode.blockStateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n            + \": add \" + block + \" to \" + datanode);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "db71de2e11cfa56a254ef4c92fea5ef4f8c19100": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4080. Add a separate logger for block state change logs to enable turning off those logs. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1407566 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/11/12 10:07 AM",
      "commitName": "db71de2e11cfa56a254ef4c92fea5ef4f8c19100",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "15/10/12 8:37 PM",
      "commitNameOld": "b7887f31fbe28d35005abdc439b2771b58c91225",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 24.6,
      "commitsBetweenForRepo": 147,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   synchronized void add(final Block block, final DatanodeInfo datanode,\n       final boolean log) {\n     LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode.getStorageID());\n     if (set \u003d\u003d null) {\n       set \u003d new LightWeightHashSet\u003cBlock\u003e();\n       node2blocks.put(datanode.getStorageID(), set);\n     }\n     if (set.add(block)) {\n       numBlocks++;\n       if (log) {\n-        NameNode.stateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n+        NameNode.blockStateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n             + \": add \" + block + \" to \" + datanode);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode.getStorageID());\n    if (set \u003d\u003d null) {\n      set \u003d new LightWeightHashSet\u003cBlock\u003e();\n      node2blocks.put(datanode.getStorageID(), set);\n    }\n    if (set.add(block)) {\n      numBlocks++;\n      if (log) {\n        NameNode.blockStateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n            + \": add \" + block + \" to \" + datanode);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "20/12/11 8:32 PM",
      "commitNameOld": "31c91706f7d17da006ef2d6c541f8dd092fae077",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 102.74,
      "commitsBetweenForRepo": 694,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   synchronized void add(final Block block, final DatanodeInfo datanode,\n       final boolean log) {\n     LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode.getStorageID());\n     if (set \u003d\u003d null) {\n       set \u003d new LightWeightHashSet\u003cBlock\u003e();\n       node2blocks.put(datanode.getStorageID(), set);\n     }\n     if (set.add(block)) {\n       numBlocks++;\n       if (log) {\n         NameNode.stateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n-            + \": add \" + block + \" to \" + datanode.getName());\n+            + \": add \" + block + \" to \" + datanode);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode.getStorageID());\n    if (set \u003d\u003d null) {\n      set \u003d new LightWeightHashSet\u003cBlock\u003e();\n      node2blocks.put(datanode.getStorageID(), set);\n    }\n    if (set.add(block)) {\n      numBlocks++;\n      if (log) {\n        NameNode.stateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n            + \": add \" + block + \" to \" + datanode);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "9a3f147fdd5421460889b266ead3a2300323cda2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2476. More CPU efficient data structure for under-replicated, over-replicated, and invalidated blocks. Contributed by Tomasz Nykiel.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1201991 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/11/11 5:13 PM",
      "commitName": "9a3f147fdd5421460889b266ead3a2300323cda2",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 82.04,
      "commitsBetweenForRepo": 586,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   synchronized void add(final Block block, final DatanodeInfo datanode,\n       final boolean log) {\n-    Collection\u003cBlock\u003e set \u003d node2blocks.get(datanode.getStorageID());\n+    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode.getStorageID());\n     if (set \u003d\u003d null) {\n-      set \u003d new HashSet\u003cBlock\u003e();\n+      set \u003d new LightWeightHashSet\u003cBlock\u003e();\n       node2blocks.put(datanode.getStorageID(), set);\n     }\n     if (set.add(block)) {\n       numBlocks++;\n       if (log) {\n         NameNode.stateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n             + \": add \" + block + \" to \" + datanode.getName());\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    LightWeightHashSet\u003cBlock\u003e set \u003d node2blocks.get(datanode.getStorageID());\n    if (set \u003d\u003d null) {\n      set \u003d new LightWeightHashSet\u003cBlock\u003e();\n      node2blocks.put(datanode.getStorageID(), set);\n    }\n    if (set.add(block)) {\n      numBlocks++;\n      if (log) {\n        NameNode.stateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n            + \": add \" + block + \" to \" + datanode.getName());\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    Collection\u003cBlock\u003e set \u003d node2blocks.get(datanode.getStorageID());\n    if (set \u003d\u003d null) {\n      set \u003d new HashSet\u003cBlock\u003e();\n      node2blocks.put(datanode.getStorageID(), set);\n    }\n    if (set.add(block)) {\n      numBlocks++;\n      if (log) {\n        NameNode.stateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n            + \": add \" + block + \" to \" + datanode.getName());\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java"
      }
    },
    "513f17d115564e49124bb744cecf36d16a144ffc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2273.  Refactor BlockManager.recentInvalidateSets to a new class.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1160475 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/08/11 3:28 PM",
      "commitName": "513f17d115564e49124bb744cecf36d16a144ffc",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,15 @@\n+  synchronized void add(final Block block, final DatanodeInfo datanode,\n+      final boolean log) {\n+    Collection\u003cBlock\u003e set \u003d node2blocks.get(datanode.getStorageID());\n+    if (set \u003d\u003d null) {\n+      set \u003d new HashSet\u003cBlock\u003e();\n+      node2blocks.put(datanode.getStorageID(), set);\n+    }\n+    if (set.add(block)) {\n+      numBlocks++;\n+      if (log) {\n+        NameNode.stateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n+            + \": add \" + block + \" to \" + datanode.getName());\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void add(final Block block, final DatanodeInfo datanode,\n      final boolean log) {\n    Collection\u003cBlock\u003e set \u003d node2blocks.get(datanode.getStorageID());\n    if (set \u003d\u003d null) {\n      set \u003d new HashSet\u003cBlock\u003e();\n      node2blocks.put(datanode.getStorageID(), set);\n    }\n    if (set.add(block)) {\n      numBlocks++;\n      if (log) {\n        NameNode.stateChangeLog.info(\"BLOCK* \" + getClass().getSimpleName()\n            + \": add \" + block + \" to \" + datanode.getName());\n      }\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java"
    }
  }
}