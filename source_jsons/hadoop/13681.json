{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockPlacementPolicyDefault.java",
  "functionName": "chooseTargetInOrder",
  "functionId": "chooseTargetInOrder___numOfReplicas-int__writer-Node__excludedNodes-Set__Node__(modifiers-final)__blocksize-long(modifiers-final)__maxNodesPerRack-int(modifiers-final)__results-List__DatanodeStorageInfo__(modifiers-final)__avoidStaleNodes-boolean(modifiers-final)__newBlock-boolean(modifiers-final)__storageTypes-EnumMap__StorageType,Integer__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
  "functionStartLine": 514,
  "functionEndLine": 564,
  "numCommitsSeen": 99,
  "timeTaken": 2176,
  "changeHistory": [
    "bccdfeee0aaef9cb98d09ee39909b63fdcbeeafc",
    "d505c8acd30d6f40d0632fe9c93c886a4499a9fc"
  ],
  "changeHistoryShort": {
    "bccdfeee0aaef9cb98d09ee39909b63fdcbeeafc": "Ybodychange",
    "d505c8acd30d6f40d0632fe9c93c886a4499a9fc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "bccdfeee0aaef9cb98d09ee39909b63fdcbeeafc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13155. BlockPlacementPolicyDefault.chooseTargetInOrder Not Checking Return Value for NULL. Contributed by Zsolt Venczel.\n",
      "commitDate": "04/06/18 7:03 AM",
      "commitName": "bccdfeee0aaef9cb98d09ee39909b63fdcbeeafc",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "17/05/18 7:43 PM",
      "commitNameOld": "f749517cc78fc761cecff21e8b7f65fb719bfca2",
      "commitAuthorOld": "Yiqun Lin",
      "daysBetweenCommits": 17.47,
      "commitsBetweenForRepo": 132,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,51 @@\n   protected Node chooseTargetInOrder(int numOfReplicas, \n                                  Node writer,\n                                  final Set\u003cNode\u003e excludedNodes,\n                                  final long blocksize,\n                                  final int maxNodesPerRack,\n                                  final List\u003cDatanodeStorageInfo\u003e results,\n                                  final boolean avoidStaleNodes,\n                                  final boolean newBlock,\n                                  EnumMap\u003cStorageType, Integer\u003e storageTypes)\n                                  throws NotEnoughReplicasException {\n     final int numOfResults \u003d results.size();\n     if (numOfResults \u003d\u003d 0) {\n-      writer \u003d chooseLocalStorage(writer, excludedNodes, blocksize,\n-          maxNodesPerRack, results, avoidStaleNodes, storageTypes, true)\n-          .getDatanodeDescriptor();\n+      DatanodeStorageInfo storageInfo \u003d chooseLocalStorage(writer,\n+          excludedNodes, blocksize, maxNodesPerRack, results, avoidStaleNodes,\n+          storageTypes, true);\n+\n+      writer \u003d (storageInfo !\u003d null) ? storageInfo.getDatanodeDescriptor()\n+                                     : null;\n+\n       if (--numOfReplicas \u003d\u003d 0) {\n         return writer;\n       }\n     }\n     final DatanodeDescriptor dn0 \u003d results.get(0).getDatanodeDescriptor();\n     if (numOfResults \u003c\u003d 1) {\n       chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n           results, avoidStaleNodes, storageTypes);\n       if (--numOfReplicas \u003d\u003d 0) {\n         return writer;\n       }\n     }\n     if (numOfResults \u003c\u003d 2) {\n       final DatanodeDescriptor dn1 \u003d results.get(1).getDatanodeDescriptor();\n       if (clusterMap.isOnSameRack(dn0, dn1)) {\n         chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n             results, avoidStaleNodes, storageTypes);\n       } else if (newBlock){\n         chooseLocalRack(dn1, excludedNodes, blocksize, maxNodesPerRack,\n             results, avoidStaleNodes, storageTypes);\n       } else {\n         chooseLocalRack(writer, excludedNodes, blocksize, maxNodesPerRack,\n             results, avoidStaleNodes, storageTypes);\n       }\n       if (--numOfReplicas \u003d\u003d 0) {\n         return writer;\n       }\n     }\n     chooseRandom(numOfReplicas, NodeBase.ROOT, excludedNodes, blocksize,\n         maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n     return writer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected Node chooseTargetInOrder(int numOfReplicas, \n                                 Node writer,\n                                 final Set\u003cNode\u003e excludedNodes,\n                                 final long blocksize,\n                                 final int maxNodesPerRack,\n                                 final List\u003cDatanodeStorageInfo\u003e results,\n                                 final boolean avoidStaleNodes,\n                                 final boolean newBlock,\n                                 EnumMap\u003cStorageType, Integer\u003e storageTypes)\n                                 throws NotEnoughReplicasException {\n    final int numOfResults \u003d results.size();\n    if (numOfResults \u003d\u003d 0) {\n      DatanodeStorageInfo storageInfo \u003d chooseLocalStorage(writer,\n          excludedNodes, blocksize, maxNodesPerRack, results, avoidStaleNodes,\n          storageTypes, true);\n\n      writer \u003d (storageInfo !\u003d null) ? storageInfo.getDatanodeDescriptor()\n                                     : null;\n\n      if (--numOfReplicas \u003d\u003d 0) {\n        return writer;\n      }\n    }\n    final DatanodeDescriptor dn0 \u003d results.get(0).getDatanodeDescriptor();\n    if (numOfResults \u003c\u003d 1) {\n      chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n          results, avoidStaleNodes, storageTypes);\n      if (--numOfReplicas \u003d\u003d 0) {\n        return writer;\n      }\n    }\n    if (numOfResults \u003c\u003d 2) {\n      final DatanodeDescriptor dn1 \u003d results.get(1).getDatanodeDescriptor();\n      if (clusterMap.isOnSameRack(dn0, dn1)) {\n        chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      } else if (newBlock){\n        chooseLocalRack(dn1, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      } else {\n        chooseLocalRack(writer, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      }\n      if (--numOfReplicas \u003d\u003d 0) {\n        return writer;\n      }\n    }\n    chooseRandom(numOfReplicas, NodeBase.ROOT, excludedNodes, blocksize,\n        maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n    return writer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
      "extendedDetails": {}
    },
    "d505c8acd30d6f40d0632fe9c93c886a4499a9fc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8073. Split BlockPlacementPolicyDefault.chooseTarget(..) so it can be easily overrided. (Contributed by Walter Su)\n",
      "commitDate": "07/04/15 9:26 PM",
      "commitName": "d505c8acd30d6f40d0632fe9c93c886a4499a9fc",
      "commitAuthor": "Vinayakumar B",
      "diff": "@@ -0,0 +1,47 @@\n+  protected Node chooseTargetInOrder(int numOfReplicas, \n+                                 Node writer,\n+                                 final Set\u003cNode\u003e excludedNodes,\n+                                 final long blocksize,\n+                                 final int maxNodesPerRack,\n+                                 final List\u003cDatanodeStorageInfo\u003e results,\n+                                 final boolean avoidStaleNodes,\n+                                 final boolean newBlock,\n+                                 EnumMap\u003cStorageType, Integer\u003e storageTypes)\n+                                 throws NotEnoughReplicasException {\n+    final int numOfResults \u003d results.size();\n+    if (numOfResults \u003d\u003d 0) {\n+      writer \u003d chooseLocalStorage(writer, excludedNodes, blocksize,\n+          maxNodesPerRack, results, avoidStaleNodes, storageTypes, true)\n+          .getDatanodeDescriptor();\n+      if (--numOfReplicas \u003d\u003d 0) {\n+        return writer;\n+      }\n+    }\n+    final DatanodeDescriptor dn0 \u003d results.get(0).getDatanodeDescriptor();\n+    if (numOfResults \u003c\u003d 1) {\n+      chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n+          results, avoidStaleNodes, storageTypes);\n+      if (--numOfReplicas \u003d\u003d 0) {\n+        return writer;\n+      }\n+    }\n+    if (numOfResults \u003c\u003d 2) {\n+      final DatanodeDescriptor dn1 \u003d results.get(1).getDatanodeDescriptor();\n+      if (clusterMap.isOnSameRack(dn0, dn1)) {\n+        chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n+            results, avoidStaleNodes, storageTypes);\n+      } else if (newBlock){\n+        chooseLocalRack(dn1, excludedNodes, blocksize, maxNodesPerRack,\n+            results, avoidStaleNodes, storageTypes);\n+      } else {\n+        chooseLocalRack(writer, excludedNodes, blocksize, maxNodesPerRack,\n+            results, avoidStaleNodes, storageTypes);\n+      }\n+      if (--numOfReplicas \u003d\u003d 0) {\n+        return writer;\n+      }\n+    }\n+    chooseRandom(numOfReplicas, NodeBase.ROOT, excludedNodes, blocksize,\n+        maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n+    return writer;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected Node chooseTargetInOrder(int numOfReplicas, \n                                 Node writer,\n                                 final Set\u003cNode\u003e excludedNodes,\n                                 final long blocksize,\n                                 final int maxNodesPerRack,\n                                 final List\u003cDatanodeStorageInfo\u003e results,\n                                 final boolean avoidStaleNodes,\n                                 final boolean newBlock,\n                                 EnumMap\u003cStorageType, Integer\u003e storageTypes)\n                                 throws NotEnoughReplicasException {\n    final int numOfResults \u003d results.size();\n    if (numOfResults \u003d\u003d 0) {\n      writer \u003d chooseLocalStorage(writer, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, storageTypes, true)\n          .getDatanodeDescriptor();\n      if (--numOfReplicas \u003d\u003d 0) {\n        return writer;\n      }\n    }\n    final DatanodeDescriptor dn0 \u003d results.get(0).getDatanodeDescriptor();\n    if (numOfResults \u003c\u003d 1) {\n      chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n          results, avoidStaleNodes, storageTypes);\n      if (--numOfReplicas \u003d\u003d 0) {\n        return writer;\n      }\n    }\n    if (numOfResults \u003c\u003d 2) {\n      final DatanodeDescriptor dn1 \u003d results.get(1).getDatanodeDescriptor();\n      if (clusterMap.isOnSameRack(dn0, dn1)) {\n        chooseRemoteRack(1, dn0, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      } else if (newBlock){\n        chooseLocalRack(dn1, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      } else {\n        chooseLocalRack(writer, excludedNodes, blocksize, maxNodesPerRack,\n            results, avoidStaleNodes, storageTypes);\n      }\n      if (--numOfReplicas \u003d\u003d 0) {\n        return writer;\n      }\n    }\n    chooseRandom(numOfReplicas, NodeBase.ROOT, excludedNodes, blocksize,\n        maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n    return writer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java"
    }
  }
}