{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CachePoolInfo.java",
  "functionName": "validate",
  "functionId": "validate___info-CachePoolInfo",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
  "functionStartLine": 217,
  "functionEndLine": 239,
  "numCommitsSeen": 21,
  "timeTaken": 3229,
  "changeHistory": [
    "5f6b4157a40e974ccc6a56c39dbd35c54f393fbd",
    "0d6aa5d60948a7966da0ca1c3344a37c1d32f2e9",
    "b9ae3087c0f83bfeeea47ded8e19932b46fd2350",
    "991c453ca3ac141a3f286f74af8401f83c38b230",
    "13edb391d06c479720202eb5ac81f1c71fe64748",
    "a0d9a155a4a4258f628e927e096ecf6673f788ec",
    "3a9cd79e9ddd5a9499e28633ccccdc9eef22b813"
  ],
  "changeHistoryShort": {
    "5f6b4157a40e974ccc6a56c39dbd35c54f393fbd": "Ybodychange",
    "0d6aa5d60948a7966da0ca1c3344a37c1d32f2e9": "Yfilerename",
    "b9ae3087c0f83bfeeea47ded8e19932b46fd2350": "Ybodychange",
    "991c453ca3ac141a3f286f74af8401f83c38b230": "Ybodychange",
    "13edb391d06c479720202eb5ac81f1c71fe64748": "Ybodychange",
    "a0d9a155a4a4258f628e927e096ecf6673f788ec": "Ymultichange(Ymovefromfile,Ybodychange,Yrename,Yparameterchange)",
    "3a9cd79e9ddd5a9499e28633ccccdc9eef22b813": "Yintroduced"
  },
  "changeHistoryDetails": {
    "5f6b4157a40e974ccc6a56c39dbd35c54f393fbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10328. Add per-cache-pool default replication num configuration (xupeng via cmccabe)\n",
      "commitDate": "20/06/16 10:42 AM",
      "commitName": "5f6b4157a40e974ccc6a56c39dbd35c54f393fbd",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 260.96,
      "commitsBetweenForRepo": 1718,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,23 @@\n   public static void validate(CachePoolInfo info) throws IOException {\n     if (info \u003d\u003d null) {\n       throw new InvalidRequestException(\"CachePoolInfo is null\");\n     }\n     if ((info.getLimit() !\u003d null) \u0026\u0026 (info.getLimit() \u003c 0)) {\n       throw new InvalidRequestException(\"Limit is negative.\");\n     }\n+    if ((info.getDefaultReplication() !\u003d null)\n+            \u0026\u0026 (info.getDefaultReplication() \u003c 0)) {\n+      throw new InvalidRequestException(\"Default Replication is negative\");\n+    }\n+\n     if (info.getMaxRelativeExpiryMs() !\u003d null) {\n       long maxRelativeExpiryMs \u003d info.getMaxRelativeExpiryMs();\n       if (maxRelativeExpiryMs \u003c 0l) {\n         throw new InvalidRequestException(\"Max relative expiry is negative.\");\n       }\n       if (maxRelativeExpiryMs \u003e Expiration.MAX_RELATIVE_EXPIRY_MS) {\n         throw new InvalidRequestException(\"Max relative expiry is too big.\");\n       }\n     }\n     validateName(info.poolName);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static void validate(CachePoolInfo info) throws IOException {\n    if (info \u003d\u003d null) {\n      throw new InvalidRequestException(\"CachePoolInfo is null\");\n    }\n    if ((info.getLimit() !\u003d null) \u0026\u0026 (info.getLimit() \u003c 0)) {\n      throw new InvalidRequestException(\"Limit is negative.\");\n    }\n    if ((info.getDefaultReplication() !\u003d null)\n            \u0026\u0026 (info.getDefaultReplication() \u003c 0)) {\n      throw new InvalidRequestException(\"Default Replication is negative\");\n    }\n\n    if (info.getMaxRelativeExpiryMs() !\u003d null) {\n      long maxRelativeExpiryMs \u003d info.getMaxRelativeExpiryMs();\n      if (maxRelativeExpiryMs \u003c 0l) {\n        throw new InvalidRequestException(\"Max relative expiry is negative.\");\n      }\n      if (maxRelativeExpiryMs \u003e Expiration.MAX_RELATIVE_EXPIRY_MS) {\n        throw new InvalidRequestException(\"Max relative expiry is too big.\");\n      }\n    }\n    validateName(info.poolName);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
      "extendedDetails": {}
    },
    "0d6aa5d60948a7966da0ca1c3344a37c1d32f2e9": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8237. Move all protocol classes used by ClientProtocol to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "04/05/15 3:00 PM",
      "commitName": "0d6aa5d60948a7966da0ca1c3344a37c1d32f2e9",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "04/05/15 1:39 PM",
      "commitNameOld": "bf70c5ae2824a9139c1aa9d7c14020018881cec2",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static void validate(CachePoolInfo info) throws IOException {\n    if (info \u003d\u003d null) {\n      throw new InvalidRequestException(\"CachePoolInfo is null\");\n    }\n    if ((info.getLimit() !\u003d null) \u0026\u0026 (info.getLimit() \u003c 0)) {\n      throw new InvalidRequestException(\"Limit is negative.\");\n    }\n    if (info.getMaxRelativeExpiryMs() !\u003d null) {\n      long maxRelativeExpiryMs \u003d info.getMaxRelativeExpiryMs();\n      if (maxRelativeExpiryMs \u003c 0l) {\n        throw new InvalidRequestException(\"Max relative expiry is negative.\");\n      }\n      if (maxRelativeExpiryMs \u003e Expiration.MAX_RELATIVE_EXPIRY_MS) {\n        throw new InvalidRequestException(\"Max relative expiry is too big.\");\n      }\n    }\n    validateName(info.poolName);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java"
      }
    },
    "b9ae3087c0f83bfeeea47ded8e19932b46fd2350": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5636. Enforce a max TTL per cache pool (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1552841 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/13 3:27 PM",
      "commitName": "b9ae3087c0f83bfeeea47ded8e19932b46fd2350",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "17/12/13 10:47 AM",
      "commitNameOld": "991c453ca3ac141a3f286f74af8401f83c38b230",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 3.19,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,18 @@\n   public static void validate(CachePoolInfo info) throws IOException {\n     if (info \u003d\u003d null) {\n       throw new InvalidRequestException(\"CachePoolInfo is null\");\n     }\n     if ((info.getLimit() !\u003d null) \u0026\u0026 (info.getLimit() \u003c 0)) {\n       throw new InvalidRequestException(\"Limit is negative.\");\n     }\n+    if (info.getMaxRelativeExpiryMs() !\u003d null) {\n+      long maxRelativeExpiryMs \u003d info.getMaxRelativeExpiryMs();\n+      if (maxRelativeExpiryMs \u003c 0l) {\n+        throw new InvalidRequestException(\"Max relative expiry is negative.\");\n+      }\n+      if (maxRelativeExpiryMs \u003e Expiration.MAX_RELATIVE_EXPIRY_MS) {\n+        throw new InvalidRequestException(\"Max relative expiry is too big.\");\n+      }\n+    }\n     validateName(info.poolName);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static void validate(CachePoolInfo info) throws IOException {\n    if (info \u003d\u003d null) {\n      throw new InvalidRequestException(\"CachePoolInfo is null\");\n    }\n    if ((info.getLimit() !\u003d null) \u0026\u0026 (info.getLimit() \u003c 0)) {\n      throw new InvalidRequestException(\"Limit is negative.\");\n    }\n    if (info.getMaxRelativeExpiryMs() !\u003d null) {\n      long maxRelativeExpiryMs \u003d info.getMaxRelativeExpiryMs();\n      if (maxRelativeExpiryMs \u003c 0l) {\n        throw new InvalidRequestException(\"Max relative expiry is negative.\");\n      }\n      if (maxRelativeExpiryMs \u003e Expiration.MAX_RELATIVE_EXPIRY_MS) {\n        throw new InvalidRequestException(\"Max relative expiry is too big.\");\n      }\n    }\n    validateName(info.poolName);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
      "extendedDetails": {}
    },
    "991c453ca3ac141a3f286f74af8401f83c38b230": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 10:47 AM",
      "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "27/11/13 9:55 AM",
      "commitNameOld": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 20.04,
      "commitsBetweenForRepo": 112,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,9 @@\n   public static void validate(CachePoolInfo info) throws IOException {\n     if (info \u003d\u003d null) {\n       throw new InvalidRequestException(\"CachePoolInfo is null\");\n     }\n-    if ((info.getWeight() !\u003d null) \u0026\u0026 (info.getWeight() \u003c 0)) {\n-      throw new InvalidRequestException(\"CachePool weight is negative.\");\n+    if ((info.getLimit() !\u003d null) \u0026\u0026 (info.getLimit() \u003c 0)) {\n+      throw new InvalidRequestException(\"Limit is negative.\");\n     }\n     validateName(info.poolName);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static void validate(CachePoolInfo info) throws IOException {\n    if (info \u003d\u003d null) {\n      throw new InvalidRequestException(\"CachePoolInfo is null\");\n    }\n    if ((info.getLimit() !\u003d null) \u0026\u0026 (info.getLimit() \u003c 0)) {\n      throw new InvalidRequestException(\"Limit is negative.\");\n    }\n    validateName(info.poolName);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
      "extendedDetails": {}
    },
    "13edb391d06c479720202eb5ac81f1c71fe64748": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 9:55 AM",
      "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "21/11/13 9:12 AM",
      "commitNameOld": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 6.03,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,6 +1,9 @@\n   public static void validate(CachePoolInfo info) throws IOException {\n     if (info \u003d\u003d null) {\n-      throw new IOException(\"CachePoolInfo is null\");\n+      throw new InvalidRequestException(\"CachePoolInfo is null\");\n+    }\n+    if ((info.getWeight() !\u003d null) \u0026\u0026 (info.getWeight() \u003c 0)) {\n+      throw new InvalidRequestException(\"CachePool weight is negative.\");\n     }\n     validateName(info.poolName);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static void validate(CachePoolInfo info) throws IOException {\n    if (info \u003d\u003d null) {\n      throw new InvalidRequestException(\"CachePoolInfo is null\");\n    }\n    if ((info.getWeight() !\u003d null) \u0026\u0026 (info.getWeight() \u003c 0)) {\n      throw new InvalidRequestException(\"CachePool weight is negative.\");\n    }\n    validateName(info.poolName);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
      "extendedDetails": {}
    },
    "a0d9a155a4a4258f628e927e096ecf6673f788ec": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "HDFS-5236. Change PathBasedCacheDirective APIs to be a single value rather than batch. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1525183 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/09/13 5:20 PM",
      "commitName": "a0d9a155a4a4258f628e927e096ecf6673f788ec",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-5236. Change PathBasedCacheDirective APIs to be a single value rather than batch. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1525183 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "20/09/13 5:20 PM",
          "commitName": "a0d9a155a4a4258f628e927e096ecf6673f788ec",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "19/09/13 4:42 PM",
          "commitNameOld": "aae86e4f3f415a45833498a9e720247c0307d9f1",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 1.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,6 @@\n-  public static void validateName(String name) throws IOException {\n-    if (name.isEmpty()) {\n-      // Empty pool names are not allowed because they would be highly\n-      // confusing.  They would also break the ability to list all pools\n-      // by starting with prevKey \u003d \"\"\n-      throw new IOException(\"invalid empty cache pool name\");\n+  public static void validate(CachePoolInfo info) throws IOException {\n+    if (info \u003d\u003d null) {\n+      throw new IOException(\"CachePoolInfo is null\");\n     }\n+    validateName(info.poolName);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void validate(CachePoolInfo info) throws IOException {\n    if (info \u003d\u003d null) {\n      throw new IOException(\"CachePoolInfo is null\");\n    }\n    validateName(info.poolName);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CachePool.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
            "oldMethodName": "validateName",
            "newMethodName": "validate"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5236. Change PathBasedCacheDirective APIs to be a single value rather than batch. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1525183 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "20/09/13 5:20 PM",
          "commitName": "a0d9a155a4a4258f628e927e096ecf6673f788ec",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "19/09/13 4:42 PM",
          "commitNameOld": "aae86e4f3f415a45833498a9e720247c0307d9f1",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 1.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,6 @@\n-  public static void validateName(String name) throws IOException {\n-    if (name.isEmpty()) {\n-      // Empty pool names are not allowed because they would be highly\n-      // confusing.  They would also break the ability to list all pools\n-      // by starting with prevKey \u003d \"\"\n-      throw new IOException(\"invalid empty cache pool name\");\n+  public static void validate(CachePoolInfo info) throws IOException {\n+    if (info \u003d\u003d null) {\n+      throw new IOException(\"CachePoolInfo is null\");\n     }\n+    validateName(info.poolName);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void validate(CachePoolInfo info) throws IOException {\n    if (info \u003d\u003d null) {\n      throw new IOException(\"CachePoolInfo is null\");\n    }\n    validateName(info.poolName);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "HDFS-5236. Change PathBasedCacheDirective APIs to be a single value rather than batch. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1525183 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "20/09/13 5:20 PM",
          "commitName": "a0d9a155a4a4258f628e927e096ecf6673f788ec",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "19/09/13 4:42 PM",
          "commitNameOld": "aae86e4f3f415a45833498a9e720247c0307d9f1",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 1.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,6 @@\n-  public static void validateName(String name) throws IOException {\n-    if (name.isEmpty()) {\n-      // Empty pool names are not allowed because they would be highly\n-      // confusing.  They would also break the ability to list all pools\n-      // by starting with prevKey \u003d \"\"\n-      throw new IOException(\"invalid empty cache pool name\");\n+  public static void validate(CachePoolInfo info) throws IOException {\n+    if (info \u003d\u003d null) {\n+      throw new IOException(\"CachePoolInfo is null\");\n     }\n+    validateName(info.poolName);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void validate(CachePoolInfo info) throws IOException {\n    if (info \u003d\u003d null) {\n      throw new IOException(\"CachePoolInfo is null\");\n    }\n    validateName(info.poolName);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
          "extendedDetails": {
            "oldValue": "validateName",
            "newValue": "validate"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5236. Change PathBasedCacheDirective APIs to be a single value rather than batch. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1525183 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "20/09/13 5:20 PM",
          "commitName": "a0d9a155a4a4258f628e927e096ecf6673f788ec",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "19/09/13 4:42 PM",
          "commitNameOld": "aae86e4f3f415a45833498a9e720247c0307d9f1",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 1.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,6 @@\n-  public static void validateName(String name) throws IOException {\n-    if (name.isEmpty()) {\n-      // Empty pool names are not allowed because they would be highly\n-      // confusing.  They would also break the ability to list all pools\n-      // by starting with prevKey \u003d \"\"\n-      throw new IOException(\"invalid empty cache pool name\");\n+  public static void validate(CachePoolInfo info) throws IOException {\n+    if (info \u003d\u003d null) {\n+      throw new IOException(\"CachePoolInfo is null\");\n     }\n+    validateName(info.poolName);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void validate(CachePoolInfo info) throws IOException {\n    if (info \u003d\u003d null) {\n      throw new IOException(\"CachePoolInfo is null\");\n    }\n    validateName(info.poolName);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java",
          "extendedDetails": {
            "oldValue": "[name-String]",
            "newValue": "[info-CachePoolInfo]"
          }
        }
      ]
    },
    "3a9cd79e9ddd5a9499e28633ccccdc9eef22b813": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5120. Add command-line support for manipulating cache pools.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1521240 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/09/13 11:53 AM",
      "commitName": "3a9cd79e9ddd5a9499e28633ccccdc9eef22b813",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,8 @@\n+  public static void validateName(String name) throws IOException {\n+    if (name.isEmpty()) {\n+      // Empty pool names are not allowed because they would be highly\n+      // confusing.  They would also break the ability to list all pools\n+      // by starting with prevKey \u003d \"\"\n+      throw new IOException(\"invalid empty cache pool name\");\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static void validateName(String name) throws IOException {\n    if (name.isEmpty()) {\n      // Empty pool names are not allowed because they would be highly\n      // confusing.  They would also break the ability to list all pools\n      // by starting with prevKey \u003d \"\"\n      throw new IOException(\"invalid empty cache pool name\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CachePool.java"
    }
  }
}