{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HadoopLogsAnalyzer.java",
  "functionName": "processJobLine",
  "functionId": "processJobLine___line-ParsedLine",
  "sourceFilePath": "hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java",
  "functionStartLine": 746,
  "functionEndLine": 956,
  "numCommitsSeen": 13,
  "timeTaken": 5009,
  "changeHistory": [
    "10325d97329c214bb3899c8535df5a366bc86d2f",
    "a238f931ea7dce0ca620d1798156c84ff77097ff",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "10325d97329c214bb3899c8535df5a366bc86d2f": "Yfilerename",
    "a238f931ea7dce0ca620d1798156c84ff77097ff": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "10325d97329c214bb3899c8535df5a366bc86d2f": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3582. Move successfully passing MR1 tests to MR2 maven tree.(ahmed via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1233090 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/01/12 2:10 PM",
      "commitName": "10325d97329c214bb3899c8535df5a366bc86d2f",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "18/01/12 10:20 AM",
      "commitNameOld": "8b2f6909ec7df5cffb5ef417f5c9cffdee43e38a",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.16,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void processJobLine(ParsedLine line) throws JsonProcessingException,\n      IOException {\n    try {\n      if (version \u003d\u003d 0 || version \u003d\u003d 1) {\n        // determine the job type if this is the declaration line\n        String jobID \u003d line.get(\"JOBID\");\n\n        String user \u003d line.get(\"USER\");\n\n        String jobPriority \u003d line.get(\"JOB_PRIORITY\");\n\n        String submitTime \u003d line.get(\"SUBMIT_TIME\");\n\n        String jobName \u003d line.get(\"JOBNAME\");\n\n        String launchTime \u003d line.get(\"LAUNCH_TIME\");\n\n        String finishTime \u003d line.get(\"FINISH_TIME\");\n\n        String status \u003d line.get(\"JOB_STATUS\");\n\n        String totalMaps \u003d line.get(\"TOTAL_MAPS\");\n\n        String totalReduces \u003d line.get(\"TOTAL_REDUCES\");\n\n        /*\n         * If the job appears new [the ID is different from the most recent one,\n         * if any] we make a new LoggedJob.\n         */\n        if (jobID !\u003d null\n            \u0026\u0026 jobTraceGen !\u003d null\n            \u0026\u0026 (jobBeingTraced \u003d\u003d null \n                || !jobID.equals(jobBeingTraced.getJobID().toString()))) {\n          // push out the old job if there is one, even though it did\u0027t get\n          // mated\n          // with a conf.\n\n          finalizeJob();\n\n          jobBeingTraced \u003d new LoggedJob(jobID);\n\n          tasksInCurrentJob \u003d new HashMap\u003cString, LoggedTask\u003e();\n          attemptsInCurrentJob \u003d new HashMap\u003cString, LoggedTaskAttempt\u003e();\n\n          // initialize all the per-job statistics gathering places\n          successfulMapAttemptTimes \u003d\n              new Histogram[ParsedHost.numberOfDistances() + 1];\n          for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n            successfulMapAttemptTimes[i] \u003d new Histogram();\n          }\n\n          successfulReduceAttemptTimes \u003d new Histogram();\n          failedMapAttemptTimes \u003d\n              new Histogram[ParsedHost.numberOfDistances() + 1];\n          for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n            failedMapAttemptTimes[i] \u003d new Histogram();\n          }\n\n          failedReduceAttemptTimes \u003d new Histogram();\n          successfulNthMapperAttempts \u003d new Histogram();\n          successfulNthReducerAttempts \u003d new Histogram();\n          mapperLocality \u003d new Histogram();\n        }\n\n        // here we fill in all the stuff the trace might need\n        if (jobBeingTraced !\u003d null) {\n          if (user !\u003d null) {\n            jobBeingTraced.setUser(user);\n          }\n\n          if (jobPriority !\u003d null) {\n            jobBeingTraced.setPriority(LoggedJob.JobPriority\n                .valueOf(jobPriority));\n          }\n\n          if (totalMaps !\u003d null) {\n            jobBeingTraced.setTotalMaps(Integer.parseInt(totalMaps));\n          }\n\n          if (totalReduces !\u003d null) {\n            jobBeingTraced.setTotalReduces(Integer.parseInt(totalReduces));\n          }\n\n          if (submitTime !\u003d null) {\n            jobBeingTraced.setSubmitTime(Long.parseLong(submitTime));\n          }\n\n          if (launchTime !\u003d null) {\n            jobBeingTraced.setLaunchTime(Long.parseLong(launchTime));\n          }\n\n          if (finishTime !\u003d null) {\n            jobBeingTraced.setFinishTime(Long.parseLong(finishTime));\n            if (status !\u003d null) {\n              jobBeingTraced.setOutcome(Pre21JobHistoryConstants.Values\n                  .valueOf(status));\n            }\n\n            maybeMateJobAndConf();\n          }\n        }\n\n        if (jobName !\u003d null) {\n          // we\u0027ll make it java unless the name parses out\n          Matcher m \u003d streamingJobnamePattern.matcher(jobName);\n\n          thisJobType \u003d LoggedJob.JobType.JAVA;\n\n          if (m.matches()) {\n            thisJobType \u003d LoggedJob.JobType.STREAMING;\n          }\n        }\n        if (submitTime !\u003d null) {\n          submitTimeCurrentJob \u003d Long.parseLong(submitTime);\n\n          currentJobID \u003d jobID;\n\n          taskAttemptStartTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptShuffleEndTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptSortEndTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskMapAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n\n          launchTimeCurrentJob \u003d 0L;\n        } else if (launchTime !\u003d null \u0026\u0026 jobID !\u003d null\n            \u0026\u0026 currentJobID.equals(jobID)) {\n          launchTimeCurrentJob \u003d Long.parseLong(launchTime);\n        } else if (finishTime !\u003d null \u0026\u0026 jobID !\u003d null\n            \u0026\u0026 currentJobID.equals(jobID)) {\n          long endTime \u003d Long.parseLong(finishTime);\n\n          if (launchTimeCurrentJob !\u003d 0) {\n            String jobResultText \u003d line.get(\"JOB_STATUS\");\n\n            JobOutcome thisOutcome \u003d\n                ((jobResultText !\u003d null \u0026\u0026 \"SUCCESS\".equals(jobResultText))\n                    ? JobOutcome.SUCCESS : JobOutcome.FAILURE);\n\n            if (submitTimeCurrentJob !\u003d 0L) {\n              canonicalDistributionsEnter(delayTimeDists, thisOutcome,\n                  thisJobType, launchTimeCurrentJob - submitTimeCurrentJob);\n            }\n\n            if (launchTimeCurrentJob !\u003d 0L) {\n              canonicalDistributionsEnter(runTimeDists, thisOutcome,\n                  thisJobType, endTime - launchTimeCurrentJob);\n            }\n\n            // Now we process the hash tables with successful task attempts\n\n            Histogram currentJobMapTimes \u003d new Histogram();\n            Histogram currentJobShuffleTimes \u003d new Histogram();\n            Histogram currentJobSortTimes \u003d new Histogram();\n            Histogram currentJobReduceTimes \u003d new Histogram();\n\n            Iterator\u003cMap.Entry\u003cString, Long\u003e\u003e taskIter \u003d\n                taskAttemptStartTimes.entrySet().iterator();\n\n            while (taskIter.hasNext()) {\n              Map.Entry\u003cString, Long\u003e entry \u003d taskIter.next();\n\n              long startTime \u003d entry.getValue();\n\n              // Map processing\n              Long mapEndTime \u003d taskMapAttemptFinishTimes.get(entry.getKey());\n\n              if (mapEndTime !\u003d null) {\n                currentJobMapTimes.enter(mapEndTime - startTime);\n\n                canonicalDistributionsEnter(mapTimeDists, thisOutcome,\n                    thisJobType, mapEndTime - startTime);\n              }\n\n              // Reduce processing\n              Long shuffleEnd \u003d\n                  taskReduceAttemptShuffleEndTimes.get(entry.getKey());\n              Long sortEnd \u003d taskReduceAttemptSortEndTimes.get(entry.getKey());\n              Long reduceEnd \u003d taskReduceAttemptFinishTimes.get(entry.getKey());\n\n              if (shuffleEnd !\u003d null \u0026\u0026 sortEnd !\u003d null \u0026\u0026 reduceEnd !\u003d null) {\n                currentJobShuffleTimes.enter(shuffleEnd - startTime);\n                currentJobSortTimes.enter(sortEnd - shuffleEnd);\n                currentJobReduceTimes.enter(reduceEnd - sortEnd);\n\n                canonicalDistributionsEnter(shuffleTimeDists, thisOutcome,\n                    thisJobType, shuffleEnd - startTime);\n                canonicalDistributionsEnter(sortTimeDists, thisOutcome,\n                    thisJobType, sortEnd - shuffleEnd);\n                canonicalDistributionsEnter(reduceTimeDists, thisOutcome,\n                    thisJobType, reduceEnd - sortEnd);\n              }\n            }\n\n            // Here we save out the task information\n            incorporateSpread(currentJobMapTimes, mapTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobShuffleTimes, shuffleTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobSortTimes, sortTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobReduceTimes, reduceTimeSpreadDists,\n                thisOutcome, thisJobType);\n          }\n        }\n      }\n    } catch (NumberFormatException e) {\n      LOG.warn(\n          \"HadoopLogsAnalyzer.processJobLine: bad numerical format, at line \"\n              + lineNumber + \".\", e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java",
        "newPath": "hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java"
      }
    },
    "a238f931ea7dce0ca620d1798156c84ff77097ff": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-778. Rumen Anonymizer. (Amar Kamat and Chris Douglas via amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1215141 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/12/11 6:20 AM",
      "commitName": "a238f931ea7dce0ca620d1798156c84ff77097ff",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "31/10/11 10:27 AM",
      "commitNameOld": "9db078212f5a37154925cc8872f9adaeca0ed371",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 45.87,
      "commitsBetweenForRepo": 279,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,211 +1,211 @@\n   private void processJobLine(ParsedLine line) throws JsonProcessingException,\n       IOException {\n     try {\n       if (version \u003d\u003d 0 || version \u003d\u003d 1) {\n         // determine the job type if this is the declaration line\n         String jobID \u003d line.get(\"JOBID\");\n \n         String user \u003d line.get(\"USER\");\n \n         String jobPriority \u003d line.get(\"JOB_PRIORITY\");\n \n         String submitTime \u003d line.get(\"SUBMIT_TIME\");\n \n         String jobName \u003d line.get(\"JOBNAME\");\n \n         String launchTime \u003d line.get(\"LAUNCH_TIME\");\n \n         String finishTime \u003d line.get(\"FINISH_TIME\");\n \n         String status \u003d line.get(\"JOB_STATUS\");\n \n         String totalMaps \u003d line.get(\"TOTAL_MAPS\");\n \n         String totalReduces \u003d line.get(\"TOTAL_REDUCES\");\n \n         /*\n          * If the job appears new [the ID is different from the most recent one,\n          * if any] we make a new LoggedJob.\n          */\n         if (jobID !\u003d null\n             \u0026\u0026 jobTraceGen !\u003d null\n-            \u0026\u0026 (jobBeingTraced \u003d\u003d null || !jobID.equals(jobBeingTraced\n-                .getJobID()))) {\n+            \u0026\u0026 (jobBeingTraced \u003d\u003d null \n+                || !jobID.equals(jobBeingTraced.getJobID().toString()))) {\n           // push out the old job if there is one, even though it did\u0027t get\n           // mated\n           // with a conf.\n \n           finalizeJob();\n \n           jobBeingTraced \u003d new LoggedJob(jobID);\n \n           tasksInCurrentJob \u003d new HashMap\u003cString, LoggedTask\u003e();\n           attemptsInCurrentJob \u003d new HashMap\u003cString, LoggedTaskAttempt\u003e();\n \n           // initialize all the per-job statistics gathering places\n           successfulMapAttemptTimes \u003d\n               new Histogram[ParsedHost.numberOfDistances() + 1];\n           for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n             successfulMapAttemptTimes[i] \u003d new Histogram();\n           }\n \n           successfulReduceAttemptTimes \u003d new Histogram();\n           failedMapAttemptTimes \u003d\n               new Histogram[ParsedHost.numberOfDistances() + 1];\n           for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n             failedMapAttemptTimes[i] \u003d new Histogram();\n           }\n \n           failedReduceAttemptTimes \u003d new Histogram();\n           successfulNthMapperAttempts \u003d new Histogram();\n           successfulNthReducerAttempts \u003d new Histogram();\n           mapperLocality \u003d new Histogram();\n         }\n \n         // here we fill in all the stuff the trace might need\n         if (jobBeingTraced !\u003d null) {\n           if (user !\u003d null) {\n             jobBeingTraced.setUser(user);\n           }\n \n           if (jobPriority !\u003d null) {\n             jobBeingTraced.setPriority(LoggedJob.JobPriority\n                 .valueOf(jobPriority));\n           }\n \n           if (totalMaps !\u003d null) {\n             jobBeingTraced.setTotalMaps(Integer.parseInt(totalMaps));\n           }\n \n           if (totalReduces !\u003d null) {\n             jobBeingTraced.setTotalReduces(Integer.parseInt(totalReduces));\n           }\n \n           if (submitTime !\u003d null) {\n             jobBeingTraced.setSubmitTime(Long.parseLong(submitTime));\n           }\n \n           if (launchTime !\u003d null) {\n             jobBeingTraced.setLaunchTime(Long.parseLong(launchTime));\n           }\n \n           if (finishTime !\u003d null) {\n             jobBeingTraced.setFinishTime(Long.parseLong(finishTime));\n             if (status !\u003d null) {\n               jobBeingTraced.setOutcome(Pre21JobHistoryConstants.Values\n                   .valueOf(status));\n             }\n \n             maybeMateJobAndConf();\n           }\n         }\n \n         if (jobName !\u003d null) {\n           // we\u0027ll make it java unless the name parses out\n           Matcher m \u003d streamingJobnamePattern.matcher(jobName);\n \n           thisJobType \u003d LoggedJob.JobType.JAVA;\n \n           if (m.matches()) {\n             thisJobType \u003d LoggedJob.JobType.STREAMING;\n           }\n         }\n         if (submitTime !\u003d null) {\n           submitTimeCurrentJob \u003d Long.parseLong(submitTime);\n \n           currentJobID \u003d jobID;\n \n           taskAttemptStartTimes \u003d new HashMap\u003cString, Long\u003e();\n           taskReduceAttemptShuffleEndTimes \u003d new HashMap\u003cString, Long\u003e();\n           taskReduceAttemptSortEndTimes \u003d new HashMap\u003cString, Long\u003e();\n           taskMapAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n           taskReduceAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n \n           launchTimeCurrentJob \u003d 0L;\n         } else if (launchTime !\u003d null \u0026\u0026 jobID !\u003d null\n             \u0026\u0026 currentJobID.equals(jobID)) {\n           launchTimeCurrentJob \u003d Long.parseLong(launchTime);\n         } else if (finishTime !\u003d null \u0026\u0026 jobID !\u003d null\n             \u0026\u0026 currentJobID.equals(jobID)) {\n           long endTime \u003d Long.parseLong(finishTime);\n \n           if (launchTimeCurrentJob !\u003d 0) {\n             String jobResultText \u003d line.get(\"JOB_STATUS\");\n \n             JobOutcome thisOutcome \u003d\n                 ((jobResultText !\u003d null \u0026\u0026 \"SUCCESS\".equals(jobResultText))\n                     ? JobOutcome.SUCCESS : JobOutcome.FAILURE);\n \n             if (submitTimeCurrentJob !\u003d 0L) {\n               canonicalDistributionsEnter(delayTimeDists, thisOutcome,\n                   thisJobType, launchTimeCurrentJob - submitTimeCurrentJob);\n             }\n \n             if (launchTimeCurrentJob !\u003d 0L) {\n               canonicalDistributionsEnter(runTimeDists, thisOutcome,\n                   thisJobType, endTime - launchTimeCurrentJob);\n             }\n \n             // Now we process the hash tables with successful task attempts\n \n             Histogram currentJobMapTimes \u003d new Histogram();\n             Histogram currentJobShuffleTimes \u003d new Histogram();\n             Histogram currentJobSortTimes \u003d new Histogram();\n             Histogram currentJobReduceTimes \u003d new Histogram();\n \n             Iterator\u003cMap.Entry\u003cString, Long\u003e\u003e taskIter \u003d\n                 taskAttemptStartTimes.entrySet().iterator();\n \n             while (taskIter.hasNext()) {\n               Map.Entry\u003cString, Long\u003e entry \u003d taskIter.next();\n \n               long startTime \u003d entry.getValue();\n \n               // Map processing\n               Long mapEndTime \u003d taskMapAttemptFinishTimes.get(entry.getKey());\n \n               if (mapEndTime !\u003d null) {\n                 currentJobMapTimes.enter(mapEndTime - startTime);\n \n                 canonicalDistributionsEnter(mapTimeDists, thisOutcome,\n                     thisJobType, mapEndTime - startTime);\n               }\n \n               // Reduce processing\n               Long shuffleEnd \u003d\n                   taskReduceAttemptShuffleEndTimes.get(entry.getKey());\n               Long sortEnd \u003d taskReduceAttemptSortEndTimes.get(entry.getKey());\n               Long reduceEnd \u003d taskReduceAttemptFinishTimes.get(entry.getKey());\n \n               if (shuffleEnd !\u003d null \u0026\u0026 sortEnd !\u003d null \u0026\u0026 reduceEnd !\u003d null) {\n                 currentJobShuffleTimes.enter(shuffleEnd - startTime);\n                 currentJobSortTimes.enter(sortEnd - shuffleEnd);\n                 currentJobReduceTimes.enter(reduceEnd - sortEnd);\n \n                 canonicalDistributionsEnter(shuffleTimeDists, thisOutcome,\n                     thisJobType, shuffleEnd - startTime);\n                 canonicalDistributionsEnter(sortTimeDists, thisOutcome,\n                     thisJobType, sortEnd - shuffleEnd);\n                 canonicalDistributionsEnter(reduceTimeDists, thisOutcome,\n                     thisJobType, reduceEnd - sortEnd);\n               }\n             }\n \n             // Here we save out the task information\n             incorporateSpread(currentJobMapTimes, mapTimeSpreadDists,\n                 thisOutcome, thisJobType);\n             incorporateSpread(currentJobShuffleTimes, shuffleTimeSpreadDists,\n                 thisOutcome, thisJobType);\n             incorporateSpread(currentJobSortTimes, sortTimeSpreadDists,\n                 thisOutcome, thisJobType);\n             incorporateSpread(currentJobReduceTimes, reduceTimeSpreadDists,\n                 thisOutcome, thisJobType);\n           }\n         }\n       }\n     } catch (NumberFormatException e) {\n       LOG.warn(\n           \"HadoopLogsAnalyzer.processJobLine: bad numerical format, at line \"\n               + lineNumber + \".\", e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processJobLine(ParsedLine line) throws JsonProcessingException,\n      IOException {\n    try {\n      if (version \u003d\u003d 0 || version \u003d\u003d 1) {\n        // determine the job type if this is the declaration line\n        String jobID \u003d line.get(\"JOBID\");\n\n        String user \u003d line.get(\"USER\");\n\n        String jobPriority \u003d line.get(\"JOB_PRIORITY\");\n\n        String submitTime \u003d line.get(\"SUBMIT_TIME\");\n\n        String jobName \u003d line.get(\"JOBNAME\");\n\n        String launchTime \u003d line.get(\"LAUNCH_TIME\");\n\n        String finishTime \u003d line.get(\"FINISH_TIME\");\n\n        String status \u003d line.get(\"JOB_STATUS\");\n\n        String totalMaps \u003d line.get(\"TOTAL_MAPS\");\n\n        String totalReduces \u003d line.get(\"TOTAL_REDUCES\");\n\n        /*\n         * If the job appears new [the ID is different from the most recent one,\n         * if any] we make a new LoggedJob.\n         */\n        if (jobID !\u003d null\n            \u0026\u0026 jobTraceGen !\u003d null\n            \u0026\u0026 (jobBeingTraced \u003d\u003d null \n                || !jobID.equals(jobBeingTraced.getJobID().toString()))) {\n          // push out the old job if there is one, even though it did\u0027t get\n          // mated\n          // with a conf.\n\n          finalizeJob();\n\n          jobBeingTraced \u003d new LoggedJob(jobID);\n\n          tasksInCurrentJob \u003d new HashMap\u003cString, LoggedTask\u003e();\n          attemptsInCurrentJob \u003d new HashMap\u003cString, LoggedTaskAttempt\u003e();\n\n          // initialize all the per-job statistics gathering places\n          successfulMapAttemptTimes \u003d\n              new Histogram[ParsedHost.numberOfDistances() + 1];\n          for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n            successfulMapAttemptTimes[i] \u003d new Histogram();\n          }\n\n          successfulReduceAttemptTimes \u003d new Histogram();\n          failedMapAttemptTimes \u003d\n              new Histogram[ParsedHost.numberOfDistances() + 1];\n          for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n            failedMapAttemptTimes[i] \u003d new Histogram();\n          }\n\n          failedReduceAttemptTimes \u003d new Histogram();\n          successfulNthMapperAttempts \u003d new Histogram();\n          successfulNthReducerAttempts \u003d new Histogram();\n          mapperLocality \u003d new Histogram();\n        }\n\n        // here we fill in all the stuff the trace might need\n        if (jobBeingTraced !\u003d null) {\n          if (user !\u003d null) {\n            jobBeingTraced.setUser(user);\n          }\n\n          if (jobPriority !\u003d null) {\n            jobBeingTraced.setPriority(LoggedJob.JobPriority\n                .valueOf(jobPriority));\n          }\n\n          if (totalMaps !\u003d null) {\n            jobBeingTraced.setTotalMaps(Integer.parseInt(totalMaps));\n          }\n\n          if (totalReduces !\u003d null) {\n            jobBeingTraced.setTotalReduces(Integer.parseInt(totalReduces));\n          }\n\n          if (submitTime !\u003d null) {\n            jobBeingTraced.setSubmitTime(Long.parseLong(submitTime));\n          }\n\n          if (launchTime !\u003d null) {\n            jobBeingTraced.setLaunchTime(Long.parseLong(launchTime));\n          }\n\n          if (finishTime !\u003d null) {\n            jobBeingTraced.setFinishTime(Long.parseLong(finishTime));\n            if (status !\u003d null) {\n              jobBeingTraced.setOutcome(Pre21JobHistoryConstants.Values\n                  .valueOf(status));\n            }\n\n            maybeMateJobAndConf();\n          }\n        }\n\n        if (jobName !\u003d null) {\n          // we\u0027ll make it java unless the name parses out\n          Matcher m \u003d streamingJobnamePattern.matcher(jobName);\n\n          thisJobType \u003d LoggedJob.JobType.JAVA;\n\n          if (m.matches()) {\n            thisJobType \u003d LoggedJob.JobType.STREAMING;\n          }\n        }\n        if (submitTime !\u003d null) {\n          submitTimeCurrentJob \u003d Long.parseLong(submitTime);\n\n          currentJobID \u003d jobID;\n\n          taskAttemptStartTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptShuffleEndTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptSortEndTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskMapAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n\n          launchTimeCurrentJob \u003d 0L;\n        } else if (launchTime !\u003d null \u0026\u0026 jobID !\u003d null\n            \u0026\u0026 currentJobID.equals(jobID)) {\n          launchTimeCurrentJob \u003d Long.parseLong(launchTime);\n        } else if (finishTime !\u003d null \u0026\u0026 jobID !\u003d null\n            \u0026\u0026 currentJobID.equals(jobID)) {\n          long endTime \u003d Long.parseLong(finishTime);\n\n          if (launchTimeCurrentJob !\u003d 0) {\n            String jobResultText \u003d line.get(\"JOB_STATUS\");\n\n            JobOutcome thisOutcome \u003d\n                ((jobResultText !\u003d null \u0026\u0026 \"SUCCESS\".equals(jobResultText))\n                    ? JobOutcome.SUCCESS : JobOutcome.FAILURE);\n\n            if (submitTimeCurrentJob !\u003d 0L) {\n              canonicalDistributionsEnter(delayTimeDists, thisOutcome,\n                  thisJobType, launchTimeCurrentJob - submitTimeCurrentJob);\n            }\n\n            if (launchTimeCurrentJob !\u003d 0L) {\n              canonicalDistributionsEnter(runTimeDists, thisOutcome,\n                  thisJobType, endTime - launchTimeCurrentJob);\n            }\n\n            // Now we process the hash tables with successful task attempts\n\n            Histogram currentJobMapTimes \u003d new Histogram();\n            Histogram currentJobShuffleTimes \u003d new Histogram();\n            Histogram currentJobSortTimes \u003d new Histogram();\n            Histogram currentJobReduceTimes \u003d new Histogram();\n\n            Iterator\u003cMap.Entry\u003cString, Long\u003e\u003e taskIter \u003d\n                taskAttemptStartTimes.entrySet().iterator();\n\n            while (taskIter.hasNext()) {\n              Map.Entry\u003cString, Long\u003e entry \u003d taskIter.next();\n\n              long startTime \u003d entry.getValue();\n\n              // Map processing\n              Long mapEndTime \u003d taskMapAttemptFinishTimes.get(entry.getKey());\n\n              if (mapEndTime !\u003d null) {\n                currentJobMapTimes.enter(mapEndTime - startTime);\n\n                canonicalDistributionsEnter(mapTimeDists, thisOutcome,\n                    thisJobType, mapEndTime - startTime);\n              }\n\n              // Reduce processing\n              Long shuffleEnd \u003d\n                  taskReduceAttemptShuffleEndTimes.get(entry.getKey());\n              Long sortEnd \u003d taskReduceAttemptSortEndTimes.get(entry.getKey());\n              Long reduceEnd \u003d taskReduceAttemptFinishTimes.get(entry.getKey());\n\n              if (shuffleEnd !\u003d null \u0026\u0026 sortEnd !\u003d null \u0026\u0026 reduceEnd !\u003d null) {\n                currentJobShuffleTimes.enter(shuffleEnd - startTime);\n                currentJobSortTimes.enter(sortEnd - shuffleEnd);\n                currentJobReduceTimes.enter(reduceEnd - sortEnd);\n\n                canonicalDistributionsEnter(shuffleTimeDists, thisOutcome,\n                    thisJobType, shuffleEnd - startTime);\n                canonicalDistributionsEnter(sortTimeDists, thisOutcome,\n                    thisJobType, sortEnd - shuffleEnd);\n                canonicalDistributionsEnter(reduceTimeDists, thisOutcome,\n                    thisJobType, reduceEnd - sortEnd);\n              }\n            }\n\n            // Here we save out the task information\n            incorporateSpread(currentJobMapTimes, mapTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobShuffleTimes, shuffleTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobSortTimes, sortTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobReduceTimes, reduceTimeSpreadDists,\n                thisOutcome, thisJobType);\n          }\n        }\n      }\n    } catch (NumberFormatException e) {\n      LOG.warn(\n          \"HadoopLogsAnalyzer.processJobLine: bad numerical format, at line \"\n              + lineNumber + \".\", e);\n    }\n  }",
      "path": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void processJobLine(ParsedLine line) throws JsonProcessingException,\n      IOException {\n    try {\n      if (version \u003d\u003d 0 || version \u003d\u003d 1) {\n        // determine the job type if this is the declaration line\n        String jobID \u003d line.get(\"JOBID\");\n\n        String user \u003d line.get(\"USER\");\n\n        String jobPriority \u003d line.get(\"JOB_PRIORITY\");\n\n        String submitTime \u003d line.get(\"SUBMIT_TIME\");\n\n        String jobName \u003d line.get(\"JOBNAME\");\n\n        String launchTime \u003d line.get(\"LAUNCH_TIME\");\n\n        String finishTime \u003d line.get(\"FINISH_TIME\");\n\n        String status \u003d line.get(\"JOB_STATUS\");\n\n        String totalMaps \u003d line.get(\"TOTAL_MAPS\");\n\n        String totalReduces \u003d line.get(\"TOTAL_REDUCES\");\n\n        /*\n         * If the job appears new [the ID is different from the most recent one,\n         * if any] we make a new LoggedJob.\n         */\n        if (jobID !\u003d null\n            \u0026\u0026 jobTraceGen !\u003d null\n            \u0026\u0026 (jobBeingTraced \u003d\u003d null || !jobID.equals(jobBeingTraced\n                .getJobID()))) {\n          // push out the old job if there is one, even though it did\u0027t get\n          // mated\n          // with a conf.\n\n          finalizeJob();\n\n          jobBeingTraced \u003d new LoggedJob(jobID);\n\n          tasksInCurrentJob \u003d new HashMap\u003cString, LoggedTask\u003e();\n          attemptsInCurrentJob \u003d new HashMap\u003cString, LoggedTaskAttempt\u003e();\n\n          // initialize all the per-job statistics gathering places\n          successfulMapAttemptTimes \u003d\n              new Histogram[ParsedHost.numberOfDistances() + 1];\n          for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n            successfulMapAttemptTimes[i] \u003d new Histogram();\n          }\n\n          successfulReduceAttemptTimes \u003d new Histogram();\n          failedMapAttemptTimes \u003d\n              new Histogram[ParsedHost.numberOfDistances() + 1];\n          for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n            failedMapAttemptTimes[i] \u003d new Histogram();\n          }\n\n          failedReduceAttemptTimes \u003d new Histogram();\n          successfulNthMapperAttempts \u003d new Histogram();\n          successfulNthReducerAttempts \u003d new Histogram();\n          mapperLocality \u003d new Histogram();\n        }\n\n        // here we fill in all the stuff the trace might need\n        if (jobBeingTraced !\u003d null) {\n          if (user !\u003d null) {\n            jobBeingTraced.setUser(user);\n          }\n\n          if (jobPriority !\u003d null) {\n            jobBeingTraced.setPriority(LoggedJob.JobPriority\n                .valueOf(jobPriority));\n          }\n\n          if (totalMaps !\u003d null) {\n            jobBeingTraced.setTotalMaps(Integer.parseInt(totalMaps));\n          }\n\n          if (totalReduces !\u003d null) {\n            jobBeingTraced.setTotalReduces(Integer.parseInt(totalReduces));\n          }\n\n          if (submitTime !\u003d null) {\n            jobBeingTraced.setSubmitTime(Long.parseLong(submitTime));\n          }\n\n          if (launchTime !\u003d null) {\n            jobBeingTraced.setLaunchTime(Long.parseLong(launchTime));\n          }\n\n          if (finishTime !\u003d null) {\n            jobBeingTraced.setFinishTime(Long.parseLong(finishTime));\n            if (status !\u003d null) {\n              jobBeingTraced.setOutcome(Pre21JobHistoryConstants.Values\n                  .valueOf(status));\n            }\n\n            maybeMateJobAndConf();\n          }\n        }\n\n        if (jobName !\u003d null) {\n          // we\u0027ll make it java unless the name parses out\n          Matcher m \u003d streamingJobnamePattern.matcher(jobName);\n\n          thisJobType \u003d LoggedJob.JobType.JAVA;\n\n          if (m.matches()) {\n            thisJobType \u003d LoggedJob.JobType.STREAMING;\n          }\n        }\n        if (submitTime !\u003d null) {\n          submitTimeCurrentJob \u003d Long.parseLong(submitTime);\n\n          currentJobID \u003d jobID;\n\n          taskAttemptStartTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptShuffleEndTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptSortEndTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskMapAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n\n          launchTimeCurrentJob \u003d 0L;\n        } else if (launchTime !\u003d null \u0026\u0026 jobID !\u003d null\n            \u0026\u0026 currentJobID.equals(jobID)) {\n          launchTimeCurrentJob \u003d Long.parseLong(launchTime);\n        } else if (finishTime !\u003d null \u0026\u0026 jobID !\u003d null\n            \u0026\u0026 currentJobID.equals(jobID)) {\n          long endTime \u003d Long.parseLong(finishTime);\n\n          if (launchTimeCurrentJob !\u003d 0) {\n            String jobResultText \u003d line.get(\"JOB_STATUS\");\n\n            JobOutcome thisOutcome \u003d\n                ((jobResultText !\u003d null \u0026\u0026 \"SUCCESS\".equals(jobResultText))\n                    ? JobOutcome.SUCCESS : JobOutcome.FAILURE);\n\n            if (submitTimeCurrentJob !\u003d 0L) {\n              canonicalDistributionsEnter(delayTimeDists, thisOutcome,\n                  thisJobType, launchTimeCurrentJob - submitTimeCurrentJob);\n            }\n\n            if (launchTimeCurrentJob !\u003d 0L) {\n              canonicalDistributionsEnter(runTimeDists, thisOutcome,\n                  thisJobType, endTime - launchTimeCurrentJob);\n            }\n\n            // Now we process the hash tables with successful task attempts\n\n            Histogram currentJobMapTimes \u003d new Histogram();\n            Histogram currentJobShuffleTimes \u003d new Histogram();\n            Histogram currentJobSortTimes \u003d new Histogram();\n            Histogram currentJobReduceTimes \u003d new Histogram();\n\n            Iterator\u003cMap.Entry\u003cString, Long\u003e\u003e taskIter \u003d\n                taskAttemptStartTimes.entrySet().iterator();\n\n            while (taskIter.hasNext()) {\n              Map.Entry\u003cString, Long\u003e entry \u003d taskIter.next();\n\n              long startTime \u003d entry.getValue();\n\n              // Map processing\n              Long mapEndTime \u003d taskMapAttemptFinishTimes.get(entry.getKey());\n\n              if (mapEndTime !\u003d null) {\n                currentJobMapTimes.enter(mapEndTime - startTime);\n\n                canonicalDistributionsEnter(mapTimeDists, thisOutcome,\n                    thisJobType, mapEndTime - startTime);\n              }\n\n              // Reduce processing\n              Long shuffleEnd \u003d\n                  taskReduceAttemptShuffleEndTimes.get(entry.getKey());\n              Long sortEnd \u003d taskReduceAttemptSortEndTimes.get(entry.getKey());\n              Long reduceEnd \u003d taskReduceAttemptFinishTimes.get(entry.getKey());\n\n              if (shuffleEnd !\u003d null \u0026\u0026 sortEnd !\u003d null \u0026\u0026 reduceEnd !\u003d null) {\n                currentJobShuffleTimes.enter(shuffleEnd - startTime);\n                currentJobSortTimes.enter(sortEnd - shuffleEnd);\n                currentJobReduceTimes.enter(reduceEnd - sortEnd);\n\n                canonicalDistributionsEnter(shuffleTimeDists, thisOutcome,\n                    thisJobType, shuffleEnd - startTime);\n                canonicalDistributionsEnter(sortTimeDists, thisOutcome,\n                    thisJobType, sortEnd - shuffleEnd);\n                canonicalDistributionsEnter(reduceTimeDists, thisOutcome,\n                    thisJobType, reduceEnd - sortEnd);\n              }\n            }\n\n            // Here we save out the task information\n            incorporateSpread(currentJobMapTimes, mapTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobShuffleTimes, shuffleTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobSortTimes, sortTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobReduceTimes, reduceTimeSpreadDists,\n                thisOutcome, thisJobType);\n          }\n        }\n      }\n    } catch (NumberFormatException e) {\n      LOG.warn(\n          \"HadoopLogsAnalyzer.processJobLine: bad numerical format, at line \"\n              + lineNumber + \".\", e);\n    }\n  }",
      "path": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java",
        "newPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void processJobLine(ParsedLine line) throws JsonProcessingException,\n      IOException {\n    try {\n      if (version \u003d\u003d 0 || version \u003d\u003d 1) {\n        // determine the job type if this is the declaration line\n        String jobID \u003d line.get(\"JOBID\");\n\n        String user \u003d line.get(\"USER\");\n\n        String jobPriority \u003d line.get(\"JOB_PRIORITY\");\n\n        String submitTime \u003d line.get(\"SUBMIT_TIME\");\n\n        String jobName \u003d line.get(\"JOBNAME\");\n\n        String launchTime \u003d line.get(\"LAUNCH_TIME\");\n\n        String finishTime \u003d line.get(\"FINISH_TIME\");\n\n        String status \u003d line.get(\"JOB_STATUS\");\n\n        String totalMaps \u003d line.get(\"TOTAL_MAPS\");\n\n        String totalReduces \u003d line.get(\"TOTAL_REDUCES\");\n\n        /*\n         * If the job appears new [the ID is different from the most recent one,\n         * if any] we make a new LoggedJob.\n         */\n        if (jobID !\u003d null\n            \u0026\u0026 jobTraceGen !\u003d null\n            \u0026\u0026 (jobBeingTraced \u003d\u003d null || !jobID.equals(jobBeingTraced\n                .getJobID()))) {\n          // push out the old job if there is one, even though it did\u0027t get\n          // mated\n          // with a conf.\n\n          finalizeJob();\n\n          jobBeingTraced \u003d new LoggedJob(jobID);\n\n          tasksInCurrentJob \u003d new HashMap\u003cString, LoggedTask\u003e();\n          attemptsInCurrentJob \u003d new HashMap\u003cString, LoggedTaskAttempt\u003e();\n\n          // initialize all the per-job statistics gathering places\n          successfulMapAttemptTimes \u003d\n              new Histogram[ParsedHost.numberOfDistances() + 1];\n          for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n            successfulMapAttemptTimes[i] \u003d new Histogram();\n          }\n\n          successfulReduceAttemptTimes \u003d new Histogram();\n          failedMapAttemptTimes \u003d\n              new Histogram[ParsedHost.numberOfDistances() + 1];\n          for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n            failedMapAttemptTimes[i] \u003d new Histogram();\n          }\n\n          failedReduceAttemptTimes \u003d new Histogram();\n          successfulNthMapperAttempts \u003d new Histogram();\n          successfulNthReducerAttempts \u003d new Histogram();\n          mapperLocality \u003d new Histogram();\n        }\n\n        // here we fill in all the stuff the trace might need\n        if (jobBeingTraced !\u003d null) {\n          if (user !\u003d null) {\n            jobBeingTraced.setUser(user);\n          }\n\n          if (jobPriority !\u003d null) {\n            jobBeingTraced.setPriority(LoggedJob.JobPriority\n                .valueOf(jobPriority));\n          }\n\n          if (totalMaps !\u003d null) {\n            jobBeingTraced.setTotalMaps(Integer.parseInt(totalMaps));\n          }\n\n          if (totalReduces !\u003d null) {\n            jobBeingTraced.setTotalReduces(Integer.parseInt(totalReduces));\n          }\n\n          if (submitTime !\u003d null) {\n            jobBeingTraced.setSubmitTime(Long.parseLong(submitTime));\n          }\n\n          if (launchTime !\u003d null) {\n            jobBeingTraced.setLaunchTime(Long.parseLong(launchTime));\n          }\n\n          if (finishTime !\u003d null) {\n            jobBeingTraced.setFinishTime(Long.parseLong(finishTime));\n            if (status !\u003d null) {\n              jobBeingTraced.setOutcome(Pre21JobHistoryConstants.Values\n                  .valueOf(status));\n            }\n\n            maybeMateJobAndConf();\n          }\n        }\n\n        if (jobName !\u003d null) {\n          // we\u0027ll make it java unless the name parses out\n          Matcher m \u003d streamingJobnamePattern.matcher(jobName);\n\n          thisJobType \u003d LoggedJob.JobType.JAVA;\n\n          if (m.matches()) {\n            thisJobType \u003d LoggedJob.JobType.STREAMING;\n          }\n        }\n        if (submitTime !\u003d null) {\n          submitTimeCurrentJob \u003d Long.parseLong(submitTime);\n\n          currentJobID \u003d jobID;\n\n          taskAttemptStartTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptShuffleEndTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptSortEndTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskMapAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n\n          launchTimeCurrentJob \u003d 0L;\n        } else if (launchTime !\u003d null \u0026\u0026 jobID !\u003d null\n            \u0026\u0026 currentJobID.equals(jobID)) {\n          launchTimeCurrentJob \u003d Long.parseLong(launchTime);\n        } else if (finishTime !\u003d null \u0026\u0026 jobID !\u003d null\n            \u0026\u0026 currentJobID.equals(jobID)) {\n          long endTime \u003d Long.parseLong(finishTime);\n\n          if (launchTimeCurrentJob !\u003d 0) {\n            String jobResultText \u003d line.get(\"JOB_STATUS\");\n\n            JobOutcome thisOutcome \u003d\n                ((jobResultText !\u003d null \u0026\u0026 \"SUCCESS\".equals(jobResultText))\n                    ? JobOutcome.SUCCESS : JobOutcome.FAILURE);\n\n            if (submitTimeCurrentJob !\u003d 0L) {\n              canonicalDistributionsEnter(delayTimeDists, thisOutcome,\n                  thisJobType, launchTimeCurrentJob - submitTimeCurrentJob);\n            }\n\n            if (launchTimeCurrentJob !\u003d 0L) {\n              canonicalDistributionsEnter(runTimeDists, thisOutcome,\n                  thisJobType, endTime - launchTimeCurrentJob);\n            }\n\n            // Now we process the hash tables with successful task attempts\n\n            Histogram currentJobMapTimes \u003d new Histogram();\n            Histogram currentJobShuffleTimes \u003d new Histogram();\n            Histogram currentJobSortTimes \u003d new Histogram();\n            Histogram currentJobReduceTimes \u003d new Histogram();\n\n            Iterator\u003cMap.Entry\u003cString, Long\u003e\u003e taskIter \u003d\n                taskAttemptStartTimes.entrySet().iterator();\n\n            while (taskIter.hasNext()) {\n              Map.Entry\u003cString, Long\u003e entry \u003d taskIter.next();\n\n              long startTime \u003d entry.getValue();\n\n              // Map processing\n              Long mapEndTime \u003d taskMapAttemptFinishTimes.get(entry.getKey());\n\n              if (mapEndTime !\u003d null) {\n                currentJobMapTimes.enter(mapEndTime - startTime);\n\n                canonicalDistributionsEnter(mapTimeDists, thisOutcome,\n                    thisJobType, mapEndTime - startTime);\n              }\n\n              // Reduce processing\n              Long shuffleEnd \u003d\n                  taskReduceAttemptShuffleEndTimes.get(entry.getKey());\n              Long sortEnd \u003d taskReduceAttemptSortEndTimes.get(entry.getKey());\n              Long reduceEnd \u003d taskReduceAttemptFinishTimes.get(entry.getKey());\n\n              if (shuffleEnd !\u003d null \u0026\u0026 sortEnd !\u003d null \u0026\u0026 reduceEnd !\u003d null) {\n                currentJobShuffleTimes.enter(shuffleEnd - startTime);\n                currentJobSortTimes.enter(sortEnd - shuffleEnd);\n                currentJobReduceTimes.enter(reduceEnd - sortEnd);\n\n                canonicalDistributionsEnter(shuffleTimeDists, thisOutcome,\n                    thisJobType, shuffleEnd - startTime);\n                canonicalDistributionsEnter(sortTimeDists, thisOutcome,\n                    thisJobType, sortEnd - shuffleEnd);\n                canonicalDistributionsEnter(reduceTimeDists, thisOutcome,\n                    thisJobType, reduceEnd - sortEnd);\n              }\n            }\n\n            // Here we save out the task information\n            incorporateSpread(currentJobMapTimes, mapTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobShuffleTimes, shuffleTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobSortTimes, sortTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobReduceTimes, reduceTimeSpreadDists,\n                thisOutcome, thisJobType);\n          }\n        }\n      }\n    } catch (NumberFormatException e) {\n      LOG.warn(\n          \"HadoopLogsAnalyzer.processJobLine: bad numerical format, at line \"\n              + lineNumber + \".\", e);\n    }\n  }",
      "path": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java",
        "newPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,211 @@\n+  private void processJobLine(ParsedLine line) throws JsonProcessingException,\n+      IOException {\n+    try {\n+      if (version \u003d\u003d 0 || version \u003d\u003d 1) {\n+        // determine the job type if this is the declaration line\n+        String jobID \u003d line.get(\"JOBID\");\n+\n+        String user \u003d line.get(\"USER\");\n+\n+        String jobPriority \u003d line.get(\"JOB_PRIORITY\");\n+\n+        String submitTime \u003d line.get(\"SUBMIT_TIME\");\n+\n+        String jobName \u003d line.get(\"JOBNAME\");\n+\n+        String launchTime \u003d line.get(\"LAUNCH_TIME\");\n+\n+        String finishTime \u003d line.get(\"FINISH_TIME\");\n+\n+        String status \u003d line.get(\"JOB_STATUS\");\n+\n+        String totalMaps \u003d line.get(\"TOTAL_MAPS\");\n+\n+        String totalReduces \u003d line.get(\"TOTAL_REDUCES\");\n+\n+        /*\n+         * If the job appears new [the ID is different from the most recent one,\n+         * if any] we make a new LoggedJob.\n+         */\n+        if (jobID !\u003d null\n+            \u0026\u0026 jobTraceGen !\u003d null\n+            \u0026\u0026 (jobBeingTraced \u003d\u003d null || !jobID.equals(jobBeingTraced\n+                .getJobID()))) {\n+          // push out the old job if there is one, even though it did\u0027t get\n+          // mated\n+          // with a conf.\n+\n+          finalizeJob();\n+\n+          jobBeingTraced \u003d new LoggedJob(jobID);\n+\n+          tasksInCurrentJob \u003d new HashMap\u003cString, LoggedTask\u003e();\n+          attemptsInCurrentJob \u003d new HashMap\u003cString, LoggedTaskAttempt\u003e();\n+\n+          // initialize all the per-job statistics gathering places\n+          successfulMapAttemptTimes \u003d\n+              new Histogram[ParsedHost.numberOfDistances() + 1];\n+          for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n+            successfulMapAttemptTimes[i] \u003d new Histogram();\n+          }\n+\n+          successfulReduceAttemptTimes \u003d new Histogram();\n+          failedMapAttemptTimes \u003d\n+              new Histogram[ParsedHost.numberOfDistances() + 1];\n+          for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n+            failedMapAttemptTimes[i] \u003d new Histogram();\n+          }\n+\n+          failedReduceAttemptTimes \u003d new Histogram();\n+          successfulNthMapperAttempts \u003d new Histogram();\n+          successfulNthReducerAttempts \u003d new Histogram();\n+          mapperLocality \u003d new Histogram();\n+        }\n+\n+        // here we fill in all the stuff the trace might need\n+        if (jobBeingTraced !\u003d null) {\n+          if (user !\u003d null) {\n+            jobBeingTraced.setUser(user);\n+          }\n+\n+          if (jobPriority !\u003d null) {\n+            jobBeingTraced.setPriority(LoggedJob.JobPriority\n+                .valueOf(jobPriority));\n+          }\n+\n+          if (totalMaps !\u003d null) {\n+            jobBeingTraced.setTotalMaps(Integer.parseInt(totalMaps));\n+          }\n+\n+          if (totalReduces !\u003d null) {\n+            jobBeingTraced.setTotalReduces(Integer.parseInt(totalReduces));\n+          }\n+\n+          if (submitTime !\u003d null) {\n+            jobBeingTraced.setSubmitTime(Long.parseLong(submitTime));\n+          }\n+\n+          if (launchTime !\u003d null) {\n+            jobBeingTraced.setLaunchTime(Long.parseLong(launchTime));\n+          }\n+\n+          if (finishTime !\u003d null) {\n+            jobBeingTraced.setFinishTime(Long.parseLong(finishTime));\n+            if (status !\u003d null) {\n+              jobBeingTraced.setOutcome(Pre21JobHistoryConstants.Values\n+                  .valueOf(status));\n+            }\n+\n+            maybeMateJobAndConf();\n+          }\n+        }\n+\n+        if (jobName !\u003d null) {\n+          // we\u0027ll make it java unless the name parses out\n+          Matcher m \u003d streamingJobnamePattern.matcher(jobName);\n+\n+          thisJobType \u003d LoggedJob.JobType.JAVA;\n+\n+          if (m.matches()) {\n+            thisJobType \u003d LoggedJob.JobType.STREAMING;\n+          }\n+        }\n+        if (submitTime !\u003d null) {\n+          submitTimeCurrentJob \u003d Long.parseLong(submitTime);\n+\n+          currentJobID \u003d jobID;\n+\n+          taskAttemptStartTimes \u003d new HashMap\u003cString, Long\u003e();\n+          taskReduceAttemptShuffleEndTimes \u003d new HashMap\u003cString, Long\u003e();\n+          taskReduceAttemptSortEndTimes \u003d new HashMap\u003cString, Long\u003e();\n+          taskMapAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n+          taskReduceAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n+\n+          launchTimeCurrentJob \u003d 0L;\n+        } else if (launchTime !\u003d null \u0026\u0026 jobID !\u003d null\n+            \u0026\u0026 currentJobID.equals(jobID)) {\n+          launchTimeCurrentJob \u003d Long.parseLong(launchTime);\n+        } else if (finishTime !\u003d null \u0026\u0026 jobID !\u003d null\n+            \u0026\u0026 currentJobID.equals(jobID)) {\n+          long endTime \u003d Long.parseLong(finishTime);\n+\n+          if (launchTimeCurrentJob !\u003d 0) {\n+            String jobResultText \u003d line.get(\"JOB_STATUS\");\n+\n+            JobOutcome thisOutcome \u003d\n+                ((jobResultText !\u003d null \u0026\u0026 \"SUCCESS\".equals(jobResultText))\n+                    ? JobOutcome.SUCCESS : JobOutcome.FAILURE);\n+\n+            if (submitTimeCurrentJob !\u003d 0L) {\n+              canonicalDistributionsEnter(delayTimeDists, thisOutcome,\n+                  thisJobType, launchTimeCurrentJob - submitTimeCurrentJob);\n+            }\n+\n+            if (launchTimeCurrentJob !\u003d 0L) {\n+              canonicalDistributionsEnter(runTimeDists, thisOutcome,\n+                  thisJobType, endTime - launchTimeCurrentJob);\n+            }\n+\n+            // Now we process the hash tables with successful task attempts\n+\n+            Histogram currentJobMapTimes \u003d new Histogram();\n+            Histogram currentJobShuffleTimes \u003d new Histogram();\n+            Histogram currentJobSortTimes \u003d new Histogram();\n+            Histogram currentJobReduceTimes \u003d new Histogram();\n+\n+            Iterator\u003cMap.Entry\u003cString, Long\u003e\u003e taskIter \u003d\n+                taskAttemptStartTimes.entrySet().iterator();\n+\n+            while (taskIter.hasNext()) {\n+              Map.Entry\u003cString, Long\u003e entry \u003d taskIter.next();\n+\n+              long startTime \u003d entry.getValue();\n+\n+              // Map processing\n+              Long mapEndTime \u003d taskMapAttemptFinishTimes.get(entry.getKey());\n+\n+              if (mapEndTime !\u003d null) {\n+                currentJobMapTimes.enter(mapEndTime - startTime);\n+\n+                canonicalDistributionsEnter(mapTimeDists, thisOutcome,\n+                    thisJobType, mapEndTime - startTime);\n+              }\n+\n+              // Reduce processing\n+              Long shuffleEnd \u003d\n+                  taskReduceAttemptShuffleEndTimes.get(entry.getKey());\n+              Long sortEnd \u003d taskReduceAttemptSortEndTimes.get(entry.getKey());\n+              Long reduceEnd \u003d taskReduceAttemptFinishTimes.get(entry.getKey());\n+\n+              if (shuffleEnd !\u003d null \u0026\u0026 sortEnd !\u003d null \u0026\u0026 reduceEnd !\u003d null) {\n+                currentJobShuffleTimes.enter(shuffleEnd - startTime);\n+                currentJobSortTimes.enter(sortEnd - shuffleEnd);\n+                currentJobReduceTimes.enter(reduceEnd - sortEnd);\n+\n+                canonicalDistributionsEnter(shuffleTimeDists, thisOutcome,\n+                    thisJobType, shuffleEnd - startTime);\n+                canonicalDistributionsEnter(sortTimeDists, thisOutcome,\n+                    thisJobType, sortEnd - shuffleEnd);\n+                canonicalDistributionsEnter(reduceTimeDists, thisOutcome,\n+                    thisJobType, reduceEnd - sortEnd);\n+              }\n+            }\n+\n+            // Here we save out the task information\n+            incorporateSpread(currentJobMapTimes, mapTimeSpreadDists,\n+                thisOutcome, thisJobType);\n+            incorporateSpread(currentJobShuffleTimes, shuffleTimeSpreadDists,\n+                thisOutcome, thisJobType);\n+            incorporateSpread(currentJobSortTimes, sortTimeSpreadDists,\n+                thisOutcome, thisJobType);\n+            incorporateSpread(currentJobReduceTimes, reduceTimeSpreadDists,\n+                thisOutcome, thisJobType);\n+          }\n+        }\n+      }\n+    } catch (NumberFormatException e) {\n+      LOG.warn(\n+          \"HadoopLogsAnalyzer.processJobLine: bad numerical format, at line \"\n+              + lineNumber + \".\", e);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void processJobLine(ParsedLine line) throws JsonProcessingException,\n      IOException {\n    try {\n      if (version \u003d\u003d 0 || version \u003d\u003d 1) {\n        // determine the job type if this is the declaration line\n        String jobID \u003d line.get(\"JOBID\");\n\n        String user \u003d line.get(\"USER\");\n\n        String jobPriority \u003d line.get(\"JOB_PRIORITY\");\n\n        String submitTime \u003d line.get(\"SUBMIT_TIME\");\n\n        String jobName \u003d line.get(\"JOBNAME\");\n\n        String launchTime \u003d line.get(\"LAUNCH_TIME\");\n\n        String finishTime \u003d line.get(\"FINISH_TIME\");\n\n        String status \u003d line.get(\"JOB_STATUS\");\n\n        String totalMaps \u003d line.get(\"TOTAL_MAPS\");\n\n        String totalReduces \u003d line.get(\"TOTAL_REDUCES\");\n\n        /*\n         * If the job appears new [the ID is different from the most recent one,\n         * if any] we make a new LoggedJob.\n         */\n        if (jobID !\u003d null\n            \u0026\u0026 jobTraceGen !\u003d null\n            \u0026\u0026 (jobBeingTraced \u003d\u003d null || !jobID.equals(jobBeingTraced\n                .getJobID()))) {\n          // push out the old job if there is one, even though it did\u0027t get\n          // mated\n          // with a conf.\n\n          finalizeJob();\n\n          jobBeingTraced \u003d new LoggedJob(jobID);\n\n          tasksInCurrentJob \u003d new HashMap\u003cString, LoggedTask\u003e();\n          attemptsInCurrentJob \u003d new HashMap\u003cString, LoggedTaskAttempt\u003e();\n\n          // initialize all the per-job statistics gathering places\n          successfulMapAttemptTimes \u003d\n              new Histogram[ParsedHost.numberOfDistances() + 1];\n          for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n            successfulMapAttemptTimes[i] \u003d new Histogram();\n          }\n\n          successfulReduceAttemptTimes \u003d new Histogram();\n          failedMapAttemptTimes \u003d\n              new Histogram[ParsedHost.numberOfDistances() + 1];\n          for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n            failedMapAttemptTimes[i] \u003d new Histogram();\n          }\n\n          failedReduceAttemptTimes \u003d new Histogram();\n          successfulNthMapperAttempts \u003d new Histogram();\n          successfulNthReducerAttempts \u003d new Histogram();\n          mapperLocality \u003d new Histogram();\n        }\n\n        // here we fill in all the stuff the trace might need\n        if (jobBeingTraced !\u003d null) {\n          if (user !\u003d null) {\n            jobBeingTraced.setUser(user);\n          }\n\n          if (jobPriority !\u003d null) {\n            jobBeingTraced.setPriority(LoggedJob.JobPriority\n                .valueOf(jobPriority));\n          }\n\n          if (totalMaps !\u003d null) {\n            jobBeingTraced.setTotalMaps(Integer.parseInt(totalMaps));\n          }\n\n          if (totalReduces !\u003d null) {\n            jobBeingTraced.setTotalReduces(Integer.parseInt(totalReduces));\n          }\n\n          if (submitTime !\u003d null) {\n            jobBeingTraced.setSubmitTime(Long.parseLong(submitTime));\n          }\n\n          if (launchTime !\u003d null) {\n            jobBeingTraced.setLaunchTime(Long.parseLong(launchTime));\n          }\n\n          if (finishTime !\u003d null) {\n            jobBeingTraced.setFinishTime(Long.parseLong(finishTime));\n            if (status !\u003d null) {\n              jobBeingTraced.setOutcome(Pre21JobHistoryConstants.Values\n                  .valueOf(status));\n            }\n\n            maybeMateJobAndConf();\n          }\n        }\n\n        if (jobName !\u003d null) {\n          // we\u0027ll make it java unless the name parses out\n          Matcher m \u003d streamingJobnamePattern.matcher(jobName);\n\n          thisJobType \u003d LoggedJob.JobType.JAVA;\n\n          if (m.matches()) {\n            thisJobType \u003d LoggedJob.JobType.STREAMING;\n          }\n        }\n        if (submitTime !\u003d null) {\n          submitTimeCurrentJob \u003d Long.parseLong(submitTime);\n\n          currentJobID \u003d jobID;\n\n          taskAttemptStartTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptShuffleEndTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptSortEndTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskMapAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n          taskReduceAttemptFinishTimes \u003d new HashMap\u003cString, Long\u003e();\n\n          launchTimeCurrentJob \u003d 0L;\n        } else if (launchTime !\u003d null \u0026\u0026 jobID !\u003d null\n            \u0026\u0026 currentJobID.equals(jobID)) {\n          launchTimeCurrentJob \u003d Long.parseLong(launchTime);\n        } else if (finishTime !\u003d null \u0026\u0026 jobID !\u003d null\n            \u0026\u0026 currentJobID.equals(jobID)) {\n          long endTime \u003d Long.parseLong(finishTime);\n\n          if (launchTimeCurrentJob !\u003d 0) {\n            String jobResultText \u003d line.get(\"JOB_STATUS\");\n\n            JobOutcome thisOutcome \u003d\n                ((jobResultText !\u003d null \u0026\u0026 \"SUCCESS\".equals(jobResultText))\n                    ? JobOutcome.SUCCESS : JobOutcome.FAILURE);\n\n            if (submitTimeCurrentJob !\u003d 0L) {\n              canonicalDistributionsEnter(delayTimeDists, thisOutcome,\n                  thisJobType, launchTimeCurrentJob - submitTimeCurrentJob);\n            }\n\n            if (launchTimeCurrentJob !\u003d 0L) {\n              canonicalDistributionsEnter(runTimeDists, thisOutcome,\n                  thisJobType, endTime - launchTimeCurrentJob);\n            }\n\n            // Now we process the hash tables with successful task attempts\n\n            Histogram currentJobMapTimes \u003d new Histogram();\n            Histogram currentJobShuffleTimes \u003d new Histogram();\n            Histogram currentJobSortTimes \u003d new Histogram();\n            Histogram currentJobReduceTimes \u003d new Histogram();\n\n            Iterator\u003cMap.Entry\u003cString, Long\u003e\u003e taskIter \u003d\n                taskAttemptStartTimes.entrySet().iterator();\n\n            while (taskIter.hasNext()) {\n              Map.Entry\u003cString, Long\u003e entry \u003d taskIter.next();\n\n              long startTime \u003d entry.getValue();\n\n              // Map processing\n              Long mapEndTime \u003d taskMapAttemptFinishTimes.get(entry.getKey());\n\n              if (mapEndTime !\u003d null) {\n                currentJobMapTimes.enter(mapEndTime - startTime);\n\n                canonicalDistributionsEnter(mapTimeDists, thisOutcome,\n                    thisJobType, mapEndTime - startTime);\n              }\n\n              // Reduce processing\n              Long shuffleEnd \u003d\n                  taskReduceAttemptShuffleEndTimes.get(entry.getKey());\n              Long sortEnd \u003d taskReduceAttemptSortEndTimes.get(entry.getKey());\n              Long reduceEnd \u003d taskReduceAttemptFinishTimes.get(entry.getKey());\n\n              if (shuffleEnd !\u003d null \u0026\u0026 sortEnd !\u003d null \u0026\u0026 reduceEnd !\u003d null) {\n                currentJobShuffleTimes.enter(shuffleEnd - startTime);\n                currentJobSortTimes.enter(sortEnd - shuffleEnd);\n                currentJobReduceTimes.enter(reduceEnd - sortEnd);\n\n                canonicalDistributionsEnter(shuffleTimeDists, thisOutcome,\n                    thisJobType, shuffleEnd - startTime);\n                canonicalDistributionsEnter(sortTimeDists, thisOutcome,\n                    thisJobType, sortEnd - shuffleEnd);\n                canonicalDistributionsEnter(reduceTimeDists, thisOutcome,\n                    thisJobType, reduceEnd - sortEnd);\n              }\n            }\n\n            // Here we save out the task information\n            incorporateSpread(currentJobMapTimes, mapTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobShuffleTimes, shuffleTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobSortTimes, sortTimeSpreadDists,\n                thisOutcome, thisJobType);\n            incorporateSpread(currentJobReduceTimes, reduceTimeSpreadDists,\n                thisOutcome, thisJobType);\n          }\n        }\n      }\n    } catch (NumberFormatException e) {\n      LOG.warn(\n          \"HadoopLogsAnalyzer.processJobLine: bad numerical format, at line \"\n              + lineNumber + \".\", e);\n    }\n  }",
      "path": "mapreduce/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java"
    }
  }
}