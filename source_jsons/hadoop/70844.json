{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PageBlobOutputStream.java",
  "functionName": "writeInternal",
  "functionId": "writeInternal___data-byte[](modifiers-final)__offset-int__length-int",
  "sourceFilePath": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobOutputStream.java",
  "functionStartLine": 541,
  "functionEndLine": 562,
  "numCommitsSeen": 8,
  "timeTaken": 1005,
  "changeHistory": [
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40"
  ],
  "changeHistoryShort": {
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40": "Yintroduced"
  },
  "changeHistoryDetails": {
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-10809. hadoop-azure: page blob support. Contributed by Dexter Bradshaw, Mostafa Elhemali, Eric Hanson, and Mike Liddell.\n",
      "commitDate": "08/10/14 2:20 PM",
      "commitName": "2217e2f8ff418b88eac6ad36cafe3a9795a11f40",
      "commitAuthor": "cnauroth",
      "diff": "@@ -0,0 +1,22 @@\n+  private synchronized void writeInternal(final byte[] data, int offset,\n+      int length) throws IOException {\n+    while (length \u003e 0) {\n+      checkStreamState();\n+      final int availableBufferBytes \u003d MAX_DATA_BYTES_PER_REQUEST\n+          - this.outBuffer.size();\n+      final int nextWrite \u003d Math.min(availableBufferBytes, length);\n+\n+      outBuffer.write(data, offset, nextWrite);\n+      offset +\u003d nextWrite;\n+      length -\u003d nextWrite;\n+\n+      if (outBuffer.size() \u003e MAX_DATA_BYTES_PER_REQUEST) {\n+        throw new RuntimeException(\"Internal error: maximum write size \" +\n+            Integer.toString(MAX_DATA_BYTES_PER_REQUEST) + \"exceeded.\");\n+      }\n+\n+      if (outBuffer.size() \u003d\u003d MAX_DATA_BYTES_PER_REQUEST) {\n+        flushIOBuffers();\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void writeInternal(final byte[] data, int offset,\n      int length) throws IOException {\n    while (length \u003e 0) {\n      checkStreamState();\n      final int availableBufferBytes \u003d MAX_DATA_BYTES_PER_REQUEST\n          - this.outBuffer.size();\n      final int nextWrite \u003d Math.min(availableBufferBytes, length);\n\n      outBuffer.write(data, offset, nextWrite);\n      offset +\u003d nextWrite;\n      length -\u003d nextWrite;\n\n      if (outBuffer.size() \u003e MAX_DATA_BYTES_PER_REQUEST) {\n        throw new RuntimeException(\"Internal error: maximum write size \" +\n            Integer.toString(MAX_DATA_BYTES_PER_REQUEST) + \"exceeded.\");\n      }\n\n      if (outBuffer.size() \u003d\u003d MAX_DATA_BYTES_PER_REQUEST) {\n        flushIOBuffers();\n      }\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobOutputStream.java"
    }
  }
}