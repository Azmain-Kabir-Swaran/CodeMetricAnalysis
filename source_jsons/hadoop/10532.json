{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BPServiceActor.java",
  "functionName": "cacheReport",
  "functionId": "cacheReport",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
  "functionStartLine": 469,
  "functionEndLine": 499,
  "numCommitsSeen": 125,
  "timeTaken": 3425,
  "changeHistory": [
    "528378784fe14e7069dd0471f3c4c478544b57c8",
    "ed72daa5df97669906234e8ac9a406d78136b206",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
    "950e0644b79e0c0514dd036dcf19b9645df0982f",
    "916ab9286b6006571649d21c74d9ae70273a3ddc",
    "9673baa7e8b43fa6300080f72ebce0189ea775e5"
  ],
  "changeHistoryShort": {
    "528378784fe14e7069dd0471f3c4c478544b57c8": "Ybodychange",
    "ed72daa5df97669906234e8ac9a406d78136b206": "Ybodychange",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14": "Ybodychange",
    "950e0644b79e0c0514dd036dcf19b9645df0982f": "Ybodychange",
    "916ab9286b6006571649d21c74d9ae70273a3ddc": "Ybodychange",
    "9673baa7e8b43fa6300080f72ebce0189ea775e5": "Ybodychange"
  },
  "changeHistoryDetails": {
    "528378784fe14e7069dd0471f3c4c478544b57c8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12012. Fix spelling mistakes in BPServiceActor.java. Contributed by chencan.\n",
      "commitDate": "17/08/19 4:37 AM",
      "commitName": "528378784fe14e7069dd0471f3c4c478544b57c8",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "05/05/19 4:03 AM",
      "commitNameOld": "69b903bbd8e2dafac6b2cb1d748ea666b6f877cf",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 104.02,
      "commitsBetweenForRepo": 830,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   DatanodeCommand cacheReport() throws IOException {\n     // If caching is disabled, do not send a cache report\n     if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n       return null;\n     }\n     // send cache report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     final long startTime \u003d monotonicNow();\n     if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Sending cacheReport from service actor: \" + this);\n       }\n       lastCacheReport \u003d startTime;\n \n       String bpid \u003d bpos.getBlockPoolId();\n       List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n       long createTime \u003d monotonicNow();\n \n       cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n       long sendTime \u003d monotonicNow();\n       long createCost \u003d createTime - startTime;\n       long sendCost \u003d sendTime - createTime;\n       dn.getMetrics().addCacheReport(sendCost);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"CacheReport of \" + blockIds.size()\n-            + \" block(s) took \" + createCost + \" msec to generate and \"\n+            + \" block(s) took \" + createCost + \" msecs to generate and \"\n             + sendCost + \" msecs for RPC and NN processing\");\n       }\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand cacheReport() throws IOException {\n    // If caching is disabled, do not send a cache report\n    if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n      return null;\n    }\n    // send cache report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    final long startTime \u003d monotonicNow();\n    if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Sending cacheReport from service actor: \" + this);\n      }\n      lastCacheReport \u003d startTime;\n\n      String bpid \u003d bpos.getBlockPoolId();\n      List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n      long createTime \u003d monotonicNow();\n\n      cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n      long sendTime \u003d monotonicNow();\n      long createCost \u003d createTime - startTime;\n      long sendCost \u003d sendTime - createTime;\n      dn.getMetrics().addCacheReport(sendCost);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"CacheReport of \" + blockIds.size()\n            + \" block(s) took \" + createCost + \" msecs to generate and \"\n            + sendCost + \" msecs for RPC and NN processing\");\n      }\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "ed72daa5df97669906234e8ac9a406d78136b206": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7978. Add LOG.isDebugEnabled() guard for some LOG.debug(..). Contributed by Walter Su.\n",
      "commitDate": "01/04/15 12:54 PM",
      "commitName": "ed72daa5df97669906234e8ac9a406d78136b206",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/03/15 3:25 PM",
      "commitNameOld": "1a495fbb489c9e9a23b341a52696d10e9e272b04",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 1.9,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,31 @@\n   DatanodeCommand cacheReport() throws IOException {\n     // If caching is disabled, do not send a cache report\n     if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n       return null;\n     }\n     // send cache report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     final long startTime \u003d monotonicNow();\n     if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Sending cacheReport from service actor: \" + this);\n       }\n       lastCacheReport \u003d startTime;\n \n       String bpid \u003d bpos.getBlockPoolId();\n       List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n       long createTime \u003d monotonicNow();\n \n       cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n       long sendTime \u003d monotonicNow();\n       long createCost \u003d createTime - startTime;\n       long sendCost \u003d sendTime - createTime;\n       dn.getMetrics().addCacheReport(sendCost);\n-      LOG.debug(\"CacheReport of \" + blockIds.size()\n-          + \" block(s) took \" + createCost + \" msec to generate and \"\n-          + sendCost + \" msecs for RPC and NN processing\");\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"CacheReport of \" + blockIds.size()\n+            + \" block(s) took \" + createCost + \" msec to generate and \"\n+            + sendCost + \" msecs for RPC and NN processing\");\n+      }\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand cacheReport() throws IOException {\n    // If caching is disabled, do not send a cache report\n    if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n      return null;\n    }\n    // send cache report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    final long startTime \u003d monotonicNow();\n    if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Sending cacheReport from service actor: \" + this);\n      }\n      lastCacheReport \u003d startTime;\n\n      String bpid \u003d bpos.getBlockPoolId();\n      List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n      long createTime \u003d monotonicNow();\n\n      cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n      long sendTime \u003d monotonicNow();\n      long createCost \u003d createTime - startTime;\n      long sendCost \u003d sendTime - createTime;\n      dn.getMetrics().addCacheReport(sendCost);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"CacheReport of \" + blockIds.size()\n            + \" block(s) took \" + createCost + \" msec to generate and \"\n            + sendCost + \" msecs for RPC and NN processing\");\n      }\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "13/03/15 12:23 PM",
      "commitNameOld": "d324164a51a43d72c02567248bd9f0f12b244a40",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 6.99,
      "commitsBetweenForRepo": 79,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   DatanodeCommand cacheReport() throws IOException {\n     // If caching is disabled, do not send a cache report\n     if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n       return null;\n     }\n     // send cache report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n-    final long startTime \u003d Time.monotonicNow();\n+    final long startTime \u003d monotonicNow();\n     if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Sending cacheReport from service actor: \" + this);\n       }\n       lastCacheReport \u003d startTime;\n \n       String bpid \u003d bpos.getBlockPoolId();\n       List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n-      long createTime \u003d Time.monotonicNow();\n+      long createTime \u003d monotonicNow();\n \n       cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n-      long sendTime \u003d Time.monotonicNow();\n+      long sendTime \u003d monotonicNow();\n       long createCost \u003d createTime - startTime;\n       long sendCost \u003d sendTime - createTime;\n       dn.getMetrics().addCacheReport(sendCost);\n       LOG.debug(\"CacheReport of \" + blockIds.size()\n           + \" block(s) took \" + createCost + \" msec to generate and \"\n           + sendCost + \" msecs for RPC and NN processing\");\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand cacheReport() throws IOException {\n    // If caching is disabled, do not send a cache report\n    if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n      return null;\n    }\n    // send cache report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    final long startTime \u003d monotonicNow();\n    if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Sending cacheReport from service actor: \" + this);\n      }\n      lastCacheReport \u003d startTime;\n\n      String bpid \u003d bpos.getBlockPoolId();\n      List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n      long createTime \u003d monotonicNow();\n\n      cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n      long sendTime \u003d monotonicNow();\n      long createCost \u003d createTime - startTime;\n      long sendCost \u003d sendTime - createTime;\n      dn.getMetrics().addCacheReport(sendCost);\n      LOG.debug(\"CacheReport of \" + blockIds.size()\n          + \" block(s) took \" + createCost + \" msec to generate and \"\n          + sendCost + \" msecs for RPC and NN processing\");\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5153. Datanode should send block reports for each storage in a separate message. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563254 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/01/14 1:00 PM",
      "commitName": "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "15/12/13 4:58 PM",
      "commitNameOld": "938565925adb9d866e8c6951361cd5582076e013",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 46.83,
      "commitsBetweenForRepo": 246,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   DatanodeCommand cacheReport() throws IOException {\n     // If caching is disabled, do not send a cache report\n     if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n       return null;\n     }\n     // send cache report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n-    long startTime \u003d Time.monotonicNow();\n+    final long startTime \u003d Time.monotonicNow();\n     if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Sending cacheReport from service actor: \" + this);\n       }\n       lastCacheReport \u003d startTime;\n \n       String bpid \u003d bpos.getBlockPoolId();\n       List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n       long createTime \u003d Time.monotonicNow();\n \n       cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n       long sendTime \u003d Time.monotonicNow();\n       long createCost \u003d createTime - startTime;\n       long sendCost \u003d sendTime - createTime;\n       dn.getMetrics().addCacheReport(sendCost);\n       LOG.debug(\"CacheReport of \" + blockIds.size()\n           + \" block(s) took \" + createCost + \" msec to generate and \"\n           + sendCost + \" msecs for RPC and NN processing\");\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand cacheReport() throws IOException {\n    // If caching is disabled, do not send a cache report\n    if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n      return null;\n    }\n    // send cache report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    final long startTime \u003d Time.monotonicNow();\n    if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Sending cacheReport from service actor: \" + this);\n      }\n      lastCacheReport \u003d startTime;\n\n      String bpid \u003d bpos.getBlockPoolId();\n      List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n      long createTime \u003d Time.monotonicNow();\n\n      cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n      long sendTime \u003d Time.monotonicNow();\n      long createCost \u003d createTime - startTime;\n      long sendCost \u003d sendTime - createTime;\n      dn.getMetrics().addCacheReport(sendCost);\n      LOG.debug(\"CacheReport of \" + blockIds.size()\n          + \" block(s) took \" + createCost + \" msec to generate and \"\n          + sendCost + \" msecs for RPC and NN processing\");\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "950e0644b79e0c0514dd036dcf19b9645df0982f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5626. dfsadmin report shows incorrect values (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548000 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/12/13 6:59 PM",
      "commitName": "950e0644b79e0c0514dd036dcf19b9645df0982f",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "20/11/13 1:31 PM",
      "commitNameOld": "916ab9286b6006571649d21c74d9ae70273a3ddc",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 14.23,
      "commitsBetweenForRepo": 66,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   DatanodeCommand cacheReport() throws IOException {\n     // If caching is disabled, do not send a cache report\n     if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n       return null;\n     }\n     // send cache report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     long startTime \u003d Time.monotonicNow();\n     if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Sending cacheReport from service actor: \" + this);\n       }\n       lastCacheReport \u003d startTime;\n \n       String bpid \u003d bpos.getBlockPoolId();\n       List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n       long createTime \u003d Time.monotonicNow();\n \n       cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n       long sendTime \u003d Time.monotonicNow();\n       long createCost \u003d createTime - startTime;\n       long sendCost \u003d sendTime - createTime;\n       dn.getMetrics().addCacheReport(sendCost);\n-      LOG.info(\"CacheReport of \" + blockIds.size()\n+      LOG.debug(\"CacheReport of \" + blockIds.size()\n           + \" block(s) took \" + createCost + \" msec to generate and \"\n           + sendCost + \" msecs for RPC and NN processing\");\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand cacheReport() throws IOException {\n    // If caching is disabled, do not send a cache report\n    if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n      return null;\n    }\n    // send cache report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d Time.monotonicNow();\n    if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Sending cacheReport from service actor: \" + this);\n      }\n      lastCacheReport \u003d startTime;\n\n      String bpid \u003d bpos.getBlockPoolId();\n      List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n      long createTime \u003d Time.monotonicNow();\n\n      cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n      long sendTime \u003d Time.monotonicNow();\n      long createCost \u003d createTime - startTime;\n      long sendCost \u003d sendTime - createTime;\n      dn.getMetrics().addCacheReport(sendCost);\n      LOG.debug(\"CacheReport of \" + blockIds.size()\n          + \" block(s) took \" + createCost + \" msec to generate and \"\n          + sendCost + \" msecs for RPC and NN processing\");\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "916ab9286b6006571649d21c74d9ae70273a3ddc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5451. Add byte and file statistics to PathBasedCacheEntry. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543958 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/11/13 1:31 PM",
      "commitName": "916ab9286b6006571649d21c74d9ae70273a3ddc",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "20/11/13 8:27 AM",
      "commitNameOld": "04cf2a768c0fb1c2c5c80d2480aa072ec7e43c3f",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.21,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   DatanodeCommand cacheReport() throws IOException {\n     // If caching is disabled, do not send a cache report\n     if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n       return null;\n     }\n     // send cache report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     long startTime \u003d Time.monotonicNow();\n     if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Sending cacheReport from service actor: \" + this);\n       }\n       lastCacheReport \u003d startTime;\n \n       String bpid \u003d bpos.getBlockPoolId();\n       List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n       long createTime \u003d Time.monotonicNow();\n \n       cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n       long sendTime \u003d Time.monotonicNow();\n       long createCost \u003d createTime - startTime;\n       long sendCost \u003d sendTime - createTime;\n       dn.getMetrics().addCacheReport(sendCost);\n       LOG.info(\"CacheReport of \" + blockIds.size()\n-          + \" blocks took \" + createCost + \" msec to generate and \"\n+          + \" block(s) took \" + createCost + \" msec to generate and \"\n           + sendCost + \" msecs for RPC and NN processing\");\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand cacheReport() throws IOException {\n    // If caching is disabled, do not send a cache report\n    if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n      return null;\n    }\n    // send cache report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d Time.monotonicNow();\n    if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Sending cacheReport from service actor: \" + this);\n      }\n      lastCacheReport \u003d startTime;\n\n      String bpid \u003d bpos.getBlockPoolId();\n      List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n      long createTime \u003d Time.monotonicNow();\n\n      cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n      long sendTime \u003d Time.monotonicNow();\n      long createCost \u003d createTime - startTime;\n      long sendCost \u003d sendTime - createTime;\n      dn.getMetrics().addCacheReport(sendCost);\n      LOG.info(\"CacheReport of \" + blockIds.size()\n          + \" block(s) took \" + createCost + \" msec to generate and \"\n          + sendCost + \" msecs for RPC and NN processing\");\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    },
    "9673baa7e8b43fa6300080f72ebce0189ea775e5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5320. Add datanode caching metrics. Contributed by Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1540796 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/11/13 10:30 AM",
      "commitName": "9673baa7e8b43fa6300080f72ebce0189ea775e5",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "21/10/13 12:29 PM",
      "commitNameOld": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 20.96,
      "commitsBetweenForRepo": 85,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   DatanodeCommand cacheReport() throws IOException {\n     // If caching is disabled, do not send a cache report\n-    if (dn.getFSDataset().getDnCacheCapacity() \u003d\u003d 0) {\n+    if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n       return null;\n     }\n     // send cache report if timer has expired.\n     DatanodeCommand cmd \u003d null;\n     long startTime \u003d Time.monotonicNow();\n     if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Sending cacheReport from service actor: \" + this);\n       }\n       lastCacheReport \u003d startTime;\n \n       String bpid \u003d bpos.getBlockPoolId();\n       List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n       long createTime \u003d Time.monotonicNow();\n \n       cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n       long sendTime \u003d Time.monotonicNow();\n       long createCost \u003d createTime - startTime;\n       long sendCost \u003d sendTime - createTime;\n       dn.getMetrics().addCacheReport(sendCost);\n       LOG.info(\"CacheReport of \" + blockIds.size()\n           + \" blocks took \" + createCost + \" msec to generate and \"\n           + sendCost + \" msecs for RPC and NN processing\");\n     }\n     return cmd;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeCommand cacheReport() throws IOException {\n    // If caching is disabled, do not send a cache report\n    if (dn.getFSDataset().getCacheCapacity() \u003d\u003d 0) {\n      return null;\n    }\n    // send cache report if timer has expired.\n    DatanodeCommand cmd \u003d null;\n    long startTime \u003d Time.monotonicNow();\n    if (startTime - lastCacheReport \u003e dnConf.cacheReportInterval) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Sending cacheReport from service actor: \" + this);\n      }\n      lastCacheReport \u003d startTime;\n\n      String bpid \u003d bpos.getBlockPoolId();\n      List\u003cLong\u003e blockIds \u003d dn.getFSDataset().getCacheReport(bpid);\n      long createTime \u003d Time.monotonicNow();\n\n      cmd \u003d bpNamenode.cacheReport(bpRegistration, bpid, blockIds);\n      long sendTime \u003d Time.monotonicNow();\n      long createCost \u003d createTime - startTime;\n      long sendCost \u003d sendTime - createTime;\n      dn.getMetrics().addCacheReport(sendCost);\n      LOG.info(\"CacheReport of \" + blockIds.size()\n          + \" blocks took \" + createCost + \" msec to generate and \"\n          + sendCost + \" msecs for RPC and NN processing\");\n    }\n    return cmd;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
      "extendedDetails": {}
    }
  }
}