{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "getStorageReports",
  "functionId": "getStorageReports___bpid-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 159,
  "functionEndLine": 182,
  "numCommitsSeen": 197,
  "timeTaken": 7691,
  "changeHistory": [
    "bb8a6eea52cb1e2c3d0b7f8b49a1bab9e4255acd",
    "5f23abfa30ea29a5474513c463b4d462c0e824ee",
    "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c",
    "3b173d95171d01ab55042b1162569d1cf14a8d43",
    "f8a9329f2b8e768fe6730fc05436e973344b9132"
  ],
  "changeHistoryShort": {
    "bb8a6eea52cb1e2c3d0b7f8b49a1bab9e4255acd": "Ybodychange",
    "5f23abfa30ea29a5474513c463b4d462c0e824ee": "Ybodychange",
    "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1": "Ybodychange",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": "Ybodychange",
    "3b173d95171d01ab55042b1162569d1cf14a8d43": "Ybodychange",
    "f8a9329f2b8e768fe6730fc05436e973344b9132": "Ybodychange"
  },
  "changeHistoryDetails": {
    "bb8a6eea52cb1e2c3d0b7f8b49a1bab9e4255acd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7060. Avoid taking locks when sending heartbeats from the DataNode. Contributed by Jiandan Yang.\n",
      "commitDate": "07/11/17 6:22 PM",
      "commitName": "bb8a6eea52cb1e2c3d0b7f8b49a1bab9e4255acd",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "25/09/17 9:25 AM",
      "commitNameOld": "02e2a9b1152b0e144fcf43bec2fce26d8a6c6dbc",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 43.41,
      "commitsBetweenForRepo": 405,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public StorageReport[] getStorageReports(String bpid)\n       throws IOException {\n     List\u003cStorageReport\u003e reports;\n-    synchronized (statsLock) {\n-      List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n-      reports \u003d new ArrayList\u003c\u003e(curVolumes.size());\n-      for (FsVolumeImpl volume : curVolumes) {\n-        try (FsVolumeReference ref \u003d volume.obtainReference()) {\n-          StorageReport sr \u003d new StorageReport(volume.toDatanodeStorage(),\n-              false,\n-              volume.getCapacity(),\n-              volume.getDfsUsed(),\n-              volume.getAvailable(),\n-              volume.getBlockPoolUsed(bpid),\n-              volume.getNonDfsUsed());\n-          reports.add(sr);\n-        } catch (ClosedChannelException e) {\n-          continue;\n-        }\n+    // Volumes are the references from a copy-on-write snapshot, so the\n+    // access on the volume metrics doesn\u0027t require an additional lock.\n+    List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n+    reports \u003d new ArrayList\u003c\u003e(curVolumes.size());\n+    for (FsVolumeImpl volume : curVolumes) {\n+      try (FsVolumeReference ref \u003d volume.obtainReference()) {\n+        StorageReport sr \u003d new StorageReport(volume.toDatanodeStorage(),\n+            false,\n+            volume.getCapacity(),\n+            volume.getDfsUsed(),\n+            volume.getAvailable(),\n+            volume.getBlockPoolUsed(bpid),\n+            volume.getNonDfsUsed());\n+        reports.add(sr);\n+      } catch (ClosedChannelException e) {\n+        continue;\n       }\n     }\n \n     return reports.toArray(new StorageReport[reports.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public StorageReport[] getStorageReports(String bpid)\n      throws IOException {\n    List\u003cStorageReport\u003e reports;\n    // Volumes are the references from a copy-on-write snapshot, so the\n    // access on the volume metrics doesn\u0027t require an additional lock.\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n    reports \u003d new ArrayList\u003c\u003e(curVolumes.size());\n    for (FsVolumeImpl volume : curVolumes) {\n      try (FsVolumeReference ref \u003d volume.obtainReference()) {\n        StorageReport sr \u003d new StorageReport(volume.toDatanodeStorage(),\n            false,\n            volume.getCapacity(),\n            volume.getDfsUsed(),\n            volume.getAvailable(),\n            volume.getBlockPoolUsed(bpid),\n            volume.getNonDfsUsed());\n        reports.add(sr);\n      } catch (ClosedChannelException e) {\n        continue;\n      }\n    }\n\n    return reports.toArray(new StorageReport[reports.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "5f23abfa30ea29a5474513c463b4d462c0e824ee": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9038. DFS reserved space is erroneously counted towards non-DFS used. (Brahma Reddy Battula)\n",
      "commitDate": "06/09/16 1:37 PM",
      "commitName": "5f23abfa30ea29a5474513c463b4d462c0e824ee",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "02/09/16 3:33 PM",
      "commitNameOld": "07650bc37a3c78ecc6566d813778d0954d0b06b0",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 3.92,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   public StorageReport[] getStorageReports(String bpid)\n       throws IOException {\n     List\u003cStorageReport\u003e reports;\n     synchronized (statsLock) {\n       List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n       reports \u003d new ArrayList\u003c\u003e(curVolumes.size());\n       for (FsVolumeImpl volume : curVolumes) {\n         try (FsVolumeReference ref \u003d volume.obtainReference()) {\n           StorageReport sr \u003d new StorageReport(volume.toDatanodeStorage(),\n               false,\n               volume.getCapacity(),\n               volume.getDfsUsed(),\n               volume.getAvailable(),\n-              volume.getBlockPoolUsed(bpid));\n+              volume.getBlockPoolUsed(bpid),\n+              volume.getNonDfsUsed());\n           reports.add(sr);\n         } catch (ClosedChannelException e) {\n           continue;\n         }\n       }\n     }\n \n     return reports.toArray(new StorageReport[reports.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public StorageReport[] getStorageReports(String bpid)\n      throws IOException {\n    List\u003cStorageReport\u003e reports;\n    synchronized (statsLock) {\n      List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n      reports \u003d new ArrayList\u003c\u003e(curVolumes.size());\n      for (FsVolumeImpl volume : curVolumes) {\n        try (FsVolumeReference ref \u003d volume.obtainReference()) {\n          StorageReport sr \u003d new StorageReport(volume.toDatanodeStorage(),\n              false,\n              volume.getCapacity(),\n              volume.getDfsUsed(),\n              volume.getAvailable(),\n              volume.getBlockPoolUsed(bpid),\n              volume.getNonDfsUsed());\n          reports.add(sr);\n        } catch (ClosedChannelException e) {\n          continue;\n        }\n      }\n    }\n\n    return reports.toArray(new StorageReport[reports.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7758. Retire FsDatasetSpi#getVolumes() and use FsDatasetSpi#getVolumeRefs() instead (Lei (Eddy) Xu via Colin P. McCabe)\n",
      "commitDate": "05/05/15 11:08 AM",
      "commitName": "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.05,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public StorageReport[] getStorageReports(String bpid)\n       throws IOException {\n     List\u003cStorageReport\u003e reports;\n     synchronized (statsLock) {\n-      List\u003cFsVolumeImpl\u003e curVolumes \u003d getVolumes();\n+      List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n       reports \u003d new ArrayList\u003c\u003e(curVolumes.size());\n       for (FsVolumeImpl volume : curVolumes) {\n         try (FsVolumeReference ref \u003d volume.obtainReference()) {\n           StorageReport sr \u003d new StorageReport(volume.toDatanodeStorage(),\n               false,\n               volume.getCapacity(),\n               volume.getDfsUsed(),\n               volume.getAvailable(),\n               volume.getBlockPoolUsed(bpid));\n           reports.add(sr);\n         } catch (ClosedChannelException e) {\n           continue;\n         }\n       }\n     }\n \n     return reports.toArray(new StorageReport[reports.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public StorageReport[] getStorageReports(String bpid)\n      throws IOException {\n    List\u003cStorageReport\u003e reports;\n    synchronized (statsLock) {\n      List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n      reports \u003d new ArrayList\u003c\u003e(curVolumes.size());\n      for (FsVolumeImpl volume : curVolumes) {\n        try (FsVolumeReference ref \u003d volume.obtainReference()) {\n          StorageReport sr \u003d new StorageReport(volume.toDatanodeStorage(),\n              false,\n              volume.getCapacity(),\n              volume.getDfsUsed(),\n              volume.getAvailable(),\n              volume.getBlockPoolUsed(bpid));\n          reports.add(sr);\n        } catch (ClosedChannelException e) {\n          continue;\n        }\n      }\n    }\n\n    return reports.toArray(new StorageReport[reports.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)\n",
      "commitDate": "20/01/15 7:05 PM",
      "commitName": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "13/01/15 12:24 AM",
      "commitNameOld": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 7.78,
      "commitsBetweenForRepo": 49,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,23 @@\n   public StorageReport[] getStorageReports(String bpid)\n       throws IOException {\n-    StorageReport[] reports;\n+    List\u003cStorageReport\u003e reports;\n     synchronized (statsLock) {\n       List\u003cFsVolumeImpl\u003e curVolumes \u003d getVolumes();\n-      reports \u003d new StorageReport[curVolumes.size()];\n-      int i \u003d 0;\n+      reports \u003d new ArrayList\u003c\u003e(curVolumes.size());\n       for (FsVolumeImpl volume : curVolumes) {\n-        reports[i++] \u003d new StorageReport(volume.toDatanodeStorage(),\n-                                         false,\n-                                         volume.getCapacity(),\n-                                         volume.getDfsUsed(),\n-                                         volume.getAvailable(),\n-                                         volume.getBlockPoolUsed(bpid));\n+        try (FsVolumeReference ref \u003d volume.obtainReference()) {\n+          StorageReport sr \u003d new StorageReport(volume.toDatanodeStorage(),\n+              false,\n+              volume.getCapacity(),\n+              volume.getDfsUsed(),\n+              volume.getAvailable(),\n+              volume.getBlockPoolUsed(bpid));\n+          reports.add(sr);\n+        } catch (ClosedChannelException e) {\n+          continue;\n+        }\n       }\n     }\n \n-    return reports;\n+    return reports.toArray(new StorageReport[reports.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public StorageReport[] getStorageReports(String bpid)\n      throws IOException {\n    List\u003cStorageReport\u003e reports;\n    synchronized (statsLock) {\n      List\u003cFsVolumeImpl\u003e curVolumes \u003d getVolumes();\n      reports \u003d new ArrayList\u003c\u003e(curVolumes.size());\n      for (FsVolumeImpl volume : curVolumes) {\n        try (FsVolumeReference ref \u003d volume.obtainReference()) {\n          StorageReport sr \u003d new StorageReport(volume.toDatanodeStorage(),\n              false,\n              volume.getCapacity(),\n              volume.getDfsUsed(),\n              volume.getAvailable(),\n              volume.getBlockPoolUsed(bpid));\n          reports.add(sr);\n        } catch (ClosedChannelException e) {\n          continue;\n        }\n      }\n    }\n\n    return reports.toArray(new StorageReport[reports.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "3b173d95171d01ab55042b1162569d1cf14a8d43": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7531. Improve the concurrent access on FsVolumeList (Lei Xu via Colin P. McCabe)\n",
      "commitDate": "17/12/14 4:41 PM",
      "commitName": "3b173d95171d01ab55042b1162569d1cf14a8d43",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "11/12/14 12:36 PM",
      "commitNameOld": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.17,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,19 @@\n   public StorageReport[] getStorageReports(String bpid)\n       throws IOException {\n     StorageReport[] reports;\n     synchronized (statsLock) {\n-      reports \u003d new StorageReport[volumes.volumes.size()];\n+      List\u003cFsVolumeImpl\u003e curVolumes \u003d getVolumes();\n+      reports \u003d new StorageReport[curVolumes.size()];\n       int i \u003d 0;\n-      for (FsVolumeImpl volume : volumes.volumes) {\n+      for (FsVolumeImpl volume : curVolumes) {\n         reports[i++] \u003d new StorageReport(volume.toDatanodeStorage(),\n                                          false,\n                                          volume.getCapacity(),\n                                          volume.getDfsUsed(),\n                                          volume.getAvailable(),\n                                          volume.getBlockPoolUsed(bpid));\n       }\n     }\n \n     return reports;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public StorageReport[] getStorageReports(String bpid)\n      throws IOException {\n    StorageReport[] reports;\n    synchronized (statsLock) {\n      List\u003cFsVolumeImpl\u003e curVolumes \u003d getVolumes();\n      reports \u003d new StorageReport[curVolumes.size()];\n      int i \u003d 0;\n      for (FsVolumeImpl volume : curVolumes) {\n        reports[i++] \u003d new StorageReport(volume.toDatanodeStorage(),\n                                         false,\n                                         volume.getCapacity(),\n                                         volume.getDfsUsed(),\n                                         volume.getAvailable(),\n                                         volume.getBlockPoolUsed(bpid));\n      }\n    }\n\n    return reports;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "f8a9329f2b8e768fe6730fc05436e973344b9132": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5667. Include DatanodeStorage in StorageReport. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1555929 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/01/14 9:28 AM",
      "commitName": "f8a9329f2b8e768fe6730fc05436e973344b9132",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "11/12/13 11:01 PM",
      "commitNameOld": "fba994ffe20d387e8ed875e727fc3d93f7097101",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 25.44,
      "commitsBetweenForRepo": 98,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public StorageReport[] getStorageReports(String bpid)\n       throws IOException {\n     StorageReport[] reports;\n     synchronized (statsLock) {\n       reports \u003d new StorageReport[volumes.volumes.size()];\n       int i \u003d 0;\n       for (FsVolumeImpl volume : volumes.volumes) {\n-        reports[i++] \u003d new StorageReport(volume.getStorageID(),\n+        reports[i++] \u003d new StorageReport(volume.toDatanodeStorage(),\n                                          false,\n                                          volume.getCapacity(),\n                                          volume.getDfsUsed(),\n                                          volume.getAvailable(),\n                                          volume.getBlockPoolUsed(bpid));\n       }\n     }\n \n     return reports;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public StorageReport[] getStorageReports(String bpid)\n      throws IOException {\n    StorageReport[] reports;\n    synchronized (statsLock) {\n      reports \u003d new StorageReport[volumes.volumes.size()];\n      int i \u003d 0;\n      for (FsVolumeImpl volume : volumes.volumes) {\n        reports[i++] \u003d new StorageReport(volume.toDatanodeStorage(),\n                                         false,\n                                         volume.getCapacity(),\n                                         volume.getDfsUsed(),\n                                         volume.getAvailable(),\n                                         volume.getBlockPoolUsed(bpid));\n      }\n    }\n\n    return reports;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    }
  }
}