{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DBNameNodeConnector.java",
  "functionName": "getVolumeInfoFromStorageReports",
  "functionId": "getVolumeInfoFromStorageReports___node-DiskBalancerDataNode__reports-StorageReport[]",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/DBNameNodeConnector.java",
  "functionStartLine": 131,
  "functionEndLine": 159,
  "numCommitsSeen": 4,
  "timeTaken": 1001,
  "changeHistory": [
    "121e1e1280c7b019f6d2cc3ba9eae1ead0dd8408",
    "64ccb232ccf204991a28fa0211917fa935ad30c5",
    "30c6ebd69919a477a582e599fb253ffe5c2982e1"
  ],
  "changeHistoryShort": {
    "121e1e1280c7b019f6d2cc3ba9eae1ead0dd8408": "Ybodychange",
    "64ccb232ccf204991a28fa0211917fa935ad30c5": "Ybodychange",
    "30c6ebd69919a477a582e599fb253ffe5c2982e1": "Yintroduced"
  },
  "changeHistoryDetails": {
    "121e1e1280c7b019f6d2cc3ba9eae1ead0dd8408": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13175. Add more information for checking argument in DiskBalancerVolume.\nContributed by  Lei (Eddy) Xu.\n",
      "commitDate": "20/02/18 7:16 PM",
      "commitName": "121e1e1280c7b019f6d2cc3ba9eae1ead0dd8408",
      "commitAuthor": "Anu Engineer",
      "commitDateOld": "23/06/16 6:21 PM",
      "commitNameOld": "64ccb232ccf204991a28fa0211917fa935ad30c5",
      "commitAuthorOld": "Anu Engineer",
      "daysBetweenCommits": 607.08,
      "commitsBetweenForRepo": 4049,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,29 @@\n   private void getVolumeInfoFromStorageReports(DiskBalancerDataNode node,\n                                                StorageReport[] reports)\n       throws Exception {\n     Preconditions.checkNotNull(node);\n     Preconditions.checkNotNull(reports);\n     for (StorageReport report : reports) {\n       DatanodeStorage storage \u003d report.getStorage();\n       DiskBalancerVolume volume \u003d new DiskBalancerVolume();\n       volume.setCapacity(report.getCapacity());\n       volume.setFailed(report.isFailed());\n       volume.setUsed(report.getDfsUsed());\n \n       // TODO : Should we do BlockPool level balancing at all ?\n       // Does it make sense ? Balancer does do that. Right now\n       // we only deal with volumes and not blockPools\n \n-      volume.setUsed(report.getDfsUsed());\n-\n       volume.setUuid(storage.getStorageID());\n \n       // we will skip this volume for disk balancer if\n       // it is read-only since we will not be able to delete\n       // or if it is already failed.\n       volume.setSkip((storage.getState() \u003d\u003d DatanodeStorage.State\n           .READ_ONLY_SHARED) || report.isFailed());\n       volume.setStorageType(storage.getStorageType().name());\n       volume.setIsTransient(storage.getStorageType().isTransient());\n       node.addVolume(volume);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void getVolumeInfoFromStorageReports(DiskBalancerDataNode node,\n                                               StorageReport[] reports)\n      throws Exception {\n    Preconditions.checkNotNull(node);\n    Preconditions.checkNotNull(reports);\n    for (StorageReport report : reports) {\n      DatanodeStorage storage \u003d report.getStorage();\n      DiskBalancerVolume volume \u003d new DiskBalancerVolume();\n      volume.setCapacity(report.getCapacity());\n      volume.setFailed(report.isFailed());\n      volume.setUsed(report.getDfsUsed());\n\n      // TODO : Should we do BlockPool level balancing at all ?\n      // Does it make sense ? Balancer does do that. Right now\n      // we only deal with volumes and not blockPools\n\n      volume.setUuid(storage.getStorageID());\n\n      // we will skip this volume for disk balancer if\n      // it is read-only since we will not be able to delete\n      // or if it is already failed.\n      volume.setSkip((storage.getState() \u003d\u003d DatanodeStorage.State\n          .READ_ONLY_SHARED) || report.isFailed());\n      volume.setStorageType(storage.getStorageType().name());\n      volume.setIsTransient(storage.getStorageType().isTransient());\n      node.addVolume(volume);\n    }\n\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/DBNameNodeConnector.java",
      "extendedDetails": {}
    },
    "64ccb232ccf204991a28fa0211917fa935ad30c5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10478. DiskBalancer: resolve volume path names. Contributed by Anu Engineer.\n",
      "commitDate": "23/06/16 6:21 PM",
      "commitName": "64ccb232ccf204991a28fa0211917fa935ad30c5",
      "commitAuthor": "Anu Engineer",
      "commitDateOld": "23/06/16 6:18 PM",
      "commitNameOld": "747227e9dea10ac6b5f601b7cf4dcc418b10d9c8",
      "commitAuthorOld": "Anu Engineer",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,31 @@\n   private void getVolumeInfoFromStorageReports(DiskBalancerDataNode node,\n                                                StorageReport[] reports)\n       throws Exception {\n     Preconditions.checkNotNull(node);\n     Preconditions.checkNotNull(reports);\n     for (StorageReport report : reports) {\n       DatanodeStorage storage \u003d report.getStorage();\n       DiskBalancerVolume volume \u003d new DiskBalancerVolume();\n       volume.setCapacity(report.getCapacity());\n       volume.setFailed(report.isFailed());\n       volume.setUsed(report.getDfsUsed());\n \n       // TODO : Should we do BlockPool level balancing at all ?\n       // Does it make sense ? Balancer does do that. Right now\n       // we only deal with volumes and not blockPools\n \n       volume.setUsed(report.getDfsUsed());\n \n       volume.setUuid(storage.getStorageID());\n \n       // we will skip this volume for disk balancer if\n       // it is read-only since we will not be able to delete\n       // or if it is already failed.\n       volume.setSkip((storage.getState() \u003d\u003d DatanodeStorage.State\n           .READ_ONLY_SHARED) || report.isFailed());\n       volume.setStorageType(storage.getStorageType().name());\n       volume.setIsTransient(storage.getStorageType().isTransient());\n-      //volume.setPath(storage.getVolumePath());\n       node.addVolume(volume);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void getVolumeInfoFromStorageReports(DiskBalancerDataNode node,\n                                               StorageReport[] reports)\n      throws Exception {\n    Preconditions.checkNotNull(node);\n    Preconditions.checkNotNull(reports);\n    for (StorageReport report : reports) {\n      DatanodeStorage storage \u003d report.getStorage();\n      DiskBalancerVolume volume \u003d new DiskBalancerVolume();\n      volume.setCapacity(report.getCapacity());\n      volume.setFailed(report.isFailed());\n      volume.setUsed(report.getDfsUsed());\n\n      // TODO : Should we do BlockPool level balancing at all ?\n      // Does it make sense ? Balancer does do that. Right now\n      // we only deal with volumes and not blockPools\n\n      volume.setUsed(report.getDfsUsed());\n\n      volume.setUuid(storage.getStorageID());\n\n      // we will skip this volume for disk balancer if\n      // it is read-only since we will not be able to delete\n      // or if it is already failed.\n      volume.setSkip((storage.getState() \u003d\u003d DatanodeStorage.State\n          .READ_ONLY_SHARED) || report.isFailed());\n      volume.setStorageType(storage.getStorageType().name());\n      volume.setIsTransient(storage.getStorageType().isTransient());\n      node.addVolume(volume);\n    }\n\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/DBNameNodeConnector.java",
      "extendedDetails": {}
    },
    "30c6ebd69919a477a582e599fb253ffe5c2982e1": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9449. DiskBalancer: Add connectors. Contributed by Anu Engineer\n",
      "commitDate": "23/06/16 6:18 PM",
      "commitName": "30c6ebd69919a477a582e599fb253ffe5c2982e1",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "diff": "@@ -0,0 +1,32 @@\n+  private void getVolumeInfoFromStorageReports(DiskBalancerDataNode node,\n+                                               StorageReport[] reports)\n+      throws Exception {\n+    Preconditions.checkNotNull(node);\n+    Preconditions.checkNotNull(reports);\n+    for (StorageReport report : reports) {\n+      DatanodeStorage storage \u003d report.getStorage();\n+      DiskBalancerVolume volume \u003d new DiskBalancerVolume();\n+      volume.setCapacity(report.getCapacity());\n+      volume.setFailed(report.isFailed());\n+      volume.setUsed(report.getDfsUsed());\n+\n+      // TODO : Should we do BlockPool level balancing at all ?\n+      // Does it make sense ? Balancer does do that. Right now\n+      // we only deal with volumes and not blockPools\n+\n+      volume.setUsed(report.getDfsUsed());\n+\n+      volume.setUuid(storage.getStorageID());\n+\n+      // we will skip this volume for disk balancer if\n+      // it is read-only since we will not be able to delete\n+      // or if it is already failed.\n+      volume.setSkip((storage.getState() \u003d\u003d DatanodeStorage.State\n+          .READ_ONLY_SHARED) || report.isFailed());\n+      volume.setStorageType(storage.getStorageType().name());\n+      volume.setIsTransient(storage.getStorageType().isTransient());\n+      //volume.setPath(storage.getVolumePath());\n+      node.addVolume(volume);\n+    }\n+\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void getVolumeInfoFromStorageReports(DiskBalancerDataNode node,\n                                               StorageReport[] reports)\n      throws Exception {\n    Preconditions.checkNotNull(node);\n    Preconditions.checkNotNull(reports);\n    for (StorageReport report : reports) {\n      DatanodeStorage storage \u003d report.getStorage();\n      DiskBalancerVolume volume \u003d new DiskBalancerVolume();\n      volume.setCapacity(report.getCapacity());\n      volume.setFailed(report.isFailed());\n      volume.setUsed(report.getDfsUsed());\n\n      // TODO : Should we do BlockPool level balancing at all ?\n      // Does it make sense ? Balancer does do that. Right now\n      // we only deal with volumes and not blockPools\n\n      volume.setUsed(report.getDfsUsed());\n\n      volume.setUuid(storage.getStorageID());\n\n      // we will skip this volume for disk balancer if\n      // it is read-only since we will not be able to delete\n      // or if it is already failed.\n      volume.setSkip((storage.getState() \u003d\u003d DatanodeStorage.State\n          .READ_ONLY_SHARED) || report.isFailed());\n      volume.setStorageType(storage.getStorageType().name());\n      volume.setIsTransient(storage.getStorageType().isTransient());\n      //volume.setPath(storage.getVolumePath());\n      node.addVolume(volume);\n    }\n\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/DBNameNodeConnector.java"
    }
  }
}