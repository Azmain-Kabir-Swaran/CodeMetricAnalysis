{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Dispatcher.java",
  "functionName": "dispatch",
  "functionId": "dispatch",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
  "functionStartLine": 351,
  "functionEndLine": 431,
  "numCommitsSeen": 110,
  "timeTaken": 14202,
  "changeHistory": [
    "c966a3837af1c1a1c4a441f491b0d76d5c9e5d78",
    "a3954ccab148bddc290cb96528e63ff19799bcc9",
    "2f73396b5901fd5fe29f6cd76fc1b3134b854b37",
    "e24a923db50879f7dbe5d2afac0e6757089fb07d",
    "f6367c5f44a88cb5eb7edffb015b10b657504a61",
    "74b3dd514c86b46197e2e19d9824a423715cab30",
    "1037ee580f87e6bf13155834c36f26794381678b",
    "3aac4758b007a56e3d66998d457b2156effca528",
    "b56daff6a186599764b046248565918b894ec116",
    "9ef03a4c5bb5573eadc7d04e371c4af2dc6bae37",
    "673280df24f0228bf01777035ceeab8807da8c40",
    "4da8490b512a33a255ed27309860859388d7c168",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "ae71a671a3b4b454aa393c2974b6f1f16dd61405",
    "0d85f7e59146cc3e9a040c2203995f3efd8ed4eb",
    "a26aa6bd0716da89853566961390d711511084e3",
    "195961a7c1da86421761162836766b1de07930fd",
    "6554994fab2d8a2a139fb71ed54be144f4057e08",
    "471b1368e2a81b4d9850f0f4d98d31df1451354c",
    "e60673697d5046c29c52bbabdfe80506f99773e4",
    "c3cf331dc91e2beef2afeed11105084843b02858",
    "83b9933db3349e6a6faf23bce35c9d4ce3f7bcf2",
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54",
    "3b54223c0f32d42a84436c670d80b791a8e9696d",
    "6b2e615f5fa034d679be0de8fb300b878a2d801a",
    "b4c41f341bb7ffff6a428b7e9557591e7685e4f0",
    "1c602f719833dae48e2bea63b98f76bbdc5a65b4",
    "907fb15ee8c150e5ecc0560b7374441c57a84122",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "e4df14f8f151413a8ec0972a21e31a0b51fe0fb0",
    "be7dd8333a7e56e732171db0781786987de03195",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "c966a3837af1c1a1c4a441f491b0d76d5c9e5d78": "Ybodychange",
    "a3954ccab148bddc290cb96528e63ff19799bcc9": "Ybodychange",
    "2f73396b5901fd5fe29f6cd76fc1b3134b854b37": "Ybodychange",
    "e24a923db50879f7dbe5d2afac0e6757089fb07d": "Ybodychange",
    "f6367c5f44a88cb5eb7edffb015b10b657504a61": "Ybodychange",
    "74b3dd514c86b46197e2e19d9824a423715cab30": "Ybodychange",
    "1037ee580f87e6bf13155834c36f26794381678b": "Ybodychange",
    "3aac4758b007a56e3d66998d457b2156effca528": "Ybodychange",
    "b56daff6a186599764b046248565918b894ec116": "Ybodychange",
    "9ef03a4c5bb5573eadc7d04e371c4af2dc6bae37": "Ybodychange",
    "673280df24f0228bf01777035ceeab8807da8c40": "Ybodychange",
    "4da8490b512a33a255ed27309860859388d7c168": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "ae71a671a3b4b454aa393c2974b6f1f16dd61405": "Ybodychange",
    "0d85f7e59146cc3e9a040c2203995f3efd8ed4eb": "Ybodychange",
    "a26aa6bd0716da89853566961390d711511084e3": "Ybodychange",
    "195961a7c1da86421761162836766b1de07930fd": "Ybodychange",
    "6554994fab2d8a2a139fb71ed54be144f4057e08": "Ybodychange",
    "471b1368e2a81b4d9850f0f4d98d31df1451354c": "Ybodychange",
    "e60673697d5046c29c52bbabdfe80506f99773e4": "Ybodychange",
    "c3cf331dc91e2beef2afeed11105084843b02858": "Ymultichange(Ymovefromfile,Ybodychange)",
    "83b9933db3349e6a6faf23bce35c9d4ce3f7bcf2": "Ybodychange",
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d": "Ybodychange",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": "Ybodychange",
    "3b54223c0f32d42a84436c670d80b791a8e9696d": "Ybodychange",
    "6b2e615f5fa034d679be0de8fb300b878a2d801a": "Ybodychange",
    "b4c41f341bb7ffff6a428b7e9557591e7685e4f0": "Ybodychange",
    "1c602f719833dae48e2bea63b98f76bbdc5a65b4": "Ybodychange",
    "907fb15ee8c150e5ecc0560b7374441c57a84122": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ybodychange",
    "e4df14f8f151413a8ec0972a21e31a0b51fe0fb0": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c966a3837af1c1a1c4a441f491b0d76d5c9e5d78": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13174. hdfs mover -p /path times out after 20 min. Contributed by Istvan Fajth.\n",
      "commitDate": "15/06/18 1:36 PM",
      "commitName": "c966a3837af1c1a1c4a441f491b0d76d5c9e5d78",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "07/03/18 11:27 AM",
      "commitNameOld": "88fba00caa8c8e26f70deb9be5b534e7482620a1",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 100.05,
      "commitsBetweenForRepo": 1371,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,81 @@\n     private void dispatch() {\n-      LOG.info(\"Start moving \" + this);\n-      assert !(reportedBlock instanceof DBlockStriped);\n-\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n+        if (source.isIterationOver()){\n+          LOG.info(\"Cancel moving \" + this +\n+              \" as iteration is already cancelled due to\" +\n+              \" dfs.balancer.max-iteration-time is passed.\");\n+          throw new IOException(\"Block move cancelled.\");\n+        }\n+        LOG.info(\"Start moving \" + this);\n+        assert !(reportedBlock instanceof DBlockStriped);\n+\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().\n                 getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                 HdfsConstants.READ_TIMEOUT);\n \n         // Set read timeout so that it doesn\u0027t hang forever against\n         // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n         // twice within the client read timeout period (every 30 seconds by\n         // default). Here, we make it give up after 5 minutes of no response.\n         sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             reportedBlock.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb,\n             new StorageType[]{target.storageType}, new String[0]);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n         target.getDDatanode().setHasSuccess();\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this, e);\n         target.getDDatanode().setHasFailure();\n         // Check that the failure is due to block pinning errors.\n         if (e instanceof BlockPinningException) {\n           // Pinned block can\u0027t be moved. Add this block into failure list.\n           // Later in the next iteration mover will exclude these blocks from\n           // pending moves.\n           target.getDDatanode().addBlockPinningFailures(this);\n           return;\n         }\n \n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        if (source.isIterationOver()){\n          LOG.info(\"Cancel moving \" + this +\n              \" as iteration is already cancelled due to\" +\n              \" dfs.balancer.max-iteration-time is passed.\");\n          throw new IOException(\"Block move cancelled.\");\n        }\n        LOG.info(\"Start moving \" + this);\n        assert !(reportedBlock instanceof DBlockStriped);\n\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().\n                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                HdfsConstants.READ_TIMEOUT);\n\n        // Set read timeout so that it doesn\u0027t hang forever against\n        // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n        // twice within the client read timeout period (every 30 seconds by\n        // default). Here, we make it give up after 5 minutes of no response.\n        sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb,\n            new StorageType[]{target.storageType}, new String[0]);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this, e);\n        target.getDDatanode().setHasFailure();\n        // Check that the failure is due to block pinning errors.\n        if (e instanceof BlockPinningException) {\n          // Pinned block can\u0027t be moved. Add this block into failure list.\n          // Later in the next iteration mover will exclude these blocks from\n          // pending moves.\n          target.getDDatanode().addBlockPinningFailures(this);\n          return;\n        }\n\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "a3954ccab148bddc290cb96528e63ff19799bcc9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
      "commitDate": "05/05/17 12:01 PM",
      "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "26/04/17 5:28 PM",
      "commitNameOld": "28eb2aabebd15c15a357d86e23ca407d3c85211c",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 8.77,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,75 @@\n     private void dispatch() {\n       LOG.info(\"Start moving \" + this);\n       assert !(reportedBlock instanceof DBlockStriped);\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().\n                 getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                 HdfsConstants.READ_TIMEOUT);\n \n         // Set read timeout so that it doesn\u0027t hang forever against\n         // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n         // twice within the client read timeout period (every 30 seconds by\n         // default). Here, we make it give up after 5 minutes of no response.\n         sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             reportedBlock.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb,\n-            new StorageType[]{target.storageType});\n+            new StorageType[]{target.storageType}, new String[0]);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n         target.getDDatanode().setHasSuccess();\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this, e);\n         target.getDDatanode().setHasFailure();\n         // Check that the failure is due to block pinning errors.\n         if (e instanceof BlockPinningException) {\n           // Pinned block can\u0027t be moved. Add this block into failure list.\n           // Later in the next iteration mover will exclude these blocks from\n           // pending moves.\n           target.getDDatanode().addBlockPinningFailures(this);\n           return;\n         }\n \n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n      assert !(reportedBlock instanceof DBlockStriped);\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().\n                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                HdfsConstants.READ_TIMEOUT);\n\n        // Set read timeout so that it doesn\u0027t hang forever against\n        // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n        // twice within the client read timeout period (every 30 seconds by\n        // default). Here, we make it give up after 5 minutes of no response.\n        sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb,\n            new StorageType[]{target.storageType}, new String[0]);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this, e);\n        target.getDDatanode().setHasFailure();\n        // Check that the failure is due to block pinning errors.\n        if (e instanceof BlockPinningException) {\n          // Pinned block can\u0027t be moved. Add this block into failure list.\n          // Later in the next iteration mover will exclude these blocks from\n          // pending moves.\n          target.getDDatanode().addBlockPinningFailures(this);\n          return;\n        }\n\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "2f73396b5901fd5fe29f6cd76fc1b3134b854b37": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6708. StorageType should be encoded in the block token. Contributed by Ewan Higgs\n",
      "commitDate": "25/04/17 11:57 PM",
      "commitName": "2f73396b5901fd5fe29f6cd76fc1b3134b854b37",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "05/02/17 9:15 PM",
      "commitNameOld": "9cbbd1eae893b21212c9bc9e6745c6859317a667",
      "commitAuthorOld": "Yiqun Lin",
      "daysBetweenCommits": 79.07,
      "commitsBetweenForRepo": 455,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,75 @@\n     private void dispatch() {\n       LOG.info(\"Start moving \" + this);\n       assert !(reportedBlock instanceof DBlockStriped);\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().\n                 getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                 HdfsConstants.READ_TIMEOUT);\n \n         // Set read timeout so that it doesn\u0027t hang forever against\n         // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n         // twice within the client read timeout period (every 30 seconds by\n         // default). Here, we make it give up after 5 minutes of no response.\n         sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             reportedBlock.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n-        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n+        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb,\n+            new StorageType[]{target.storageType});\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n         target.getDDatanode().setHasSuccess();\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this, e);\n         target.getDDatanode().setHasFailure();\n         // Check that the failure is due to block pinning errors.\n         if (e instanceof BlockPinningException) {\n           // Pinned block can\u0027t be moved. Add this block into failure list.\n           // Later in the next iteration mover will exclude these blocks from\n           // pending moves.\n           target.getDDatanode().addBlockPinningFailures(this);\n           return;\n         }\n \n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n      assert !(reportedBlock instanceof DBlockStriped);\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().\n                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                HdfsConstants.READ_TIMEOUT);\n\n        // Set read timeout so that it doesn\u0027t hang forever against\n        // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n        // twice within the client read timeout period (every 30 seconds by\n        // default). Here, we make it give up after 5 minutes of no response.\n        sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb,\n            new StorageType[]{target.storageType});\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this, e);\n        target.getDDatanode().setHasFailure();\n        // Check that the failure is due to block pinning errors.\n        if (e instanceof BlockPinningException) {\n          // Pinned block can\u0027t be moved. Add this block into failure list.\n          // Later in the next iteration mover will exclude these blocks from\n          // pending moves.\n          target.getDDatanode().addBlockPinningFailures(this);\n          return;\n        }\n\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "e24a923db50879f7dbe5d2afac0e6757089fb07d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11164: Mover should avoid unnecessary retries if the block is pinned. Contributed by Rakesh R\n",
      "commitDate": "13/12/16 5:09 PM",
      "commitName": "e24a923db50879f7dbe5d2afac0e6757089fb07d",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "21/11/16 8:13 AM",
      "commitNameOld": "49a09179e3fadae090126261be0a7fe0aa48798e",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 22.37,
      "commitsBetweenForRepo": 143,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,74 @@\n     private void dispatch() {\n       LOG.info(\"Start moving \" + this);\n       assert !(reportedBlock instanceof DBlockStriped);\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().\n                 getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                 HdfsConstants.READ_TIMEOUT);\n \n         // Set read timeout so that it doesn\u0027t hang forever against\n         // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n         // twice within the client read timeout period (every 30 seconds by\n         // default). Here, we make it give up after 5 minutes of no response.\n         sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             reportedBlock.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n         target.getDDatanode().setHasSuccess();\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this, e);\n         target.getDDatanode().setHasFailure();\n+        // Check that the failure is due to block pinning errors.\n+        if (e instanceof BlockPinningException) {\n+          // Pinned block can\u0027t be moved. Add this block into failure list.\n+          // Later in the next iteration mover will exclude these blocks from\n+          // pending moves.\n+          target.getDDatanode().addBlockPinningFailures(this);\n+          return;\n+        }\n+\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n      assert !(reportedBlock instanceof DBlockStriped);\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().\n                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                HdfsConstants.READ_TIMEOUT);\n\n        // Set read timeout so that it doesn\u0027t hang forever against\n        // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n        // twice within the client read timeout period (every 30 seconds by\n        // default). Here, we make it give up after 5 minutes of no response.\n        sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this, e);\n        target.getDDatanode().setHasFailure();\n        // Check that the failure is due to block pinning errors.\n        if (e instanceof BlockPinningException) {\n          // Pinned block can\u0027t be moved. Add this block into failure list.\n          // Later in the next iteration mover will exclude these blocks from\n          // pending moves.\n          target.getDDatanode().addBlockPinningFailures(this);\n          return;\n        }\n\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "f6367c5f44a88cb5eb7edffb015b10b657504a61": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11015. Enforce timeout in balancer. Contributed by Kihwal Lee.\n",
      "commitDate": "25/10/16 10:19 AM",
      "commitName": "f6367c5f44a88cb5eb7edffb015b10b657504a61",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "17/10/16 5:45 PM",
      "commitNameOld": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthorOld": "Ming Ma",
      "daysBetweenCommits": 7.69,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,65 @@\n     private void dispatch() {\n       LOG.info(\"Start moving \" + this);\n       assert !(reportedBlock instanceof DBlockStriped);\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().\n                 getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                 HdfsConstants.READ_TIMEOUT);\n \n+        // Set read timeout so that it doesn\u0027t hang forever against\n+        // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n+        // twice within the client read timeout period (every 30 seconds by\n+        // default). Here, we make it give up after 5 minutes of no response.\n+        sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             reportedBlock.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n         target.getDDatanode().setHasSuccess();\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this, e);\n         target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n      assert !(reportedBlock instanceof DBlockStriped);\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().\n                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                HdfsConstants.READ_TIMEOUT);\n\n        // Set read timeout so that it doesn\u0027t hang forever against\n        // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n        // twice within the client read timeout period (every 30 seconds by\n        // default). Here, we make it give up after 5 minutes of no response.\n        sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this, e);\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "74b3dd514c86b46197e2e19d9824a423715cab30": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10876. Dispatcher#dispatch should log IOException stacktrace. Contributed by Manoj Govindassamy.\n",
      "commitDate": "23/09/16 1:26 PM",
      "commitName": "74b3dd514c86b46197e2e19d9824a423715cab30",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "04/08/16 9:45 AM",
      "commitNameOld": "8c9b44eaffb776e71a41358864cb5c28ecd8bfec",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 50.15,
      "commitsBetweenForRepo": 305,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n     private void dispatch() {\n       LOG.info(\"Start moving \" + this);\n       assert !(reportedBlock instanceof DBlockStriped);\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().\n                 getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                 HdfsConstants.READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             reportedBlock.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n         target.getDDatanode().setHasSuccess();\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n-        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n+        LOG.warn(\"Failed to move \" + this, e);\n         target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n      assert !(reportedBlock instanceof DBlockStriped);\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().\n                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                HdfsConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this, e);\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "1037ee580f87e6bf13155834c36f26794381678b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9100. HDFS Balancer does not respect dfs.client.use.datanode.hostname. Contributed by Casey Brotherton.\n",
      "commitDate": "02/10/15 12:00 PM",
      "commitName": "1037ee580f87e6bf13155834c36f26794381678b",
      "commitAuthor": "Yongjun Zhang",
      "commitDateOld": "29/09/15 1:39 AM",
      "commitNameOld": "8fd55202468b28422b0df888641c9b08906fe4a7",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.43,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,60 @@\n     private void dispatch() {\n       LOG.info(\"Start moving \" + this);\n       assert !(reportedBlock instanceof DBlockStriped);\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n-            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n-            HdfsConstants.READ_TIMEOUT);\n+            NetUtils.createSocketAddr(target.getDatanodeInfo().\n+                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n+                HdfsConstants.READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             reportedBlock.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n         target.getDDatanode().setHasSuccess();\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n      assert !(reportedBlock instanceof DBlockStriped);\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().\n                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                HdfsConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "3aac4758b007a56e3d66998d457b2156effca528": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8803. Move DfsClientConf to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "19/08/15 11:28 AM",
      "commitName": "3aac4758b007a56e3d66998d457b2156effca528",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "18/08/15 7:25 PM",
      "commitNameOld": "7ecbfd44aa57f5f54c214b7fdedda2500be76f51",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 0.67,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,58 @@\n     private void dispatch() {\n       LOG.info(\"Start moving \" + this);\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n-            HdfsServerConstants.READ_TIMEOUT);\n+            HdfsConstants.READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(block.getNumBytes());\n         target.getDDatanode().setHasSuccess();\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(block.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "b56daff6a186599764b046248565918b894ec116": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8818. Changes the global moveExecutor to per datanode executors and changes MAX_SIZE_TO_MOVE to be configurable.\n",
      "commitDate": "10/08/15 4:52 PM",
      "commitName": "b56daff6a186599764b046248565918b894ec116",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "13/07/15 3:12 PM",
      "commitNameOld": "9ef03a4c5bb5573eadc7d04e371c4af2dc6bae37",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 28.07,
      "commitsBetweenForRepo": 150,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,58 @@\n     private void dispatch() {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Start moving \" + this);\n-      }\n+      LOG.info(\"Start moving \" + this);\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(block.getNumBytes());\n         target.getDDatanode().setHasSuccess();\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(block.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "9ef03a4c5bb5573eadc7d04e371c4af2dc6bae37": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8541. Mover should exit with NO_MOVE_PROGRESS if there is no move progress.  Contributed by Surendra Singh Lilhore\n",
      "commitDate": "13/07/15 3:12 PM",
      "commitName": "9ef03a4c5bb5573eadc7d04e371c4af2dc6bae37",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "05/05/15 3:41 PM",
      "commitNameOld": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 68.98,
      "commitsBetweenForRepo": 565,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,60 @@\n     private void dispatch() {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Start moving \" + this);\n       }\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(block.getNumBytes());\n+        target.getDDatanode().setHasSuccess();\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(block.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "673280df24f0228bf01777035ceeab8807da8c40": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7621. Erasure Coding: update the Balancer/Mover data migration logic. Contributed by Walter Su.\n",
      "commitDate": "03/06/15 11:51 AM",
      "commitName": "673280df24f0228bf01777035ceeab8807da8c40",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "05/05/15 3:41 PM",
      "commitNameOld": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 28.84,
      "commitsBetweenForRepo": 380,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,60 @@\n     private void dispatch() {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Start moving \" + this);\n       }\n+      assert !(reportedBlock instanceof DBlockStriped);\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n-            block.getBlock());\n+            reportedBlock.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n-        nnc.getBytesMoved().addAndGet(block.getNumBytes());\n+        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n      assert !(reportedBlock instanceof DBlockStriped);\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "4da8490b512a33a255ed27309860859388d7c168": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
      "commitDate": "05/05/15 3:41 PM",
      "commitName": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.23,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,59 @@\n     private void dispatch() {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Start moving \" + this);\n       }\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-            HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n+            ioFileBufferSize));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n-            HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n+            ioFileBufferSize));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/04/15 1:05 PM",
      "commitNameOld": "5639bf02da716b3ecda785979b3d08cdca15972d",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 3.87,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,59 @@\n     private void dispatch() {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Start moving \" + this);\n       }\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-            HdfsConstants.IO_FILE_BUFFER_SIZE));\n+            HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n-            HdfsConstants.IO_FILE_BUFFER_SIZE));\n+            HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         nnc.getBytesMoved().addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "ae71a671a3b4b454aa393c2974b6f1f16dd61405": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7364. Balancer always shows zero Bytes Already Moved. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "06/11/14 5:48 PM",
      "commitName": "ae71a671a3b4b454aa393c2974b6f1f16dd61405",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "28/10/14 6:11 AM",
      "commitNameOld": "58c0bb9ed9f4a2491395b63c68046562a73526c9",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 9.53,
      "commitsBetweenForRepo": 121,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,59 @@\n     private void dispatch() {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Start moving \" + this);\n       }\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n-        bytesMoved.addAndGet(block.getNumBytes());\n+        nnc.getBytesMoved().addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(delayAfterErrors);\n         target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "0d85f7e59146cc3e9a040c2203995f3efd8ed4eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7034. Archival Storage: Fix TestBlockPlacement and TestStorageMover. Contributed by Jing Zhao.\n",
      "commitDate": "11/09/14 1:00 PM",
      "commitName": "0d85f7e59146cc3e9a040c2203995f3efd8ed4eb",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/08/14 2:20 PM",
      "commitNameOld": "a26aa6bd0716da89853566961390d711511084e3",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 14.94,
      "commitsBetweenForRepo": 128,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,59 @@\n     private void dispatch() {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Start moving \" + this);\n       }\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n-        proxySource.activateDelay(DELAY_AFTER_ERROR);\n-        target.getDDatanode().activateDelay(DELAY_AFTER_ERROR);\n+        proxySource.activateDelay(delayAfterErrors);\n+        target.getDDatanode().activateDelay(delayAfterErrors);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "a26aa6bd0716da89853566961390d711511084e3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6944. Archival Storage: add retry and termination logic for Mover. Contributed by Jing Zhao.\n",
      "commitDate": "27/08/14 2:20 PM",
      "commitName": "a26aa6bd0716da89853566961390d711511084e3",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/08/14 10:38 AM",
      "commitNameOld": "8ea20b53a861a2771c206afaacf8e7783568c4b1",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.15,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,59 @@\n     private void dispatch() {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Start moving \" + this);\n       }\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n+        target.getDDatanode().setHasFailure();\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n         target.getDDatanode().activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.getDDatanode().activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "195961a7c1da86421761162836766b1de07930fd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6247. Avoid timeouts for replaceBlock() call by sending intermediate responses to Balancer (vinayakumarb)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1617799 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/08/14 11:43 AM",
      "commitName": "195961a7c1da86421761162836766b1de07930fd",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "13/08/14 11:36 AM",
      "commitNameOld": "6554994fab2d8a2a139fb71ed54be144f4057e08",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,58 @@\n     private void dispatch() {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Start moving \" + this);\n       }\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n-        /*\n-         * Unfortunately we don\u0027t have a good way to know if the Datanode is\n-         * taking a really long time to move a block, OR something has gone\n-         * wrong and it\u0027s never going to finish. To deal with this scenario, we\n-         * set a long timeout (20 minutes) to avoid hanging indefinitely.\n-         */\n-        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n         target.getDDatanode().activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.getDDatanode().activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "6554994fab2d8a2a139fb71ed54be144f4057e08": {
      "type": "Ybodychange",
      "commitMessage": "Reverted\nMerged revision(s) 1617784 from hadoop/common/trunk:\nHDFS-6847. Avoid timeouts for replaceBlock() call by sending intermediate responses to Balancer (Contributed by Vinayakumar B.)\n........\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1617794 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/08/14 11:36 AM",
      "commitName": "6554994fab2d8a2a139fb71ed54be144f4057e08",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "13/08/14 11:06 AM",
      "commitNameOld": "471b1368e2a81b4d9850f0f4d98d31df1451354c",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,65 @@\n     private void dispatch() {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Start moving \" + this);\n       }\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n+        /*\n+         * Unfortunately we don\u0027t have a good way to know if the Datanode is\n+         * taking a really long time to move a block, OR something has gone\n+         * wrong and it\u0027s never going to finish. To deal with this scenario, we\n+         * set a long timeout (20 minutes) to avoid hanging indefinitely.\n+         */\n+        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n         target.getDDatanode().activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        /*\n         * Unfortunately we don\u0027t have a good way to know if the Datanode is\n         * taking a really long time to move a block, OR something has gone\n         * wrong and it\u0027s never going to finish. To deal with this scenario, we\n         * set a long timeout (20 minutes) to avoid hanging indefinitely.\n         */\n        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.getDDatanode().activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "471b1368e2a81b4d9850f0f4d98d31df1451354c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6847. Avoid timeouts for replaceBlock() call by sending intermediate responses to Balancer (Contributed by Vinayakumar B.)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1617784 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/08/14 11:06 AM",
      "commitName": "471b1368e2a81b4d9850f0f4d98d31df1451354c",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "11/08/14 11:01 AM",
      "commitNameOld": "e60673697d5046c29c52bbabdfe80506f99773e4",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.0,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,58 @@\n     private void dispatch() {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Start moving \" + this);\n       }\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n-        /*\n-         * Unfortunately we don\u0027t have a good way to know if the Datanode is\n-         * taking a really long time to move a block, OR something has gone\n-         * wrong and it\u0027s never going to finish. To deal with this scenario, we\n-         * set a long timeout (20 minutes) to avoid hanging indefinitely.\n-         */\n-        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n         final KeyManager km \u003d nnc.getKeyManager(); \n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n             unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         // Proxy or target may have some issues, delay before using these nodes\n         // further in order to avoid a potential storm of \"threads quota\n         // exceeded\" warnings when the dispatcher gets out of sync with work\n         // going on in datanodes.\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n         target.getDDatanode().activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n         target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.getDDatanode().activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "e60673697d5046c29c52bbabdfe80506f99773e4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6837. Code cleanup for Balancer and Dispatcher. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1617337 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/08/14 11:01 AM",
      "commitName": "e60673697d5046c29c52bbabdfe80506f99773e4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "08/08/14 2:33 PM",
      "commitNameOld": "c3cf331dc91e2beef2afeed11105084843b02858",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.85,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,65 @@\n     private void dispatch() {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Start moving \" + this);\n       }\n \n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n-            NetUtils.createSocketAddr(target.getDatanode().getXferAddr()),\n+            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n         /*\n          * Unfortunately we don\u0027t have a good way to know if the Datanode is\n          * taking a really long time to move a block, OR something has gone\n          * wrong and it\u0027s never going to finish. To deal with this scenario, we\n-         * set a long timeout (20 minutes) to avoid hanging the balancer\n-         * indefinitely.\n+         * set a long timeout (20 minutes) to avoid hanging indefinitely.\n          */\n         sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n             block.getBlock());\n-        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d keyManager.getAccessToken(eb);\n+        final KeyManager km \u003d nnc.getKeyManager(); \n+        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n-            unbufIn, keyManager, accessToken, target.getDatanode());\n+            unbufIn, km, accessToken, target.getDatanodeInfo());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n         sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n-        /*\n-         * proxy or target may have an issue, insert a small delay before using\n-         * these nodes further. This avoids a potential storm of\n-         * \"threads quota exceeded\" Warnings when the balancer gets out of sync\n-         * with work going on in datanode.\n-         */\n+        // Proxy or target may have some issues, delay before using these nodes\n+        // further in order to avoid a potential storm of \"threads quota\n+        // exceeded\" warnings when the dispatcher gets out of sync with work\n+        // going on in datanodes.\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n-        target.getBalancerDatanode().activateDelay(DELAY_AFTER_ERROR);\n+        target.getDDatanode().activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n \n         proxySource.removePendingBlock(this);\n-        target.getBalancerDatanode().removePendingBlock(this);\n+        target.getDDatanode().removePendingBlock(this);\n \n         synchronized (this) {\n           reset();\n         }\n         synchronized (Dispatcher.this) {\n           Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        /*\n         * Unfortunately we don\u0027t have a good way to know if the Datanode is\n         * taking a really long time to move a block, OR something has gone\n         * wrong and it\u0027s never going to finish. To deal with this scenario, we\n         * set a long timeout (20 minutes) to avoid hanging indefinitely.\n         */\n        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        final KeyManager km \u003d nnc.getKeyManager(); \n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d km.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.getDDatanode().activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "extendedDetails": {}
    },
    "c3cf331dc91e2beef2afeed11105084843b02858": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-6828. Separate block replica dispatching from Balancer. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616889 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/08/14 2:33 PM",
      "commitName": "c3cf331dc91e2beef2afeed11105084843b02858",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-6828. Separate block replica dispatching from Balancer. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616889 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/08/14 2:33 PM",
          "commitName": "c3cf331dc91e2beef2afeed11105084843b02858",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "08/08/14 2:22 PM",
          "commitNameOld": "05d1bf4157e6660610f11951845e59899260596e",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,67 @@\n     private void dispatch() {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Start moving \" + this);\n+      }\n+\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanode().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n-        /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n-         * taking a really long time to move a block, OR something has\n-         * gone wrong and it\u0027s never going to finish. To deal with this \n-         * scenario, we set a long timeout (20 minutes) to avoid hanging\n-         * the balancer indefinitely.\n+        /*\n+         * Unfortunately we don\u0027t have a good way to know if the Datanode is\n+         * taking a really long time to move a block, OR something has gone\n+         * wrong and it\u0027s never going to finish. To deal with this scenario, we\n+         * set a long timeout (20 minutes) to avoid hanging the balancer\n+         * indefinitely.\n          */\n         sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n-        \n+\n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n-        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(), block.getBlock());\n+        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n+            block.getBlock());\n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d keyManager.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n-          unbufIn, keyManager, accessToken, target.getDatanode());\n+            unbufIn, keyManager, accessToken, target.getDatanode());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n-        \n-        sendRequest(out, eb, StorageType.DEFAULT, accessToken);\n+\n+        sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n-        /* proxy or target may have an issue, insert a small delay\n-         * before using these nodes further. This avoids a potential storm\n-         * of \"threads quota exceeded\" Warnings when the balancer\n-         * gets out of sync with work going on in datanode.\n+        /*\n+         * proxy or target may have an issue, insert a small delay before using\n+         * these nodes further. This avoids a potential storm of\n+         * \"threads quota exceeded\" Warnings when the balancer gets out of sync\n+         * with work going on in datanode.\n          */\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n         target.getBalancerDatanode().activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n-        \n+\n         proxySource.removePendingBlock(this);\n         target.getBalancerDatanode().removePendingBlock(this);\n \n-        synchronized (this ) {\n+        synchronized (this) {\n           reset();\n         }\n-        synchronized (Balancer.this) {\n-          Balancer.this.notifyAll();\n+        synchronized (Dispatcher.this) {\n+          Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanode().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        /*\n         * Unfortunately we don\u0027t have a good way to know if the Datanode is\n         * taking a really long time to move a block, OR something has gone\n         * wrong and it\u0027s never going to finish. To deal with this scenario, we\n         * set a long timeout (20 minutes) to avoid hanging the balancer\n         * indefinitely.\n         */\n        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d keyManager.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, keyManager, accessToken, target.getDatanode());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        /*\n         * proxy or target may have an issue, insert a small delay before using\n         * these nodes further. This avoids a potential storm of\n         * \"threads quota exceeded\" Warnings when the balancer gets out of sync\n         * with work going on in datanode.\n         */\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.getBalancerDatanode().activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getBalancerDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
            "oldMethodName": "dispatch",
            "newMethodName": "dispatch"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6828. Separate block replica dispatching from Balancer. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616889 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/08/14 2:33 PM",
          "commitName": "c3cf331dc91e2beef2afeed11105084843b02858",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "08/08/14 2:22 PM",
          "commitNameOld": "05d1bf4157e6660610f11951845e59899260596e",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,67 @@\n     private void dispatch() {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Start moving \" + this);\n+      }\n+\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanode().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n-        /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n-         * taking a really long time to move a block, OR something has\n-         * gone wrong and it\u0027s never going to finish. To deal with this \n-         * scenario, we set a long timeout (20 minutes) to avoid hanging\n-         * the balancer indefinitely.\n+        /*\n+         * Unfortunately we don\u0027t have a good way to know if the Datanode is\n+         * taking a really long time to move a block, OR something has gone\n+         * wrong and it\u0027s never going to finish. To deal with this scenario, we\n+         * set a long timeout (20 minutes) to avoid hanging the balancer\n+         * indefinitely.\n          */\n         sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n-        \n+\n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n-        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(), block.getBlock());\n+        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n+            block.getBlock());\n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d keyManager.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n-          unbufIn, keyManager, accessToken, target.getDatanode());\n+            unbufIn, keyManager, accessToken, target.getDatanode());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n-        \n-        sendRequest(out, eb, StorageType.DEFAULT, accessToken);\n+\n+        sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n-        /* proxy or target may have an issue, insert a small delay\n-         * before using these nodes further. This avoids a potential storm\n-         * of \"threads quota exceeded\" Warnings when the balancer\n-         * gets out of sync with work going on in datanode.\n+        /*\n+         * proxy or target may have an issue, insert a small delay before using\n+         * these nodes further. This avoids a potential storm of\n+         * \"threads quota exceeded\" Warnings when the balancer gets out of sync\n+         * with work going on in datanode.\n          */\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n         target.getBalancerDatanode().activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n-        \n+\n         proxySource.removePendingBlock(this);\n         target.getBalancerDatanode().removePendingBlock(this);\n \n-        synchronized (this ) {\n+        synchronized (this) {\n           reset();\n         }\n-        synchronized (Balancer.this) {\n-          Balancer.this.notifyAll();\n+        synchronized (Dispatcher.this) {\n+          Dispatcher.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void dispatch() {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Start moving \" + this);\n      }\n\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanode().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        /*\n         * Unfortunately we don\u0027t have a good way to know if the Datanode is\n         * taking a really long time to move a block, OR something has gone\n         * wrong and it\u0027s never going to finish. To deal with this scenario, we\n         * set a long timeout (20 minutes) to avoid hanging the balancer\n         * indefinitely.\n         */\n        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(),\n            block.getBlock());\n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d keyManager.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n            unbufIn, keyManager, accessToken, target.getDatanode());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        /*\n         * proxy or target may have an issue, insert a small delay before using\n         * these nodes further. This avoids a potential storm of\n         * \"threads quota exceeded\" Warnings when the balancer gets out of sync\n         * with work going on in datanode.\n         */\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.getBalancerDatanode().activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getBalancerDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
          "extendedDetails": {}
        }
      ]
    },
    "83b9933db3349e6a6faf23bce35c9d4ce3f7bcf2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6809. Move Balancer\u0027s inner classes MovedBlocks and Matcher as to standalone classes and separates KeyManager from NameNodeConnector.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616422 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/14 12:18 AM",
      "commitName": "83b9933db3349e6a6faf23bce35c9d4ce3f7bcf2",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/08/14 7:12 AM",
      "commitNameOld": "7e12b1912f8cdbe6d88ac0b8eb71d7c4dc1bf78e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.71,
      "commitsBetweenForRepo": 41,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.getDatanode().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n         /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n          * taking a really long time to move a block, OR something has\n          * gone wrong and it\u0027s never going to finish. To deal with this \n          * scenario, we set a long timeout (20 minutes) to avoid hanging\n          * the balancer indefinitely.\n          */\n         sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n         \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n-        ExtendedBlock eb \u003d new ExtendedBlock(nnc.blockpoolID, block.getBlock());\n-        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d nnc.getAccessToken(eb);\n+        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(), block.getBlock());\n+        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d keyManager.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n-          unbufIn, nnc, accessToken, target.getDatanode());\n+          unbufIn, keyManager, accessToken, target.getDatanode());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         \n         sendRequest(out, eb, StorageType.DEFAULT, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         /* proxy or target may have an issue, insert a small delay\n          * before using these nodes further. This avoids a potential storm\n          * of \"threads quota exceeded\" Warnings when the balancer\n          * gets out of sync with work going on in datanode.\n          */\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n         target.getBalancerDatanode().activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.getBalancerDatanode().removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanode().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n         * taking a really long time to move a block, OR something has\n         * gone wrong and it\u0027s never going to finish. To deal with this \n         * scenario, we set a long timeout (20 minutes) to avoid hanging\n         * the balancer indefinitely.\n         */\n        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n        \n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.getBlockpoolID(), block.getBlock());\n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d keyManager.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n          unbufIn, keyManager, accessToken, target.getDatanode());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        \n        sendRequest(out, eb, StorageType.DEFAULT, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        /* proxy or target may have an issue, insert a small delay\n         * before using these nodes further. This avoids a potential storm\n         * of \"threads quota exceeded\" Warnings when the balancer\n         * gets out of sync with work going on in datanode.\n         */\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.getBalancerDatanode().activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.getBalancerDatanode().removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6685. Balancer should preserve storage type of replicas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615015 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/07/14 6:05 PM",
      "commitName": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/07/14 11:02 PM",
      "commitNameOld": "b8b8f3f5e7214d6fcfc30e1b94ff097e52868f4f",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n-            NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n+            NetUtils.createSocketAddr(target.getDatanode().getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n         /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n          * taking a really long time to move a block, OR something has\n          * gone wrong and it\u0027s never going to finish. To deal with this \n          * scenario, we set a long timeout (20 minutes) to avoid hanging\n          * the balancer indefinitely.\n          */\n         sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n         \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.blockpoolID, block.getBlock());\n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d nnc.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n-          unbufIn, nnc, accessToken, target.datanode);\n+          unbufIn, nnc, accessToken, target.getDatanode());\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         \n         sendRequest(out, eb, StorageType.DEFAULT, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         /* proxy or target may have an issue, insert a small delay\n          * before using these nodes further. This avoids a potential storm\n          * of \"threads quota exceeded\" Warnings when the balancer\n          * gets out of sync with work going on in datanode.\n          */\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n-        target.activateDelay(DELAY_AFTER_ERROR);\n+        target.getBalancerDatanode().activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n-        target.removePendingBlock(this);\n+        target.getBalancerDatanode().removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanode().getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n         * taking a really long time to move a block, OR something has\n         * gone wrong and it\u0027s never going to finish. To deal with this \n         * scenario, we set a long timeout (20 minutes) to avoid hanging\n         * the balancer indefinitely.\n         */\n        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n        \n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.blockpoolID, block.getBlock());\n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d nnc.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n          unbufIn, nnc, accessToken, target.getDatanode());\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        \n        sendRequest(out, eb, StorageType.DEFAULT, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        /* proxy or target may have an issue, insert a small delay\n         * before using these nodes further. This avoids a potential storm\n         * of \"threads quota exceeded\" Warnings when the balancer\n         * gets out of sync with work going on in datanode.\n         */\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.getBalancerDatanode().activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.getBalancerDatanode().removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/14 12:41 AM",
      "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "14/07/14 11:10 AM",
      "commitNameOld": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 7.56,
      "commitsBetweenForRepo": 68,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n         /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n          * taking a really long time to move a block, OR something has\n          * gone wrong and it\u0027s never going to finish. To deal with this \n          * scenario, we set a long timeout (20 minutes) to avoid hanging\n          * the balancer indefinitely.\n          */\n         sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n         \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         ExtendedBlock eb \u003d new ExtendedBlock(nnc.blockpoolID, block.getBlock());\n         Token\u003cBlockTokenIdentifier\u003e accessToken \u003d nnc.getAccessToken(eb);\n         IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n           unbufIn, nnc, accessToken, target.datanode);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         \n-        sendRequest(out, eb, accessToken);\n+        sendRequest(out, eb, StorageType.DEFAULT, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         /* proxy or target may have an issue, insert a small delay\n          * before using these nodes further. This avoids a potential storm\n          * of \"threads quota exceeded\" Warnings when the balancer\n          * gets out of sync with work going on in datanode.\n          */\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n         target.activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n         * taking a really long time to move a block, OR something has\n         * gone wrong and it\u0027s never going to finish. To deal with this \n         * scenario, we set a long timeout (20 minutes) to avoid hanging\n         * the balancer indefinitely.\n         */\n        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n        \n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.blockpoolID, block.getBlock());\n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d nnc.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n          unbufIn, nnc, accessToken, target.datanode);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        \n        sendRequest(out, eb, StorageType.DEFAULT, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        /* proxy or target may have an issue, insert a small delay\n         * before using these nodes further. This avoids a potential storm\n         * of \"threads quota exceeded\" Warnings when the balancer\n         * gets out of sync with work going on in datanode.\n         */\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "3b54223c0f32d42a84436c670d80b791a8e9696d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/07/14 11:10 AM",
      "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "25/06/14 12:27 PM",
      "commitNameOld": "e3612e442809310c67bc2ed4376e028c4ab8d597",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 18.95,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,60 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n         /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n          * taking a really long time to move a block, OR something has\n          * gone wrong and it\u0027s never going to finish. To deal with this \n          * scenario, we set a long timeout (20 minutes) to avoid hanging\n          * the balancer indefinitely.\n          */\n         sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n         \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n-        if (nnc.getDataEncryptionKey() !\u003d null) {\n-          IOStreamPair encryptedStreams \u003d\n-              DataTransferEncryptor.getEncryptedStreams(\n-                  unbufOut, unbufIn, nnc.getDataEncryptionKey());\n-          unbufOut \u003d encryptedStreams.out;\n-          unbufIn \u003d encryptedStreams.in;\n-        }\n+        ExtendedBlock eb \u003d new ExtendedBlock(nnc.blockpoolID, block.getBlock());\n+        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d nnc.getAccessToken(eb);\n+        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n+          unbufIn, nnc, accessToken, target.datanode);\n+        unbufOut \u003d saslStreams.out;\n+        unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         \n-        sendRequest(out);\n+        sendRequest(out, eb, accessToken);\n         receiveResponse(in);\n         bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         /* proxy or target may have an issue, insert a small delay\n          * before using these nodes further. This avoids a potential storm\n          * of \"threads quota exceeded\" Warnings when the balancer\n          * gets out of sync with work going on in datanode.\n          */\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n         target.activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n         * taking a really long time to move a block, OR something has\n         * gone wrong and it\u0027s never going to finish. To deal with this \n         * scenario, we set a long timeout (20 minutes) to avoid hanging\n         * the balancer indefinitely.\n         */\n        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n        \n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        ExtendedBlock eb \u003d new ExtendedBlock(nnc.blockpoolID, block.getBlock());\n        Token\u003cBlockTokenIdentifier\u003e accessToken \u003d nnc.getAccessToken(eb);\n        IOStreamPair saslStreams \u003d saslClient.socketSend(sock, unbufOut,\n          unbufIn, nnc, accessToken, target.datanode);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        \n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        /* proxy or target may have an issue, insert a small delay\n         * before using these nodes further. This avoids a potential storm\n         * of \"threads quota exceeded\" Warnings when the balancer\n         * gets out of sync with work going on in datanode.\n         */\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "6b2e615f5fa034d679be0de8fb300b878a2d801a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6433. Replace BytesMoved class with AtomicLong. Contributed by Benoy Antony.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596742 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/05/14 9:50 PM",
      "commitName": "6b2e615f5fa034d679be0de8fb300b878a2d801a",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "03/05/14 4:02 AM",
      "commitNameOld": "b2f65c276da2c4420a0974a7e2d75e081abf5d63",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 18.74,
      "commitsBetweenForRepo": 109,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n         /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n          * taking a really long time to move a block, OR something has\n          * gone wrong and it\u0027s never going to finish. To deal with this \n          * scenario, we set a long timeout (20 minutes) to avoid hanging\n          * the balancer indefinitely.\n          */\n         sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n \n         sock.setKeepAlive(true);\n         \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         if (nnc.getDataEncryptionKey() !\u003d null) {\n           IOStreamPair encryptedStreams \u003d\n               DataTransferEncryptor.getEncryptedStreams(\n                   unbufOut, unbufIn, nnc.getDataEncryptionKey());\n           unbufOut \u003d encryptedStreams.out;\n           unbufIn \u003d encryptedStreams.in;\n         }\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         \n         sendRequest(out);\n         receiveResponse(in);\n-        bytesMoved.inc(block.getNumBytes());\n+        bytesMoved.addAndGet(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n         /* proxy or target may have an issue, insert a small delay\n          * before using these nodes further. This avoids a potential storm\n          * of \"threads quota exceeded\" Warnings when the balancer\n          * gets out of sync with work going on in datanode.\n          */\n         proxySource.activateDelay(DELAY_AFTER_ERROR);\n         target.activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n         * taking a really long time to move a block, OR something has\n         * gone wrong and it\u0027s never going to finish. To deal with this \n         * scenario, we set a long timeout (20 minutes) to avoid hanging\n         * the balancer indefinitely.\n         */\n        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n        \n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        if (nnc.getDataEncryptionKey() !\u003d null) {\n          IOStreamPair encryptedStreams \u003d\n              DataTransferEncryptor.getEncryptedStreams(\n                  unbufOut, unbufIn, nnc.getDataEncryptionKey());\n          unbufOut \u003d encryptedStreams.out;\n          unbufIn \u003d encryptedStreams.in;\n        }\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        \n        sendRequest(out);\n        receiveResponse(in);\n        bytesMoved.addAndGet(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        /* proxy or target may have an issue, insert a small delay\n         * before using these nodes further. This avoids a potential storm\n         * of \"threads quota exceeded\" Warnings when the balancer\n         * gets out of sync with work going on in datanode.\n         */\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "b4c41f341bb7ffff6a428b7e9557591e7685e4f0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6166. Change Balancer socket read timeout to 20 minutes and add 10 seconds delay after error.  Contributed by Nathan Roberts\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1583018 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/03/14 9:23 AM",
      "commitName": "b4c41f341bb7ffff6a428b7e9557591e7685e4f0",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "24/03/14 4:32 PM",
      "commitNameOld": "c2ef7e239eb0e81cf8a3e971378e9e696202de67",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 4.7,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,61 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n-        sock.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n+        /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n+         * taking a really long time to move a block, OR something has\n+         * gone wrong and it\u0027s never going to finish. To deal with this \n+         * scenario, we set a long timeout (20 minutes) to avoid hanging\n+         * the balancer indefinitely.\n+         */\n+        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n+\n         sock.setKeepAlive(true);\n         \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         if (nnc.getDataEncryptionKey() !\u003d null) {\n           IOStreamPair encryptedStreams \u003d\n               DataTransferEncryptor.getEncryptedStreams(\n                   unbufOut, unbufIn, nnc.getDataEncryptionKey());\n           unbufOut \u003d encryptedStreams.out;\n           unbufIn \u003d encryptedStreams.in;\n         }\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         \n         sendRequest(out);\n         receiveResponse(in);\n         bytesMoved.inc(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n+        /* proxy or target may have an issue, insert a small delay\n+         * before using these nodes further. This avoids a potential storm\n+         * of \"threads quota exceeded\" Warnings when the balancer\n+         * gets out of sync with work going on in datanode.\n+         */\n+        proxySource.activateDelay(DELAY_AFTER_ERROR);\n+        target.activateDelay(DELAY_AFTER_ERROR);\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        /* Unfortunately we don\u0027t have a good way to know if the Datanode is\n         * taking a really long time to move a block, OR something has\n         * gone wrong and it\u0027s never going to finish. To deal with this \n         * scenario, we set a long timeout (20 minutes) to avoid hanging\n         * the balancer indefinitely.\n         */\n        sock.setSoTimeout(BLOCK_MOVE_READ_TIMEOUT);\n\n        sock.setKeepAlive(true);\n        \n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        if (nnc.getDataEncryptionKey() !\u003d null) {\n          IOStreamPair encryptedStreams \u003d\n              DataTransferEncryptor.getEncryptedStreams(\n                  unbufOut, unbufIn, nnc.getDataEncryptionKey());\n          unbufOut \u003d encryptedStreams.out;\n          unbufIn \u003d encryptedStreams.in;\n        }\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        \n        sendRequest(out);\n        receiveResponse(in);\n        bytesMoved.inc(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n        /* proxy or target may have an issue, insert a small delay\n         * before using these nodes further. This avoids a potential storm\n         * of \"threads quota exceeded\" Warnings when the balancer\n         * gets out of sync with work going on in datanode.\n         */\n        proxySource.activateDelay(DELAY_AFTER_ERROR);\n        target.activateDelay(DELAY_AFTER_ERROR);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "1c602f719833dae48e2bea63b98f76bbdc5a65b4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5806. Balancer should set soTimeout to avoid indefinite hangs. Contributed by Nathan Roberts.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1560548 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/01/14 2:33 PM",
      "commitName": "1c602f719833dae48e2bea63b98f76bbdc5a65b4",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "11/12/13 3:18 PM",
      "commitNameOld": "cd083aa80711092e50365e20f0b507551000f915",
      "commitAuthorOld": "",
      "daysBetweenCommits": 41.97,
      "commitsBetweenForRepo": 194,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,47 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n+        sock.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n         sock.setKeepAlive(true);\n         \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         if (nnc.getDataEncryptionKey() !\u003d null) {\n           IOStreamPair encryptedStreams \u003d\n               DataTransferEncryptor.getEncryptedStreams(\n                   unbufOut, unbufIn, nnc.getDataEncryptionKey());\n           unbufOut \u003d encryptedStreams.out;\n           unbufIn \u003d encryptedStreams.in;\n         }\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         \n         sendRequest(out);\n         receiveResponse(in);\n         bytesMoved.inc(block.getNumBytes());\n         LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        sock.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n        sock.setKeepAlive(true);\n        \n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        if (nnc.getDataEncryptionKey() !\u003d null) {\n          IOStreamPair encryptedStreams \u003d\n              DataTransferEncryptor.getEncryptedStreams(\n                  unbufOut, unbufIn, nnc.getDataEncryptionKey());\n          unbufOut \u003d encryptedStreams.out;\n          unbufIn \u003d encryptedStreams.in;\n        }\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        \n        sendRequest(out);\n        receiveResponse(in);\n        bytesMoved.inc(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "907fb15ee8c150e5ecc0560b7374441c57a84122": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5480. Update Balancer for HDFS-2832. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1540547 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/11/13 12:59 PM",
      "commitName": "907fb15ee8c150e5ecc0560b7374441c57a84122",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "17/10/13 11:54 AM",
      "commitNameOld": "f8d5755a69d7b4f230adbbfd88ea73df7a83b4f0",
      "commitAuthorOld": "",
      "daysBetweenCommits": 24.09,
      "commitsBetweenForRepo": 121,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,46 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n         sock.setKeepAlive(true);\n         \n         OutputStream unbufOut \u003d sock.getOutputStream();\n         InputStream unbufIn \u003d sock.getInputStream();\n         if (nnc.getDataEncryptionKey() !\u003d null) {\n           IOStreamPair encryptedStreams \u003d\n               DataTransferEncryptor.getEncryptedStreams(\n                   unbufOut, unbufIn, nnc.getDataEncryptionKey());\n           unbufOut \u003d encryptedStreams.out;\n           unbufIn \u003d encryptedStreams.in;\n         }\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n             HdfsConstants.IO_FILE_BUFFER_SIZE));\n         \n         sendRequest(out);\n         receiveResponse(in);\n         bytesMoved.inc(block.getNumBytes());\n-        LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n-              \" from \"+ source.getDisplayName() + \" to \" +\n-              target.getDisplayName() + \" through \" +\n-              proxySource.getDisplayName() +\n-              \" is succeeded.\" );\n+        LOG.info(\"Successfully moved \" + this);\n       } catch (IOException e) {\n-        LOG.warn(\"Error moving block \"+block.getBlockId()+\n-            \" from \" + source.getDisplayName() + \" to \" +\n-            target.getDisplayName() + \" through \" +\n-            proxySource.getDisplayName() +\n-            \": \"+e.getMessage());\n+        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        sock.setKeepAlive(true);\n        \n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        if (nnc.getDataEncryptionKey() !\u003d null) {\n          IOStreamPair encryptedStreams \u003d\n              DataTransferEncryptor.getEncryptedStreams(\n                  unbufOut, unbufIn, nnc.getDataEncryptionKey());\n          unbufOut \u003d encryptedStreams.out;\n          unbufIn \u003d encryptedStreams.in;\n        }\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        \n        sendRequest(out);\n        receiveResponse(in);\n        bytesMoved.inc(block.getNumBytes());\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this + \": \" + e.getMessage());\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "15/07/12 7:58 PM",
      "commitNameOld": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 22.57,
      "commitsBetweenForRepo": 106,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,54 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n         sock.setKeepAlive(true);\n-        out \u003d new DataOutputStream( new BufferedOutputStream(\n-            sock.getOutputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n+        \n+        OutputStream unbufOut \u003d sock.getOutputStream();\n+        InputStream unbufIn \u003d sock.getInputStream();\n+        if (nnc.getDataEncryptionKey() !\u003d null) {\n+          IOStreamPair encryptedStreams \u003d\n+              DataTransferEncryptor.getEncryptedStreams(\n+                  unbufOut, unbufIn, nnc.getDataEncryptionKey());\n+          unbufOut \u003d encryptedStreams.out;\n+          unbufIn \u003d encryptedStreams.in;\n+        }\n+        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n+            HdfsConstants.IO_FILE_BUFFER_SIZE));\n+        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n+            HdfsConstants.IO_FILE_BUFFER_SIZE));\n+        \n         sendRequest(out);\n-        in \u003d new DataInputStream( new BufferedInputStream(\n-            sock.getInputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n         receiveResponse(in);\n         bytesMoved.inc(block.getNumBytes());\n         LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n               \" from \"+ source.getDisplayName() + \" to \" +\n               target.getDisplayName() + \" through \" +\n               proxySource.getDisplayName() +\n               \" is succeeded.\" );\n       } catch (IOException e) {\n         LOG.warn(\"Error moving block \"+block.getBlockId()+\n             \" from \" + source.getDisplayName() + \" to \" +\n             target.getDisplayName() + \" through \" +\n             proxySource.getDisplayName() +\n             \": \"+e.getMessage());\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        sock.setKeepAlive(true);\n        \n        OutputStream unbufOut \u003d sock.getOutputStream();\n        InputStream unbufIn \u003d sock.getInputStream();\n        if (nnc.getDataEncryptionKey() !\u003d null) {\n          IOStreamPair encryptedStreams \u003d\n              DataTransferEncryptor.getEncryptedStreams(\n                  unbufOut, unbufIn, nnc.getDataEncryptionKey());\n          unbufOut \u003d encryptedStreams.out;\n          unbufIn \u003d encryptedStreams.in;\n        }\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        in \u003d new DataInputStream(new BufferedInputStream(unbufIn,\n            HdfsConstants.IO_FILE_BUFFER_SIZE));\n        \n        sendRequest(out);\n        receiveResponse(in);\n        bytesMoved.inc(block.getNumBytes());\n        LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n              \" from \"+ source.getDisplayName() + \" to \" +\n              target.getDisplayName() + \" through \" +\n              proxySource.getDisplayName() +\n              \" is succeeded.\" );\n      } catch (IOException e) {\n        LOG.warn(\"Error moving block \"+block.getBlockId()+\n            \" from \" + source.getDisplayName() + \" to \" +\n            target.getDisplayName() + \" through \" +\n            proxySource.getDisplayName() +\n            \": \"+e.getMessage());\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "e4df14f8f151413a8ec0972a21e31a0b51fe0fb0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3417. Rename BalancerDatanode#getName to getDisplayName to be consistent with Datanode. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1338767 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/05/12 9:01 AM",
      "commitName": "e4df14f8f151413a8ec0972a21e31a0b51fe0fb0",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "30/04/12 11:44 PM",
      "commitNameOld": "d37ec9d09e02f16396c296e72ff8c1b7c3e6ed10",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 14.39,
      "commitsBetweenForRepo": 96,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(\n             NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n             HdfsServerConstants.READ_TIMEOUT);\n         sock.setKeepAlive(true);\n         out \u003d new DataOutputStream( new BufferedOutputStream(\n             sock.getOutputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n         sendRequest(out);\n         in \u003d new DataInputStream( new BufferedInputStream(\n             sock.getInputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n         receiveResponse(in);\n         bytesMoved.inc(block.getNumBytes());\n         LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n-              \" from \"+ source.getName() + \" to \" +\n-              target.getName() + \" through \" +\n-              proxySource.getName() +\n+              \" from \"+ source.getDisplayName() + \" to \" +\n+              target.getDisplayName() + \" through \" +\n+              proxySource.getDisplayName() +\n               \" is succeeded.\" );\n       } catch (IOException e) {\n         LOG.warn(\"Error moving block \"+block.getBlockId()+\n-            \" from \" + source.getName() + \" to \" +\n-            target.getName() + \" through \" +\n-            proxySource.getName() +\n+            \" from \" + source.getDisplayName() + \" to \" +\n+            target.getDisplayName() + \" through \" +\n+            proxySource.getDisplayName() +\n             \": \"+e.getMessage());\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        sock.setKeepAlive(true);\n        out \u003d new DataOutputStream( new BufferedOutputStream(\n            sock.getOutputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n        sendRequest(out);\n        in \u003d new DataInputStream( new BufferedInputStream(\n            sock.getInputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n        receiveResponse(in);\n        bytesMoved.inc(block.getNumBytes());\n        LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n              \" from \"+ source.getDisplayName() + \" to \" +\n              target.getDisplayName() + \" through \" +\n              proxySource.getDisplayName() +\n              \" is succeeded.\" );\n      } catch (IOException e) {\n        LOG.warn(\"Error moving block \"+block.getBlockId()+\n            \" from \" + source.getDisplayName() + \" to \" +\n            target.getDisplayName() + \" through \" +\n            proxySource.getDisplayName() +\n            \": \"+e.getMessage());\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "31/03/12 9:20 AM",
      "commitNameOld": "ff897e75c9c61e1a971d01f94e2a5abef3be9061",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 1.24,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,43 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n-        sock.connect(NetUtils.createSocketAddr(\n-            target.datanode.getName()), HdfsServerConstants.READ_TIMEOUT);\n+        sock.connect(\n+            NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n+            HdfsServerConstants.READ_TIMEOUT);\n         sock.setKeepAlive(true);\n         out \u003d new DataOutputStream( new BufferedOutputStream(\n             sock.getOutputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n         sendRequest(out);\n         in \u003d new DataInputStream( new BufferedInputStream(\n             sock.getInputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n         receiveResponse(in);\n         bytesMoved.inc(block.getNumBytes());\n         LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n               \" from \"+ source.getName() + \" to \" +\n               target.getName() + \" through \" +\n               proxySource.getName() +\n               \" is succeeded.\" );\n       } catch (IOException e) {\n         LOG.warn(\"Error moving block \"+block.getBlockId()+\n             \" from \" + source.getName() + \" to \" +\n             target.getName() + \" through \" +\n             proxySource.getName() +\n             \": \"+e.getMessage());\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.datanode.getXferAddr()),\n            HdfsServerConstants.READ_TIMEOUT);\n        sock.setKeepAlive(true);\n        out \u003d new DataOutputStream( new BufferedOutputStream(\n            sock.getOutputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n        sendRequest(out);\n        in \u003d new DataInputStream( new BufferedInputStream(\n            sock.getInputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n        receiveResponse(in);\n        bytesMoved.inc(block.getNumBytes());\n        LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n              \" from \"+ source.getName() + \" to \" +\n              target.getName() + \" through \" +\n              proxySource.getName() +\n              \" is succeeded.\" );\n      } catch (IOException e) {\n        LOG.warn(\"Error moving block \"+block.getBlockId()+\n            \" from \" + source.getName() + \" to \" +\n            target.getName() + \" through \" +\n            proxySource.getName() +\n            \": \"+e.getMessage());\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1620. Rename HdfsConstants -\u003e HdfsServerConstants, FSConstants -\u003e HdfsConstants. (Harsh J Chouraria via atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165096 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/11 12:30 PM",
      "commitName": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 10.8,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(NetUtils.createSocketAddr(\n-            target.datanode.getName()), HdfsConstants.READ_TIMEOUT);\n+            target.datanode.getName()), HdfsServerConstants.READ_TIMEOUT);\n         sock.setKeepAlive(true);\n         out \u003d new DataOutputStream( new BufferedOutputStream(\n-            sock.getOutputStream(), FSConstants.IO_FILE_BUFFER_SIZE));\n+            sock.getOutputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n         sendRequest(out);\n         in \u003d new DataInputStream( new BufferedInputStream(\n-            sock.getInputStream(), FSConstants.IO_FILE_BUFFER_SIZE));\n+            sock.getInputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n         receiveResponse(in);\n         bytesMoved.inc(block.getNumBytes());\n         LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n               \" from \"+ source.getName() + \" to \" +\n               target.getName() + \" through \" +\n               proxySource.getName() +\n               \" is succeeded.\" );\n       } catch (IOException e) {\n         LOG.warn(\"Error moving block \"+block.getBlockId()+\n             \" from \" + source.getName() + \" to \" +\n             target.getName() + \" through \" +\n             proxySource.getName() +\n             \": \"+e.getMessage());\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(NetUtils.createSocketAddr(\n            target.datanode.getName()), HdfsServerConstants.READ_TIMEOUT);\n        sock.setKeepAlive(true);\n        out \u003d new DataOutputStream( new BufferedOutputStream(\n            sock.getOutputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n        sendRequest(out);\n        in \u003d new DataInputStream( new BufferedInputStream(\n            sock.getInputStream(), HdfsConstants.IO_FILE_BUFFER_SIZE));\n        receiveResponse(in);\n        bytesMoved.inc(block.getNumBytes());\n        LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n              \" from \"+ source.getName() + \" to \" +\n              target.getName() + \" through \" +\n              proxySource.getName() +\n              \" is succeeded.\" );\n      } catch (IOException e) {\n        LOG.warn(\"Error moving block \"+block.getBlockId()+\n            \" from \" + source.getName() + \" to \" +\n            target.getName() + \" through \" +\n            proxySource.getName() +\n            \": \"+e.getMessage());\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(NetUtils.createSocketAddr(\n            target.datanode.getName()), HdfsConstants.READ_TIMEOUT);\n        sock.setKeepAlive(true);\n        out \u003d new DataOutputStream( new BufferedOutputStream(\n            sock.getOutputStream(), FSConstants.IO_FILE_BUFFER_SIZE));\n        sendRequest(out);\n        in \u003d new DataInputStream( new BufferedInputStream(\n            sock.getInputStream(), FSConstants.IO_FILE_BUFFER_SIZE));\n        receiveResponse(in);\n        bytesMoved.inc(block.getNumBytes());\n        LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n              \" from \"+ source.getName() + \" to \" +\n              target.getName() + \" through \" +\n              proxySource.getName() +\n              \" is succeeded.\" );\n      } catch (IOException e) {\n        LOG.warn(\"Error moving block \"+block.getBlockId()+\n            \" from \" + source.getName() + \" to \" +\n            target.getName() + \" through \" +\n            proxySource.getName() +\n            \": \"+e.getMessage());\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(NetUtils.createSocketAddr(\n            target.datanode.getName()), HdfsConstants.READ_TIMEOUT);\n        sock.setKeepAlive(true);\n        out \u003d new DataOutputStream( new BufferedOutputStream(\n            sock.getOutputStream(), FSConstants.IO_FILE_BUFFER_SIZE));\n        sendRequest(out);\n        in \u003d new DataInputStream( new BufferedInputStream(\n            sock.getInputStream(), FSConstants.IO_FILE_BUFFER_SIZE));\n        receiveResponse(in);\n        bytesMoved.inc(block.getNumBytes());\n        LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n              \" from \"+ source.getName() + \" to \" +\n              target.getName() + \" through \" +\n              proxySource.getName() +\n              \" is succeeded.\" );\n      } catch (IOException e) {\n        LOG.warn(\"Error moving block \"+block.getBlockId()+\n            \" from \" + source.getName() + \" to \" +\n            target.getName() + \" through \" +\n            proxySource.getName() +\n            \": \"+e.getMessage());\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java"
      }
    },
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2241. Remove implementing FSConstants interface to just get the constants from the interface. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1156420 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/08/11 5:46 PM",
      "commitName": "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "19/07/11 7:23 AM",
      "commitNameOld": "710e5a960e8af1d4c73e386041096aacfee8b828",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 22.43,
      "commitsBetweenForRepo": 80,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n     private void dispatch() {\n       Socket sock \u003d new Socket();\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock.connect(NetUtils.createSocketAddr(\n             target.datanode.getName()), HdfsConstants.READ_TIMEOUT);\n         sock.setKeepAlive(true);\n         out \u003d new DataOutputStream( new BufferedOutputStream(\n-            sock.getOutputStream(), FSConstants.BUFFER_SIZE));\n+            sock.getOutputStream(), FSConstants.IO_FILE_BUFFER_SIZE));\n         sendRequest(out);\n         in \u003d new DataInputStream( new BufferedInputStream(\n-            sock.getInputStream(), FSConstants.BUFFER_SIZE));\n+            sock.getInputStream(), FSConstants.IO_FILE_BUFFER_SIZE));\n         receiveResponse(in);\n         bytesMoved.inc(block.getNumBytes());\n         LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n               \" from \"+ source.getName() + \" to \" +\n               target.getName() + \" through \" +\n               proxySource.getName() +\n               \" is succeeded.\" );\n       } catch (IOException e) {\n         LOG.warn(\"Error moving block \"+block.getBlockId()+\n             \" from \" + source.getName() + \" to \" +\n             target.getName() + \" through \" +\n             proxySource.getName() +\n             \": \"+e.getMessage());\n       } finally {\n         IOUtils.closeStream(out);\n         IOUtils.closeStream(in);\n         IOUtils.closeSocket(sock);\n         \n         proxySource.removePendingBlock(this);\n         target.removePendingBlock(this);\n \n         synchronized (this ) {\n           reset();\n         }\n         synchronized (Balancer.this) {\n           Balancer.this.notifyAll();\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(NetUtils.createSocketAddr(\n            target.datanode.getName()), HdfsConstants.READ_TIMEOUT);\n        sock.setKeepAlive(true);\n        out \u003d new DataOutputStream( new BufferedOutputStream(\n            sock.getOutputStream(), FSConstants.IO_FILE_BUFFER_SIZE));\n        sendRequest(out);\n        in \u003d new DataInputStream( new BufferedInputStream(\n            sock.getInputStream(), FSConstants.IO_FILE_BUFFER_SIZE));\n        receiveResponse(in);\n        bytesMoved.inc(block.getNumBytes());\n        LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n              \" from \"+ source.getName() + \" to \" +\n              target.getName() + \" through \" +\n              proxySource.getName() +\n              \" is succeeded.\" );\n      } catch (IOException e) {\n        LOG.warn(\"Error moving block \"+block.getBlockId()+\n            \" from \" + source.getName() + \" to \" +\n            target.getName() + \" through \" +\n            proxySource.getName() +\n            \": \"+e.getMessage());\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,42 @@\n+    private void dispatch() {\n+      Socket sock \u003d new Socket();\n+      DataOutputStream out \u003d null;\n+      DataInputStream in \u003d null;\n+      try {\n+        sock.connect(NetUtils.createSocketAddr(\n+            target.datanode.getName()), HdfsConstants.READ_TIMEOUT);\n+        sock.setKeepAlive(true);\n+        out \u003d new DataOutputStream( new BufferedOutputStream(\n+            sock.getOutputStream(), FSConstants.BUFFER_SIZE));\n+        sendRequest(out);\n+        in \u003d new DataInputStream( new BufferedInputStream(\n+            sock.getInputStream(), FSConstants.BUFFER_SIZE));\n+        receiveResponse(in);\n+        bytesMoved.inc(block.getNumBytes());\n+        LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n+              \" from \"+ source.getName() + \" to \" +\n+              target.getName() + \" through \" +\n+              proxySource.getName() +\n+              \" is succeeded.\" );\n+      } catch (IOException e) {\n+        LOG.warn(\"Error moving block \"+block.getBlockId()+\n+            \" from \" + source.getName() + \" to \" +\n+            target.getName() + \" through \" +\n+            proxySource.getName() +\n+            \": \"+e.getMessage());\n+      } finally {\n+        IOUtils.closeStream(out);\n+        IOUtils.closeStream(in);\n+        IOUtils.closeSocket(sock);\n+        \n+        proxySource.removePendingBlock(this);\n+        target.removePendingBlock(this);\n+\n+        synchronized (this ) {\n+          reset();\n+        }\n+        synchronized (Balancer.this) {\n+          Balancer.this.notifyAll();\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void dispatch() {\n      Socket sock \u003d new Socket();\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock.connect(NetUtils.createSocketAddr(\n            target.datanode.getName()), HdfsConstants.READ_TIMEOUT);\n        sock.setKeepAlive(true);\n        out \u003d new DataOutputStream( new BufferedOutputStream(\n            sock.getOutputStream(), FSConstants.BUFFER_SIZE));\n        sendRequest(out);\n        in \u003d new DataInputStream( new BufferedInputStream(\n            sock.getInputStream(), FSConstants.BUFFER_SIZE));\n        receiveResponse(in);\n        bytesMoved.inc(block.getNumBytes());\n        LOG.info( \"Moving block \" + block.getBlock().getBlockId() +\n              \" from \"+ source.getName() + \" to \" +\n              target.getName() + \" through \" +\n              proxySource.getName() +\n              \" is succeeded.\" );\n      } catch (IOException e) {\n        LOG.warn(\"Error moving block \"+block.getBlockId()+\n            \" from \" + source.getName() + \" to \" +\n            target.getName() + \" through \" +\n            proxySource.getName() +\n            \": \"+e.getMessage());\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n        \n        proxySource.removePendingBlock(this);\n        target.removePendingBlock(this);\n\n        synchronized (this ) {\n          reset();\n        }\n        synchronized (Balancer.this) {\n          Balancer.this.notifyAll();\n        }\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java"
    }
  }
}