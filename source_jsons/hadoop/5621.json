{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CacheManager.java",
  "functionName": "processCacheReportImpl",
  "functionId": "processCacheReportImpl___datanode-DatanodeDescriptor(modifiers-final)__blockIds-List__Long__(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
  "functionStartLine": 1010,
  "functionEndLine": 1043,
  "numCommitsSeen": 78,
  "timeTaken": 4440,
  "changeHistory": [
    "93e23a99157c30b51752fc49748c3c210745a187",
    "07e4fb1455abc33584fc666ef745abe256ebd7d1",
    "13edb391d06c479720202eb5ac81f1c71fe64748",
    "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
    "40eb94ade3161d93e7a762a839004748f6d0ae89"
  ],
  "changeHistoryShort": {
    "93e23a99157c30b51752fc49748c3c210745a187": "Ybodychange",
    "07e4fb1455abc33584fc666ef745abe256ebd7d1": "Ybodychange",
    "13edb391d06c479720202eb5ac81f1c71fe64748": "Ybodychange",
    "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48": "Ymultichange(Yparameterchange,Ybodychange)",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": "Ymultichange(Ymovefromfile,Ymodifierchange,Yexceptionschange,Ybodychange,Yrename,Yparameterchange)",
    "40eb94ade3161d93e7a762a839004748f6d0ae89": "Yintroduced"
  },
  "changeHistoryDetails": {
    "93e23a99157c30b51752fc49748c3c210745a187": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6613. Improve logging in caching classes. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1607697 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/07/14 10:13 AM",
      "commitName": "93e23a99157c30b51752fc49748c3c210745a187",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "15/05/14 6:18 PM",
      "commitNameOld": "8f48760663070529ff09927d1772010fffe5f438",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 48.66,
      "commitsBetweenForRepo": 297,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,34 @@\n   private void processCacheReportImpl(final DatanodeDescriptor datanode,\n       final List\u003cLong\u003e blockIds) {\n     CachedBlocksList cached \u003d datanode.getCached();\n     cached.clear();\n     CachedBlocksList cachedList \u003d datanode.getCached();\n     CachedBlocksList pendingCachedList \u003d datanode.getPendingCached();\n     for (Iterator\u003cLong\u003e iter \u003d blockIds.iterator(); iter.hasNext(); ) {\n       long blockId \u003d iter.next();\n+      LOG.trace(\"Cache report from datanode {} has block {}\", datanode,\n+          blockId);\n       CachedBlock cachedBlock \u003d\n           new CachedBlock(blockId, (short)0, false);\n       CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n       // Add the block ID from the cache report to the cachedBlocks map\n       // if it\u0027s not already there.\n       if (prevCachedBlock !\u003d null) {\n         cachedBlock \u003d prevCachedBlock;\n       } else {\n         cachedBlocks.put(cachedBlock);\n+        LOG.trace(\"Added block {}  to cachedBlocks\", cachedBlock);\n       }\n       // Add the block to the datanode\u0027s implicit cached block list\n       // if it\u0027s not already there.  Similarly, remove it from the pending\n       // cached block list if it exists there.\n       if (!cachedBlock.isPresent(cachedList)) {\n         cachedList.add(cachedBlock);\n+        LOG.trace(\"Added block {} to CACHED list.\", cachedBlock);\n       }\n       if (cachedBlock.isPresent(pendingCachedList)) {\n         pendingCachedList.remove(cachedBlock);\n+        LOG.trace(\"Removed block {} from PENDING_CACHED list.\", cachedBlock);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final List\u003cLong\u003e blockIds) {\n    CachedBlocksList cached \u003d datanode.getCached();\n    cached.clear();\n    CachedBlocksList cachedList \u003d datanode.getCached();\n    CachedBlocksList pendingCachedList \u003d datanode.getPendingCached();\n    for (Iterator\u003cLong\u003e iter \u003d blockIds.iterator(); iter.hasNext(); ) {\n      long blockId \u003d iter.next();\n      LOG.trace(\"Cache report from datanode {} has block {}\", datanode,\n          blockId);\n      CachedBlock cachedBlock \u003d\n          new CachedBlock(blockId, (short)0, false);\n      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n      // Add the block ID from the cache report to the cachedBlocks map\n      // if it\u0027s not already there.\n      if (prevCachedBlock !\u003d null) {\n        cachedBlock \u003d prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n        LOG.trace(\"Added block {}  to cachedBlocks\", cachedBlock);\n      }\n      // Add the block to the datanode\u0027s implicit cached block list\n      // if it\u0027s not already there.  Similarly, remove it from the pending\n      // cached block list if it exists there.\n      if (!cachedBlock.isPresent(cachedList)) {\n        cachedList.add(cachedBlock);\n        LOG.trace(\"Added block {} to CACHED list.\", cachedBlock);\n      }\n      if (cachedBlock.isPresent(pendingCachedList)) {\n        pendingCachedList.remove(cachedBlock);\n        LOG.trace(\"Removed block {} from PENDING_CACHED list.\", cachedBlock);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "07e4fb1455abc33584fc666ef745abe256ebd7d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5708. The CacheManager throws a NPE in the DataNode logs when processing cache reports that refer to a block not known to the BlockManager. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1554594 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/12/13 4:01 PM",
      "commitName": "07e4fb1455abc33584fc666ef745abe256ebd7d1",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "20/12/13 3:27 PM",
      "commitNameOld": "b9ae3087c0f83bfeeea47ded8e19932b46fd2350",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 11.02,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,29 @@\n   private void processCacheReportImpl(final DatanodeDescriptor datanode,\n       final List\u003cLong\u003e blockIds) {\n     CachedBlocksList cached \u003d datanode.getCached();\n     cached.clear();\n+    CachedBlocksList cachedList \u003d datanode.getCached();\n+    CachedBlocksList pendingCachedList \u003d datanode.getPendingCached();\n     for (Iterator\u003cLong\u003e iter \u003d blockIds.iterator(); iter.hasNext(); ) {\n-      Block block \u003d new Block(iter.next());\n-      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n-      if (!blockInfo.isComplete()) {\n-        LOG.warn(\"Ignoring block id \" + block.getBlockId() + \", because \" +\n-            \"it is in not complete yet.  It is in state \" + \n-            blockInfo.getBlockUCState());\n-        continue;\n-      }\n-      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n-          blockManager.getCorruptReplicas(blockInfo);\n-      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n-        // The NameNode will eventually remove or update the corrupt block.\n-        // Until then, we pretend that it isn\u0027t cached.\n-        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n-            \" because it is corrupt.\");\n-        continue;\n-      }\n+      long blockId \u003d iter.next();\n       CachedBlock cachedBlock \u003d\n-          new CachedBlock(block.getBlockId(), (short)0, false);\n+          new CachedBlock(blockId, (short)0, false);\n       CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n-      // Use the existing CachedBlock if it\u0027s present; otherwise,\n-      // insert a new one.\n+      // Add the block ID from the cache report to the cachedBlocks map\n+      // if it\u0027s not already there.\n       if (prevCachedBlock !\u003d null) {\n         cachedBlock \u003d prevCachedBlock;\n       } else {\n         cachedBlocks.put(cachedBlock);\n       }\n-      if (!cachedBlock.isPresent(datanode.getCached())) {\n-        datanode.getCached().add(cachedBlock);\n+      // Add the block to the datanode\u0027s implicit cached block list\n+      // if it\u0027s not already there.  Similarly, remove it from the pending\n+      // cached block list if it exists there.\n+      if (!cachedBlock.isPresent(cachedList)) {\n+        cachedList.add(cachedBlock);\n       }\n-      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n-        datanode.getPendingCached().remove(cachedBlock);\n+      if (cachedBlock.isPresent(pendingCachedList)) {\n+        pendingCachedList.remove(cachedBlock);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final List\u003cLong\u003e blockIds) {\n    CachedBlocksList cached \u003d datanode.getCached();\n    cached.clear();\n    CachedBlocksList cachedList \u003d datanode.getCached();\n    CachedBlocksList pendingCachedList \u003d datanode.getPendingCached();\n    for (Iterator\u003cLong\u003e iter \u003d blockIds.iterator(); iter.hasNext(); ) {\n      long blockId \u003d iter.next();\n      CachedBlock cachedBlock \u003d\n          new CachedBlock(blockId, (short)0, false);\n      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n      // Add the block ID from the cache report to the cachedBlocks map\n      // if it\u0027s not already there.\n      if (prevCachedBlock !\u003d null) {\n        cachedBlock \u003d prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n      }\n      // Add the block to the datanode\u0027s implicit cached block list\n      // if it\u0027s not already there.  Similarly, remove it from the pending\n      // cached block list if it exists there.\n      if (!cachedBlock.isPresent(cachedList)) {\n        cachedList.add(cachedBlock);\n      }\n      if (cachedBlock.isPresent(pendingCachedList)) {\n        pendingCachedList.remove(cachedBlock);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "13edb391d06c479720202eb5ac81f1c71fe64748": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 9:55 AM",
      "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "21/11/13 9:12 AM",
      "commitNameOld": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 6.03,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,40 @@\n   private void processCacheReportImpl(final DatanodeDescriptor datanode,\n       final List\u003cLong\u003e blockIds) {\n     CachedBlocksList cached \u003d datanode.getCached();\n     cached.clear();\n     for (Iterator\u003cLong\u003e iter \u003d blockIds.iterator(); iter.hasNext(); ) {\n       Block block \u003d new Block(iter.next());\n       BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n-      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n-        // The NameNode will eventually remove or update the out-of-date block.\n-        // Until then, we pretend that it isn\u0027t cached.\n-        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n-          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n-        continue;\n-      }\n       if (!blockInfo.isComplete()) {\n         LOG.warn(\"Ignoring block id \" + block.getBlockId() + \", because \" +\n             \"it is in not complete yet.  It is in state \" + \n             blockInfo.getBlockUCState());\n         continue;\n       }\n       Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n           blockManager.getCorruptReplicas(blockInfo);\n       if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n         // The NameNode will eventually remove or update the corrupt block.\n         // Until then, we pretend that it isn\u0027t cached.\n         LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n             \" because it is corrupt.\");\n         continue;\n       }\n       CachedBlock cachedBlock \u003d\n           new CachedBlock(block.getBlockId(), (short)0, false);\n       CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n       // Use the existing CachedBlock if it\u0027s present; otherwise,\n       // insert a new one.\n       if (prevCachedBlock !\u003d null) {\n         cachedBlock \u003d prevCachedBlock;\n       } else {\n         cachedBlocks.put(cachedBlock);\n       }\n       if (!cachedBlock.isPresent(datanode.getCached())) {\n         datanode.getCached().add(cachedBlock);\n       }\n       if (cachedBlock.isPresent(datanode.getPendingCached())) {\n         datanode.getPendingCached().remove(cachedBlock);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final List\u003cLong\u003e blockIds) {\n    CachedBlocksList cached \u003d datanode.getCached();\n    cached.clear();\n    for (Iterator\u003cLong\u003e iter \u003d blockIds.iterator(); iter.hasNext(); ) {\n      Block block \u003d new Block(iter.next());\n      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n      if (!blockInfo.isComplete()) {\n        LOG.warn(\"Ignoring block id \" + block.getBlockId() + \", because \" +\n            \"it is in not complete yet.  It is in state \" + \n            blockInfo.getBlockUCState());\n        continue;\n      }\n      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n          blockManager.getCorruptReplicas(blockInfo);\n      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n        // The NameNode will eventually remove or update the corrupt block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n            \" because it is corrupt.\");\n        continue;\n      }\n      CachedBlock cachedBlock \u003d\n          new CachedBlock(block.getBlockId(), (short)0, false);\n      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n      // Use the existing CachedBlock if it\u0027s present; otherwise,\n      // insert a new one.\n      if (prevCachedBlock !\u003d null) {\n        cachedBlock \u003d prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n      }\n      if (!cachedBlock.isPresent(datanode.getCached())) {\n        datanode.getCached().add(cachedBlock);\n      }\n      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n        datanode.getPendingCached().remove(cachedBlock);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5378. In CacheReport, don\u0027t send genstamp and length on the wire (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1534334 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/10/13 12:29 PM",
      "commitName": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5378. In CacheReport, don\u0027t send genstamp and length on the wire (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1534334 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/10/13 12:29 PM",
          "commitName": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "18/10/13 3:15 PM",
          "commitNameOld": "d61af9781086073152113d97106f708ea1cf6e8c",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 2.88,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,47 @@\n   private void processCacheReportImpl(final DatanodeDescriptor datanode,\n-      final BlockListAsLongs report) {\n+      final List\u003cLong\u003e blockIds) {\n     CachedBlocksList cached \u003d datanode.getCached();\n     cached.clear();\n-    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n-    while (itBR.hasNext()) {\n-      Block block \u003d itBR.next();\n-      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n-      if (iState !\u003d ReplicaState.FINALIZED) {\n-        LOG.error(\"Cached block report contained unfinalized block \" + block);\n-        continue;\n-      }\n+    for (Iterator\u003cLong\u003e iter \u003d blockIds.iterator(); iter.hasNext(); ) {\n+      Block block \u003d new Block(iter.next());\n       BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n       if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n         // The NameNode will eventually remove or update the out-of-date block.\n         // Until then, we pretend that it isn\u0027t cached.\n         LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n           block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n         continue;\n       }\n+      if (!blockInfo.isComplete()) {\n+        LOG.warn(\"Ignoring block id \" + block.getBlockId() + \", because \" +\n+            \"it is in not complete yet.  It is in state \" + \n+            blockInfo.getBlockUCState());\n+        continue;\n+      }\n       Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n           blockManager.getCorruptReplicas(blockInfo);\n       if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n         // The NameNode will eventually remove or update the corrupt block.\n         // Until then, we pretend that it isn\u0027t cached.\n         LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n             \" because it is corrupt.\");\n         continue;\n       }\n       CachedBlock cachedBlock \u003d\n           new CachedBlock(block.getBlockId(), (short)0, false);\n       CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n       // Use the existing CachedBlock if it\u0027s present; otherwise,\n       // insert a new one.\n       if (prevCachedBlock !\u003d null) {\n         cachedBlock \u003d prevCachedBlock;\n       } else {\n         cachedBlocks.put(cachedBlock);\n       }\n       if (!cachedBlock.isPresent(datanode.getCached())) {\n         datanode.getCached().add(cachedBlock);\n       }\n       if (cachedBlock.isPresent(datanode.getPendingCached())) {\n         datanode.getPendingCached().remove(cachedBlock);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final List\u003cLong\u003e blockIds) {\n    CachedBlocksList cached \u003d datanode.getCached();\n    cached.clear();\n    for (Iterator\u003cLong\u003e iter \u003d blockIds.iterator(); iter.hasNext(); ) {\n      Block block \u003d new Block(iter.next());\n      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n        // The NameNode will eventually remove or update the out-of-date block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n        continue;\n      }\n      if (!blockInfo.isComplete()) {\n        LOG.warn(\"Ignoring block id \" + block.getBlockId() + \", because \" +\n            \"it is in not complete yet.  It is in state \" + \n            blockInfo.getBlockUCState());\n        continue;\n      }\n      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n          blockManager.getCorruptReplicas(blockInfo);\n      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n        // The NameNode will eventually remove or update the corrupt block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n            \" because it is corrupt.\");\n        continue;\n      }\n      CachedBlock cachedBlock \u003d\n          new CachedBlock(block.getBlockId(), (short)0, false);\n      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n      // Use the existing CachedBlock if it\u0027s present; otherwise,\n      // insert a new one.\n      if (prevCachedBlock !\u003d null) {\n        cachedBlock \u003d prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n      }\n      if (!cachedBlock.isPresent(datanode.getCached())) {\n        datanode.getCached().add(cachedBlock);\n      }\n      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n        datanode.getPendingCached().remove(cachedBlock);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[datanode-DatanodeDescriptor(modifiers-final), report-BlockListAsLongs(modifiers-final)]",
            "newValue": "[datanode-DatanodeDescriptor(modifiers-final), blockIds-List\u003cLong\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5378. In CacheReport, don\u0027t send genstamp and length on the wire (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1534334 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/10/13 12:29 PM",
          "commitName": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "18/10/13 3:15 PM",
          "commitNameOld": "d61af9781086073152113d97106f708ea1cf6e8c",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 2.88,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,47 @@\n   private void processCacheReportImpl(final DatanodeDescriptor datanode,\n-      final BlockListAsLongs report) {\n+      final List\u003cLong\u003e blockIds) {\n     CachedBlocksList cached \u003d datanode.getCached();\n     cached.clear();\n-    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n-    while (itBR.hasNext()) {\n-      Block block \u003d itBR.next();\n-      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n-      if (iState !\u003d ReplicaState.FINALIZED) {\n-        LOG.error(\"Cached block report contained unfinalized block \" + block);\n-        continue;\n-      }\n+    for (Iterator\u003cLong\u003e iter \u003d blockIds.iterator(); iter.hasNext(); ) {\n+      Block block \u003d new Block(iter.next());\n       BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n       if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n         // The NameNode will eventually remove or update the out-of-date block.\n         // Until then, we pretend that it isn\u0027t cached.\n         LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n           block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n         continue;\n       }\n+      if (!blockInfo.isComplete()) {\n+        LOG.warn(\"Ignoring block id \" + block.getBlockId() + \", because \" +\n+            \"it is in not complete yet.  It is in state \" + \n+            blockInfo.getBlockUCState());\n+        continue;\n+      }\n       Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n           blockManager.getCorruptReplicas(blockInfo);\n       if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n         // The NameNode will eventually remove or update the corrupt block.\n         // Until then, we pretend that it isn\u0027t cached.\n         LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n             \" because it is corrupt.\");\n         continue;\n       }\n       CachedBlock cachedBlock \u003d\n           new CachedBlock(block.getBlockId(), (short)0, false);\n       CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n       // Use the existing CachedBlock if it\u0027s present; otherwise,\n       // insert a new one.\n       if (prevCachedBlock !\u003d null) {\n         cachedBlock \u003d prevCachedBlock;\n       } else {\n         cachedBlocks.put(cachedBlock);\n       }\n       if (!cachedBlock.isPresent(datanode.getCached())) {\n         datanode.getCached().add(cachedBlock);\n       }\n       if (cachedBlock.isPresent(datanode.getPendingCached())) {\n         datanode.getPendingCached().remove(cachedBlock);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final List\u003cLong\u003e blockIds) {\n    CachedBlocksList cached \u003d datanode.getCached();\n    cached.clear();\n    for (Iterator\u003cLong\u003e iter \u003d blockIds.iterator(); iter.hasNext(); ) {\n      Block block \u003d new Block(iter.next());\n      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n        // The NameNode will eventually remove or update the out-of-date block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n        continue;\n      }\n      if (!blockInfo.isComplete()) {\n        LOG.warn(\"Ignoring block id \" + block.getBlockId() + \", because \" +\n            \"it is in not complete yet.  It is in state \" + \n            blockInfo.getBlockUCState());\n        continue;\n      }\n      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n          blockManager.getCorruptReplicas(blockInfo);\n      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n        // The NameNode will eventually remove or update the corrupt block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n            \" because it is corrupt.\");\n        continue;\n      }\n      CachedBlock cachedBlock \u003d\n          new CachedBlock(block.getBlockId(), (short)0, false);\n      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n      // Use the existing CachedBlock if it\u0027s present; otherwise,\n      // insert a new one.\n      if (prevCachedBlock !\u003d null) {\n        cachedBlock \u003d prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n      }\n      if (!cachedBlock.isPresent(datanode.getCached())) {\n        datanode.getCached().add(cachedBlock);\n      }\n      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n        datanode.getPendingCached().remove(cachedBlock);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Yexceptionschange,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 3:15 PM",
      "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/10/13 1:23 PM",
          "commitNameOld": "8da82eba1c84f828617a13a6f785a9b6cfc057a5",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,47 @@\n-  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n-      final BlockListAsLongs newReport) throws IOException {\n-    if (!isCachingEnabled) {\n-      String error \u003d \"cacheReport received from datanode \" + nodeID\n-          + \" but caching is disabled on the namenode (\"\n-          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n-      LOG.warn(error + \", ignoring\");\n-      throw new IOException(error);\n-    }\n-    namesystem.writeLock();\n-    final long startTime \u003d Time.now(); //after acquiring write lock\n-    final long endTime;\n-    try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive) {\n-        throw new IOException(\n-            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n+      final BlockListAsLongs report) {\n+    CachedBlocksList cached \u003d datanode.getCached();\n+    cached.clear();\n+    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n+    while (itBR.hasNext()) {\n+      Block block \u003d itBR.next();\n+      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n+      if (iState !\u003d ReplicaState.FINALIZED) {\n+        LOG.error(\"Cached block report contained unfinalized block \" + block);\n+        continue;\n       }\n-\n-      // TODO: do an optimized initial cache report while in startup safemode\n-      if (namesystem.isInStartupSafeMode()) {\n-        blockLogInfo(\"#processCacheReport: \"\n-            + \"discarded cache report from \" + nodeID\n-            + \" because namenode still in startup phase\");\n-        return;\n+      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n+      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n+        // The NameNode will eventually remove or update the out-of-date block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n+          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n+        continue;\n       }\n-\n-      processReport(node, newReport);\n-\n-      // TODO: process postponed blocks reported while a standby\n-      //rescanPostponedMisreplicatedBlocks();\n-    } finally {\n-      endTime \u003d Time.now();\n-      namesystem.writeUnlock();\n+      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n+          blockManager.getCorruptReplicas(blockInfo);\n+      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n+        // The NameNode will eventually remove or update the corrupt block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n+            \" because it is corrupt.\");\n+        continue;\n+      }\n+      CachedBlock cachedBlock \u003d\n+          new CachedBlock(block.getBlockId(), (short)0, false);\n+      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n+      // Use the existing CachedBlock if it\u0027s present; otherwise,\n+      // insert a new one.\n+      if (prevCachedBlock !\u003d null) {\n+        cachedBlock \u003d prevCachedBlock;\n+      } else {\n+        cachedBlocks.put(cachedBlock);\n+      }\n+      if (!cachedBlock.isPresent(datanode.getCached())) {\n+        datanode.getCached().add(cachedBlock);\n+      }\n+      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n+        datanode.getPendingCached().remove(cachedBlock);\n+      }\n     }\n-\n-    // Log the block report processing stats from Namenode perspective\n-    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n-    if (metrics !\u003d null) {\n-      metrics.addCacheBlockReport((int) (endTime - startTime));\n-    }\n-    blockLogInfo(\"#processCacheReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n-        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final BlockListAsLongs report) {\n    CachedBlocksList cached \u003d datanode.getCached();\n    cached.clear();\n    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n    while (itBR.hasNext()) {\n      Block block \u003d itBR.next();\n      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n      if (iState !\u003d ReplicaState.FINALIZED) {\n        LOG.error(\"Cached block report contained unfinalized block \" + block);\n        continue;\n      }\n      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n        // The NameNode will eventually remove or update the out-of-date block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n        continue;\n      }\n      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n          blockManager.getCorruptReplicas(blockInfo);\n      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n        // The NameNode will eventually remove or update the corrupt block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n            \" because it is corrupt.\");\n        continue;\n      }\n      CachedBlock cachedBlock \u003d\n          new CachedBlock(block.getBlockId(), (short)0, false);\n      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n      // Use the existing CachedBlock if it\u0027s present; otherwise,\n      // insert a new one.\n      if (prevCachedBlock !\u003d null) {\n        cachedBlock \u003d prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n      }\n      if (!cachedBlock.isPresent(datanode.getCached())) {\n        datanode.getCached().add(cachedBlock);\n      }\n      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n        datanode.getPendingCached().remove(cachedBlock);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationManager.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
            "oldMethodName": "processCacheReport",
            "newMethodName": "processCacheReportImpl"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/10/13 1:23 PM",
          "commitNameOld": "8da82eba1c84f828617a13a6f785a9b6cfc057a5",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,47 @@\n-  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n-      final BlockListAsLongs newReport) throws IOException {\n-    if (!isCachingEnabled) {\n-      String error \u003d \"cacheReport received from datanode \" + nodeID\n-          + \" but caching is disabled on the namenode (\"\n-          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n-      LOG.warn(error + \", ignoring\");\n-      throw new IOException(error);\n-    }\n-    namesystem.writeLock();\n-    final long startTime \u003d Time.now(); //after acquiring write lock\n-    final long endTime;\n-    try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive) {\n-        throw new IOException(\n-            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n+      final BlockListAsLongs report) {\n+    CachedBlocksList cached \u003d datanode.getCached();\n+    cached.clear();\n+    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n+    while (itBR.hasNext()) {\n+      Block block \u003d itBR.next();\n+      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n+      if (iState !\u003d ReplicaState.FINALIZED) {\n+        LOG.error(\"Cached block report contained unfinalized block \" + block);\n+        continue;\n       }\n-\n-      // TODO: do an optimized initial cache report while in startup safemode\n-      if (namesystem.isInStartupSafeMode()) {\n-        blockLogInfo(\"#processCacheReport: \"\n-            + \"discarded cache report from \" + nodeID\n-            + \" because namenode still in startup phase\");\n-        return;\n+      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n+      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n+        // The NameNode will eventually remove or update the out-of-date block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n+          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n+        continue;\n       }\n-\n-      processReport(node, newReport);\n-\n-      // TODO: process postponed blocks reported while a standby\n-      //rescanPostponedMisreplicatedBlocks();\n-    } finally {\n-      endTime \u003d Time.now();\n-      namesystem.writeUnlock();\n+      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n+          blockManager.getCorruptReplicas(blockInfo);\n+      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n+        // The NameNode will eventually remove or update the corrupt block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n+            \" because it is corrupt.\");\n+        continue;\n+      }\n+      CachedBlock cachedBlock \u003d\n+          new CachedBlock(block.getBlockId(), (short)0, false);\n+      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n+      // Use the existing CachedBlock if it\u0027s present; otherwise,\n+      // insert a new one.\n+      if (prevCachedBlock !\u003d null) {\n+        cachedBlock \u003d prevCachedBlock;\n+      } else {\n+        cachedBlocks.put(cachedBlock);\n+      }\n+      if (!cachedBlock.isPresent(datanode.getCached())) {\n+        datanode.getCached().add(cachedBlock);\n+      }\n+      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n+        datanode.getPendingCached().remove(cachedBlock);\n+      }\n     }\n-\n-    // Log the block report processing stats from Namenode perspective\n-    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n-    if (metrics !\u003d null) {\n-      metrics.addCacheBlockReport((int) (endTime - startTime));\n-    }\n-    blockLogInfo(\"#processCacheReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n-        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final BlockListAsLongs report) {\n    CachedBlocksList cached \u003d datanode.getCached();\n    cached.clear();\n    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n    while (itBR.hasNext()) {\n      Block block \u003d itBR.next();\n      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n      if (iState !\u003d ReplicaState.FINALIZED) {\n        LOG.error(\"Cached block report contained unfinalized block \" + block);\n        continue;\n      }\n      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n        // The NameNode will eventually remove or update the out-of-date block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n        continue;\n      }\n      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n          blockManager.getCorruptReplicas(blockInfo);\n      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n        // The NameNode will eventually remove or update the corrupt block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n            \" because it is corrupt.\");\n        continue;\n      }\n      CachedBlock cachedBlock \u003d\n          new CachedBlock(block.getBlockId(), (short)0, false);\n      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n      // Use the existing CachedBlock if it\u0027s present; otherwise,\n      // insert a new one.\n      if (prevCachedBlock !\u003d null) {\n        cachedBlock \u003d prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n      }\n      if (!cachedBlock.isPresent(datanode.getCached())) {\n        datanode.getCached().add(cachedBlock);\n      }\n      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n        datanode.getPendingCached().remove(cachedBlock);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/10/13 1:23 PM",
          "commitNameOld": "8da82eba1c84f828617a13a6f785a9b6cfc057a5",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,47 @@\n-  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n-      final BlockListAsLongs newReport) throws IOException {\n-    if (!isCachingEnabled) {\n-      String error \u003d \"cacheReport received from datanode \" + nodeID\n-          + \" but caching is disabled on the namenode (\"\n-          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n-      LOG.warn(error + \", ignoring\");\n-      throw new IOException(error);\n-    }\n-    namesystem.writeLock();\n-    final long startTime \u003d Time.now(); //after acquiring write lock\n-    final long endTime;\n-    try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive) {\n-        throw new IOException(\n-            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n+      final BlockListAsLongs report) {\n+    CachedBlocksList cached \u003d datanode.getCached();\n+    cached.clear();\n+    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n+    while (itBR.hasNext()) {\n+      Block block \u003d itBR.next();\n+      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n+      if (iState !\u003d ReplicaState.FINALIZED) {\n+        LOG.error(\"Cached block report contained unfinalized block \" + block);\n+        continue;\n       }\n-\n-      // TODO: do an optimized initial cache report while in startup safemode\n-      if (namesystem.isInStartupSafeMode()) {\n-        blockLogInfo(\"#processCacheReport: \"\n-            + \"discarded cache report from \" + nodeID\n-            + \" because namenode still in startup phase\");\n-        return;\n+      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n+      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n+        // The NameNode will eventually remove or update the out-of-date block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n+          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n+        continue;\n       }\n-\n-      processReport(node, newReport);\n-\n-      // TODO: process postponed blocks reported while a standby\n-      //rescanPostponedMisreplicatedBlocks();\n-    } finally {\n-      endTime \u003d Time.now();\n-      namesystem.writeUnlock();\n+      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n+          blockManager.getCorruptReplicas(blockInfo);\n+      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n+        // The NameNode will eventually remove or update the corrupt block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n+            \" because it is corrupt.\");\n+        continue;\n+      }\n+      CachedBlock cachedBlock \u003d\n+          new CachedBlock(block.getBlockId(), (short)0, false);\n+      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n+      // Use the existing CachedBlock if it\u0027s present; otherwise,\n+      // insert a new one.\n+      if (prevCachedBlock !\u003d null) {\n+        cachedBlock \u003d prevCachedBlock;\n+      } else {\n+        cachedBlocks.put(cachedBlock);\n+      }\n+      if (!cachedBlock.isPresent(datanode.getCached())) {\n+        datanode.getCached().add(cachedBlock);\n+      }\n+      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n+        datanode.getPendingCached().remove(cachedBlock);\n+      }\n     }\n-\n-    // Log the block report processing stats from Namenode perspective\n-    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n-    if (metrics !\u003d null) {\n-      metrics.addCacheBlockReport((int) (endTime - startTime));\n-    }\n-    blockLogInfo(\"#processCacheReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n-        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final BlockListAsLongs report) {\n    CachedBlocksList cached \u003d datanode.getCached();\n    cached.clear();\n    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n    while (itBR.hasNext()) {\n      Block block \u003d itBR.next();\n      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n      if (iState !\u003d ReplicaState.FINALIZED) {\n        LOG.error(\"Cached block report contained unfinalized block \" + block);\n        continue;\n      }\n      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n        // The NameNode will eventually remove or update the out-of-date block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n        continue;\n      }\n      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n          blockManager.getCorruptReplicas(blockInfo);\n      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n        // The NameNode will eventually remove or update the corrupt block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n            \" because it is corrupt.\");\n        continue;\n      }\n      CachedBlock cachedBlock \u003d\n          new CachedBlock(block.getBlockId(), (short)0, false);\n      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n      // Use the existing CachedBlock if it\u0027s present; otherwise,\n      // insert a new one.\n      if (prevCachedBlock !\u003d null) {\n        cachedBlock \u003d prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n      }\n      if (!cachedBlock.isPresent(datanode.getCached())) {\n        datanode.getCached().add(cachedBlock);\n      }\n      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n        datanode.getPendingCached().remove(cachedBlock);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[IOException]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/10/13 1:23 PM",
          "commitNameOld": "8da82eba1c84f828617a13a6f785a9b6cfc057a5",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,47 @@\n-  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n-      final BlockListAsLongs newReport) throws IOException {\n-    if (!isCachingEnabled) {\n-      String error \u003d \"cacheReport received from datanode \" + nodeID\n-          + \" but caching is disabled on the namenode (\"\n-          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n-      LOG.warn(error + \", ignoring\");\n-      throw new IOException(error);\n-    }\n-    namesystem.writeLock();\n-    final long startTime \u003d Time.now(); //after acquiring write lock\n-    final long endTime;\n-    try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive) {\n-        throw new IOException(\n-            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n+      final BlockListAsLongs report) {\n+    CachedBlocksList cached \u003d datanode.getCached();\n+    cached.clear();\n+    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n+    while (itBR.hasNext()) {\n+      Block block \u003d itBR.next();\n+      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n+      if (iState !\u003d ReplicaState.FINALIZED) {\n+        LOG.error(\"Cached block report contained unfinalized block \" + block);\n+        continue;\n       }\n-\n-      // TODO: do an optimized initial cache report while in startup safemode\n-      if (namesystem.isInStartupSafeMode()) {\n-        blockLogInfo(\"#processCacheReport: \"\n-            + \"discarded cache report from \" + nodeID\n-            + \" because namenode still in startup phase\");\n-        return;\n+      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n+      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n+        // The NameNode will eventually remove or update the out-of-date block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n+          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n+        continue;\n       }\n-\n-      processReport(node, newReport);\n-\n-      // TODO: process postponed blocks reported while a standby\n-      //rescanPostponedMisreplicatedBlocks();\n-    } finally {\n-      endTime \u003d Time.now();\n-      namesystem.writeUnlock();\n+      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n+          blockManager.getCorruptReplicas(blockInfo);\n+      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n+        // The NameNode will eventually remove or update the corrupt block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n+            \" because it is corrupt.\");\n+        continue;\n+      }\n+      CachedBlock cachedBlock \u003d\n+          new CachedBlock(block.getBlockId(), (short)0, false);\n+      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n+      // Use the existing CachedBlock if it\u0027s present; otherwise,\n+      // insert a new one.\n+      if (prevCachedBlock !\u003d null) {\n+        cachedBlock \u003d prevCachedBlock;\n+      } else {\n+        cachedBlocks.put(cachedBlock);\n+      }\n+      if (!cachedBlock.isPresent(datanode.getCached())) {\n+        datanode.getCached().add(cachedBlock);\n+      }\n+      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n+        datanode.getPendingCached().remove(cachedBlock);\n+      }\n     }\n-\n-    // Log the block report processing stats from Namenode perspective\n-    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n-    if (metrics !\u003d null) {\n-      metrics.addCacheBlockReport((int) (endTime - startTime));\n-    }\n-    blockLogInfo(\"#processCacheReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n-        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final BlockListAsLongs report) {\n    CachedBlocksList cached \u003d datanode.getCached();\n    cached.clear();\n    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n    while (itBR.hasNext()) {\n      Block block \u003d itBR.next();\n      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n      if (iState !\u003d ReplicaState.FINALIZED) {\n        LOG.error(\"Cached block report contained unfinalized block \" + block);\n        continue;\n      }\n      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n        // The NameNode will eventually remove or update the out-of-date block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n        continue;\n      }\n      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n          blockManager.getCorruptReplicas(blockInfo);\n      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n        // The NameNode will eventually remove or update the corrupt block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n            \" because it is corrupt.\");\n        continue;\n      }\n      CachedBlock cachedBlock \u003d\n          new CachedBlock(block.getBlockId(), (short)0, false);\n      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n      // Use the existing CachedBlock if it\u0027s present; otherwise,\n      // insert a new one.\n      if (prevCachedBlock !\u003d null) {\n        cachedBlock \u003d prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n      }\n      if (!cachedBlock.isPresent(datanode.getCached())) {\n        datanode.getCached().add(cachedBlock);\n      }\n      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n        datanode.getPendingCached().remove(cachedBlock);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/10/13 1:23 PM",
          "commitNameOld": "8da82eba1c84f828617a13a6f785a9b6cfc057a5",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,47 @@\n-  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n-      final BlockListAsLongs newReport) throws IOException {\n-    if (!isCachingEnabled) {\n-      String error \u003d \"cacheReport received from datanode \" + nodeID\n-          + \" but caching is disabled on the namenode (\"\n-          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n-      LOG.warn(error + \", ignoring\");\n-      throw new IOException(error);\n-    }\n-    namesystem.writeLock();\n-    final long startTime \u003d Time.now(); //after acquiring write lock\n-    final long endTime;\n-    try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive) {\n-        throw new IOException(\n-            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n+      final BlockListAsLongs report) {\n+    CachedBlocksList cached \u003d datanode.getCached();\n+    cached.clear();\n+    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n+    while (itBR.hasNext()) {\n+      Block block \u003d itBR.next();\n+      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n+      if (iState !\u003d ReplicaState.FINALIZED) {\n+        LOG.error(\"Cached block report contained unfinalized block \" + block);\n+        continue;\n       }\n-\n-      // TODO: do an optimized initial cache report while in startup safemode\n-      if (namesystem.isInStartupSafeMode()) {\n-        blockLogInfo(\"#processCacheReport: \"\n-            + \"discarded cache report from \" + nodeID\n-            + \" because namenode still in startup phase\");\n-        return;\n+      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n+      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n+        // The NameNode will eventually remove or update the out-of-date block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n+          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n+        continue;\n       }\n-\n-      processReport(node, newReport);\n-\n-      // TODO: process postponed blocks reported while a standby\n-      //rescanPostponedMisreplicatedBlocks();\n-    } finally {\n-      endTime \u003d Time.now();\n-      namesystem.writeUnlock();\n+      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n+          blockManager.getCorruptReplicas(blockInfo);\n+      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n+        // The NameNode will eventually remove or update the corrupt block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n+            \" because it is corrupt.\");\n+        continue;\n+      }\n+      CachedBlock cachedBlock \u003d\n+          new CachedBlock(block.getBlockId(), (short)0, false);\n+      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n+      // Use the existing CachedBlock if it\u0027s present; otherwise,\n+      // insert a new one.\n+      if (prevCachedBlock !\u003d null) {\n+        cachedBlock \u003d prevCachedBlock;\n+      } else {\n+        cachedBlocks.put(cachedBlock);\n+      }\n+      if (!cachedBlock.isPresent(datanode.getCached())) {\n+        datanode.getCached().add(cachedBlock);\n+      }\n+      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n+        datanode.getPendingCached().remove(cachedBlock);\n+      }\n     }\n-\n-    // Log the block report processing stats from Namenode perspective\n-    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n-    if (metrics !\u003d null) {\n-      metrics.addCacheBlockReport((int) (endTime - startTime));\n-    }\n-    blockLogInfo(\"#processCacheReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n-        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final BlockListAsLongs report) {\n    CachedBlocksList cached \u003d datanode.getCached();\n    cached.clear();\n    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n    while (itBR.hasNext()) {\n      Block block \u003d itBR.next();\n      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n      if (iState !\u003d ReplicaState.FINALIZED) {\n        LOG.error(\"Cached block report contained unfinalized block \" + block);\n        continue;\n      }\n      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n        // The NameNode will eventually remove or update the out-of-date block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n        continue;\n      }\n      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n          blockManager.getCorruptReplicas(blockInfo);\n      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n        // The NameNode will eventually remove or update the corrupt block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n            \" because it is corrupt.\");\n        continue;\n      }\n      CachedBlock cachedBlock \u003d\n          new CachedBlock(block.getBlockId(), (short)0, false);\n      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n      // Use the existing CachedBlock if it\u0027s present; otherwise,\n      // insert a new one.\n      if (prevCachedBlock !\u003d null) {\n        cachedBlock \u003d prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n      }\n      if (!cachedBlock.isPresent(datanode.getCached())) {\n        datanode.getCached().add(cachedBlock);\n      }\n      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n        datanode.getPendingCached().remove(cachedBlock);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "processCacheReport",
            "newValue": "processCacheReportImpl"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/10/13 1:23 PM",
          "commitNameOld": "8da82eba1c84f828617a13a6f785a9b6cfc057a5",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,47 @@\n-  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n-      final BlockListAsLongs newReport) throws IOException {\n-    if (!isCachingEnabled) {\n-      String error \u003d \"cacheReport received from datanode \" + nodeID\n-          + \" but caching is disabled on the namenode (\"\n-          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n-      LOG.warn(error + \", ignoring\");\n-      throw new IOException(error);\n-    }\n-    namesystem.writeLock();\n-    final long startTime \u003d Time.now(); //after acquiring write lock\n-    final long endTime;\n-    try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive) {\n-        throw new IOException(\n-            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n+      final BlockListAsLongs report) {\n+    CachedBlocksList cached \u003d datanode.getCached();\n+    cached.clear();\n+    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n+    while (itBR.hasNext()) {\n+      Block block \u003d itBR.next();\n+      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n+      if (iState !\u003d ReplicaState.FINALIZED) {\n+        LOG.error(\"Cached block report contained unfinalized block \" + block);\n+        continue;\n       }\n-\n-      // TODO: do an optimized initial cache report while in startup safemode\n-      if (namesystem.isInStartupSafeMode()) {\n-        blockLogInfo(\"#processCacheReport: \"\n-            + \"discarded cache report from \" + nodeID\n-            + \" because namenode still in startup phase\");\n-        return;\n+      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n+      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n+        // The NameNode will eventually remove or update the out-of-date block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n+          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n+        continue;\n       }\n-\n-      processReport(node, newReport);\n-\n-      // TODO: process postponed blocks reported while a standby\n-      //rescanPostponedMisreplicatedBlocks();\n-    } finally {\n-      endTime \u003d Time.now();\n-      namesystem.writeUnlock();\n+      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n+          blockManager.getCorruptReplicas(blockInfo);\n+      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n+        // The NameNode will eventually remove or update the corrupt block.\n+        // Until then, we pretend that it isn\u0027t cached.\n+        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n+            \" because it is corrupt.\");\n+        continue;\n+      }\n+      CachedBlock cachedBlock \u003d\n+          new CachedBlock(block.getBlockId(), (short)0, false);\n+      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n+      // Use the existing CachedBlock if it\u0027s present; otherwise,\n+      // insert a new one.\n+      if (prevCachedBlock !\u003d null) {\n+        cachedBlock \u003d prevCachedBlock;\n+      } else {\n+        cachedBlocks.put(cachedBlock);\n+      }\n+      if (!cachedBlock.isPresent(datanode.getCached())) {\n+        datanode.getCached().add(cachedBlock);\n+      }\n+      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n+        datanode.getPendingCached().remove(cachedBlock);\n+      }\n     }\n-\n-    // Log the block report processing stats from Namenode perspective\n-    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n-    if (metrics !\u003d null) {\n-      metrics.addCacheBlockReport((int) (endTime - startTime));\n-    }\n-    blockLogInfo(\"#processCacheReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n-        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processCacheReportImpl(final DatanodeDescriptor datanode,\n      final BlockListAsLongs report) {\n    CachedBlocksList cached \u003d datanode.getCached();\n    cached.clear();\n    BlockReportIterator itBR \u003d report.getBlockReportIterator();\n    while (itBR.hasNext()) {\n      Block block \u003d itBR.next();\n      ReplicaState iState \u003d itBR.getCurrentReplicaState();\n      if (iState !\u003d ReplicaState.FINALIZED) {\n        LOG.error(\"Cached block report contained unfinalized block \" + block);\n        continue;\n      }\n      BlockInfo blockInfo \u003d blockManager.getStoredBlock(block);\n      if (blockInfo.getGenerationStamp() \u003c block.getGenerationStamp()) {\n        // The NameNode will eventually remove or update the out-of-date block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Genstamp in cache report disagrees with our genstamp for \" +\n          block + \": expected genstamp \" + blockInfo.getGenerationStamp());\n        continue;\n      }\n      Collection\u003cDatanodeDescriptor\u003e corruptReplicas \u003d\n          blockManager.getCorruptReplicas(blockInfo);\n      if ((corruptReplicas !\u003d null) \u0026\u0026 corruptReplicas.contains(datanode)) {\n        // The NameNode will eventually remove or update the corrupt block.\n        // Until then, we pretend that it isn\u0027t cached.\n        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n            \" because it is corrupt.\");\n        continue;\n      }\n      CachedBlock cachedBlock \u003d\n          new CachedBlock(block.getBlockId(), (short)0, false);\n      CachedBlock prevCachedBlock \u003d cachedBlocks.get(cachedBlock);\n      // Use the existing CachedBlock if it\u0027s present; otherwise,\n      // insert a new one.\n      if (prevCachedBlock !\u003d null) {\n        cachedBlock \u003d prevCachedBlock;\n      } else {\n        cachedBlocks.put(cachedBlock);\n      }\n      if (!cachedBlock.isPresent(datanode.getCached())) {\n        datanode.getCached().add(cachedBlock);\n      }\n      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n        datanode.getPendingCached().remove(cachedBlock);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[nodeID-DatanodeID(modifiers-final), poolId-String(modifiers-final), newReport-BlockListAsLongs(modifiers-final)]",
            "newValue": "[datanode-DatanodeDescriptor(modifiers-final), report-BlockListAsLongs(modifiers-final)]"
          }
        }
      ]
    },
    "40eb94ade3161d93e7a762a839004748f6d0ae89": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5053. NameNode should invoke DataNode APIs to coordinate caching. (Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1523145 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/09/13 4:27 PM",
      "commitName": "40eb94ade3161d93e7a762a839004748f6d0ae89",
      "commitAuthor": "Andrew Wang",
      "diff": "@@ -0,0 +1,45 @@\n+  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n+      final BlockListAsLongs newReport) throws IOException {\n+    if (!isCachingEnabled) {\n+      String error \u003d \"cacheReport received from datanode \" + nodeID\n+          + \" but caching is disabled on the namenode (\"\n+          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n+      LOG.warn(error + \", ignoring\");\n+      throw new IOException(error);\n+    }\n+    namesystem.writeLock();\n+    final long startTime \u003d Time.now(); //after acquiring write lock\n+    final long endTime;\n+    try {\n+      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n+      if (node \u003d\u003d null || !node.isAlive) {\n+        throw new IOException(\n+            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+      }\n+\n+      // TODO: do an optimized initial cache report while in startup safemode\n+      if (namesystem.isInStartupSafeMode()) {\n+        blockLogInfo(\"#processCacheReport: \"\n+            + \"discarded cache report from \" + nodeID\n+            + \" because namenode still in startup phase\");\n+        return;\n+      }\n+\n+      processReport(node, newReport);\n+\n+      // TODO: process postponed blocks reported while a standby\n+      //rescanPostponedMisreplicatedBlocks();\n+    } finally {\n+      endTime \u003d Time.now();\n+      namesystem.writeUnlock();\n+    }\n+\n+    // Log the block report processing stats from Namenode perspective\n+    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n+    if (metrics !\u003d null) {\n+      metrics.addCacheBlockReport((int) (endTime - startTime));\n+    }\n+    blockLogInfo(\"#processCacheReport: from \"\n+        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n+        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n      final BlockListAsLongs newReport) throws IOException {\n    if (!isCachingEnabled) {\n      String error \u003d \"cacheReport received from datanode \" + nodeID\n          + \" but caching is disabled on the namenode (\"\n          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n      LOG.warn(error + \", ignoring\");\n      throw new IOException(error);\n    }\n    namesystem.writeLock();\n    final long startTime \u003d Time.now(); //after acquiring write lock\n    final long endTime;\n    try {\n      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // TODO: do an optimized initial cache report while in startup safemode\n      if (namesystem.isInStartupSafeMode()) {\n        blockLogInfo(\"#processCacheReport: \"\n            + \"discarded cache report from \" + nodeID\n            + \" because namenode still in startup phase\");\n        return;\n      }\n\n      processReport(node, newReport);\n\n      // TODO: process postponed blocks reported while a standby\n      //rescanPostponedMisreplicatedBlocks();\n    } finally {\n      endTime \u003d Time.now();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    blockLogInfo(\"#processCacheReport: from \"\n        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationManager.java"
    }
  }
}