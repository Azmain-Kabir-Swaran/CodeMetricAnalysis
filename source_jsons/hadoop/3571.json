{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "openInfo",
  "functionId": "openInfo___refreshLocatedBlocks-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 245,
  "functionEndLine": 273,
  "numCommitsSeen": 228,
  "timeTaken": 9036,
  "changeHistory": [
    "3123895db0518552eb357093fddf0e8ff1a6cadf",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "bff5999d07e9416a22846c849487e509ede55040",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
    "b0947f93056939d419882c13d4acf3e935a66e62",
    "d28b98242854ff7f9d615e1c9d6a5b7584ce2498",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "3123895db0518552eb357093fddf0e8ff1a6cadf": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Yexceptionschange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "bff5999d07e9416a22846c849487e509ede55040": "Ymultichange(Yparameterchange,Ybodychange)",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f": "Ymultichange(Ymodifierchange,Ybodychange)",
    "b0947f93056939d419882c13d4acf3e935a66e62": "Ybodychange",
    "d28b98242854ff7f9d615e1c9d6a5b7584ce2498": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3123895db0518552eb357093fddf0e8ff1a6cadf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12318. Fix IOException condition for openInfo in DFSInputStream. Contributed by legend.\n",
      "commitDate": "24/08/17 12:26 AM",
      "commitName": "3123895db0518552eb357093fddf0e8ff1a6cadf",
      "commitAuthor": "John Zhuge",
      "commitDateOld": "21/08/17 1:45 PM",
      "commitNameOld": "b6bfb2fcb2391d51b8de97c01c1290880779132e",
      "commitAuthorOld": "John Zhuge",
      "daysBetweenCommits": 2.45,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,29 @@\n   void openInfo(boolean refreshLocatedBlocks) throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     synchronized(infoLock) {\n       lastBlockBeingWrittenLength \u003d\n           fetchLocatedBlocksAndGetLastBlockLength(refreshLocatedBlocks);\n       int retriesForLastBlockLength \u003d conf.getRetryTimesForGetLastBlockLength();\n       while (retriesForLastBlockLength \u003e 0) {\n         // Getting last block length as -1 is a special case. When cluster\n         // restarts, DNs may not report immediately. At this time partial block\n         // locations will not be available with NN for getting the length. Lets\n         // retry for 3 times to get the length.\n         if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n           DFSClient.LOG.warn(\"Last block locations not available. \"\n               + \"Datanodes might not have reported blocks completely.\"\n               + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n           waitFor(conf.getRetryIntervalForGetLastBlockLength());\n           lastBlockBeingWrittenLength \u003d\n               fetchLocatedBlocksAndGetLastBlockLength(true);\n         } else {\n           break;\n         }\n         retriesForLastBlockLength--;\n       }\n-      if (retriesForLastBlockLength \u003d\u003d 0) {\n+      if (lastBlockBeingWrittenLength \u003d\u003d -1\n+          \u0026\u0026 retriesForLastBlockLength \u003d\u003d 0) {\n         throw new IOException(\"Could not obtain the last block locations.\");\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void openInfo(boolean refreshLocatedBlocks) throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    synchronized(infoLock) {\n      lastBlockBeingWrittenLength \u003d\n          fetchLocatedBlocksAndGetLastBlockLength(refreshLocatedBlocks);\n      int retriesForLastBlockLength \u003d conf.getRetryTimesForGetLastBlockLength();\n      while (retriesForLastBlockLength \u003e 0) {\n        // Getting last block length as -1 is a special case. When cluster\n        // restarts, DNs may not report immediately. At this time partial block\n        // locations will not be available with NN for getting the length. Lets\n        // retry for 3 times to get the length.\n        if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n          DFSClient.LOG.warn(\"Last block locations not available. \"\n              + \"Datanodes might not have reported blocks completely.\"\n              + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n          waitFor(conf.getRetryIntervalForGetLastBlockLength());\n          lastBlockBeingWrittenLength \u003d\n              fetchLocatedBlocksAndGetLastBlockLength(true);\n        } else {\n          break;\n        }\n        retriesForLastBlockLength--;\n      }\n      if (lastBlockBeingWrittenLength \u003d\u003d -1\n          \u0026\u0026 retriesForLastBlockLength \u003d\u003d 0) {\n        throw new IOException(\"Could not obtain the last block locations.\");\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,28 @@\n-  void openInfo(boolean refreshLocatedBlocks) throws IOException,\n-      UnresolvedLinkException {\n+  void openInfo(boolean refreshLocatedBlocks) throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     synchronized(infoLock) {\n       lastBlockBeingWrittenLength \u003d\n           fetchLocatedBlocksAndGetLastBlockLength(refreshLocatedBlocks);\n       int retriesForLastBlockLength \u003d conf.getRetryTimesForGetLastBlockLength();\n       while (retriesForLastBlockLength \u003e 0) {\n         // Getting last block length as -1 is a special case. When cluster\n         // restarts, DNs may not report immediately. At this time partial block\n         // locations will not be available with NN for getting the length. Lets\n         // retry for 3 times to get the length.\n         if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n           DFSClient.LOG.warn(\"Last block locations not available. \"\n               + \"Datanodes might not have reported blocks completely.\"\n               + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n           waitFor(conf.getRetryIntervalForGetLastBlockLength());\n           lastBlockBeingWrittenLength \u003d\n               fetchLocatedBlocksAndGetLastBlockLength(true);\n         } else {\n           break;\n         }\n         retriesForLastBlockLength--;\n       }\n       if (retriesForLastBlockLength \u003d\u003d 0) {\n         throw new IOException(\"Could not obtain the last block locations.\");\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void openInfo(boolean refreshLocatedBlocks) throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    synchronized(infoLock) {\n      lastBlockBeingWrittenLength \u003d\n          fetchLocatedBlocksAndGetLastBlockLength(refreshLocatedBlocks);\n      int retriesForLastBlockLength \u003d conf.getRetryTimesForGetLastBlockLength();\n      while (retriesForLastBlockLength \u003e 0) {\n        // Getting last block length as -1 is a special case. When cluster\n        // restarts, DNs may not report immediately. At this time partial block\n        // locations will not be available with NN for getting the length. Lets\n        // retry for 3 times to get the length.\n        if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n          DFSClient.LOG.warn(\"Last block locations not available. \"\n              + \"Datanodes might not have reported blocks completely.\"\n              + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n          waitFor(conf.getRetryIntervalForGetLastBlockLength());\n          lastBlockBeingWrittenLength \u003d\n              fetchLocatedBlocksAndGetLastBlockLength(true);\n        } else {\n          break;\n        }\n        retriesForLastBlockLength--;\n      }\n      if (retriesForLastBlockLength \u003d\u003d 0) {\n        throw new IOException(\"Could not obtain the last block locations.\");\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldValue": "[IOException, UnresolvedLinkException]",
        "newValue": "[IOException]"
      }
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void openInfo(boolean refreshLocatedBlocks) throws IOException,\n      UnresolvedLinkException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    synchronized(infoLock) {\n      lastBlockBeingWrittenLength \u003d\n          fetchLocatedBlocksAndGetLastBlockLength(refreshLocatedBlocks);\n      int retriesForLastBlockLength \u003d conf.getRetryTimesForGetLastBlockLength();\n      while (retriesForLastBlockLength \u003e 0) {\n        // Getting last block length as -1 is a special case. When cluster\n        // restarts, DNs may not report immediately. At this time partial block\n        // locations will not be available with NN for getting the length. Lets\n        // retry for 3 times to get the length.\n        if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n          DFSClient.LOG.warn(\"Last block locations not available. \"\n              + \"Datanodes might not have reported blocks completely.\"\n              + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n          waitFor(conf.getRetryIntervalForGetLastBlockLength());\n          lastBlockBeingWrittenLength \u003d\n              fetchLocatedBlocksAndGetLastBlockLength(true);\n        } else {\n          break;\n        }\n        retriesForLastBlockLength--;\n      }\n      if (retriesForLastBlockLength \u003d\u003d 0) {\n        throw new IOException(\"Could not obtain the last block locations.\");\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "bff5999d07e9416a22846c849487e509ede55040": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
      "commitDate": "02/07/15 3:41 AM",
      "commitName": "bff5999d07e9416a22846c849487e509ede55040",
      "commitAuthor": "Vinayakumar B",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
          "commitDate": "02/07/15 3:41 AM",
          "commitName": "bff5999d07e9416a22846c849487e509ede55040",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "04/06/15 10:51 AM",
          "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 27.7,
          "commitsBetweenForRepo": 196,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,29 @@\n-  void openInfo() throws IOException, UnresolvedLinkException {\n+  void openInfo(boolean refreshLocatedBlocks) throws IOException,\n+      UnresolvedLinkException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     synchronized(infoLock) {\n-      lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n+      lastBlockBeingWrittenLength \u003d\n+          fetchLocatedBlocksAndGetLastBlockLength(refreshLocatedBlocks);\n       int retriesForLastBlockLength \u003d conf.getRetryTimesForGetLastBlockLength();\n       while (retriesForLastBlockLength \u003e 0) {\n         // Getting last block length as -1 is a special case. When cluster\n         // restarts, DNs may not report immediately. At this time partial block\n         // locations will not be available with NN for getting the length. Lets\n         // retry for 3 times to get the length.\n         if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n           DFSClient.LOG.warn(\"Last block locations not available. \"\n               + \"Datanodes might not have reported blocks completely.\"\n               + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n           waitFor(conf.getRetryIntervalForGetLastBlockLength());\n-          lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n+          lastBlockBeingWrittenLength \u003d\n+              fetchLocatedBlocksAndGetLastBlockLength(true);\n         } else {\n           break;\n         }\n         retriesForLastBlockLength--;\n       }\n       if (retriesForLastBlockLength \u003d\u003d 0) {\n         throw new IOException(\"Could not obtain the last block locations.\");\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void openInfo(boolean refreshLocatedBlocks) throws IOException,\n      UnresolvedLinkException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    synchronized(infoLock) {\n      lastBlockBeingWrittenLength \u003d\n          fetchLocatedBlocksAndGetLastBlockLength(refreshLocatedBlocks);\n      int retriesForLastBlockLength \u003d conf.getRetryTimesForGetLastBlockLength();\n      while (retriesForLastBlockLength \u003e 0) {\n        // Getting last block length as -1 is a special case. When cluster\n        // restarts, DNs may not report immediately. At this time partial block\n        // locations will not be available with NN for getting the length. Lets\n        // retry for 3 times to get the length.\n        if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n          DFSClient.LOG.warn(\"Last block locations not available. \"\n              + \"Datanodes might not have reported blocks completely.\"\n              + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n          waitFor(conf.getRetryIntervalForGetLastBlockLength());\n          lastBlockBeingWrittenLength \u003d\n              fetchLocatedBlocksAndGetLastBlockLength(true);\n        } else {\n          break;\n        }\n        retriesForLastBlockLength--;\n      }\n      if (retriesForLastBlockLength \u003d\u003d 0) {\n        throw new IOException(\"Could not obtain the last block locations.\");\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[refreshLocatedBlocks-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
          "commitDate": "02/07/15 3:41 AM",
          "commitName": "bff5999d07e9416a22846c849487e509ede55040",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "04/06/15 10:51 AM",
          "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 27.7,
          "commitsBetweenForRepo": 196,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,29 @@\n-  void openInfo() throws IOException, UnresolvedLinkException {\n+  void openInfo(boolean refreshLocatedBlocks) throws IOException,\n+      UnresolvedLinkException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     synchronized(infoLock) {\n-      lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n+      lastBlockBeingWrittenLength \u003d\n+          fetchLocatedBlocksAndGetLastBlockLength(refreshLocatedBlocks);\n       int retriesForLastBlockLength \u003d conf.getRetryTimesForGetLastBlockLength();\n       while (retriesForLastBlockLength \u003e 0) {\n         // Getting last block length as -1 is a special case. When cluster\n         // restarts, DNs may not report immediately. At this time partial block\n         // locations will not be available with NN for getting the length. Lets\n         // retry for 3 times to get the length.\n         if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n           DFSClient.LOG.warn(\"Last block locations not available. \"\n               + \"Datanodes might not have reported blocks completely.\"\n               + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n           waitFor(conf.getRetryIntervalForGetLastBlockLength());\n-          lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n+          lastBlockBeingWrittenLength \u003d\n+              fetchLocatedBlocksAndGetLastBlockLength(true);\n         } else {\n           break;\n         }\n         retriesForLastBlockLength--;\n       }\n       if (retriesForLastBlockLength \u003d\u003d 0) {\n         throw new IOException(\"Could not obtain the last block locations.\");\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void openInfo(boolean refreshLocatedBlocks) throws IOException,\n      UnresolvedLinkException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    synchronized(infoLock) {\n      lastBlockBeingWrittenLength \u003d\n          fetchLocatedBlocksAndGetLastBlockLength(refreshLocatedBlocks);\n      int retriesForLastBlockLength \u003d conf.getRetryTimesForGetLastBlockLength();\n      while (retriesForLastBlockLength \u003e 0) {\n        // Getting last block length as -1 is a special case. When cluster\n        // restarts, DNs may not report immediately. At this time partial block\n        // locations will not be available with NN for getting the length. Lets\n        // retry for 3 times to get the length.\n        if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n          DFSClient.LOG.warn(\"Last block locations not available. \"\n              + \"Datanodes might not have reported blocks completely.\"\n              + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n          waitFor(conf.getRetryIntervalForGetLastBlockLength());\n          lastBlockBeingWrittenLength \u003d\n              fetchLocatedBlocksAndGetLastBlockLength(true);\n        } else {\n          break;\n        }\n        retriesForLastBlockLength--;\n      }\n      if (retriesForLastBlockLength \u003d\u003d 0) {\n        throw new IOException(\"Could not obtain the last block locations.\");\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "09/04/15 11:22 AM",
      "commitNameOld": "30acb7372ab97adf9bc86ead529c96cfe36e2396",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.14,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,26 @@\n   void openInfo() throws IOException, UnresolvedLinkException {\n+    final DfsClientConf conf \u003d dfsClient.getConf();\n     synchronized(infoLock) {\n       lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n-      int retriesForLastBlockLength \u003d dfsClient.getConf().retryTimesForGetLastBlockLength;\n+      int retriesForLastBlockLength \u003d conf.getRetryTimesForGetLastBlockLength();\n       while (retriesForLastBlockLength \u003e 0) {\n         // Getting last block length as -1 is a special case. When cluster\n         // restarts, DNs may not report immediately. At this time partial block\n         // locations will not be available with NN for getting the length. Lets\n         // retry for 3 times to get the length.\n         if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n           DFSClient.LOG.warn(\"Last block locations not available. \"\n               + \"Datanodes might not have reported blocks completely.\"\n               + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n-          waitFor(dfsClient.getConf().retryIntervalForGetLastBlockLength);\n+          waitFor(conf.getRetryIntervalForGetLastBlockLength());\n           lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n         } else {\n           break;\n         }\n         retriesForLastBlockLength--;\n       }\n       if (retriesForLastBlockLength \u003d\u003d 0) {\n         throw new IOException(\"Could not obtain the last block locations.\");\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void openInfo() throws IOException, UnresolvedLinkException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    synchronized(infoLock) {\n      lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n      int retriesForLastBlockLength \u003d conf.getRetryTimesForGetLastBlockLength();\n      while (retriesForLastBlockLength \u003e 0) {\n        // Getting last block length as -1 is a special case. When cluster\n        // restarts, DNs may not report immediately. At this time partial block\n        // locations will not be available with NN for getting the length. Lets\n        // retry for 3 times to get the length.\n        if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n          DFSClient.LOG.warn(\"Last block locations not available. \"\n              + \"Datanodes might not have reported blocks completely.\"\n              + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n          waitFor(conf.getRetryIntervalForGetLastBlockLength());\n          lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n        } else {\n          break;\n        }\n        retriesForLastBlockLength--;\n      }\n      if (retriesForLastBlockLength \u003d\u003d 0) {\n        throw new IOException(\"Could not obtain the last block locations.\");\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-6735. A minor optimization to avoid pread() be blocked by read() inside the same DFSInputStream (Lars Hofhansl via stack)\n",
      "commitDate": "02/12/14 8:57 PM",
      "commitName": "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
      "commitAuthor": "stack",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-6735. A minor optimization to avoid pread() be blocked by read() inside the same DFSInputStream (Lars Hofhansl via stack)\n",
          "commitDate": "02/12/14 8:57 PM",
          "commitName": "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
          "commitAuthor": "stack",
          "commitDateOld": "05/11/14 9:00 PM",
          "commitNameOld": "80d7d183cd4052d6e6d412ff6588d26471c85d6d",
          "commitAuthorOld": "Milan Desai",
          "daysBetweenCommits": 27.0,
          "commitsBetweenForRepo": 189,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,25 @@\n-  synchronized void openInfo() throws IOException, UnresolvedLinkException {\n-    lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n-    int retriesForLastBlockLength \u003d dfsClient.getConf().retryTimesForGetLastBlockLength;\n-    while (retriesForLastBlockLength \u003e 0) {\n-      // Getting last block length as -1 is a special case. When cluster\n-      // restarts, DNs may not report immediately. At this time partial block\n-      // locations will not be available with NN for getting the length. Lets\n-      // retry for 3 times to get the length.\n-      if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n-        DFSClient.LOG.warn(\"Last block locations not available. \"\n-            + \"Datanodes might not have reported blocks completely.\"\n-            + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n-        waitFor(dfsClient.getConf().retryIntervalForGetLastBlockLength);\n-        lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n-      } else {\n-        break;\n+  void openInfo() throws IOException, UnresolvedLinkException {\n+    synchronized(infoLock) {\n+      lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n+      int retriesForLastBlockLength \u003d dfsClient.getConf().retryTimesForGetLastBlockLength;\n+      while (retriesForLastBlockLength \u003e 0) {\n+        // Getting last block length as -1 is a special case. When cluster\n+        // restarts, DNs may not report immediately. At this time partial block\n+        // locations will not be available with NN for getting the length. Lets\n+        // retry for 3 times to get the length.\n+        if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n+          DFSClient.LOG.warn(\"Last block locations not available. \"\n+              + \"Datanodes might not have reported blocks completely.\"\n+              + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n+          waitFor(dfsClient.getConf().retryIntervalForGetLastBlockLength);\n+          lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n+        } else {\n+          break;\n+        }\n+        retriesForLastBlockLength--;\n       }\n-      retriesForLastBlockLength--;\n-    }\n-    if (retriesForLastBlockLength \u003d\u003d 0) {\n-      throw new IOException(\"Could not obtain the last block locations.\");\n+      if (retriesForLastBlockLength \u003d\u003d 0) {\n+        throw new IOException(\"Could not obtain the last block locations.\");\n+      }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void openInfo() throws IOException, UnresolvedLinkException {\n    synchronized(infoLock) {\n      lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n      int retriesForLastBlockLength \u003d dfsClient.getConf().retryTimesForGetLastBlockLength;\n      while (retriesForLastBlockLength \u003e 0) {\n        // Getting last block length as -1 is a special case. When cluster\n        // restarts, DNs may not report immediately. At this time partial block\n        // locations will not be available with NN for getting the length. Lets\n        // retry for 3 times to get the length.\n        if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n          DFSClient.LOG.warn(\"Last block locations not available. \"\n              + \"Datanodes might not have reported blocks completely.\"\n              + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n          waitFor(dfsClient.getConf().retryIntervalForGetLastBlockLength);\n          lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n        } else {\n          break;\n        }\n        retriesForLastBlockLength--;\n      }\n      if (retriesForLastBlockLength \u003d\u003d 0) {\n        throw new IOException(\"Could not obtain the last block locations.\");\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[synchronized]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6735. A minor optimization to avoid pread() be blocked by read() inside the same DFSInputStream (Lars Hofhansl via stack)\n",
          "commitDate": "02/12/14 8:57 PM",
          "commitName": "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
          "commitAuthor": "stack",
          "commitDateOld": "05/11/14 9:00 PM",
          "commitNameOld": "80d7d183cd4052d6e6d412ff6588d26471c85d6d",
          "commitAuthorOld": "Milan Desai",
          "daysBetweenCommits": 27.0,
          "commitsBetweenForRepo": 189,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,25 @@\n-  synchronized void openInfo() throws IOException, UnresolvedLinkException {\n-    lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n-    int retriesForLastBlockLength \u003d dfsClient.getConf().retryTimesForGetLastBlockLength;\n-    while (retriesForLastBlockLength \u003e 0) {\n-      // Getting last block length as -1 is a special case. When cluster\n-      // restarts, DNs may not report immediately. At this time partial block\n-      // locations will not be available with NN for getting the length. Lets\n-      // retry for 3 times to get the length.\n-      if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n-        DFSClient.LOG.warn(\"Last block locations not available. \"\n-            + \"Datanodes might not have reported blocks completely.\"\n-            + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n-        waitFor(dfsClient.getConf().retryIntervalForGetLastBlockLength);\n-        lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n-      } else {\n-        break;\n+  void openInfo() throws IOException, UnresolvedLinkException {\n+    synchronized(infoLock) {\n+      lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n+      int retriesForLastBlockLength \u003d dfsClient.getConf().retryTimesForGetLastBlockLength;\n+      while (retriesForLastBlockLength \u003e 0) {\n+        // Getting last block length as -1 is a special case. When cluster\n+        // restarts, DNs may not report immediately. At this time partial block\n+        // locations will not be available with NN for getting the length. Lets\n+        // retry for 3 times to get the length.\n+        if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n+          DFSClient.LOG.warn(\"Last block locations not available. \"\n+              + \"Datanodes might not have reported blocks completely.\"\n+              + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n+          waitFor(dfsClient.getConf().retryIntervalForGetLastBlockLength);\n+          lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n+        } else {\n+          break;\n+        }\n+        retriesForLastBlockLength--;\n       }\n-      retriesForLastBlockLength--;\n-    }\n-    if (retriesForLastBlockLength \u003d\u003d 0) {\n-      throw new IOException(\"Could not obtain the last block locations.\");\n+      if (retriesForLastBlockLength \u003d\u003d 0) {\n+        throw new IOException(\"Could not obtain the last block locations.\");\n+      }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void openInfo() throws IOException, UnresolvedLinkException {\n    synchronized(infoLock) {\n      lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n      int retriesForLastBlockLength \u003d dfsClient.getConf().retryTimesForGetLastBlockLength;\n      while (retriesForLastBlockLength \u003e 0) {\n        // Getting last block length as -1 is a special case. When cluster\n        // restarts, DNs may not report immediately. At this time partial block\n        // locations will not be available with NN for getting the length. Lets\n        // retry for 3 times to get the length.\n        if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n          DFSClient.LOG.warn(\"Last block locations not available. \"\n              + \"Datanodes might not have reported blocks completely.\"\n              + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n          waitFor(dfsClient.getConf().retryIntervalForGetLastBlockLength);\n          lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n        } else {\n          break;\n        }\n        retriesForLastBlockLength--;\n      }\n      if (retriesForLastBlockLength \u003d\u003d 0) {\n        throw new IOException(\"Could not obtain the last block locations.\");\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "b0947f93056939d419882c13d4acf3e935a66e62": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5663 make the retry time and interval value configurable in openInfo()\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1552232 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/12/13 9:21 PM",
      "commitName": "b0947f93056939d419882c13d4acf3e935a66e62",
      "commitAuthor": "Michael Stack",
      "commitDateOld": "18/12/13 3:29 PM",
      "commitNameOld": "90122f25e142ff5ae9e2610b6b8968ac5fee8f79",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 0.24,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   synchronized void openInfo() throws IOException, UnresolvedLinkException {\n     lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n-    int retriesForLastBlockLength \u003d 3;\n+    int retriesForLastBlockLength \u003d dfsClient.getConf().retryTimesForGetLastBlockLength;\n     while (retriesForLastBlockLength \u003e 0) {\n       // Getting last block length as -1 is a special case. When cluster\n       // restarts, DNs may not report immediately. At this time partial block\n       // locations will not be available with NN for getting the length. Lets\n       // retry for 3 times to get the length.\n       if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n         DFSClient.LOG.warn(\"Last block locations not available. \"\n             + \"Datanodes might not have reported blocks completely.\"\n             + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n-        waitFor(4000);\n+        waitFor(dfsClient.getConf().retryIntervalForGetLastBlockLength);\n         lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n       } else {\n         break;\n       }\n       retriesForLastBlockLength--;\n     }\n     if (retriesForLastBlockLength \u003d\u003d 0) {\n       throw new IOException(\"Could not obtain the last block locations.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void openInfo() throws IOException, UnresolvedLinkException {\n    lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n    int retriesForLastBlockLength \u003d dfsClient.getConf().retryTimesForGetLastBlockLength;\n    while (retriesForLastBlockLength \u003e 0) {\n      // Getting last block length as -1 is a special case. When cluster\n      // restarts, DNs may not report immediately. At this time partial block\n      // locations will not be available with NN for getting the length. Lets\n      // retry for 3 times to get the length.\n      if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n        DFSClient.LOG.warn(\"Last block locations not available. \"\n            + \"Datanodes might not have reported blocks completely.\"\n            + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n        waitFor(dfsClient.getConf().retryIntervalForGetLastBlockLength);\n        lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n      } else {\n        break;\n      }\n      retriesForLastBlockLength--;\n    }\n    if (retriesForLastBlockLength \u003d\u003d 0) {\n      throw new IOException(\"Could not obtain the last block locations.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "d28b98242854ff7f9d615e1c9d6a5b7584ce2498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3222. DFSInputStream#openInfo should not silently get the length as 0 when locations length is zero for last partial block. Contributed by Uma Maheswara Rao G.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1331061 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/04/12 1:18 PM",
      "commitName": "d28b98242854ff7f9d615e1c9d6a5b7584ce2498",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "18/04/12 7:34 PM",
      "commitNameOld": "c6d3537d337d71a3e566bcae824cc2377e9a9ed2",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 7.74,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,23 @@\n   synchronized void openInfo() throws IOException, UnresolvedLinkException {\n-    LocatedBlocks newInfo \u003d DFSClient.callGetBlockLocations(dfsClient.namenode, src, 0, prefetchSize);\n-    if (DFSClient.LOG.isDebugEnabled()) {\n-      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n-    }\n-    if (newInfo \u003d\u003d null) {\n-      throw new IOException(\"Cannot open filename \" + src);\n-    }\n-\n-    if (locatedBlocks !\u003d null) {\n-      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n-      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n-      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n-        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n-          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n-        }\n+    lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n+    int retriesForLastBlockLength \u003d 3;\n+    while (retriesForLastBlockLength \u003e 0) {\n+      // Getting last block length as -1 is a special case. When cluster\n+      // restarts, DNs may not report immediately. At this time partial block\n+      // locations will not be available with NN for getting the length. Lets\n+      // retry for 3 times to get the length.\n+      if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n+        DFSClient.LOG.warn(\"Last block locations not available. \"\n+            + \"Datanodes might not have reported blocks completely.\"\n+            + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n+        waitFor(4000);\n+        lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n+      } else {\n+        break;\n       }\n+      retriesForLastBlockLength--;\n     }\n-    locatedBlocks \u003d newInfo;\n-    lastBlockBeingWrittenLength \u003d 0;\n-    if (!locatedBlocks.isLastBlockComplete()) {\n-      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n-      if (last !\u003d null) {\n-        final long len \u003d readBlockLength(last);\n-        last.getBlock().setNumBytes(len);\n-        lastBlockBeingWrittenLength \u003d len; \n-      }\n+    if (retriesForLastBlockLength \u003d\u003d 0) {\n+      throw new IOException(\"Could not obtain the last block locations.\");\n     }\n-\n-    currentNode \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void openInfo() throws IOException, UnresolvedLinkException {\n    lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n    int retriesForLastBlockLength \u003d 3;\n    while (retriesForLastBlockLength \u003e 0) {\n      // Getting last block length as -1 is a special case. When cluster\n      // restarts, DNs may not report immediately. At this time partial block\n      // locations will not be available with NN for getting the length. Lets\n      // retry for 3 times to get the length.\n      if (lastBlockBeingWrittenLength \u003d\u003d -1) {\n        DFSClient.LOG.warn(\"Last block locations not available. \"\n            + \"Datanodes might not have reported blocks completely.\"\n            + \" Will retry for \" + retriesForLastBlockLength + \" times\");\n        waitFor(4000);\n        lastBlockBeingWrittenLength \u003d fetchLocatedBlocksAndGetLastBlockLength();\n      } else {\n        break;\n      }\n      retriesForLastBlockLength--;\n    }\n    if (retriesForLastBlockLength \u003d\u003d 0) {\n      throw new IOException(\"Could not obtain the last block locations.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  synchronized void openInfo() throws IOException, UnresolvedLinkException {\n    LocatedBlocks newInfo \u003d DFSClient.callGetBlockLocations(dfsClient.namenode, src, 0, prefetchSize);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    currentNode \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  synchronized void openInfo() throws IOException, UnresolvedLinkException {\n    LocatedBlocks newInfo \u003d DFSClient.callGetBlockLocations(dfsClient.namenode, src, 0, prefetchSize);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    currentNode \u003d null;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,31 @@\n+  synchronized void openInfo() throws IOException, UnresolvedLinkException {\n+    LocatedBlocks newInfo \u003d DFSClient.callGetBlockLocations(dfsClient.namenode, src, 0, prefetchSize);\n+    if (DFSClient.LOG.isDebugEnabled()) {\n+      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n+    }\n+    if (newInfo \u003d\u003d null) {\n+      throw new IOException(\"Cannot open filename \" + src);\n+    }\n+\n+    if (locatedBlocks !\u003d null) {\n+      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n+      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n+      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n+        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n+          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n+        }\n+      }\n+    }\n+    locatedBlocks \u003d newInfo;\n+    lastBlockBeingWrittenLength \u003d 0;\n+    if (!locatedBlocks.isLastBlockComplete()) {\n+      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n+      if (last !\u003d null) {\n+        final long len \u003d readBlockLength(last);\n+        last.getBlock().setNumBytes(len);\n+        lastBlockBeingWrittenLength \u003d len; \n+      }\n+    }\n+\n+    currentNode \u003d null;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void openInfo() throws IOException, UnresolvedLinkException {\n    LocatedBlocks newInfo \u003d DFSClient.callGetBlockLocations(dfsClient.namenode, src, 0, prefetchSize);\n    if (DFSClient.LOG.isDebugEnabled()) {\n      DFSClient.LOG.debug(\"newInfo \u003d \" + newInfo);\n    }\n    if (newInfo \u003d\u003d null) {\n      throw new IOException(\"Cannot open filename \" + src);\n    }\n\n    if (locatedBlocks !\u003d null) {\n      Iterator\u003cLocatedBlock\u003e oldIter \u003d locatedBlocks.getLocatedBlocks().iterator();\n      Iterator\u003cLocatedBlock\u003e newIter \u003d newInfo.getLocatedBlocks().iterator();\n      while (oldIter.hasNext() \u0026\u0026 newIter.hasNext()) {\n        if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {\n          throw new IOException(\"Blocklist for \" + src + \" has changed!\");\n        }\n      }\n    }\n    locatedBlocks \u003d newInfo;\n    lastBlockBeingWrittenLength \u003d 0;\n    if (!locatedBlocks.isLastBlockComplete()) {\n      final LocatedBlock last \u003d locatedBlocks.getLastLocatedBlock();\n      if (last !\u003d null) {\n        final long len \u003d readBlockLength(last);\n        last.getBlock().setNumBytes(len);\n        lastBlockBeingWrittenLength \u003d len; \n      }\n    }\n\n    currentNode \u003d null;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}