{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PeerCache.java",
  "functionName": "get",
  "functionId": "get___dnId-DatanodeID__isDomain-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
  "functionStartLine": 148,
  "functionEndLine": 154,
  "numCommitsSeen": 34,
  "timeTaken": 6908,
  "changeHistory": [
    "e2c9b288b223b9fd82dc12018936e13128413492",
    "4799570dfdb7987c2ac39716143341e9a3d9b7d2",
    "e0cda4895948c500a7bbc0a1a553d3698be3e317",
    "d12f465c674b3bb5102671b6d6c2746261602d7e",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
    "837e17b2eac1471d93e2eff395272063b265fee7",
    "239b2742d0e80d13c970fd062af4930e672fe903",
    "a7bcdcc0518595b7d94383606ab8e9aa711292b0",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "567aed4f2c0a3bac4ef0cd0ebd36e8672001912c",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "e2c9b288b223b9fd82dc12018936e13128413492": "Yfilerename",
    "4799570dfdb7987c2ac39716143341e9a3d9b7d2": "Ymultichange(Ymodifierchange,Ybodychange)",
    "e0cda4895948c500a7bbc0a1a553d3698be3e317": "Ybodychange",
    "d12f465c674b3bb5102671b6d6c2746261602d7e": "Ymultichange(Yparameterchange,Ybodychange)",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": "Ymultichange(Yfilerename,Yreturntypechange,Ybodychange,Yparameterchange)",
    "837e17b2eac1471d93e2eff395272063b265fee7": "Ymultichange(Yfilerename,Yreturntypechange,Ybodychange,Yparameterchange)",
    "239b2742d0e80d13c970fd062af4930e672fe903": "Ymultichange(Yfilerename,Yreturntypechange,Ybodychange,Yparameterchange)",
    "a7bcdcc0518595b7d94383606ab8e9aa711292b0": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ymultichange(Yreturntypechange,Ybodychange)",
    "567aed4f2c0a3bac4ef0cd0ebd36e8672001912c": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "e2c9b288b223b9fd82dc12018936e13128413492": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "28/08/15 2:38 PM",
      "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/08/15 2:21 PM",
      "commitNameOld": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public Peer get(DatanodeID dnId, boolean isDomain) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n    return getInternal(dnId, isDomain);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PeerCache.java"
      }
    },
    "4799570dfdb7987c2ac39716143341e9a3d9b7d2": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-7266. HDFS Peercache enabled check should not lock on object (awang via cmccabe)\n",
      "commitDate": "20/10/14 6:24 PM",
      "commitName": "4799570dfdb7987c2ac39716143341e9a3d9b7d2",
      "commitAuthor": "Colin Patrick Mccabe",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7266. HDFS Peercache enabled check should not lock on object (awang via cmccabe)\n",
          "commitDate": "20/10/14 6:24 PM",
          "commitName": "4799570dfdb7987c2ac39716143341e9a3d9b7d2",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "01/04/14 10:09 PM",
          "commitNameOld": "f93d99990a9a02ce693cd74466c2e5f127c1f560",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 201.84,
          "commitsBetweenForRepo": 1605,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,30 +1,7 @@\n-  public synchronized Peer get(DatanodeID dnId, boolean isDomain) {\n+  public Peer get(DatanodeID dnId, boolean isDomain) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n-\n-    List\u003cValue\u003e sockStreamList \u003d multimap.get(new Key(dnId, isDomain));\n-    if (sockStreamList \u003d\u003d null) {\n-      return null;\n-    }\n-\n-    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n-    while (iter.hasNext()) {\n-      Value candidate \u003d iter.next();\n-      iter.remove();\n-      long ageMs \u003d Time.monotonicNow() - candidate.getTime();\n-      Peer peer \u003d candidate.getPeer();\n-      if (ageMs \u003e\u003d expiryPeriod) {\n-        try {\n-          peer.close();\n-        } catch (IOException e) {\n-          LOG.warn(\"got IOException closing stale peer \" + peer +\n-                \", which is \" + ageMs + \" ms old\");\n-        }\n-      } else if (!peer.isClosed()) {\n-        return peer;\n-      }\n-    }\n-    return null;\n+    return getInternal(dnId, isDomain);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Peer get(DatanodeID dnId, boolean isDomain) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n    return getInternal(dnId, isDomain);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7266. HDFS Peercache enabled check should not lock on object (awang via cmccabe)\n",
          "commitDate": "20/10/14 6:24 PM",
          "commitName": "4799570dfdb7987c2ac39716143341e9a3d9b7d2",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "01/04/14 10:09 PM",
          "commitNameOld": "f93d99990a9a02ce693cd74466c2e5f127c1f560",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 201.84,
          "commitsBetweenForRepo": 1605,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,30 +1,7 @@\n-  public synchronized Peer get(DatanodeID dnId, boolean isDomain) {\n+  public Peer get(DatanodeID dnId, boolean isDomain) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n-\n-    List\u003cValue\u003e sockStreamList \u003d multimap.get(new Key(dnId, isDomain));\n-    if (sockStreamList \u003d\u003d null) {\n-      return null;\n-    }\n-\n-    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n-    while (iter.hasNext()) {\n-      Value candidate \u003d iter.next();\n-      iter.remove();\n-      long ageMs \u003d Time.monotonicNow() - candidate.getTime();\n-      Peer peer \u003d candidate.getPeer();\n-      if (ageMs \u003e\u003d expiryPeriod) {\n-        try {\n-          peer.close();\n-        } catch (IOException e) {\n-          LOG.warn(\"got IOException closing stale peer \" + peer +\n-                \", which is \" + ageMs + \" ms old\");\n-        }\n-      } else if (!peer.isClosed()) {\n-        return peer;\n-      }\n-    }\n-    return null;\n+    return getInternal(dnId, isDomain);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Peer get(DatanodeID dnId, boolean isDomain) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n    return getInternal(dnId, isDomain);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {}
        }
      ]
    },
    "e0cda4895948c500a7bbc0a1a553d3698be3e317": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4911. Reduce PeerCache timeout to be commensurate with dfs.datanode.socket.reuse.keepalive (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1565435 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/02/14 1:08 PM",
      "commitName": "e0cda4895948c500a7bbc0a1a553d3698be3e317",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "23/01/13 12:01 PM",
      "commitNameOld": "4e74c52e60213a8ddceb53bb3cc40221c65d8bf3",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 379.05,
      "commitsBetweenForRepo": 2286,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,30 @@\n   public synchronized Peer get(DatanodeID dnId, boolean isDomain) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n     List\u003cValue\u003e sockStreamList \u003d multimap.get(new Key(dnId, isDomain));\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n     Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n       Value candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.getPeer().isClosed()) {\n-        return candidate.getPeer();\n+      long ageMs \u003d Time.monotonicNow() - candidate.getTime();\n+      Peer peer \u003d candidate.getPeer();\n+      if (ageMs \u003e\u003d expiryPeriod) {\n+        try {\n+          peer.close();\n+        } catch (IOException e) {\n+          LOG.warn(\"got IOException closing stale peer \" + peer +\n+                \", which is \" + ageMs + \" ms old\");\n+        }\n+      } else if (!peer.isClosed()) {\n+        return peer;\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized Peer get(DatanodeID dnId, boolean isDomain) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cValue\u003e sockStreamList \u003d multimap.get(new Key(dnId, isDomain));\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      Value candidate \u003d iter.next();\n      iter.remove();\n      long ageMs \u003d Time.monotonicNow() - candidate.getTime();\n      Peer peer \u003d candidate.getPeer();\n      if (ageMs \u003e\u003d expiryPeriod) {\n        try {\n          peer.close();\n        } catch (IOException e) {\n          LOG.warn(\"got IOException closing stale peer \" + peer +\n                \", which is \" + ageMs + \" ms old\");\n        }\n      } else if (!peer.isClosed()) {\n        return peer;\n      }\n    }\n    return null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
      "extendedDetails": {}
    },
    "d12f465c674b3bb5102671b6d6c2746261602d7e": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4417. Fix case where local reads get disabled incorrectly. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1437616 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/01/13 10:38 AM",
      "commitName": "d12f465c674b3bb5102671b6d6c2746261602d7e",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4417. Fix case where local reads get disabled incorrectly. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1437616 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/01/13 10:38 AM",
          "commitName": "d12f465c674b3bb5102671b6d6c2746261602d7e",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "09/01/13 1:34 PM",
          "commitNameOld": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 13.88,
          "commitsBetweenForRepo": 46,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized Peer get(DatanodeID dnId) {\n+  public synchronized Peer get(DatanodeID dnId, boolean isDomain) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n+    List\u003cValue\u003e sockStreamList \u003d multimap.get(new Key(dnId, isDomain));\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n     Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n       Value candidate \u003d iter.next();\n       iter.remove();\n       if (!candidate.getPeer().isClosed()) {\n         return candidate.getPeer();\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized Peer get(DatanodeID dnId, boolean isDomain) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cValue\u003e sockStreamList \u003d multimap.get(new Key(dnId, isDomain));\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      Value candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.getPeer().isClosed()) {\n        return candidate.getPeer();\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {
            "oldValue": "[dnId-DatanodeID]",
            "newValue": "[dnId-DatanodeID, isDomain-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4417. Fix case where local reads get disabled incorrectly. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1437616 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/01/13 10:38 AM",
          "commitName": "d12f465c674b3bb5102671b6d6c2746261602d7e",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "09/01/13 1:34 PM",
          "commitNameOld": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 13.88,
          "commitsBetweenForRepo": 46,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized Peer get(DatanodeID dnId) {\n+  public synchronized Peer get(DatanodeID dnId, boolean isDomain) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n+    List\u003cValue\u003e sockStreamList \u003d multimap.get(new Key(dnId, isDomain));\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n     Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n       Value candidate \u003d iter.next();\n       iter.remove();\n       if (!candidate.getPeer().isClosed()) {\n         return candidate.getPeer();\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized Peer get(DatanodeID dnId, boolean isDomain) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cValue\u003e sockStreamList \u003d multimap.get(new Key(dnId, isDomain));\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      Value candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.getPeer().isClosed()) {\n        return candidate.getPeer();\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {}
        }
      ]
    },
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": {
      "type": "Ymultichange(Yfilerename,Yreturntypechange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:34 PM",
      "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/01/13 1:34 PM",
          "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "09/01/13 10:12 AM",
          "commitNameOld": "f6c28639005f46bc171a9a990e2ad4d7afb4ce73",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized SocketAndStreams get(SocketAddress remote) {\n+  public synchronized Peer get(DatanodeID dnId) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n+    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      SocketAndStreams candidate \u003d iter.next();\n+      Value candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.sock.isClosed()) {\n-        return candidate;\n+      if (!candidate.getPeer().isClosed()) {\n+        return candidate.getPeer();\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized Peer get(DatanodeID dnId) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      Value candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.getPeer().isClosed()) {\n        return candidate.getPeer();\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/01/13 1:34 PM",
          "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "09/01/13 10:12 AM",
          "commitNameOld": "f6c28639005f46bc171a9a990e2ad4d7afb4ce73",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized SocketAndStreams get(SocketAddress remote) {\n+  public synchronized Peer get(DatanodeID dnId) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n+    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      SocketAndStreams candidate \u003d iter.next();\n+      Value candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.sock.isClosed()) {\n-        return candidate;\n+      if (!candidate.getPeer().isClosed()) {\n+        return candidate.getPeer();\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized Peer get(DatanodeID dnId) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      Value candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.getPeer().isClosed()) {\n        return candidate.getPeer();\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {
            "oldValue": "SocketAndStreams",
            "newValue": "Peer"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/01/13 1:34 PM",
          "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "09/01/13 10:12 AM",
          "commitNameOld": "f6c28639005f46bc171a9a990e2ad4d7afb4ce73",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized SocketAndStreams get(SocketAddress remote) {\n+  public synchronized Peer get(DatanodeID dnId) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n+    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      SocketAndStreams candidate \u003d iter.next();\n+      Value candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.sock.isClosed()) {\n-        return candidate;\n+      if (!candidate.getPeer().isClosed()) {\n+        return candidate.getPeer();\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized Peer get(DatanodeID dnId) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      Value candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.getPeer().isClosed()) {\n        return candidate.getPeer();\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/01/13 1:34 PM",
          "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "09/01/13 10:12 AM",
          "commitNameOld": "f6c28639005f46bc171a9a990e2ad4d7afb4ce73",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized SocketAndStreams get(SocketAddress remote) {\n+  public synchronized Peer get(DatanodeID dnId) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n+    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      SocketAndStreams candidate \u003d iter.next();\n+      Value candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.sock.isClosed()) {\n-        return candidate;\n+      if (!candidate.getPeer().isClosed()) {\n+        return candidate.getPeer();\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized Peer get(DatanodeID dnId) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      Value candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.getPeer().isClosed()) {\n        return candidate.getPeer();\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {
            "oldValue": "[remote-SocketAddress]",
            "newValue": "[dnId-DatanodeID]"
          }
        }
      ]
    },
    "837e17b2eac1471d93e2eff395272063b265fee7": {
      "type": "Ymultichange(Yfilerename,Yreturntypechange,Ybodychange,Yparameterchange)",
      "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 6:39 PM",
      "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 6:39 PM",
          "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "08/01/13 2:43 PM",
          "commitNameOld": "4ca58bd57c11fe328ff03d52a3cf6d848f6daa00",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized Peer get(DatanodeID dnId) {\n+  public synchronized SocketAndStreams get(SocketAddress remote) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n+    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      Value candidate \u003d iter.next();\n+      SocketAndStreams candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.getPeer().isClosed()) {\n-        return candidate.getPeer();\n+      if (!candidate.sock.isClosed()) {\n+        return candidate;\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized SocketAndStreams get(SocketAddress remote) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      SocketAndStreams candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.sock.isClosed()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 6:39 PM",
          "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "08/01/13 2:43 PM",
          "commitNameOld": "4ca58bd57c11fe328ff03d52a3cf6d848f6daa00",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized Peer get(DatanodeID dnId) {\n+  public synchronized SocketAndStreams get(SocketAddress remote) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n+    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      Value candidate \u003d iter.next();\n+      SocketAndStreams candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.getPeer().isClosed()) {\n-        return candidate.getPeer();\n+      if (!candidate.sock.isClosed()) {\n+        return candidate;\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized SocketAndStreams get(SocketAddress remote) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      SocketAndStreams candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.sock.isClosed()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
          "extendedDetails": {
            "oldValue": "Peer",
            "newValue": "SocketAndStreams"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 6:39 PM",
          "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "08/01/13 2:43 PM",
          "commitNameOld": "4ca58bd57c11fe328ff03d52a3cf6d848f6daa00",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized Peer get(DatanodeID dnId) {\n+  public synchronized SocketAndStreams get(SocketAddress remote) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n+    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      Value candidate \u003d iter.next();\n+      SocketAndStreams candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.getPeer().isClosed()) {\n-        return candidate.getPeer();\n+      if (!candidate.sock.isClosed()) {\n+        return candidate;\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized SocketAndStreams get(SocketAddress remote) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      SocketAndStreams candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.sock.isClosed()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 6:39 PM",
          "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "08/01/13 2:43 PM",
          "commitNameOld": "4ca58bd57c11fe328ff03d52a3cf6d848f6daa00",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized Peer get(DatanodeID dnId) {\n+  public synchronized SocketAndStreams get(SocketAddress remote) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n+    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      Value candidate \u003d iter.next();\n+      SocketAndStreams candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.getPeer().isClosed()) {\n-        return candidate.getPeer();\n+      if (!candidate.sock.isClosed()) {\n+        return candidate;\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized SocketAndStreams get(SocketAddress remote) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      SocketAndStreams candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.sock.isClosed()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
          "extendedDetails": {
            "oldValue": "[dnId-DatanodeID]",
            "newValue": "[remote-SocketAddress]"
          }
        }
      ]
    },
    "239b2742d0e80d13c970fd062af4930e672fe903": {
      "type": "Ymultichange(Yfilerename,Yreturntypechange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 12:44 PM",
      "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 12:44 PM",
          "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "08/01/13 11:51 AM",
          "commitNameOld": "db99f7f67d173de63e5601e401b7d4daf1585288",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized SocketAndStreams get(SocketAddress remote) {\n+  public synchronized Peer get(DatanodeID dnId) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n+    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      SocketAndStreams candidate \u003d iter.next();\n+      Value candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.sock.isClosed()) {\n-        return candidate;\n+      if (!candidate.getPeer().isClosed()) {\n+        return candidate.getPeer();\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized Peer get(DatanodeID dnId) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      Value candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.getPeer().isClosed()) {\n        return candidate.getPeer();\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 12:44 PM",
          "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "08/01/13 11:51 AM",
          "commitNameOld": "db99f7f67d173de63e5601e401b7d4daf1585288",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized SocketAndStreams get(SocketAddress remote) {\n+  public synchronized Peer get(DatanodeID dnId) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n+    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      SocketAndStreams candidate \u003d iter.next();\n+      Value candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.sock.isClosed()) {\n-        return candidate;\n+      if (!candidate.getPeer().isClosed()) {\n+        return candidate.getPeer();\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized Peer get(DatanodeID dnId) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      Value candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.getPeer().isClosed()) {\n        return candidate.getPeer();\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {
            "oldValue": "SocketAndStreams",
            "newValue": "Peer"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 12:44 PM",
          "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "08/01/13 11:51 AM",
          "commitNameOld": "db99f7f67d173de63e5601e401b7d4daf1585288",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized SocketAndStreams get(SocketAddress remote) {\n+  public synchronized Peer get(DatanodeID dnId) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n+    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      SocketAndStreams candidate \u003d iter.next();\n+      Value candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.sock.isClosed()) {\n-        return candidate;\n+      if (!candidate.getPeer().isClosed()) {\n+        return candidate.getPeer();\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized Peer get(DatanodeID dnId) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      Value candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.getPeer().isClosed()) {\n        return candidate.getPeer();\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 12:44 PM",
          "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "08/01/13 11:51 AM",
          "commitNameOld": "db99f7f67d173de63e5601e401b7d4daf1585288",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  public synchronized SocketAndStreams get(SocketAddress remote) {\n+  public synchronized Peer get(DatanodeID dnId) {\n \n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n \n-    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n+    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n     if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n+    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n-      SocketAndStreams candidate \u003d iter.next();\n+      Value candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.sock.isClosed()) {\n-        return candidate;\n+      if (!candidate.getPeer().isClosed()) {\n+        return candidate.getPeer();\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized Peer get(DatanodeID dnId) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cValue\u003e sockStreamList \u003d multimap.get(dnId);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cValue\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      Value candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.getPeer().isClosed()) {\n        return candidate.getPeer();\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {
            "oldValue": "[remote-SocketAddress]",
            "newValue": "[dnId-DatanodeID]"
          }
        }
      ]
    },
    "a7bcdcc0518595b7d94383606ab8e9aa711292b0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3373. Change DFSClient input stream socket cache to global static and add a thread to cleanup expired cache entries.  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1390466 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/09/12 6:23 AM",
      "commitName": "a7bcdcc0518595b7d94383606ab8e9aa711292b0",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "07/08/12 9:40 AM",
      "commitNameOld": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 49.86,
      "commitsBetweenForRepo": 286,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,21 @@\n   public synchronized SocketAndStreams get(SocketAddress remote) {\n+\n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n-    \n-    List\u003cSocketAndStreams\u003e socklist \u003d multimap.get(remote);\n-    if (socklist \u003d\u003d null) {\n+\n+    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n+    if (sockStreamList \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cSocketAndStreams\u003e iter \u003d socklist.iterator();\n+    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n     while (iter.hasNext()) {\n       SocketAndStreams candidate \u003d iter.next();\n       iter.remove();\n       if (!candidate.sock.isClosed()) {\n         return candidate;\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized SocketAndStreams get(SocketAddress remote) {\n\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n\n    List\u003cSocketAndStreams\u003e sockStreamList \u003d multimap.get(remote);\n    if (sockStreamList \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cSocketAndStreams\u003e iter \u003d sockStreamList.iterator();\n    while (iter.hasNext()) {\n      SocketAndStreams candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.sock.isClosed()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/08/12 9:40 AM",
          "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "15/07/12 7:58 PM",
          "commitNameOld": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
          "commitAuthorOld": "Harsh J",
          "daysBetweenCommits": 22.57,
          "commitsBetweenForRepo": 106,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  public synchronized Socket get(SocketAddress remote) {\n+  public synchronized SocketAndStreams get(SocketAddress remote) {\n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n     \n-    List\u003cSocket\u003e socklist \u003d multimap.get(remote);\n+    List\u003cSocketAndStreams\u003e socklist \u003d multimap.get(remote);\n     if (socklist \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cSocket\u003e iter \u003d socklist.iterator();\n+    Iterator\u003cSocketAndStreams\u003e iter \u003d socklist.iterator();\n     while (iter.hasNext()) {\n-      Socket candidate \u003d iter.next();\n+      SocketAndStreams candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.isClosed()) {\n+      if (!candidate.sock.isClosed()) {\n         return candidate;\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized SocketAndStreams get(SocketAddress remote) {\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n    \n    List\u003cSocketAndStreams\u003e socklist \u003d multimap.get(remote);\n    if (socklist \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cSocketAndStreams\u003e iter \u003d socklist.iterator();\n    while (iter.hasNext()) {\n      SocketAndStreams candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.sock.isClosed()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
          "extendedDetails": {
            "oldValue": "Socket",
            "newValue": "SocketAndStreams"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/08/12 9:40 AM",
          "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "15/07/12 7:58 PM",
          "commitNameOld": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
          "commitAuthorOld": "Harsh J",
          "daysBetweenCommits": 22.57,
          "commitsBetweenForRepo": 106,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  public synchronized Socket get(SocketAddress remote) {\n+  public synchronized SocketAndStreams get(SocketAddress remote) {\n     if (capacity \u003c\u003d 0) { // disabled\n       return null;\n     }\n     \n-    List\u003cSocket\u003e socklist \u003d multimap.get(remote);\n+    List\u003cSocketAndStreams\u003e socklist \u003d multimap.get(remote);\n     if (socklist \u003d\u003d null) {\n       return null;\n     }\n \n-    Iterator\u003cSocket\u003e iter \u003d socklist.iterator();\n+    Iterator\u003cSocketAndStreams\u003e iter \u003d socklist.iterator();\n     while (iter.hasNext()) {\n-      Socket candidate \u003d iter.next();\n+      SocketAndStreams candidate \u003d iter.next();\n       iter.remove();\n-      if (!candidate.isClosed()) {\n+      if (!candidate.sock.isClosed()) {\n         return candidate;\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized SocketAndStreams get(SocketAddress remote) {\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n    \n    List\u003cSocketAndStreams\u003e socklist \u003d multimap.get(remote);\n    if (socklist \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cSocketAndStreams\u003e iter \u003d socklist.iterator();\n    while (iter.hasNext()) {\n      SocketAndStreams candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.sock.isClosed()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
          "extendedDetails": {}
        }
      ]
    },
    "567aed4f2c0a3bac4ef0cd0ebd36e8672001912c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3365. Enable users to disable socket caching in DFS client configuration. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1335222 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/05/12 12:56 PM",
      "commitName": "567aed4f2c0a3bac4ef0cd0ebd36e8672001912c",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 256.82,
      "commitsBetweenForRepo": 1831,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,20 @@\n   public synchronized Socket get(SocketAddress remote) {\n+    if (capacity \u003c\u003d 0) { // disabled\n+      return null;\n+    }\n+    \n     List\u003cSocket\u003e socklist \u003d multimap.get(remote);\n     if (socklist \u003d\u003d null) {\n       return null;\n     }\n \n     Iterator\u003cSocket\u003e iter \u003d socklist.iterator();\n     while (iter.hasNext()) {\n       Socket candidate \u003d iter.next();\n       iter.remove();\n       if (!candidate.isClosed()) {\n         return candidate;\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized Socket get(SocketAddress remote) {\n    if (capacity \u003c\u003d 0) { // disabled\n      return null;\n    }\n    \n    List\u003cSocket\u003e socklist \u003d multimap.get(remote);\n    if (socklist \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cSocket\u003e iter \u003d socklist.iterator();\n    while (iter.hasNext()) {\n      Socket candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.isClosed()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized Socket get(SocketAddress remote) {\n    List\u003cSocket\u003e socklist \u003d multimap.get(remote);\n    if (socklist \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cSocket\u003e iter \u003d socklist.iterator();\n    while (iter.hasNext()) {\n      Socket candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.isClosed()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized Socket get(SocketAddress remote) {\n    List\u003cSocket\u003e socklist \u003d multimap.get(remote);\n    if (socklist \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cSocket\u003e iter \u003d socklist.iterator();\n    while (iter.hasNext()) {\n      Socket candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.isClosed()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/SocketCache.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,16 @@\n+  public synchronized Socket get(SocketAddress remote) {\n+    List\u003cSocket\u003e socklist \u003d multimap.get(remote);\n+    if (socklist \u003d\u003d null) {\n+      return null;\n+    }\n+\n+    Iterator\u003cSocket\u003e iter \u003d socklist.iterator();\n+    while (iter.hasNext()) {\n+      Socket candidate \u003d iter.next();\n+      iter.remove();\n+      if (!candidate.isClosed()) {\n+        return candidate;\n+      }\n+    }\n+    return null;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized Socket get(SocketAddress remote) {\n    List\u003cSocket\u003e socklist \u003d multimap.get(remote);\n    if (socklist \u003d\u003d null) {\n      return null;\n    }\n\n    Iterator\u003cSocket\u003e iter \u003d socklist.iterator();\n    while (iter.hasNext()) {\n      Socket candidate \u003d iter.next();\n      iter.remove();\n      if (!candidate.isClosed()) {\n        return candidate;\n      }\n    }\n    return null;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/SocketCache.java"
    }
  }
}