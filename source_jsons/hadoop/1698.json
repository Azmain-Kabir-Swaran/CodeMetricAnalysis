{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Sender.java",
  "functionName": "blockGroupChecksum",
  "functionId": "blockGroupChecksum___stripedBlockInfo-StripedBlockInfo__blockToken-Token__BlockTokenIdentifier____requestedNumBytes-long__blockChecksumOptions-BlockChecksumOptions",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java",
  "functionStartLine": 282,
  "functionEndLine": 302,
  "numCommitsSeen": 36,
  "timeTaken": 3325,
  "changeHistory": [
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
    "e6cb07520f935efde3e881de8f84ee7f6e0a746f",
    "d749cf65e1ab0e0daf5be86931507183f189e855",
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720"
  ],
  "changeHistoryShort": {
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf": "Ymultichange(Yparameterchange,Ybodychange)",
    "e6cb07520f935efde3e881de8f84ee7f6e0a746f": "Ymultichange(Yparameterchange,Ybodychange)",
    "d749cf65e1ab0e0daf5be86931507183f189e855": "Ybodychange",
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13056. Expose file-level composite CRCs in HDFS which are comparable across different instances/layouts. Contributed by Dennis Huo.\n",
      "commitDate": "10/04/18 9:31 PM",
      "commitName": "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
      "commitAuthor": "Xiao Chen",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13056. Expose file-level composite CRCs in HDFS which are comparable across different instances/layouts. Contributed by Dennis Huo.\n",
          "commitDate": "10/04/18 9:31 PM",
          "commitName": "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
          "commitAuthor": "Xiao Chen",
          "commitDateOld": "05/05/17 12:01 PM",
          "commitNameOld": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 340.4,
          "commitsBetweenForRepo": 2425,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,21 @@\n   public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n-      Token\u003cBlockTokenIdentifier\u003e blockToken, long requestedNumBytes)\n-          throws IOException {\n+      Token\u003cBlockTokenIdentifier\u003e blockToken,\n+      long requestedNumBytes,\n+      BlockChecksumOptions blockChecksumOptions) throws IOException {\n     OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n         .setHeader(DataTransferProtoUtil.buildBaseHeader(\n             stripedBlockInfo.getBlock(), blockToken))\n         .setDatanodes(PBHelperClient.convertToProto(\n             stripedBlockInfo.getDatanodes()))\n         .addAllBlockTokens(PBHelperClient.convert(\n             stripedBlockInfo.getBlockTokens()))\n         .addAllBlockIndices(PBHelperClient\n             .convertBlockIndices(stripedBlockInfo.getBlockIndices()))\n         .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n             stripedBlockInfo.getErasureCodingPolicy()))\n         .setRequestedNumBytes(requestedNumBytes)\n+        .setBlockChecksumOptions(PBHelperClient.convert(blockChecksumOptions))\n         .build();\n \n     send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long requestedNumBytes,\n      BlockChecksumOptions blockChecksumOptions) throws IOException {\n    OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n        .setHeader(DataTransferProtoUtil.buildBaseHeader(\n            stripedBlockInfo.getBlock(), blockToken))\n        .setDatanodes(PBHelperClient.convertToProto(\n            stripedBlockInfo.getDatanodes()))\n        .addAllBlockTokens(PBHelperClient.convert(\n            stripedBlockInfo.getBlockTokens()))\n        .addAllBlockIndices(PBHelperClient\n            .convertBlockIndices(stripedBlockInfo.getBlockIndices()))\n        .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n            stripedBlockInfo.getErasureCodingPolicy()))\n        .setRequestedNumBytes(requestedNumBytes)\n        .setBlockChecksumOptions(PBHelperClient.convert(blockChecksumOptions))\n        .build();\n\n    send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java",
          "extendedDetails": {
            "oldValue": "[stripedBlockInfo-StripedBlockInfo, blockToken-Token\u003cBlockTokenIdentifier\u003e, requestedNumBytes-long]",
            "newValue": "[stripedBlockInfo-StripedBlockInfo, blockToken-Token\u003cBlockTokenIdentifier\u003e, requestedNumBytes-long, blockChecksumOptions-BlockChecksumOptions]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13056. Expose file-level composite CRCs in HDFS which are comparable across different instances/layouts. Contributed by Dennis Huo.\n",
          "commitDate": "10/04/18 9:31 PM",
          "commitName": "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
          "commitAuthor": "Xiao Chen",
          "commitDateOld": "05/05/17 12:01 PM",
          "commitNameOld": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 340.4,
          "commitsBetweenForRepo": 2425,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,21 @@\n   public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n-      Token\u003cBlockTokenIdentifier\u003e blockToken, long requestedNumBytes)\n-          throws IOException {\n+      Token\u003cBlockTokenIdentifier\u003e blockToken,\n+      long requestedNumBytes,\n+      BlockChecksumOptions blockChecksumOptions) throws IOException {\n     OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n         .setHeader(DataTransferProtoUtil.buildBaseHeader(\n             stripedBlockInfo.getBlock(), blockToken))\n         .setDatanodes(PBHelperClient.convertToProto(\n             stripedBlockInfo.getDatanodes()))\n         .addAllBlockTokens(PBHelperClient.convert(\n             stripedBlockInfo.getBlockTokens()))\n         .addAllBlockIndices(PBHelperClient\n             .convertBlockIndices(stripedBlockInfo.getBlockIndices()))\n         .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n             stripedBlockInfo.getErasureCodingPolicy()))\n         .setRequestedNumBytes(requestedNumBytes)\n+        .setBlockChecksumOptions(PBHelperClient.convert(blockChecksumOptions))\n         .build();\n \n     send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long requestedNumBytes,\n      BlockChecksumOptions blockChecksumOptions) throws IOException {\n    OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n        .setHeader(DataTransferProtoUtil.buildBaseHeader(\n            stripedBlockInfo.getBlock(), blockToken))\n        .setDatanodes(PBHelperClient.convertToProto(\n            stripedBlockInfo.getDatanodes()))\n        .addAllBlockTokens(PBHelperClient.convert(\n            stripedBlockInfo.getBlockTokens()))\n        .addAllBlockIndices(PBHelperClient\n            .convertBlockIndices(stripedBlockInfo.getBlockIndices()))\n        .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n            stripedBlockInfo.getErasureCodingPolicy()))\n        .setRequestedNumBytes(requestedNumBytes)\n        .setBlockChecksumOptions(PBHelperClient.convert(blockChecksumOptions))\n        .build();\n\n    send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java",
          "extendedDetails": {}
        }
      ]
    },
    "e6cb07520f935efde3e881de8f84ee7f6e0a746f": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10460. Recompute block checksum for a particular range less than file size on the fly by reconstructing missed block. Contributed by Rakesh R\n",
      "commitDate": "24/06/16 2:39 AM",
      "commitName": "e6cb07520f935efde3e881de8f84ee7f6e0a746f",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10460. Recompute block checksum for a particular range less than file size on the fly by reconstructing missed block. Contributed by Rakesh R\n",
          "commitDate": "24/06/16 2:39 AM",
          "commitName": "e6cb07520f935efde3e881de8f84ee7f6e0a746f",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "01/06/16 9:56 PM",
          "commitNameOld": "d749cf65e1ab0e0daf5be86931507183f189e855",
          "commitAuthorOld": "Kai Zheng",
          "daysBetweenCommits": 22.2,
          "commitsBetweenForRepo": 141,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,19 @@\n   public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n-         Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+      Token\u003cBlockTokenIdentifier\u003e blockToken, long requestedNumBytes)\n+          throws IOException {\n     OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n         .setHeader(DataTransferProtoUtil.buildBaseHeader(\n             stripedBlockInfo.getBlock(), blockToken))\n         .setDatanodes(PBHelperClient.convertToProto(\n             stripedBlockInfo.getDatanodes()))\n         .addAllBlockTokens(PBHelperClient.convert(\n             stripedBlockInfo.getBlockTokens()))\n         .addAllBlockIndices(PBHelperClient\n             .convertBlockIndices(stripedBlockInfo.getBlockIndices()))\n         .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n             stripedBlockInfo.getErasureCodingPolicy()))\n+        .setRequestedNumBytes(requestedNumBytes)\n         .build();\n \n     send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n      Token\u003cBlockTokenIdentifier\u003e blockToken, long requestedNumBytes)\n          throws IOException {\n    OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n        .setHeader(DataTransferProtoUtil.buildBaseHeader(\n            stripedBlockInfo.getBlock(), blockToken))\n        .setDatanodes(PBHelperClient.convertToProto(\n            stripedBlockInfo.getDatanodes()))\n        .addAllBlockTokens(PBHelperClient.convert(\n            stripedBlockInfo.getBlockTokens()))\n        .addAllBlockIndices(PBHelperClient\n            .convertBlockIndices(stripedBlockInfo.getBlockIndices()))\n        .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n            stripedBlockInfo.getErasureCodingPolicy()))\n        .setRequestedNumBytes(requestedNumBytes)\n        .build();\n\n    send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java",
          "extendedDetails": {
            "oldValue": "[stripedBlockInfo-StripedBlockInfo, blockToken-Token\u003cBlockTokenIdentifier\u003e]",
            "newValue": "[stripedBlockInfo-StripedBlockInfo, blockToken-Token\u003cBlockTokenIdentifier\u003e, requestedNumBytes-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10460. Recompute block checksum for a particular range less than file size on the fly by reconstructing missed block. Contributed by Rakesh R\n",
          "commitDate": "24/06/16 2:39 AM",
          "commitName": "e6cb07520f935efde3e881de8f84ee7f6e0a746f",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "01/06/16 9:56 PM",
          "commitNameOld": "d749cf65e1ab0e0daf5be86931507183f189e855",
          "commitAuthorOld": "Kai Zheng",
          "daysBetweenCommits": 22.2,
          "commitsBetweenForRepo": 141,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,19 @@\n   public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n-         Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+      Token\u003cBlockTokenIdentifier\u003e blockToken, long requestedNumBytes)\n+          throws IOException {\n     OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n         .setHeader(DataTransferProtoUtil.buildBaseHeader(\n             stripedBlockInfo.getBlock(), blockToken))\n         .setDatanodes(PBHelperClient.convertToProto(\n             stripedBlockInfo.getDatanodes()))\n         .addAllBlockTokens(PBHelperClient.convert(\n             stripedBlockInfo.getBlockTokens()))\n         .addAllBlockIndices(PBHelperClient\n             .convertBlockIndices(stripedBlockInfo.getBlockIndices()))\n         .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n             stripedBlockInfo.getErasureCodingPolicy()))\n+        .setRequestedNumBytes(requestedNumBytes)\n         .build();\n \n     send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n      Token\u003cBlockTokenIdentifier\u003e blockToken, long requestedNumBytes)\n          throws IOException {\n    OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n        .setHeader(DataTransferProtoUtil.buildBaseHeader(\n            stripedBlockInfo.getBlock(), blockToken))\n        .setDatanodes(PBHelperClient.convertToProto(\n            stripedBlockInfo.getDatanodes()))\n        .addAllBlockTokens(PBHelperClient.convert(\n            stripedBlockInfo.getBlockTokens()))\n        .addAllBlockIndices(PBHelperClient\n            .convertBlockIndices(stripedBlockInfo.getBlockIndices()))\n        .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n            stripedBlockInfo.getErasureCodingPolicy()))\n        .setRequestedNumBytes(requestedNumBytes)\n        .build();\n\n    send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java",
          "extendedDetails": {}
        }
      ]
    },
    "d749cf65e1ab0e0daf5be86931507183f189e855": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9833. Erasure coding: recomputing block checksum on the fly by reconstructing the missed/corrupt block data. Contributed by Rakesh R.\n",
      "commitDate": "01/06/16 9:56 PM",
      "commitName": "d749cf65e1ab0e0daf5be86931507183f189e855",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "26/03/16 7:58 PM",
      "commitNameOld": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 67.08,
      "commitsBetweenForRepo": 433,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,17 @@\n   public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n          Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n         .setHeader(DataTransferProtoUtil.buildBaseHeader(\n             stripedBlockInfo.getBlock(), blockToken))\n         .setDatanodes(PBHelperClient.convertToProto(\n             stripedBlockInfo.getDatanodes()))\n         .addAllBlockTokens(PBHelperClient.convert(\n             stripedBlockInfo.getBlockTokens()))\n+        .addAllBlockIndices(PBHelperClient\n+            .convertBlockIndices(stripedBlockInfo.getBlockIndices()))\n         .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n             stripedBlockInfo.getErasureCodingPolicy()))\n         .build();\n \n     send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n         Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n        .setHeader(DataTransferProtoUtil.buildBaseHeader(\n            stripedBlockInfo.getBlock(), blockToken))\n        .setDatanodes(PBHelperClient.convertToProto(\n            stripedBlockInfo.getDatanodes()))\n        .addAllBlockTokens(PBHelperClient.convert(\n            stripedBlockInfo.getBlockTokens()))\n        .addAllBlockIndices(PBHelperClient\n            .convertBlockIndices(stripedBlockInfo.getBlockIndices()))\n        .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n            stripedBlockInfo.getErasureCodingPolicy()))\n        .build();\n\n    send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java",
      "extendedDetails": {}
    },
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
      "commitDate": "26/03/16 7:58 PM",
      "commitName": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,15 @@\n+  public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n+         Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+    OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n+        .setHeader(DataTransferProtoUtil.buildBaseHeader(\n+            stripedBlockInfo.getBlock(), blockToken))\n+        .setDatanodes(PBHelperClient.convertToProto(\n+            stripedBlockInfo.getDatanodes()))\n+        .addAllBlockTokens(PBHelperClient.convert(\n+            stripedBlockInfo.getBlockTokens()))\n+        .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n+            stripedBlockInfo.getErasureCodingPolicy()))\n+        .build();\n+\n+    send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void blockGroupChecksum(StripedBlockInfo stripedBlockInfo,\n         Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    OpBlockGroupChecksumProto proto \u003d OpBlockGroupChecksumProto.newBuilder()\n        .setHeader(DataTransferProtoUtil.buildBaseHeader(\n            stripedBlockInfo.getBlock(), blockToken))\n        .setDatanodes(PBHelperClient.convertToProto(\n            stripedBlockInfo.getDatanodes()))\n        .addAllBlockTokens(PBHelperClient.convert(\n            stripedBlockInfo.getBlockTokens()))\n        .setEcPolicy(PBHelperClient.convertErasureCodingPolicy(\n            stripedBlockInfo.getErasureCodingPolicy()))\n        .build();\n\n    send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java"
    }
  }
}