{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "MapTask.java",
  "functionName": "getSplitDetails",
  "functionId": "getSplitDetails___file-Path__offset-long",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
  "functionStartLine": 359,
  "functionEndLine": 384,
  "numCommitsSeen": 36,
  "timeTaken": 9783,
  "changeHistory": [
    "aac5c149c7ca500b8eb810b7d4b561ff1e38ea02",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "aac5c149c7ca500b8eb810b7d4b561ff1e38ea02": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymovefromfile",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "aac5c149c7ca500b8eb810b7d4b561ff1e38ea02": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4752. Reduce MR AM memory usage through String Interning (Robert Evans via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1404177 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/10/12 7:57 AM",
      "commitName": "aac5c149c7ca500b8eb810b7d4b561ff1e38ea02",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "09/10/12 6:39 AM",
      "commitNameOld": "0aa8188d188e73e2126086e7a67d15cfc3a3c432",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 22.05,
      "commitsBetweenForRepo": 127,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n  private \u003cT\u003e T getSplitDetails(Path file, long offset) \n   throws IOException {\n    FileSystem fs \u003d file.getFileSystem(conf);\n    FSDataInputStream inFile \u003d fs.open(file);\n    inFile.seek(offset);\n-   String className \u003d Text.readString(inFile);\n+   String className \u003d StringInterner.weakIntern(Text.readString(inFile));\n    Class\u003cT\u003e cls;\n    try {\n      cls \u003d (Class\u003cT\u003e) conf.getClassByName(className);\n    } catch (ClassNotFoundException ce) {\n      IOException wrap \u003d new IOException(\"Split class \" + className + \n                                          \" not found\");\n      wrap.initCause(ce);\n      throw wrap;\n    }\n    SerializationFactory factory \u003d new SerializationFactory(conf);\n    Deserializer\u003cT\u003e deserializer \u003d \n      (Deserializer\u003cT\u003e) factory.getDeserializer(cls);\n    deserializer.open(inFile);\n    T split \u003d deserializer.deserialize(null);\n    long pos \u003d inFile.getPos();\n    getCounters().findCounter(\n        TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n    inFile.close();\n    return split;\n  }\n\\ No newline at end of file\n",
      "actualSource": " private \u003cT\u003e T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs \u003d file.getFileSystem(conf);\n   FSDataInputStream inFile \u003d fs.open(file);\n   inFile.seek(offset);\n   String className \u003d StringInterner.weakIntern(Text.readString(inFile));\n   Class\u003cT\u003e cls;\n   try {\n     cls \u003d (Class\u003cT\u003e) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap \u003d new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory \u003d new SerializationFactory(conf);\n   Deserializer\u003cT\u003e deserializer \u003d \n     (Deserializer\u003cT\u003e) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split \u003d deserializer.deserialize(null);\n   long pos \u003d inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": " private \u003cT\u003e T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs \u003d file.getFileSystem(conf);\n   FSDataInputStream inFile \u003d fs.open(file);\n   inFile.seek(offset);\n   String className \u003d Text.readString(inFile);\n   Class\u003cT\u003e cls;\n   try {\n     cls \u003d (Class\u003cT\u003e) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap \u003d new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory \u003d new SerializationFactory(conf);\n   Deserializer\u003cT\u003e deserializer \u003d \n     (Deserializer\u003cT\u003e) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split \u003d deserializer.deserialize(null);\n   long pos \u003d inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": " private \u003cT\u003e T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs \u003d file.getFileSystem(conf);\n   FSDataInputStream inFile \u003d fs.open(file);\n   inFile.seek(offset);\n   String className \u003d Text.readString(inFile);\n   Class\u003cT\u003e cls;\n   try {\n     cls \u003d (Class\u003cT\u003e) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap \u003d new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory \u003d new SerializationFactory(conf);\n   Deserializer\u003cT\u003e deserializer \u003d \n     (Deserializer\u003cT\u003e) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split \u003d deserializer.deserialize(null);\n   long pos \u003d inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
        "oldMethodName": "getSplitDetails",
        "newMethodName": "getSplitDetails"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,26 @@\n+ private \u003cT\u003e T getSplitDetails(Path file, long offset) \n+  throws IOException {\n+   FileSystem fs \u003d file.getFileSystem(conf);\n+   FSDataInputStream inFile \u003d fs.open(file);\n+   inFile.seek(offset);\n+   String className \u003d Text.readString(inFile);\n+   Class\u003cT\u003e cls;\n+   try {\n+     cls \u003d (Class\u003cT\u003e) conf.getClassByName(className);\n+   } catch (ClassNotFoundException ce) {\n+     IOException wrap \u003d new IOException(\"Split class \" + className + \n+                                         \" not found\");\n+     wrap.initCause(ce);\n+     throw wrap;\n+   }\n+   SerializationFactory factory \u003d new SerializationFactory(conf);\n+   Deserializer\u003cT\u003e deserializer \u003d \n+     (Deserializer\u003cT\u003e) factory.getDeserializer(cls);\n+   deserializer.open(inFile);\n+   T split \u003d deserializer.deserialize(null);\n+   long pos \u003d inFile.getPos();\n+   getCounters().findCounter(\n+       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n+   inFile.close();\n+   return split;\n+ }\n\\ No newline at end of file\n",
      "actualSource": " private \u003cT\u003e T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs \u003d file.getFileSystem(conf);\n   FSDataInputStream inFile \u003d fs.open(file);\n   inFile.seek(offset);\n   String className \u003d Text.readString(inFile);\n   Class\u003cT\u003e cls;\n   try {\n     cls \u003d (Class\u003cT\u003e) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap \u003d new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory \u003d new SerializationFactory(conf);\n   Deserializer\u003cT\u003e deserializer \u003d \n     (Deserializer\u003cT\u003e) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split \u003d deserializer.deserialize(null);\n   long pos \u003d inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java"
    }
  }
}