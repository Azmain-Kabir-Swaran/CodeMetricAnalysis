{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LocalResolver.java",
  "functionName": "getDatanodesSubcluster",
  "functionId": "getDatanodesSubcluster",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java",
  "functionStartLine": 129,
  "functionEndLine": 167,
  "numCommitsSeen": 6,
  "timeTaken": 1563,
  "changeHistory": [
    "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d",
    "e71bc00a471422ddb26dd54e706f09f0fe09925c"
  ],
  "changeHistoryShort": {
    "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d": "Yfilerename",
    "e71bc00a471422ddb26dd54e706f09f0fe09925c": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-13215. RBF: Move Router to its own module. Contributed by Wei Yan\n",
      "commitDate": "19/03/18 10:13 PM",
      "commitName": "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d",
      "commitAuthor": "weiy",
      "commitDateOld": "19/03/18 5:19 PM",
      "commitNameOld": "e65ff1c8be48ef4f04ed96f96ac4caef4974944d",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 0.2,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private Map\u003cString, String\u003e getDatanodesSubcluster() {\n\n    final RouterRpcServer rpcServer \u003d getRpcServer();\n    if (rpcServer \u003d\u003d null) {\n      LOG.error(\"Cannot access the Router RPC server\");\n      return null;\n    }\n\n    Map\u003cString, String\u003e ret \u003d new HashMap\u003c\u003e();\n    try {\n      // We need to get the DNs as a privileged user\n      UserGroupInformation loginUser \u003d UserGroupInformation.getLoginUser();\n      Map\u003cString, DatanodeStorageReport[]\u003e dnMap \u003d loginUser.doAs(\n          new PrivilegedAction\u003cMap\u003cString, DatanodeStorageReport[]\u003e\u003e() {\n            @Override\n            public Map\u003cString, DatanodeStorageReport[]\u003e run() {\n              try {\n                return rpcServer.getDatanodeStorageReportMap(\n                    DatanodeReportType.ALL);\n              } catch (IOException e) {\n                LOG.error(\"Cannot get the datanodes from the RPC server\", e);\n                return null;\n              }\n            }\n          });\n      for (Entry\u003cString, DatanodeStorageReport[]\u003e entry : dnMap.entrySet()) {\n        String nsId \u003d entry.getKey();\n        DatanodeStorageReport[] dns \u003d entry.getValue();\n        for (DatanodeStorageReport dn : dns) {\n          DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n          String ipAddr \u003d dnInfo.getIpAddr();\n          ret.put(ipAddr, nsId);\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Cannot get Datanodes from the Namenodes: {}\", e.getMessage());\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java"
      }
    },
    "e71bc00a471422ddb26dd54e706f09f0fe09925c": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13224. RBF: Resolvers to support mount points across multiple subclusters. Contributed by Inigo Goiri.\n",
      "commitDate": "15/03/18 10:32 AM",
      "commitName": "e71bc00a471422ddb26dd54e706f09f0fe09925c",
      "commitAuthor": "Inigo Goiri",
      "diff": "@@ -0,0 +1,39 @@\n+  private Map\u003cString, String\u003e getDatanodesSubcluster() {\n+\n+    final RouterRpcServer rpcServer \u003d getRpcServer();\n+    if (rpcServer \u003d\u003d null) {\n+      LOG.error(\"Cannot access the Router RPC server\");\n+      return null;\n+    }\n+\n+    Map\u003cString, String\u003e ret \u003d new HashMap\u003c\u003e();\n+    try {\n+      // We need to get the DNs as a privileged user\n+      UserGroupInformation loginUser \u003d UserGroupInformation.getLoginUser();\n+      Map\u003cString, DatanodeStorageReport[]\u003e dnMap \u003d loginUser.doAs(\n+          new PrivilegedAction\u003cMap\u003cString, DatanodeStorageReport[]\u003e\u003e() {\n+            @Override\n+            public Map\u003cString, DatanodeStorageReport[]\u003e run() {\n+              try {\n+                return rpcServer.getDatanodeStorageReportMap(\n+                    DatanodeReportType.ALL);\n+              } catch (IOException e) {\n+                LOG.error(\"Cannot get the datanodes from the RPC server\", e);\n+                return null;\n+              }\n+            }\n+          });\n+      for (Entry\u003cString, DatanodeStorageReport[]\u003e entry : dnMap.entrySet()) {\n+        String nsId \u003d entry.getKey();\n+        DatanodeStorageReport[] dns \u003d entry.getValue();\n+        for (DatanodeStorageReport dn : dns) {\n+          DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n+          String ipAddr \u003d dnInfo.getIpAddr();\n+          ret.put(ipAddr, nsId);\n+        }\n+      }\n+    } catch (IOException e) {\n+      LOG.error(\"Cannot get Datanodes from the Namenodes: {}\", e.getMessage());\n+    }\n+    return ret;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private Map\u003cString, String\u003e getDatanodesSubcluster() {\n\n    final RouterRpcServer rpcServer \u003d getRpcServer();\n    if (rpcServer \u003d\u003d null) {\n      LOG.error(\"Cannot access the Router RPC server\");\n      return null;\n    }\n\n    Map\u003cString, String\u003e ret \u003d new HashMap\u003c\u003e();\n    try {\n      // We need to get the DNs as a privileged user\n      UserGroupInformation loginUser \u003d UserGroupInformation.getLoginUser();\n      Map\u003cString, DatanodeStorageReport[]\u003e dnMap \u003d loginUser.doAs(\n          new PrivilegedAction\u003cMap\u003cString, DatanodeStorageReport[]\u003e\u003e() {\n            @Override\n            public Map\u003cString, DatanodeStorageReport[]\u003e run() {\n              try {\n                return rpcServer.getDatanodeStorageReportMap(\n                    DatanodeReportType.ALL);\n              } catch (IOException e) {\n                LOG.error(\"Cannot get the datanodes from the RPC server\", e);\n                return null;\n              }\n            }\n          });\n      for (Entry\u003cString, DatanodeStorageReport[]\u003e entry : dnMap.entrySet()) {\n        String nsId \u003d entry.getKey();\n        DatanodeStorageReport[] dns \u003d entry.getValue();\n        for (DatanodeStorageReport dn : dns) {\n          DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n          String ipAddr \u003d dnInfo.getIpAddr();\n          ret.put(ipAddr, nsId);\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Cannot get Datanodes from the Namenodes: {}\", e.getMessage());\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java"
    }
  }
}