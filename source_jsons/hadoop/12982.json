{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeDescriptor.java",
  "functionName": "pruneStorageMap",
  "functionId": "pruneStorageMap___reports-StorageReport[](modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
  "functionStartLine": 523,
  "functionEndLine": 561,
  "numCommitsSeen": 118,
  "timeTaken": 3364,
  "changeHistory": [
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f",
    "2e7b7e2cda67eba4c03e0a2c7892d868d235b0cf",
    "2ffd84273ac490724fe7e7825664bb6d09ef0e99",
    "1feb9569f366a29ecb43592d71ee21023162c18f",
    "ef3c3a832c2f0c1e5ccdda2ff8ef84902912955f"
  ],
  "changeHistoryShort": {
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f": "Ybodychange",
    "2e7b7e2cda67eba4c03e0a2c7892d868d235b0cf": "Ybodychange",
    "2ffd84273ac490724fe7e7825664bb6d09ef0e99": "Ybodychange",
    "1feb9569f366a29ecb43592d71ee21023162c18f": "Ybodychange",
    "ef3c3a832c2f0c1e5ccdda2ff8ef84902912955f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11530. Use HDFS specific network topology to choose datanode in BlockPlacementPolicyDefault. Contributed by Yiqun Lin and Chen Liang.\n",
      "commitDate": "04/05/17 8:54 PM",
      "commitName": "97c2e576c91c2316c2b52bfc948bae9bff8ca49f",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "17/04/17 4:56 PM",
      "commitNameOld": "8dfcd95d580bb090af7f40af0a57061518c18c8c",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 17.17,
      "commitsBetweenForRepo": 103,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,39 @@\n   private void pruneStorageMap(final StorageReport[] reports) {\n     synchronized (storageMap) {\n       LOG.debug(\"Number of storages reported in heartbeat\u003d{};\"\n               + \" Number of storages in storageMap\u003d{}\", reports.length,\n           storageMap.size());\n \n       HashMap\u003cString, DatanodeStorageInfo\u003e excessStorages;\n \n       // Init excessStorages with all known storages.\n       excessStorages \u003d new HashMap\u003c\u003e(storageMap);\n \n       // Remove storages that the DN reported in the heartbeat.\n       for (final StorageReport report : reports) {\n         excessStorages.remove(report.getStorage().getStorageID());\n       }\n \n       // For each remaining storage, remove it if there are no associated\n       // blocks.\n       for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {\n         if (storageInfo.numBlocks() \u003d\u003d 0) {\n-          storageMap.remove(storageInfo.getStorageID());\n+          DatanodeStorageInfo info \u003d\n+              storageMap.remove(storageInfo.getStorageID());\n+          if (!hasStorageType(info.getStorageType())) {\n+            // we removed a storage, and as result there is no more such storage\n+            // type, inform the parent about this.\n+            if (getParent() instanceof DFSTopologyNodeImpl) {\n+              ((DFSTopologyNodeImpl) getParent()).childRemoveStorage(getName(),\n+                  info.getStorageType());\n+            }\n+          }\n           LOG.info(\"Removed storage {} from DataNode {}\", storageInfo, this);\n         } else {\n           // This can occur until all block reports are received.\n           LOG.debug(\"Deferring removal of stale storage {} with {} blocks\",\n               storageInfo, storageInfo.numBlocks());\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void pruneStorageMap(final StorageReport[] reports) {\n    synchronized (storageMap) {\n      LOG.debug(\"Number of storages reported in heartbeat\u003d{};\"\n              + \" Number of storages in storageMap\u003d{}\", reports.length,\n          storageMap.size());\n\n      HashMap\u003cString, DatanodeStorageInfo\u003e excessStorages;\n\n      // Init excessStorages with all known storages.\n      excessStorages \u003d new HashMap\u003c\u003e(storageMap);\n\n      // Remove storages that the DN reported in the heartbeat.\n      for (final StorageReport report : reports) {\n        excessStorages.remove(report.getStorage().getStorageID());\n      }\n\n      // For each remaining storage, remove it if there are no associated\n      // blocks.\n      for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {\n        if (storageInfo.numBlocks() \u003d\u003d 0) {\n          DatanodeStorageInfo info \u003d\n              storageMap.remove(storageInfo.getStorageID());\n          if (!hasStorageType(info.getStorageType())) {\n            // we removed a storage, and as result there is no more such storage\n            // type, inform the parent about this.\n            if (getParent() instanceof DFSTopologyNodeImpl) {\n              ((DFSTopologyNodeImpl) getParent()).childRemoveStorage(getName(),\n                  info.getStorageType());\n            }\n          }\n          LOG.info(\"Removed storage {} from DataNode {}\", storageInfo, this);\n        } else {\n          // This can occur until all block reports are received.\n          LOG.debug(\"Deferring removal of stale storage {} with {} blocks\",\n              storageInfo, storageInfo.numBlocks());\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    },
    "2e7b7e2cda67eba4c03e0a2c7892d868d235b0cf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8713. Convert DatanodeDescriptor to use SLF4J logging.\n",
      "commitDate": "17/08/15 10:17 AM",
      "commitName": "2e7b7e2cda67eba4c03e0a2c7892d868d235b0cf",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "06/08/15 10:21 AM",
      "commitNameOld": "f4c523b69ba55b1fd35e8995c3011a9f546ac835",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 11.0,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,30 @@\n   private void pruneStorageMap(final StorageReport[] reports) {\n     synchronized (storageMap) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Number of storages reported in heartbeat\u003d\" + reports.length\n-            + \"; Number of storages in storageMap\u003d\" + storageMap.size());\n-      }\n+      LOG.debug(\"Number of storages reported in heartbeat\u003d{};\"\n+              + \" Number of storages in storageMap\u003d{}\", reports.length,\n+          storageMap.size());\n \n       HashMap\u003cString, DatanodeStorageInfo\u003e excessStorages;\n \n       // Init excessStorages with all known storages.\n       excessStorages \u003d new HashMap\u003c\u003e(storageMap);\n \n       // Remove storages that the DN reported in the heartbeat.\n       for (final StorageReport report : reports) {\n         excessStorages.remove(report.getStorage().getStorageID());\n       }\n \n       // For each remaining storage, remove it if there are no associated\n       // blocks.\n       for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {\n         if (storageInfo.numBlocks() \u003d\u003d 0) {\n           storageMap.remove(storageInfo.getStorageID());\n-          LOG.info(\"Removed storage \" + storageInfo + \" from DataNode\" + this);\n-        } else if (LOG.isDebugEnabled()) {\n+          LOG.info(\"Removed storage {} from DataNode {}\", storageInfo, this);\n+        } else {\n           // This can occur until all block reports are received.\n-          LOG.debug(\"Deferring removal of stale storage \" + storageInfo\n-              + \" with \" + storageInfo.numBlocks() + \" blocks\");\n+          LOG.debug(\"Deferring removal of stale storage {} with {} blocks\",\n+              storageInfo, storageInfo.numBlocks());\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void pruneStorageMap(final StorageReport[] reports) {\n    synchronized (storageMap) {\n      LOG.debug(\"Number of storages reported in heartbeat\u003d{};\"\n              + \" Number of storages in storageMap\u003d{}\", reports.length,\n          storageMap.size());\n\n      HashMap\u003cString, DatanodeStorageInfo\u003e excessStorages;\n\n      // Init excessStorages with all known storages.\n      excessStorages \u003d new HashMap\u003c\u003e(storageMap);\n\n      // Remove storages that the DN reported in the heartbeat.\n      for (final StorageReport report : reports) {\n        excessStorages.remove(report.getStorage().getStorageID());\n      }\n\n      // For each remaining storage, remove it if there are no associated\n      // blocks.\n      for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {\n        if (storageInfo.numBlocks() \u003d\u003d 0) {\n          storageMap.remove(storageInfo.getStorageID());\n          LOG.info(\"Removed storage {} from DataNode {}\", storageInfo, this);\n        } else {\n          // This can occur until all block reports are received.\n          LOG.debug(\"Deferring removal of stale storage {} with {} blocks\",\n              storageInfo, storageInfo.numBlocks());\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    },
    "2ffd84273ac490724fe7e7825664bb6d09ef0e99": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8653. Code cleanup for DatanodeManager, DatanodeDescriptor and DatanodeStorageInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "29/06/15 12:12 PM",
      "commitName": "2ffd84273ac490724fe7e7825664bb6d09ef0e99",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "12/06/15 11:38 AM",
      "commitNameOld": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 17.02,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   private void pruneStorageMap(final StorageReport[] reports) {\n     synchronized (storageMap) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Number of storages reported in heartbeat\u003d\" + reports.length\n             + \"; Number of storages in storageMap\u003d\" + storageMap.size());\n       }\n \n       HashMap\u003cString, DatanodeStorageInfo\u003e excessStorages;\n \n       // Init excessStorages with all known storages.\n-      excessStorages \u003d new HashMap\u003cString, DatanodeStorageInfo\u003e(storageMap);\n+      excessStorages \u003d new HashMap\u003c\u003e(storageMap);\n \n       // Remove storages that the DN reported in the heartbeat.\n       for (final StorageReport report : reports) {\n         excessStorages.remove(report.getStorage().getStorageID());\n       }\n \n       // For each remaining storage, remove it if there are no associated\n       // blocks.\n       for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {\n         if (storageInfo.numBlocks() \u003d\u003d 0) {\n           storageMap.remove(storageInfo.getStorageID());\n           LOG.info(\"Removed storage \" + storageInfo + \" from DataNode\" + this);\n         } else if (LOG.isDebugEnabled()) {\n           // This can occur until all block reports are received.\n           LOG.debug(\"Deferring removal of stale storage \" + storageInfo\n               + \" with \" + storageInfo.numBlocks() + \" blocks\");\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void pruneStorageMap(final StorageReport[] reports) {\n    synchronized (storageMap) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Number of storages reported in heartbeat\u003d\" + reports.length\n            + \"; Number of storages in storageMap\u003d\" + storageMap.size());\n      }\n\n      HashMap\u003cString, DatanodeStorageInfo\u003e excessStorages;\n\n      // Init excessStorages with all known storages.\n      excessStorages \u003d new HashMap\u003c\u003e(storageMap);\n\n      // Remove storages that the DN reported in the heartbeat.\n      for (final StorageReport report : reports) {\n        excessStorages.remove(report.getStorage().getStorageID());\n      }\n\n      // For each remaining storage, remove it if there are no associated\n      // blocks.\n      for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {\n        if (storageInfo.numBlocks() \u003d\u003d 0) {\n          storageMap.remove(storageInfo.getStorageID());\n          LOG.info(\"Removed storage \" + storageInfo + \" from DataNode\" + this);\n        } else if (LOG.isDebugEnabled()) {\n          // This can occur until all block reports are received.\n          LOG.debug(\"Deferring removal of stale storage \" + storageInfo\n              + \" with \" + storageInfo.numBlocks() + \" blocks\");\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    },
    "1feb9569f366a29ecb43592d71ee21023162c18f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7261. storageMap is accessed without synchronization in DatanodeDescriptor#updateHeartbeatState() (Brahma Reddy Battula via Colin P. McCabe)\n",
      "commitDate": "30/03/15 10:46 AM",
      "commitName": "1feb9569f366a29ecb43592d71ee21023162c18f",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "23/03/15 10:00 PM",
      "commitNameOld": "50ee8f4e67a66aa77c5359182f61f3e951844db6",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 6.53,
      "commitsBetweenForRepo": 56,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   private void pruneStorageMap(final StorageReport[] reports) {\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Number of storages reported in heartbeat\u003d\" + reports.length +\n-                    \"; Number of storages in storageMap\u003d\" + storageMap.size());\n-    }\n-\n-    HashMap\u003cString, DatanodeStorageInfo\u003e excessStorages;\n-\n     synchronized (storageMap) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Number of storages reported in heartbeat\u003d\" + reports.length\n+            + \"; Number of storages in storageMap\u003d\" + storageMap.size());\n+      }\n+\n+      HashMap\u003cString, DatanodeStorageInfo\u003e excessStorages;\n+\n       // Init excessStorages with all known storages.\n       excessStorages \u003d new HashMap\u003cString, DatanodeStorageInfo\u003e(storageMap);\n \n       // Remove storages that the DN reported in the heartbeat.\n       for (final StorageReport report : reports) {\n         excessStorages.remove(report.getStorage().getStorageID());\n       }\n \n       // For each remaining storage, remove it if there are no associated\n       // blocks.\n       for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {\n         if (storageInfo.numBlocks() \u003d\u003d 0) {\n           storageMap.remove(storageInfo.getStorageID());\n           LOG.info(\"Removed storage \" + storageInfo + \" from DataNode\" + this);\n         } else if (LOG.isDebugEnabled()) {\n           // This can occur until all block reports are received.\n-          LOG.debug(\"Deferring removal of stale storage \" + storageInfo +\n-                        \" with \" + storageInfo.numBlocks() + \" blocks\");\n+          LOG.debug(\"Deferring removal of stale storage \" + storageInfo\n+              + \" with \" + storageInfo.numBlocks() + \" blocks\");\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void pruneStorageMap(final StorageReport[] reports) {\n    synchronized (storageMap) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Number of storages reported in heartbeat\u003d\" + reports.length\n            + \"; Number of storages in storageMap\u003d\" + storageMap.size());\n      }\n\n      HashMap\u003cString, DatanodeStorageInfo\u003e excessStorages;\n\n      // Init excessStorages with all known storages.\n      excessStorages \u003d new HashMap\u003cString, DatanodeStorageInfo\u003e(storageMap);\n\n      // Remove storages that the DN reported in the heartbeat.\n      for (final StorageReport report : reports) {\n        excessStorages.remove(report.getStorage().getStorageID());\n      }\n\n      // For each remaining storage, remove it if there are no associated\n      // blocks.\n      for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {\n        if (storageInfo.numBlocks() \u003d\u003d 0) {\n          storageMap.remove(storageInfo.getStorageID());\n          LOG.info(\"Removed storage \" + storageInfo + \" from DataNode\" + this);\n        } else if (LOG.isDebugEnabled()) {\n          // This can occur until all block reports are received.\n          LOG.debug(\"Deferring removal of stale storage \" + storageInfo\n              + \" with \" + storageInfo.numBlocks() + \" blocks\");\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    },
    "ef3c3a832c2f0c1e5ccdda2ff8ef84902912955f": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7596. NameNode should prune dead storages from storageMap. Contributed by Arpit Agarwal.\n",
      "commitDate": "10/01/15 9:18 AM",
      "commitName": "ef3c3a832c2f0c1e5ccdda2ff8ef84902912955f",
      "commitAuthor": "cnauroth",
      "diff": "@@ -0,0 +1,31 @@\n+  private void pruneStorageMap(final StorageReport[] reports) {\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Number of storages reported in heartbeat\u003d\" + reports.length +\n+                    \"; Number of storages in storageMap\u003d\" + storageMap.size());\n+    }\n+\n+    HashMap\u003cString, DatanodeStorageInfo\u003e excessStorages;\n+\n+    synchronized (storageMap) {\n+      // Init excessStorages with all known storages.\n+      excessStorages \u003d new HashMap\u003cString, DatanodeStorageInfo\u003e(storageMap);\n+\n+      // Remove storages that the DN reported in the heartbeat.\n+      for (final StorageReport report : reports) {\n+        excessStorages.remove(report.getStorage().getStorageID());\n+      }\n+\n+      // For each remaining storage, remove it if there are no associated\n+      // blocks.\n+      for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {\n+        if (storageInfo.numBlocks() \u003d\u003d 0) {\n+          storageMap.remove(storageInfo.getStorageID());\n+          LOG.info(\"Removed storage \" + storageInfo + \" from DataNode\" + this);\n+        } else if (LOG.isDebugEnabled()) {\n+          // This can occur until all block reports are received.\n+          LOG.debug(\"Deferring removal of stale storage \" + storageInfo +\n+                        \" with \" + storageInfo.numBlocks() + \" blocks\");\n+        }\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void pruneStorageMap(final StorageReport[] reports) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Number of storages reported in heartbeat\u003d\" + reports.length +\n                    \"; Number of storages in storageMap\u003d\" + storageMap.size());\n    }\n\n    HashMap\u003cString, DatanodeStorageInfo\u003e excessStorages;\n\n    synchronized (storageMap) {\n      // Init excessStorages with all known storages.\n      excessStorages \u003d new HashMap\u003cString, DatanodeStorageInfo\u003e(storageMap);\n\n      // Remove storages that the DN reported in the heartbeat.\n      for (final StorageReport report : reports) {\n        excessStorages.remove(report.getStorage().getStorageID());\n      }\n\n      // For each remaining storage, remove it if there are no associated\n      // blocks.\n      for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {\n        if (storageInfo.numBlocks() \u003d\u003d 0) {\n          storageMap.remove(storageInfo.getStorageID());\n          LOG.info(\"Removed storage \" + storageInfo + \" from DataNode\" + this);\n        } else if (LOG.isDebugEnabled()) {\n          // This can occur until all block reports are received.\n          LOG.debug(\"Deferring removal of stale storage \" + storageInfo +\n                        \" with \" + storageInfo.numBlocks() + \" blocks\");\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java"
    }
  }
}