{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirAttrOp.java",
  "functionName": "unsetStoragePolicy",
  "functionId": "unsetStoragePolicy___fsd-FSDirectory__pc-FSPermissionChecker__bm-BlockManager__src-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
  "functionStartLine": 150,
  "functionEndLine": 154,
  "numCommitsSeen": 578,
  "timeTaken": 13679,
  "changeHistory": [
    "a700803a18fb957d2799001a2ce1dcb70f75c080",
    "84a1321f6aa0af6895564a7c47f8f264656f0294",
    "9b90e52f1ec22c18cd535af2a569defcef65b093",
    "27941a1811831e0f2144a2f463d807755cd850b2",
    "65f2a4ee600dfffa5203450261da3c1989de25a9",
    "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
    "a45ad330facc56f06ed42eb71304c49ef56dc549",
    "bb84f1fccb18c6c7373851e05d2451d55e908242",
    "b38e52b5e8f2b2ad91c2c8c4eabc232ee4753ca0"
  ],
  "changeHistoryShort": {
    "a700803a18fb957d2799001a2ce1dcb70f75c080": "Ybodychange",
    "84a1321f6aa0af6895564a7c47f8f264656f0294": "Ymultichange(Yparameterchange,Ybodychange)",
    "9b90e52f1ec22c18cd535af2a569defcef65b093": "Yreturntypechange",
    "27941a1811831e0f2144a2f463d807755cd850b2": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
    "65f2a4ee600dfffa5203450261da3c1989de25a9": "Ybodychange",
    "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yrename,Yparameterchange)",
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2": "Ymultichange(Yexceptionschange,Ybodychange)",
    "a45ad330facc56f06ed42eb71304c49ef56dc549": "Ybodychange",
    "bb84f1fccb18c6c7373851e05d2451d55e908242": "Ybodychange",
    "b38e52b5e8f2b2ad91c2c8c4eabc232ee4753ca0": "Ymultichange(Yexceptionschange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "a700803a18fb957d2799001a2ce1dcb70f75c080": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15154. Allow only hdfs superusers the ability to assign HDFS storage policies. Contributed by Siddharth Wagle.\n\nChange-Id: I32d6dd2837945b8fc026a759aa367c55daefe348\n",
      "commitDate": "25/03/20 10:28 AM",
      "commitName": "a700803a18fb957d2799001a2ce1dcb70f75c080",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "24/03/20 10:26 AM",
      "commitNameOld": "ea87d6049340d1df040047aa08ce7784c03dd69e",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 1.0,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,5 @@\n   static FileStatus unsetStoragePolicy(FSDirectory fsd, FSPermissionChecker pc,\n       BlockManager bm, String src) throws IOException {\n     return setStoragePolicy(fsd, pc, bm, src,\n-        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n+        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static FileStatus unsetStoragePolicy(FSDirectory fsd, FSPermissionChecker pc,\n      BlockManager bm, String src) throws IOException {\n    return setStoragePolicy(fsd, pc, bm, src,\n        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
      "extendedDetails": {}
    },
    "84a1321f6aa0af6895564a7c47f8f264656f0294": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
      "commitDate": "22/02/18 11:32 AM",
      "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
      "commitAuthor": "Xiaoyu Yao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
          "commitDate": "22/02/18 11:32 AM",
          "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
          "commitAuthor": "Xiaoyu Yao",
          "commitDateOld": "20/12/17 8:55 AM",
          "commitNameOld": "a78db9919065d06ced8122229530f44cc7369857",
          "commitAuthorOld": "Wei Yan",
          "daysBetweenCommits": 64.11,
          "commitsBetweenForRepo": 331,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,5 +1,5 @@\n-  static FileStatus unsetStoragePolicy(FSDirectory fsd, BlockManager bm,\n-      String src) throws IOException {\n-    return setStoragePolicy(fsd, bm, src,\n+  static FileStatus unsetStoragePolicy(FSDirectory fsd, FSPermissionChecker pc,\n+      BlockManager bm, String src) throws IOException {\n+    return setStoragePolicy(fsd, pc, bm, src,\n         HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static FileStatus unsetStoragePolicy(FSDirectory fsd, FSPermissionChecker pc,\n      BlockManager bm, String src) throws IOException {\n    return setStoragePolicy(fsd, pc, bm, src,\n        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, bm-BlockManager, src-String]",
            "newValue": "[fsd-FSDirectory, pc-FSPermissionChecker, bm-BlockManager, src-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
          "commitDate": "22/02/18 11:32 AM",
          "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
          "commitAuthor": "Xiaoyu Yao",
          "commitDateOld": "20/12/17 8:55 AM",
          "commitNameOld": "a78db9919065d06ced8122229530f44cc7369857",
          "commitAuthorOld": "Wei Yan",
          "daysBetweenCommits": 64.11,
          "commitsBetweenForRepo": 331,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,5 +1,5 @@\n-  static FileStatus unsetStoragePolicy(FSDirectory fsd, BlockManager bm,\n-      String src) throws IOException {\n-    return setStoragePolicy(fsd, bm, src,\n+  static FileStatus unsetStoragePolicy(FSDirectory fsd, FSPermissionChecker pc,\n+      BlockManager bm, String src) throws IOException {\n+    return setStoragePolicy(fsd, pc, bm, src,\n         HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static FileStatus unsetStoragePolicy(FSDirectory fsd, FSPermissionChecker pc,\n      BlockManager bm, String src) throws IOException {\n    return setStoragePolicy(fsd, pc, bm, src,\n        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "9b90e52f1ec22c18cd535af2a569defcef65b093": {
      "type": "Yreturntypechange",
      "commitMessage": "HDFS-11641. Reduce cost of audit logging by using FileStatus instead of HdfsFileStatus. Contributed by Daryn Sharp.\n",
      "commitDate": "16/05/17 9:28 AM",
      "commitName": "9b90e52f1ec22c18cd535af2a569defcef65b093",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "21/12/16 1:04 PM",
      "commitNameOld": "f6e80acd681548b14fe3f0f3d2b3aaf800d10310",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 145.81,
      "commitsBetweenForRepo": 772,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,5 @@\n-  static HdfsFileStatus unsetStoragePolicy(FSDirectory fsd, BlockManager bm,\n+  static FileStatus unsetStoragePolicy(FSDirectory fsd, BlockManager bm,\n       String src) throws IOException {\n     return setStoragePolicy(fsd, bm, src,\n         HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static FileStatus unsetStoragePolicy(FSDirectory fsd, BlockManager bm,\n      String src) throws IOException {\n    return setStoragePolicy(fsd, bm, src,\n        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
      "extendedDetails": {
        "oldValue": "HdfsFileStatus",
        "newValue": "FileStatus"
      }
    },
    "27941a1811831e0f2144a2f463d807755cd850b2": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9534. Add CLI command to clear storage policy from a path. (Contributed by Xiaobing Zhou)\n",
      "commitDate": "02/03/16 6:35 PM",
      "commitName": "27941a1811831e0f2144a2f463d807755cd850b2",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-9534. Add CLI command to clear storage policy from a path. (Contributed by Xiaobing Zhou)\n",
          "commitDate": "02/03/16 6:35 PM",
          "commitName": "27941a1811831e0f2144a2f463d807755cd850b2",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "21/10/15 4:58 PM",
          "commitNameOld": "3dadf369d550c2ae393b751cb5a184dbfe2814df",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 133.11,
          "commitsBetweenForRepo": 903,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,5 @@\n-  static HdfsFileStatus setStoragePolicy(\n-      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n-      throws IOException {\n-    if (!fsd.isStoragePolicyEnabled()) {\n-      throw new IOException(\n-          \"Failed to set storage policy since \"\n-              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n-    }\n-    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    INodesInPath iip;\n-    fsd.writeLock();\n-    try {\n-      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n-      iip \u003d fsd.getINodesInPath4Write(src);\n-\n-      if (fsd.isPermissionEnabled()) {\n-        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n-      }\n-\n-      // get the corresponding policy and make sure the policy name is valid\n-      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n-      if (policy \u003d\u003d null) {\n-        throw new HadoopIllegalArgumentException(\n-            \"Cannot find a block policy with the name \" + policyName);\n-      }\n-      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n-      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n-    } finally {\n-      fsd.writeUnlock();\n-    }\n-    return fsd.getAuditFileInfo(iip);\n+  static HdfsFileStatus unsetStoragePolicy(FSDirectory fsd, BlockManager bm,\n+      String src) throws IOException {\n+    return setStoragePolicy(fsd, bm, src,\n+        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus unsetStoragePolicy(FSDirectory fsd, BlockManager bm,\n      String src) throws IOException {\n    return setStoragePolicy(fsd, bm, src,\n        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
          "extendedDetails": {
            "oldValue": "setStoragePolicy",
            "newValue": "unsetStoragePolicy"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9534. Add CLI command to clear storage policy from a path. (Contributed by Xiaobing Zhou)\n",
          "commitDate": "02/03/16 6:35 PM",
          "commitName": "27941a1811831e0f2144a2f463d807755cd850b2",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "21/10/15 4:58 PM",
          "commitNameOld": "3dadf369d550c2ae393b751cb5a184dbfe2814df",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 133.11,
          "commitsBetweenForRepo": 903,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,5 @@\n-  static HdfsFileStatus setStoragePolicy(\n-      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n-      throws IOException {\n-    if (!fsd.isStoragePolicyEnabled()) {\n-      throw new IOException(\n-          \"Failed to set storage policy since \"\n-              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n-    }\n-    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    INodesInPath iip;\n-    fsd.writeLock();\n-    try {\n-      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n-      iip \u003d fsd.getINodesInPath4Write(src);\n-\n-      if (fsd.isPermissionEnabled()) {\n-        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n-      }\n-\n-      // get the corresponding policy and make sure the policy name is valid\n-      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n-      if (policy \u003d\u003d null) {\n-        throw new HadoopIllegalArgumentException(\n-            \"Cannot find a block policy with the name \" + policyName);\n-      }\n-      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n-      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n-    } finally {\n-      fsd.writeUnlock();\n-    }\n-    return fsd.getAuditFileInfo(iip);\n+  static HdfsFileStatus unsetStoragePolicy(FSDirectory fsd, BlockManager bm,\n+      String src) throws IOException {\n+    return setStoragePolicy(fsd, bm, src,\n+        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus unsetStoragePolicy(FSDirectory fsd, BlockManager bm,\n      String src) throws IOException {\n    return setStoragePolicy(fsd, bm, src,\n        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, bm-BlockManager, src-String, policyName-String(modifiers-final)]",
            "newValue": "[fsd-FSDirectory, bm-BlockManager, src-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9534. Add CLI command to clear storage policy from a path. (Contributed by Xiaobing Zhou)\n",
          "commitDate": "02/03/16 6:35 PM",
          "commitName": "27941a1811831e0f2144a2f463d807755cd850b2",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "21/10/15 4:58 PM",
          "commitNameOld": "3dadf369d550c2ae393b751cb5a184dbfe2814df",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 133.11,
          "commitsBetweenForRepo": 903,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,5 @@\n-  static HdfsFileStatus setStoragePolicy(\n-      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n-      throws IOException {\n-    if (!fsd.isStoragePolicyEnabled()) {\n-      throw new IOException(\n-          \"Failed to set storage policy since \"\n-              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n-    }\n-    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    INodesInPath iip;\n-    fsd.writeLock();\n-    try {\n-      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n-      iip \u003d fsd.getINodesInPath4Write(src);\n-\n-      if (fsd.isPermissionEnabled()) {\n-        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n-      }\n-\n-      // get the corresponding policy and make sure the policy name is valid\n-      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n-      if (policy \u003d\u003d null) {\n-        throw new HadoopIllegalArgumentException(\n-            \"Cannot find a block policy with the name \" + policyName);\n-      }\n-      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n-      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n-    } finally {\n-      fsd.writeUnlock();\n-    }\n-    return fsd.getAuditFileInfo(iip);\n+  static HdfsFileStatus unsetStoragePolicy(FSDirectory fsd, BlockManager bm,\n+      String src) throws IOException {\n+    return setStoragePolicy(fsd, bm, src,\n+        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus unsetStoragePolicy(FSDirectory fsd, BlockManager bm,\n      String src) throws IOException {\n    return setStoragePolicy(fsd, bm, src,\n        HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED, \"unset\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "65f2a4ee600dfffa5203450261da3c1989de25a9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7543. Avoid path resolution when getting FileStatus for audit logs. Contributed by Haohui Mai.\n",
      "commitDate": "18/12/14 11:25 AM",
      "commitName": "65f2a4ee600dfffa5203450261da3c1989de25a9",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "15/12/14 10:40 AM",
      "commitNameOld": "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.03,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,33 @@\n   static HdfsFileStatus setStoragePolicy(\n       FSDirectory fsd, BlockManager bm, String src, final String policyName)\n       throws IOException {\n     if (!fsd.isStoragePolicyEnabled()) {\n       throw new IOException(\n           \"Failed to set storage policy since \"\n               + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n     }\n     FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n+    INodesInPath iip;\n     fsd.writeLock();\n     try {\n       src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n-      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n+      iip \u003d fsd.getINodesInPath4Write(src);\n \n       if (fsd.isPermissionEnabled()) {\n         fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n       }\n \n       // get the corresponding policy and make sure the policy name is valid\n       BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n       unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n       fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n     } finally {\n       fsd.writeUnlock();\n     }\n-    return fsd.getAuditFileInfo(src, false);\n+    return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus setStoragePolicy(\n      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n      throws IOException {\n    if (!fsd.isStoragePolicyEnabled()) {\n      throw new IOException(\n          \"Failed to set storage policy since \"\n              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    INodesInPath iip;\n    fsd.writeLock();\n    try {\n      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n      iip \u003d fsd.getINodesInPath4Write(src);\n\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n    } finally {\n      fsd.writeUnlock();\n    }\n    return fsd.getAuditFileInfo(iip);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
      "extendedDetails": {}
    },
    "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "HDFS-7506. Consolidate implementation of setting inode attributes into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "15/12/14 10:40 AM",
      "commitName": "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7506. Consolidate implementation of setting inode attributes into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "15/12/14 10:40 AM",
          "commitName": "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/12/14 10:30 AM",
          "commitNameOld": "6e13fc62e1f284f22fd0089f06ce281198bc7c2a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,32 @@\n-  private void setStoragePolicyInt(String src, final String policyName)\n+  static HdfsFileStatus setStoragePolicy(\n+      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n       throws IOException {\n-    if (!isStoragePolicyEnabled) {\n-      throw new IOException(\"Failed to set storage policy since \"\n-          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n+    if (!fsd.isStoragePolicyEnabled()) {\n+      throw new IOException(\n+          \"Failed to set storage policy since \"\n+              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n     }\n-    FSPermissionChecker pc \u003d null;\n-    if (isPermissionEnabled) {\n-      pc \u003d getPermissionChecker();\n-    }\n-\n-    checkOperation(OperationCategory.WRITE);\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    waitForLoadingFSImage();\n-    HdfsFileStatus fileStat;\n-    writeLock();\n+    fsd.writeLock();\n     try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n+      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n \n-      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-\n-      if (pc !\u003d null) {\n-        dir.checkPermission(pc, iip, false, null, null, FsAction.WRITE, null, false);\n+      if (fsd.isPermissionEnabled()) {\n+        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n       }\n \n       // get the corresponding policy and make sure the policy name is valid\n-      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n+      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n-      dir.setStoragePolicy(iip, policy.getId());\n-      getEditLog().logSetStoragePolicy(src, policy.getId());\n-      fileStat \u003d getAuditFileInfo(src, false);\n+      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n+      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n     } finally {\n-      writeUnlock();\n+      fsd.writeUnlock();\n     }\n-\n-    getEditLog().logSync();\n-    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n+    return fsd.getAuditFileInfo(src, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setStoragePolicy(\n      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n      throws IOException {\n    if (!fsd.isStoragePolicyEnabled()) {\n      throw new IOException(\n          \"Failed to set storage policy since \"\n              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    fsd.writeLock();\n    try {\n      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n    } finally {\n      fsd.writeUnlock();\n    }\n    return fsd.getAuditFileInfo(src, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
            "oldMethodName": "setStoragePolicyInt",
            "newMethodName": "setStoragePolicy"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-7506. Consolidate implementation of setting inode attributes into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "15/12/14 10:40 AM",
          "commitName": "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/12/14 10:30 AM",
          "commitNameOld": "6e13fc62e1f284f22fd0089f06ce281198bc7c2a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,32 @@\n-  private void setStoragePolicyInt(String src, final String policyName)\n+  static HdfsFileStatus setStoragePolicy(\n+      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n       throws IOException {\n-    if (!isStoragePolicyEnabled) {\n-      throw new IOException(\"Failed to set storage policy since \"\n-          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n+    if (!fsd.isStoragePolicyEnabled()) {\n+      throw new IOException(\n+          \"Failed to set storage policy since \"\n+              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n     }\n-    FSPermissionChecker pc \u003d null;\n-    if (isPermissionEnabled) {\n-      pc \u003d getPermissionChecker();\n-    }\n-\n-    checkOperation(OperationCategory.WRITE);\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    waitForLoadingFSImage();\n-    HdfsFileStatus fileStat;\n-    writeLock();\n+    fsd.writeLock();\n     try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n+      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n \n-      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-\n-      if (pc !\u003d null) {\n-        dir.checkPermission(pc, iip, false, null, null, FsAction.WRITE, null, false);\n+      if (fsd.isPermissionEnabled()) {\n+        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n       }\n \n       // get the corresponding policy and make sure the policy name is valid\n-      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n+      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n-      dir.setStoragePolicy(iip, policy.getId());\n-      getEditLog().logSetStoragePolicy(src, policy.getId());\n-      fileStat \u003d getAuditFileInfo(src, false);\n+      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n+      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n     } finally {\n-      writeUnlock();\n+      fsd.writeUnlock();\n     }\n-\n-    getEditLog().logSync();\n-    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n+    return fsd.getAuditFileInfo(src, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setStoragePolicy(\n      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n      throws IOException {\n    if (!fsd.isStoragePolicyEnabled()) {\n      throw new IOException(\n          \"Failed to set storage policy since \"\n              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    fsd.writeLock();\n    try {\n      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n    } finally {\n      fsd.writeUnlock();\n    }\n    return fsd.getAuditFileInfo(src, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "HdfsFileStatus"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7506. Consolidate implementation of setting inode attributes into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "15/12/14 10:40 AM",
          "commitName": "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/12/14 10:30 AM",
          "commitNameOld": "6e13fc62e1f284f22fd0089f06ce281198bc7c2a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,32 @@\n-  private void setStoragePolicyInt(String src, final String policyName)\n+  static HdfsFileStatus setStoragePolicy(\n+      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n       throws IOException {\n-    if (!isStoragePolicyEnabled) {\n-      throw new IOException(\"Failed to set storage policy since \"\n-          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n+    if (!fsd.isStoragePolicyEnabled()) {\n+      throw new IOException(\n+          \"Failed to set storage policy since \"\n+              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n     }\n-    FSPermissionChecker pc \u003d null;\n-    if (isPermissionEnabled) {\n-      pc \u003d getPermissionChecker();\n-    }\n-\n-    checkOperation(OperationCategory.WRITE);\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    waitForLoadingFSImage();\n-    HdfsFileStatus fileStat;\n-    writeLock();\n+    fsd.writeLock();\n     try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n+      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n \n-      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-\n-      if (pc !\u003d null) {\n-        dir.checkPermission(pc, iip, false, null, null, FsAction.WRITE, null, false);\n+      if (fsd.isPermissionEnabled()) {\n+        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n       }\n \n       // get the corresponding policy and make sure the policy name is valid\n-      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n+      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n-      dir.setStoragePolicy(iip, policy.getId());\n-      getEditLog().logSetStoragePolicy(src, policy.getId());\n-      fileStat \u003d getAuditFileInfo(src, false);\n+      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n+      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n     } finally {\n-      writeUnlock();\n+      fsd.writeUnlock();\n     }\n-\n-    getEditLog().logSync();\n-    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n+    return fsd.getAuditFileInfo(src, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setStoragePolicy(\n      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n      throws IOException {\n    if (!fsd.isStoragePolicyEnabled()) {\n      throw new IOException(\n          \"Failed to set storage policy since \"\n              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    fsd.writeLock();\n    try {\n      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n    } finally {\n      fsd.writeUnlock();\n    }\n    return fsd.getAuditFileInfo(src, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7506. Consolidate implementation of setting inode attributes into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "15/12/14 10:40 AM",
          "commitName": "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/12/14 10:30 AM",
          "commitNameOld": "6e13fc62e1f284f22fd0089f06ce281198bc7c2a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,32 @@\n-  private void setStoragePolicyInt(String src, final String policyName)\n+  static HdfsFileStatus setStoragePolicy(\n+      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n       throws IOException {\n-    if (!isStoragePolicyEnabled) {\n-      throw new IOException(\"Failed to set storage policy since \"\n-          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n+    if (!fsd.isStoragePolicyEnabled()) {\n+      throw new IOException(\n+          \"Failed to set storage policy since \"\n+              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n     }\n-    FSPermissionChecker pc \u003d null;\n-    if (isPermissionEnabled) {\n-      pc \u003d getPermissionChecker();\n-    }\n-\n-    checkOperation(OperationCategory.WRITE);\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    waitForLoadingFSImage();\n-    HdfsFileStatus fileStat;\n-    writeLock();\n+    fsd.writeLock();\n     try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n+      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n \n-      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-\n-      if (pc !\u003d null) {\n-        dir.checkPermission(pc, iip, false, null, null, FsAction.WRITE, null, false);\n+      if (fsd.isPermissionEnabled()) {\n+        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n       }\n \n       // get the corresponding policy and make sure the policy name is valid\n-      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n+      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n-      dir.setStoragePolicy(iip, policy.getId());\n-      getEditLog().logSetStoragePolicy(src, policy.getId());\n-      fileStat \u003d getAuditFileInfo(src, false);\n+      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n+      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n     } finally {\n-      writeUnlock();\n+      fsd.writeUnlock();\n     }\n-\n-    getEditLog().logSync();\n-    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n+    return fsd.getAuditFileInfo(src, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setStoragePolicy(\n      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n      throws IOException {\n    if (!fsd.isStoragePolicyEnabled()) {\n      throw new IOException(\n          \"Failed to set storage policy since \"\n              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    fsd.writeLock();\n    try {\n      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n    } finally {\n      fsd.writeUnlock();\n    }\n    return fsd.getAuditFileInfo(src, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "HDFS-7506. Consolidate implementation of setting inode attributes into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "15/12/14 10:40 AM",
          "commitName": "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/12/14 10:30 AM",
          "commitNameOld": "6e13fc62e1f284f22fd0089f06ce281198bc7c2a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,32 @@\n-  private void setStoragePolicyInt(String src, final String policyName)\n+  static HdfsFileStatus setStoragePolicy(\n+      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n       throws IOException {\n-    if (!isStoragePolicyEnabled) {\n-      throw new IOException(\"Failed to set storage policy since \"\n-          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n+    if (!fsd.isStoragePolicyEnabled()) {\n+      throw new IOException(\n+          \"Failed to set storage policy since \"\n+              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n     }\n-    FSPermissionChecker pc \u003d null;\n-    if (isPermissionEnabled) {\n-      pc \u003d getPermissionChecker();\n-    }\n-\n-    checkOperation(OperationCategory.WRITE);\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    waitForLoadingFSImage();\n-    HdfsFileStatus fileStat;\n-    writeLock();\n+    fsd.writeLock();\n     try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n+      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n \n-      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-\n-      if (pc !\u003d null) {\n-        dir.checkPermission(pc, iip, false, null, null, FsAction.WRITE, null, false);\n+      if (fsd.isPermissionEnabled()) {\n+        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n       }\n \n       // get the corresponding policy and make sure the policy name is valid\n-      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n+      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n-      dir.setStoragePolicy(iip, policy.getId());\n-      getEditLog().logSetStoragePolicy(src, policy.getId());\n-      fileStat \u003d getAuditFileInfo(src, false);\n+      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n+      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n     } finally {\n-      writeUnlock();\n+      fsd.writeUnlock();\n     }\n-\n-    getEditLog().logSync();\n-    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n+    return fsd.getAuditFileInfo(src, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setStoragePolicy(\n      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n      throws IOException {\n    if (!fsd.isStoragePolicyEnabled()) {\n      throw new IOException(\n          \"Failed to set storage policy since \"\n              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    fsd.writeLock();\n    try {\n      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n    } finally {\n      fsd.writeUnlock();\n    }\n    return fsd.getAuditFileInfo(src, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
          "extendedDetails": {
            "oldValue": "setStoragePolicyInt",
            "newValue": "setStoragePolicy"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7506. Consolidate implementation of setting inode attributes into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "15/12/14 10:40 AM",
          "commitName": "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/12/14 10:30 AM",
          "commitNameOld": "6e13fc62e1f284f22fd0089f06ce281198bc7c2a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,32 @@\n-  private void setStoragePolicyInt(String src, final String policyName)\n+  static HdfsFileStatus setStoragePolicy(\n+      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n       throws IOException {\n-    if (!isStoragePolicyEnabled) {\n-      throw new IOException(\"Failed to set storage policy since \"\n-          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n+    if (!fsd.isStoragePolicyEnabled()) {\n+      throw new IOException(\n+          \"Failed to set storage policy since \"\n+              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n     }\n-    FSPermissionChecker pc \u003d null;\n-    if (isPermissionEnabled) {\n-      pc \u003d getPermissionChecker();\n-    }\n-\n-    checkOperation(OperationCategory.WRITE);\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    waitForLoadingFSImage();\n-    HdfsFileStatus fileStat;\n-    writeLock();\n+    fsd.writeLock();\n     try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n+      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n \n-      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-\n-      if (pc !\u003d null) {\n-        dir.checkPermission(pc, iip, false, null, null, FsAction.WRITE, null, false);\n+      if (fsd.isPermissionEnabled()) {\n+        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n       }\n \n       // get the corresponding policy and make sure the policy name is valid\n-      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n+      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n-      dir.setStoragePolicy(iip, policy.getId());\n-      getEditLog().logSetStoragePolicy(src, policy.getId());\n-      fileStat \u003d getAuditFileInfo(src, false);\n+      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n+      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n     } finally {\n-      writeUnlock();\n+      fsd.writeUnlock();\n     }\n-\n-    getEditLog().logSync();\n-    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n+    return fsd.getAuditFileInfo(src, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setStoragePolicy(\n      FSDirectory fsd, BlockManager bm, String src, final String policyName)\n      throws IOException {\n    if (!fsd.isStoragePolicyEnabled()) {\n      throw new IOException(\n          \"Failed to set storage policy since \"\n              + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    fsd.writeLock();\n    try {\n      src \u003d FSDirectory.resolvePath(src, pathComponents, fsd);\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d bm.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      unprotectedSetStoragePolicy(fsd, bm, iip, policy.getId());\n      fsd.getEditLog().logSetStoragePolicy(src, policy.getId());\n    } finally {\n      fsd.writeUnlock();\n    }\n    return fsd.getAuditFileInfo(src, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
          "extendedDetails": {
            "oldValue": "[src-String, policyName-String(modifiers-final)]",
            "newValue": "[fsd-FSDirectory, bm-BlockManager, src-String, policyName-String(modifiers-final)]"
          }
        }
      ]
    },
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-7474. Avoid resolving path in FSPermissionChecker. Contributed by Jing Zhao.\n",
      "commitDate": "05/12/14 2:17 PM",
      "commitName": "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-7474. Avoid resolving path in FSPermissionChecker. Contributed by Jing Zhao.\n",
          "commitDate": "05/12/14 2:17 PM",
          "commitName": "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "05/12/14 10:55 AM",
          "commitNameOld": "6a5596e3b4443462fc86f800b3c2eb839d44c3bd",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,43 @@\n   private void setStoragePolicyInt(String src, final String policyName)\n-      throws IOException, UnresolvedLinkException, AccessControlException {\n-\n+      throws IOException {\n     if (!isStoragePolicyEnabled) {\n       throw new IOException(\"Failed to set storage policy since \"\n           + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n     }\n     FSPermissionChecker pc \u003d null;\n     if (isPermissionEnabled) {\n       pc \u003d getPermissionChecker();\n     }\n \n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     waitForLoadingFSImage();\n     HdfsFileStatus fileStat;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n \n-      if (pc !\u003d null) {\n-        checkPermission(pc, src, false, null, null, FsAction.WRITE, null,\n-                        false, true);\n-      }\n-\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n+      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n+\n+      if (pc !\u003d null) {\n+        dir.checkPermission(pc, iip, false, null, null, FsAction.WRITE, null, false);\n+      }\n \n       // get the corresponding policy and make sure the policy name is valid\n       BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n-      dir.setStoragePolicy(src, policy.getId());\n+      dir.setStoragePolicy(iip, policy.getId());\n       getEditLog().logSetStoragePolicy(src, policy.getId());\n       fileStat \u003d getAuditFileInfo(src, false);\n     } finally {\n       writeUnlock();\n     }\n \n     getEditLog().logSync();\n     logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void setStoragePolicyInt(String src, final String policyName)\n      throws IOException {\n    if (!isStoragePolicyEnabled) {\n      throw new IOException(\"Failed to set storage policy since \"\n          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d null;\n    if (isPermissionEnabled) {\n      pc \u003d getPermissionChecker();\n    }\n\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    waitForLoadingFSImage();\n    HdfsFileStatus fileStat;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n\n      if (pc !\u003d null) {\n        dir.checkPermission(pc, iip, false, null, null, FsAction.WRITE, null, false);\n      }\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      dir.setStoragePolicy(iip, policy.getId());\n      getEditLog().logSetStoragePolicy(src, policy.getId());\n      fileStat \u003d getAuditFileInfo(src, false);\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[IOException, UnresolvedLinkException, AccessControlException]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7474. Avoid resolving path in FSPermissionChecker. Contributed by Jing Zhao.\n",
          "commitDate": "05/12/14 2:17 PM",
          "commitName": "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "05/12/14 10:55 AM",
          "commitNameOld": "6a5596e3b4443462fc86f800b3c2eb839d44c3bd",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,43 @@\n   private void setStoragePolicyInt(String src, final String policyName)\n-      throws IOException, UnresolvedLinkException, AccessControlException {\n-\n+      throws IOException {\n     if (!isStoragePolicyEnabled) {\n       throw new IOException(\"Failed to set storage policy since \"\n           + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n     }\n     FSPermissionChecker pc \u003d null;\n     if (isPermissionEnabled) {\n       pc \u003d getPermissionChecker();\n     }\n \n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     waitForLoadingFSImage();\n     HdfsFileStatus fileStat;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n \n-      if (pc !\u003d null) {\n-        checkPermission(pc, src, false, null, null, FsAction.WRITE, null,\n-                        false, true);\n-      }\n-\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n+      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n+\n+      if (pc !\u003d null) {\n+        dir.checkPermission(pc, iip, false, null, null, FsAction.WRITE, null, false);\n+      }\n \n       // get the corresponding policy and make sure the policy name is valid\n       BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n-      dir.setStoragePolicy(src, policy.getId());\n+      dir.setStoragePolicy(iip, policy.getId());\n       getEditLog().logSetStoragePolicy(src, policy.getId());\n       fileStat \u003d getAuditFileInfo(src, false);\n     } finally {\n       writeUnlock();\n     }\n \n     getEditLog().logSync();\n     logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void setStoragePolicyInt(String src, final String policyName)\n      throws IOException {\n    if (!isStoragePolicyEnabled) {\n      throw new IOException(\"Failed to set storage policy since \"\n          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d null;\n    if (isPermissionEnabled) {\n      pc \u003d getPermissionChecker();\n    }\n\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    waitForLoadingFSImage();\n    HdfsFileStatus fileStat;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n\n      if (pc !\u003d null) {\n        dir.checkPermission(pc, iip, false, null, null, FsAction.WRITE, null, false);\n      }\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      dir.setStoragePolicy(iip, policy.getId());\n      getEditLog().logSetStoragePolicy(src, policy.getId());\n      fileStat \u003d getAuditFileInfo(src, false);\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "a45ad330facc56f06ed42eb71304c49ef56dc549": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7171. Fix Jenkins failures in HDFS-6581 branch. (Arpit Agarwal)\n",
      "commitDate": "30/09/14 6:25 PM",
      "commitName": "a45ad330facc56f06ed42eb71304c49ef56dc549",
      "commitAuthor": "arp",
      "commitDateOld": "29/09/14 10:27 PM",
      "commitNameOld": "bb84f1fccb18c6c7373851e05d2451d55e908242",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 0.83,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,44 @@\n   private void setStoragePolicyInt(String src, final String policyName)\n       throws IOException, UnresolvedLinkException, AccessControlException {\n \n     if (!isStoragePolicyEnabled) {\n       throw new IOException(\"Failed to set storage policy since \"\n           + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n     }\n     FSPermissionChecker pc \u003d null;\n     if (isPermissionEnabled) {\n       pc \u003d getPermissionChecker();\n     }\n \n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     waitForLoadingFSImage();\n     HdfsFileStatus fileStat;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n \n       if (pc !\u003d null) {\n         checkPermission(pc, src, false, null, null, FsAction.WRITE, null,\n                         false, true);\n       }\n \n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n-      INode inode \u003d dir.getINode(src);\n \n       // get the corresponding policy and make sure the policy name is valid\n       BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n       dir.setStoragePolicy(src, policy.getId());\n       getEditLog().logSetStoragePolicy(src, policy.getId());\n       fileStat \u003d getAuditFileInfo(src, false);\n     } finally {\n       writeUnlock();\n     }\n \n     getEditLog().logSync();\n     logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void setStoragePolicyInt(String src, final String policyName)\n      throws IOException, UnresolvedLinkException, AccessControlException {\n\n    if (!isStoragePolicyEnabled) {\n      throw new IOException(\"Failed to set storage policy since \"\n          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d null;\n    if (isPermissionEnabled) {\n      pc \u003d getPermissionChecker();\n    }\n\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    waitForLoadingFSImage();\n    HdfsFileStatus fileStat;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n\n      if (pc !\u003d null) {\n        checkPermission(pc, src, false, null, null, FsAction.WRITE, null,\n                        false, true);\n      }\n\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      dir.setStoragePolicy(src, policy.getId());\n      getEditLog().logSetStoragePolicy(src, policy.getId());\n      fileStat \u003d getAuditFileInfo(src, false);\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "bb84f1fccb18c6c7373851e05d2451d55e908242": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7159. Use block storage policy to set lazy persist preference. (Arpit Agarwal)\n",
      "commitDate": "29/09/14 10:27 PM",
      "commitName": "bb84f1fccb18c6c7373851e05d2451d55e908242",
      "commitAuthor": "arp",
      "commitDateOld": "29/09/14 12:36 PM",
      "commitNameOld": "d45e7c7e856c7103752888c0395fa94985cd7670",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 0.41,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,45 @@\n   private void setStoragePolicyInt(String src, final String policyName)\n       throws IOException, UnresolvedLinkException, AccessControlException {\n \n     if (!isStoragePolicyEnabled) {\n       throw new IOException(\"Failed to set storage policy since \"\n           + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n     }\n     FSPermissionChecker pc \u003d null;\n     if (isPermissionEnabled) {\n       pc \u003d getPermissionChecker();\n     }\n \n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     waitForLoadingFSImage();\n     HdfsFileStatus fileStat;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n \n       if (pc !\u003d null) {\n         checkPermission(pc, src, false, null, null, FsAction.WRITE, null,\n                         false, true);\n       }\n \n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n+      INode inode \u003d dir.getINode(src);\n \n       // get the corresponding policy and make sure the policy name is valid\n       BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n       dir.setStoragePolicy(src, policy.getId());\n       getEditLog().logSetStoragePolicy(src, policy.getId());\n       fileStat \u003d getAuditFileInfo(src, false);\n     } finally {\n       writeUnlock();\n     }\n \n     getEditLog().logSync();\n     logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void setStoragePolicyInt(String src, final String policyName)\n      throws IOException, UnresolvedLinkException, AccessControlException {\n\n    if (!isStoragePolicyEnabled) {\n      throw new IOException(\"Failed to set storage policy since \"\n          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d null;\n    if (isPermissionEnabled) {\n      pc \u003d getPermissionChecker();\n    }\n\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    waitForLoadingFSImage();\n    HdfsFileStatus fileStat;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n\n      if (pc !\u003d null) {\n        checkPermission(pc, src, false, null, null, FsAction.WRITE, null,\n                        false, true);\n      }\n\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      INode inode \u003d dir.getINode(src);\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      dir.setStoragePolicy(src, policy.getId());\n      getEditLog().logSetStoragePolicy(src, policy.getId());\n      fileStat \u003d getAuditFileInfo(src, false);\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "b38e52b5e8f2b2ad91c2c8c4eabc232ee4753ca0": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-7093. Add config key to restrict setStoragePolicy. (Arpit Agarwal)\n",
      "commitDate": "28/09/14 7:28 PM",
      "commitName": "b38e52b5e8f2b2ad91c2c8c4eabc232ee4753ca0",
      "commitAuthor": "arp",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-7093. Add config key to restrict setStoragePolicy. (Arpit Agarwal)\n",
          "commitDate": "28/09/14 7:28 PM",
          "commitName": "b38e52b5e8f2b2ad91c2c8c4eabc232ee4753ca0",
          "commitAuthor": "arp",
          "commitDateOld": "25/09/14 6:40 PM",
          "commitNameOld": "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 3.03,
          "commitsBetweenForRepo": 21,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,44 @@\n   private void setStoragePolicyInt(String src, final String policyName)\n-      throws IOException {\n-    checkSuperuserPrivilege();\n+      throws IOException, UnresolvedLinkException, AccessControlException {\n+\n+    if (!isStoragePolicyEnabled) {\n+      throw new IOException(\"Failed to set storage policy since \"\n+          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n+    }\n+    FSPermissionChecker pc \u003d null;\n+    if (isPermissionEnabled) {\n+      pc \u003d getPermissionChecker();\n+    }\n+\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     waitForLoadingFSImage();\n     HdfsFileStatus fileStat;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n+\n+      if (pc !\u003d null) {\n+        checkPermission(pc, src, false, null, null, FsAction.WRITE, null,\n+                        false, true);\n+      }\n+\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n \n       // get the corresponding policy and make sure the policy name is valid\n       BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n       dir.setStoragePolicy(src, policy.getId());\n       getEditLog().logSetStoragePolicy(src, policy.getId());\n       fileStat \u003d getAuditFileInfo(src, false);\n     } finally {\n       writeUnlock();\n     }\n \n     getEditLog().logSync();\n     logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void setStoragePolicyInt(String src, final String policyName)\n      throws IOException, UnresolvedLinkException, AccessControlException {\n\n    if (!isStoragePolicyEnabled) {\n      throw new IOException(\"Failed to set storage policy since \"\n          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d null;\n    if (isPermissionEnabled) {\n      pc \u003d getPermissionChecker();\n    }\n\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    waitForLoadingFSImage();\n    HdfsFileStatus fileStat;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n\n      if (pc !\u003d null) {\n        checkPermission(pc, src, false, null, null, FsAction.WRITE, null,\n                        false, true);\n      }\n\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      dir.setStoragePolicy(src, policy.getId());\n      getEditLog().logSetStoragePolicy(src, policy.getId());\n      fileStat \u003d getAuditFileInfo(src, false);\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[IOException]",
            "newValue": "[IOException, UnresolvedLinkException, AccessControlException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7093. Add config key to restrict setStoragePolicy. (Arpit Agarwal)\n",
          "commitDate": "28/09/14 7:28 PM",
          "commitName": "b38e52b5e8f2b2ad91c2c8c4eabc232ee4753ca0",
          "commitAuthor": "arp",
          "commitDateOld": "25/09/14 6:40 PM",
          "commitNameOld": "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 3.03,
          "commitsBetweenForRepo": 21,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,44 @@\n   private void setStoragePolicyInt(String src, final String policyName)\n-      throws IOException {\n-    checkSuperuserPrivilege();\n+      throws IOException, UnresolvedLinkException, AccessControlException {\n+\n+    if (!isStoragePolicyEnabled) {\n+      throw new IOException(\"Failed to set storage policy since \"\n+          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n+    }\n+    FSPermissionChecker pc \u003d null;\n+    if (isPermissionEnabled) {\n+      pc \u003d getPermissionChecker();\n+    }\n+\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     waitForLoadingFSImage();\n     HdfsFileStatus fileStat;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n+\n+      if (pc !\u003d null) {\n+        checkPermission(pc, src, false, null, null, FsAction.WRITE, null,\n+                        false, true);\n+      }\n+\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n \n       // get the corresponding policy and make sure the policy name is valid\n       BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n       if (policy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\n             \"Cannot find a block policy with the name \" + policyName);\n       }\n       dir.setStoragePolicy(src, policy.getId());\n       getEditLog().logSetStoragePolicy(src, policy.getId());\n       fileStat \u003d getAuditFileInfo(src, false);\n     } finally {\n       writeUnlock();\n     }\n \n     getEditLog().logSync();\n     logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void setStoragePolicyInt(String src, final String policyName)\n      throws IOException, UnresolvedLinkException, AccessControlException {\n\n    if (!isStoragePolicyEnabled) {\n      throw new IOException(\"Failed to set storage policy since \"\n          + DFS_STORAGE_POLICY_ENABLED_KEY + \" is set to false.\");\n    }\n    FSPermissionChecker pc \u003d null;\n    if (isPermissionEnabled) {\n      pc \u003d getPermissionChecker();\n    }\n\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    waitForLoadingFSImage();\n    HdfsFileStatus fileStat;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set storage policy for \" + src);\n\n      if (pc !\u003d null) {\n        checkPermission(pc, src, false, null, null, FsAction.WRITE, null,\n                        false, true);\n      }\n\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n\n      // get the corresponding policy and make sure the policy name is valid\n      BlockStoragePolicy policy \u003d blockManager.getStoragePolicy(policyName);\n      if (policy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\n            \"Cannot find a block policy with the name \" + policyName);\n      }\n      dir.setStoragePolicy(src, policy.getId());\n      getEditLog().logSetStoragePolicy(src, policy.getId());\n      fileStat \u003d getAuditFileInfo(src, false);\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n    logAuditEvent(true, \"setStoragePolicy\", src, null, fileStat);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}