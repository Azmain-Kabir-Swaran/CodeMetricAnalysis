{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "processReport",
  "functionId": "processReport___storageInfo-DatanodeStorageInfo(modifiers-final)__report-BlockListAsLongs(modifiers-final)__context-BlockReportContext",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 2862,
  "functionEndLine": 2932,
  "numCommitsSeen": 1273,
  "timeTaken": 18837,
  "changeHistory": [
    "4ca3a6b21a3a25acf16d026c699154047b1f686b",
    "546b95f4843f3cbbbdf72d90d202cad551696082",
    "10e84c6a6e831fe2bea061fb21bd0dfe32bc9953",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "abf833a7b228fff2bca4f69cd9df99d532380038",
    "ba9371492036983a9899398907ab41fe548f29b3",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
    "390642acf35f3d599271617d30ba26c2f6406fc1",
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4"
  ],
  "changeHistoryShort": {
    "4ca3a6b21a3a25acf16d026c699154047b1f686b": "Ybodychange",
    "546b95f4843f3cbbbdf72d90d202cad551696082": "Ymodifierchange",
    "10e84c6a6e831fe2bea061fb21bd0dfe32bc9953": "Ymultichange(Yparameterchange,Ybodychange)",
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": "Ymultichange(Yparameterchange,Ybodychange)",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": "Ybodychange",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "abf833a7b228fff2bca4f69cd9df99d532380038": "Ybodychange",
    "ba9371492036983a9899398907ab41fe548f29b3": "Ybodychange",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": "Ybodychange",
    "390642acf35f3d599271617d30ba26c2f6406fc1": "Ymultichange(Yreturntypechange,Ybodychange)",
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "4ca3a6b21a3a25acf16d026c699154047b1f686b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14108. Performance improvement in BlockManager Data Structures. Contributed by Beluga Behr.\n",
      "commitDate": "28/11/18 11:25 AM",
      "commitName": "4ca3a6b21a3a25acf16d026c699154047b1f686b",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "05/11/18 9:38 PM",
      "commitNameOld": "ffc9c50e074aeca804674c6e1e6b0f1eb629e230",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 22.57,
      "commitsBetweenForRepo": 159,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,71 @@\n   Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n       final BlockListAsLongs report,\n       BlockReportContext context) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n-    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n-    Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n-    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n-    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n-    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n+    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new ArrayList\u003c\u003e();\n+    Collection\u003cBlockInfo\u003e toRemove \u003d new HashSet\u003c\u003e();\n+    Collection\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003c\u003e();\n+    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new ArrayList\u003c\u003e();\n+    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new ArrayList\u003c\u003e();\n \n     boolean sorted \u003d false;\n     String strBlockReportId \u003d \"\";\n     if (context !\u003d null) {\n       sorted \u003d context.isSorted();\n       strBlockReportId \u003d Long.toHexString(context.getReportId());\n     }\n \n     Iterable\u003cBlockReportReplica\u003e sortedReport;\n     if (!sorted) {\n       blockLog.warn(\"BLOCK* processReport 0x{}: Report from the DataNode ({}) \"\n                     + \"is unsorted. This will cause overhead on the NameNode \"\n                     + \"which needs to sort the Full BR. Please update the \"\n                     + \"DataNode to the same version of Hadoop HDFS as the \"\n                     + \"NameNode ({}).\",\n                     strBlockReportId,\n                     storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n                     VersionInfo.getVersion());\n       Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n       for (BlockReportReplica iblk : report) {\n         set.add(new BlockReportReplica(iblk));\n       }\n       sortedReport \u003d set;\n     } else {\n       sortedReport \u003d report;\n     }\n \n     reportDiffSorted(storageInfo, sortedReport,\n                      toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n \n \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (BlockInfo b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfoToAdd b : toAdd) {\n       addStoredBlock(b.stored, b.reported, storageInfo, null,\n           numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport 0x{}: logged info for {} of {} \" +\n           \"reported.\", strBlockReportId, maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report,\n      BlockReportContext context) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new ArrayList\u003c\u003e();\n    Collection\u003cBlockInfo\u003e toRemove \u003d new HashSet\u003c\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new ArrayList\u003c\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new ArrayList\u003c\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new ArrayList\u003c\u003e();\n\n    boolean sorted \u003d false;\n    String strBlockReportId \u003d \"\";\n    if (context !\u003d null) {\n      sorted \u003d context.isSorted();\n      strBlockReportId \u003d Long.toHexString(context.getReportId());\n    }\n\n    Iterable\u003cBlockReportReplica\u003e sortedReport;\n    if (!sorted) {\n      blockLog.warn(\"BLOCK* processReport 0x{}: Report from the DataNode ({}) \"\n                    + \"is unsorted. This will cause overhead on the NameNode \"\n                    + \"which needs to sort the Full BR. Please update the \"\n                    + \"DataNode to the same version of Hadoop HDFS as the \"\n                    + \"NameNode ({}).\",\n                    strBlockReportId,\n                    storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n                    VersionInfo.getVersion());\n      Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n      for (BlockReportReplica iblk : report) {\n        set.add(new BlockReportReplica(iblk));\n      }\n      sortedReport \u003d set;\n    } else {\n      sortedReport \u003d report;\n    }\n\n    reportDiffSorted(storageInfo, sortedReport,\n                     toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n\n\n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (BlockInfo b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.stored, b.reported, storageInfo, null,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport 0x{}: logged info for {} of {} \" +\n          \"reported.\", strBlockReportId, maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "546b95f4843f3cbbbdf72d90d202cad551696082": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-11673. [READ] Handle failures of Datanode with PROVIDED storage\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "546b95f4843f3cbbbdf72d90d202cad551696082",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "d65df0f27395792c6e25f5e03b6ba1765e2ba925",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,71 @@\n-  private Collection\u003cBlock\u003e processReport(\n+  Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n       final BlockListAsLongs report,\n       BlockReportContext context) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n     Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n \n     boolean sorted \u003d false;\n     String strBlockReportId \u003d \"\";\n     if (context !\u003d null) {\n       sorted \u003d context.isSorted();\n       strBlockReportId \u003d Long.toHexString(context.getReportId());\n     }\n \n     Iterable\u003cBlockReportReplica\u003e sortedReport;\n     if (!sorted) {\n       blockLog.warn(\"BLOCK* processReport 0x{}: Report from the DataNode ({}) \"\n                     + \"is unsorted. This will cause overhead on the NameNode \"\n                     + \"which needs to sort the Full BR. Please update the \"\n                     + \"DataNode to the same version of Hadoop HDFS as the \"\n                     + \"NameNode ({}).\",\n                     strBlockReportId,\n                     storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n                     VersionInfo.getVersion());\n       Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n       for (BlockReportReplica iblk : report) {\n         set.add(new BlockReportReplica(iblk));\n       }\n       sortedReport \u003d set;\n     } else {\n       sortedReport \u003d report;\n     }\n \n     reportDiffSorted(storageInfo, sortedReport,\n                      toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n \n \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (BlockInfo b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfoToAdd b : toAdd) {\n       addStoredBlock(b.stored, b.reported, storageInfo, null,\n           numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport 0x{}: logged info for {} of {} \" +\n           \"reported.\", strBlockReportId, maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report,\n      BlockReportContext context) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n\n    boolean sorted \u003d false;\n    String strBlockReportId \u003d \"\";\n    if (context !\u003d null) {\n      sorted \u003d context.isSorted();\n      strBlockReportId \u003d Long.toHexString(context.getReportId());\n    }\n\n    Iterable\u003cBlockReportReplica\u003e sortedReport;\n    if (!sorted) {\n      blockLog.warn(\"BLOCK* processReport 0x{}: Report from the DataNode ({}) \"\n                    + \"is unsorted. This will cause overhead on the NameNode \"\n                    + \"which needs to sort the Full BR. Please update the \"\n                    + \"DataNode to the same version of Hadoop HDFS as the \"\n                    + \"NameNode ({}).\",\n                    strBlockReportId,\n                    storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n                    VersionInfo.getVersion());\n      Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n      for (BlockReportReplica iblk : report) {\n        set.add(new BlockReportReplica(iblk));\n      }\n      sortedReport \u003d set;\n    } else {\n      sortedReport \u003d report;\n    }\n\n    reportDiffSorted(storageInfo, sortedReport,\n                     toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n\n\n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (BlockInfo b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.stored, b.reported, storageInfo, null,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport 0x{}: logged info for {} of {} \" +\n          \"reported.\", strBlockReportId, maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[]"
      }
    },
    "10e84c6a6e831fe2bea061fb21bd0dfe32bc9953": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10694. processReport() should print blockReportId in each log message. Contributed by Yuanbo Liu.",
      "commitDate": "10/08/16 11:07 AM",
      "commitName": "10e84c6a6e831fe2bea061fb21bd0dfe32bc9953",
      "commitAuthor": "Yuanbo Liu",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10694. processReport() should print blockReportId in each log message. Contributed by Yuanbo Liu.",
          "commitDate": "10/08/16 11:07 AM",
          "commitName": "10e84c6a6e831fe2bea061fb21bd0dfe32bc9953",
          "commitAuthor": "Yuanbo Liu",
          "commitDateOld": "09/08/16 9:56 AM",
          "commitNameOld": "b10c936020e2616609dcb3b2126e8c34328c10ca",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 1.05,
          "commitsBetweenForRepo": 12,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,62 +1,71 @@\n   private Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n-      final BlockListAsLongs report, final boolean sorted) throws IOException {\n+      final BlockListAsLongs report,\n+      BlockReportContext context) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n     Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n \n+    boolean sorted \u003d false;\n+    String strBlockReportId \u003d \"\";\n+    if (context !\u003d null) {\n+      sorted \u003d context.isSorted();\n+      strBlockReportId \u003d Long.toHexString(context.getReportId());\n+    }\n+\n     Iterable\u003cBlockReportReplica\u003e sortedReport;\n     if (!sorted) {\n-      blockLog.warn(\"BLOCK* processReport: Report from the DataNode ({}) is \"\n-                    + \"unsorted. This will cause overhead on the NameNode \"\n+      blockLog.warn(\"BLOCK* processReport 0x{}: Report from the DataNode ({}) \"\n+                    + \"is unsorted. This will cause overhead on the NameNode \"\n                     + \"which needs to sort the Full BR. Please update the \"\n                     + \"DataNode to the same version of Hadoop HDFS as the \"\n                     + \"NameNode ({}).\",\n+                    strBlockReportId,\n                     storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n                     VersionInfo.getVersion());\n       Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n       for (BlockReportReplica iblk : report) {\n         set.add(new BlockReportReplica(iblk));\n       }\n       sortedReport \u003d set;\n     } else {\n       sortedReport \u003d report;\n     }\n \n     reportDiffSorted(storageInfo, sortedReport,\n                      toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n \n \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (BlockInfo b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfoToAdd b : toAdd) {\n       addStoredBlock(b.stored, b.reported, storageInfo, null,\n           numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n-      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n-          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n+      blockLog.info(\"BLOCK* processReport 0x{}: logged info for {} of {} \" +\n+          \"reported.\", strBlockReportId, maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report,\n      BlockReportContext context) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n\n    boolean sorted \u003d false;\n    String strBlockReportId \u003d \"\";\n    if (context !\u003d null) {\n      sorted \u003d context.isSorted();\n      strBlockReportId \u003d Long.toHexString(context.getReportId());\n    }\n\n    Iterable\u003cBlockReportReplica\u003e sortedReport;\n    if (!sorted) {\n      blockLog.warn(\"BLOCK* processReport 0x{}: Report from the DataNode ({}) \"\n                    + \"is unsorted. This will cause overhead on the NameNode \"\n                    + \"which needs to sort the Full BR. Please update the \"\n                    + \"DataNode to the same version of Hadoop HDFS as the \"\n                    + \"NameNode ({}).\",\n                    strBlockReportId,\n                    storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n                    VersionInfo.getVersion());\n      Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n      for (BlockReportReplica iblk : report) {\n        set.add(new BlockReportReplica(iblk));\n      }\n      sortedReport \u003d set;\n    } else {\n      sortedReport \u003d report;\n    }\n\n    reportDiffSorted(storageInfo, sortedReport,\n                     toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n\n\n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (BlockInfo b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.stored, b.reported, storageInfo, null,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport 0x{}: logged info for {} of {} \" +\n          \"reported.\", strBlockReportId, maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[storageInfo-DatanodeStorageInfo(modifiers-final), report-BlockListAsLongs(modifiers-final), sorted-boolean(modifiers-final)]",
            "newValue": "[storageInfo-DatanodeStorageInfo(modifiers-final), report-BlockListAsLongs(modifiers-final), context-BlockReportContext]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10694. processReport() should print blockReportId in each log message. Contributed by Yuanbo Liu.",
          "commitDate": "10/08/16 11:07 AM",
          "commitName": "10e84c6a6e831fe2bea061fb21bd0dfe32bc9953",
          "commitAuthor": "Yuanbo Liu",
          "commitDateOld": "09/08/16 9:56 AM",
          "commitNameOld": "b10c936020e2616609dcb3b2126e8c34328c10ca",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 1.05,
          "commitsBetweenForRepo": 12,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,62 +1,71 @@\n   private Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n-      final BlockListAsLongs report, final boolean sorted) throws IOException {\n+      final BlockListAsLongs report,\n+      BlockReportContext context) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n     Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n \n+    boolean sorted \u003d false;\n+    String strBlockReportId \u003d \"\";\n+    if (context !\u003d null) {\n+      sorted \u003d context.isSorted();\n+      strBlockReportId \u003d Long.toHexString(context.getReportId());\n+    }\n+\n     Iterable\u003cBlockReportReplica\u003e sortedReport;\n     if (!sorted) {\n-      blockLog.warn(\"BLOCK* processReport: Report from the DataNode ({}) is \"\n-                    + \"unsorted. This will cause overhead on the NameNode \"\n+      blockLog.warn(\"BLOCK* processReport 0x{}: Report from the DataNode ({}) \"\n+                    + \"is unsorted. This will cause overhead on the NameNode \"\n                     + \"which needs to sort the Full BR. Please update the \"\n                     + \"DataNode to the same version of Hadoop HDFS as the \"\n                     + \"NameNode ({}).\",\n+                    strBlockReportId,\n                     storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n                     VersionInfo.getVersion());\n       Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n       for (BlockReportReplica iblk : report) {\n         set.add(new BlockReportReplica(iblk));\n       }\n       sortedReport \u003d set;\n     } else {\n       sortedReport \u003d report;\n     }\n \n     reportDiffSorted(storageInfo, sortedReport,\n                      toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n \n \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (BlockInfo b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfoToAdd b : toAdd) {\n       addStoredBlock(b.stored, b.reported, storageInfo, null,\n           numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n-      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n-          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n+      blockLog.info(\"BLOCK* processReport 0x{}: logged info for {} of {} \" +\n+          \"reported.\", strBlockReportId, maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report,\n      BlockReportContext context) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n\n    boolean sorted \u003d false;\n    String strBlockReportId \u003d \"\";\n    if (context !\u003d null) {\n      sorted \u003d context.isSorted();\n      strBlockReportId \u003d Long.toHexString(context.getReportId());\n    }\n\n    Iterable\u003cBlockReportReplica\u003e sortedReport;\n    if (!sorted) {\n      blockLog.warn(\"BLOCK* processReport 0x{}: Report from the DataNode ({}) \"\n                    + \"is unsorted. This will cause overhead on the NameNode \"\n                    + \"which needs to sort the Full BR. Please update the \"\n                    + \"DataNode to the same version of Hadoop HDFS as the \"\n                    + \"NameNode ({}).\",\n                    strBlockReportId,\n                    storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n                    VersionInfo.getVersion());\n      Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n      for (BlockReportReplica iblk : report) {\n        set.add(new BlockReportReplica(iblk));\n      }\n      sortedReport \u003d set;\n    } else {\n      sortedReport \u003d report;\n    }\n\n    reportDiffSorted(storageInfo, sortedReport,\n                     toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n\n\n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (BlockInfo b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.stored, b.reported, storageInfo, null,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport 0x{}: logged info for {} of {} \" +\n          \"reported.\", strBlockReportId, maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9260. Improve the performance and GC friendliness of NameNode startup and full block reports (Staffan Friberg via cmccabe)\n",
      "commitDate": "02/02/16 11:23 AM",
      "commitName": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
      "commitAuthor": "Colin Patrick Mccabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9260. Improve the performance and GC friendliness of NameNode startup and full block reports (Staffan Friberg via cmccabe)\n",
          "commitDate": "02/02/16 11:23 AM",
          "commitName": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "31/01/16 11:54 PM",
          "commitNameOld": "e418bd1fb0568ce7ae22f588fea2dd9c95567383",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 1.48,
          "commitsBetweenForRepo": 16,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,62 @@\n   private Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n-      final BlockListAsLongs report) throws IOException {\n+      final BlockListAsLongs report, final boolean sorted) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n     Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n-    reportDiff(storageInfo, report,\n-        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n-   \n+\n+    Iterable\u003cBlockReportReplica\u003e sortedReport;\n+    if (!sorted) {\n+      blockLog.warn(\"BLOCK* processReport: Report from the DataNode ({}) is \"\n+                    + \"unsorted. This will cause overhead on the NameNode \"\n+                    + \"which needs to sort the Full BR. Please update the \"\n+                    + \"DataNode to the same version of Hadoop HDFS as the \"\n+                    + \"NameNode ({}).\",\n+                    storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n+                    VersionInfo.getVersion());\n+      Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n+      for (BlockReportReplica iblk : report) {\n+        set.add(new BlockReportReplica(iblk));\n+      }\n+      sortedReport \u003d set;\n+    } else {\n+      sortedReport \u003d report;\n+    }\n+\n+    reportDiffSorted(storageInfo, sortedReport,\n+                     toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n+\n+\n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (BlockInfo b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfoToAdd b : toAdd) {\n       addStoredBlock(b.stored, b.reported, storageInfo, null,\n           numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n           \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report, final boolean sorted) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n\n    Iterable\u003cBlockReportReplica\u003e sortedReport;\n    if (!sorted) {\n      blockLog.warn(\"BLOCK* processReport: Report from the DataNode ({}) is \"\n                    + \"unsorted. This will cause overhead on the NameNode \"\n                    + \"which needs to sort the Full BR. Please update the \"\n                    + \"DataNode to the same version of Hadoop HDFS as the \"\n                    + \"NameNode ({}).\",\n                    storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n                    VersionInfo.getVersion());\n      Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n      for (BlockReportReplica iblk : report) {\n        set.add(new BlockReportReplica(iblk));\n      }\n      sortedReport \u003d set;\n    } else {\n      sortedReport \u003d report;\n    }\n\n    reportDiffSorted(storageInfo, sortedReport,\n                     toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n\n\n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (BlockInfo b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.stored, b.reported, storageInfo, null,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[storageInfo-DatanodeStorageInfo(modifiers-final), report-BlockListAsLongs(modifiers-final)]",
            "newValue": "[storageInfo-DatanodeStorageInfo(modifiers-final), report-BlockListAsLongs(modifiers-final), sorted-boolean(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9260. Improve the performance and GC friendliness of NameNode startup and full block reports (Staffan Friberg via cmccabe)\n",
          "commitDate": "02/02/16 11:23 AM",
          "commitName": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "31/01/16 11:54 PM",
          "commitNameOld": "e418bd1fb0568ce7ae22f588fea2dd9c95567383",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 1.48,
          "commitsBetweenForRepo": 16,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,62 @@\n   private Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n-      final BlockListAsLongs report) throws IOException {\n+      final BlockListAsLongs report, final boolean sorted) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n     Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n-    reportDiff(storageInfo, report,\n-        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n-   \n+\n+    Iterable\u003cBlockReportReplica\u003e sortedReport;\n+    if (!sorted) {\n+      blockLog.warn(\"BLOCK* processReport: Report from the DataNode ({}) is \"\n+                    + \"unsorted. This will cause overhead on the NameNode \"\n+                    + \"which needs to sort the Full BR. Please update the \"\n+                    + \"DataNode to the same version of Hadoop HDFS as the \"\n+                    + \"NameNode ({}).\",\n+                    storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n+                    VersionInfo.getVersion());\n+      Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n+      for (BlockReportReplica iblk : report) {\n+        set.add(new BlockReportReplica(iblk));\n+      }\n+      sortedReport \u003d set;\n+    } else {\n+      sortedReport \u003d report;\n+    }\n+\n+    reportDiffSorted(storageInfo, sortedReport,\n+                     toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n+\n+\n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (BlockInfo b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfoToAdd b : toAdd) {\n       addStoredBlock(b.stored, b.reported, storageInfo, null,\n           numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n           \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report, final boolean sorted) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n\n    Iterable\u003cBlockReportReplica\u003e sortedReport;\n    if (!sorted) {\n      blockLog.warn(\"BLOCK* processReport: Report from the DataNode ({}) is \"\n                    + \"unsorted. This will cause overhead on the NameNode \"\n                    + \"which needs to sort the Full BR. Please update the \"\n                    + \"DataNode to the same version of Hadoop HDFS as the \"\n                    + \"NameNode ({}).\",\n                    storageInfo.getDatanodeDescriptor().getDatanodeUuid(),\n                    VersionInfo.getVersion());\n      Set\u003cBlockReportReplica\u003e set \u003d new FoldedTreeSet\u003c\u003e();\n      for (BlockReportReplica iblk : report) {\n        set.add(new BlockReportReplica(iblk));\n      }\n      sortedReport \u003d set;\n    } else {\n      sortedReport \u003d report;\n    }\n\n    reportDiffSorted(storageInfo, sortedReport,\n                     toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n\n\n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (BlockInfo b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.stored, b.reported, storageInfo, null,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\"\n\nThis reverts commit de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "31/07/15 4:15 PM",
      "commitNameOld": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 5.75,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,41 @@\n   private Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n       final BlockListAsLongs report) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n-    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n-    Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n-    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n-    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n-    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n+    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n+    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n+    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n+    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n+    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     reportDiff(storageInfo, report,\n         toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n-\n+   \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n-    for (StatefulBlockInfo b : toUC) {\n+    for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n-    for (BlockInfo b : toRemove) {\n+    for (Block b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n-    for (BlockInfoToAdd b : toAdd) {\n-      addStoredBlock(b.getStored(), b.getReported(), storageInfo, null,\n-          numBlocksLogged \u003c maxNumBlocksToLog);\n+    for (BlockInfo b : toAdd) {\n+      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n           \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n   \n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (Block b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\n",
      "commitDate": "26/06/15 10:49 AM",
      "commitName": "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/06/15 2:42 PM",
      "commitNameOld": "afe9ea3c12e1f5a71922400eadb642960bc87ca1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,42 @@\n   private Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n       final BlockListAsLongs report) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n-    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n-    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n-    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n-    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n-    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n+    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n+    Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n+    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n+    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n+    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n     reportDiff(storageInfo, report,\n         toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n-   \n+\n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n-    for (StatefulBlockInfo b : toUC) { \n+    for (StatefulBlockInfo b : toUC) {\n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n-    for (Block b : toRemove) {\n+    for (BlockInfo b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n-    for (BlockInfo b : toAdd) {\n-      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n+    for (BlockInfoToAdd b : toAdd) {\n+      addStoredBlock(b.getStored(), b.getReported(), storageInfo, null,\n+          numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n           \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n\n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) {\n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (BlockInfo b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.getStored(), b.getReported(), storageInfo, null,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "19/05/15 11:05 AM",
      "commitNameOld": "8860e352c394372e4eb3ebdf82ea899567f34e4e",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 8.19,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   private Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n       final BlockListAsLongs report) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n-    Collection\u003cBlockInfoContiguous\u003e toAdd \u003d new LinkedList\u003cBlockInfoContiguous\u003e();\n+    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n     Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     reportDiff(storageInfo, report,\n         toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n    \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (Block b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n-    for (BlockInfoContiguous b : toAdd) {\n+    for (BlockInfo b : toAdd) {\n       addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n           \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n   \n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (Block b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "abf833a7b228fff2bca4f69cd9df99d532380038": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7907. Erasure Coding: track invalid, corrupt, and under-recovery striped blocks in NameNode. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:43 AM",
      "commitName": "abf833a7b228fff2bca4f69cd9df99d532380038",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:43 AM",
      "commitNameOld": "ea2e60fbcc79c65ec571224bd3f57c262a5d9114",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n   private Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n       final BlockListAsLongs report) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n     Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n-    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n-    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n-    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n-    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n+    Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n+    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n+    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n+    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n     reportDiff(storageInfo, report,\n         toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n    \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n-    for (Block b : toRemove) {\n+    for (BlockInfo b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfoToAdd b : toAdd) {\n       addStoredBlock(b.stored, b.reported, storageInfo, null,\n           numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n           \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockInfo\u003e toRemove \u003d new TreeSet\u003c\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003c\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003c\u003e();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n   \n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (BlockInfo b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.stored, b.reported, storageInfo, null,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "ba9371492036983a9899398907ab41fe548f29b3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7716. Erasure Coding: extend BlockInfo to handle EC info. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "ba9371492036983a9899398907ab41fe548f29b3",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:03 AM",
      "commitNameOld": "0c1da5a0300f015a7e39f2b40a73fb06c65a78c8",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,42 @@\n   private Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n       final BlockListAsLongs report) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n-    Collection\u003cBlockInfoContiguous\u003e toAdd \u003d new LinkedList\u003cBlockInfoContiguous\u003e();\n+    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n     Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     reportDiff(storageInfo, report,\n         toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n    \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (Block b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n-    for (BlockInfoContiguous b : toAdd) {\n-      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n+    for (BlockInfoToAdd b : toAdd) {\n+      addStoredBlock(b.stored, b.reported, storageInfo, null,\n+          numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n           \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfoToAdd\u003e toAdd \u003d new LinkedList\u003c\u003e();\n    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n   \n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (Block b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfoToAdd b : toAdd) {\n      addStoredBlock(b.stored, b.reported, storageInfo, null,\n          numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "04/02/15 11:31 AM",
      "commitNameOld": "9175105eeaecf0a1d60b57989b73ce45cee4689b",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.01,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   private Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n       final BlockListAsLongs report) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n-    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n+    Collection\u003cBlockInfoContiguous\u003e toAdd \u003d new LinkedList\u003cBlockInfoContiguous\u003e();\n     Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     reportDiff(storageInfo, report,\n         toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n    \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (Block b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n-    for (BlockInfo b : toAdd) {\n+    for (BlockInfoContiguous b : toAdd) {\n       addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n           \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfoContiguous\u003e toAdd \u003d new LinkedList\u003cBlockInfoContiguous\u003e();\n    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n   \n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (Block b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfoContiguous b : toAdd) {\n      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "3ae38ec7dfa1aaf451cf889cec6cf862379af32a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7712. Switch blockStateChangeLog to use slf4j.\n",
      "commitDate": "03/02/15 3:01 PM",
      "commitName": "3ae38ec7dfa1aaf451cf889cec6cf862379af32a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/01/15 11:33 AM",
      "commitNameOld": "951b3608a8cb1d9063b9be9c740b524c137b816f",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.14,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   private Collection\u003cBlock\u003e processReport(\n       final DatanodeStorageInfo storageInfo,\n       final BlockListAsLongs report) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n     Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n     Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     reportDiff(storageInfo, report,\n         toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n    \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (Block b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfo b : toAdd) {\n       addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n-      blockLog.info(\"BLOCK* processReport: logged info for \" + maxNumBlocksToLog\n-          + \" of \" + numBlocksLogged + \" reported.\");\n+      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n+          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n     }\n     for (Block b : toInvalidate) {\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n \n     return toInvalidate;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n   \n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (Block b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" +\n          \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "390642acf35f3d599271617d30ba26c2f6406fc1": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-7503. Namenode restart after large deletions can cause slow processReport (Arpit Agarwal)\n",
      "commitDate": "10/12/14 11:44 PM",
      "commitName": "390642acf35f3d599271617d30ba26c2f6406fc1",
      "commitAuthor": "arp",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-7503. Namenode restart after large deletions can cause slow processReport (Arpit Agarwal)\n",
          "commitDate": "10/12/14 11:44 PM",
          "commitName": "390642acf35f3d599271617d30ba26c2f6406fc1",
          "commitAuthor": "arp",
          "commitDateOld": "26/11/14 9:57 AM",
          "commitNameOld": "058af60c56207907f2bedf76df4284e86d923e0c",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 14.57,
          "commitsBetweenForRepo": 100,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,41 @@\n-  private void processReport(final DatanodeStorageInfo storageInfo,\n-                             final BlockListAsLongs report) throws IOException {\n+  private Collection\u003cBlock\u003e processReport(\n+      final DatanodeStorageInfo storageInfo,\n+      final BlockListAsLongs report) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n     Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n     Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     reportDiff(storageInfo, report,\n         toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n    \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (Block b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfo b : toAdd) {\n       addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for \" + maxNumBlocksToLog\n           + \" of \" + numBlocksLogged + \" reported.\");\n     }\n     for (Block b : toInvalidate) {\n-      blockLog.info(\"BLOCK* processReport: \"\n-          + b + \" on \" + node + \" size \" + b.getNumBytes()\n-          + \" does not belong to any file\");\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n+\n+    return toInvalidate;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n   \n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (Block b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for \" + maxNumBlocksToLog\n          + \" of \" + numBlocksLogged + \" reported.\");\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "Collection\u003cBlock\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7503. Namenode restart after large deletions can cause slow processReport (Arpit Agarwal)\n",
          "commitDate": "10/12/14 11:44 PM",
          "commitName": "390642acf35f3d599271617d30ba26c2f6406fc1",
          "commitAuthor": "arp",
          "commitDateOld": "26/11/14 9:57 AM",
          "commitNameOld": "058af60c56207907f2bedf76df4284e86d923e0c",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 14.57,
          "commitsBetweenForRepo": 100,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,41 @@\n-  private void processReport(final DatanodeStorageInfo storageInfo,\n-                             final BlockListAsLongs report) throws IOException {\n+  private Collection\u003cBlock\u003e processReport(\n+      final DatanodeStorageInfo storageInfo,\n+      final BlockListAsLongs report) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n     Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n     Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n     reportDiff(storageInfo, report,\n         toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n    \n     DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n       addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (Block b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfo b : toAdd) {\n       addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for \" + maxNumBlocksToLog\n           + \" of \" + numBlocksLogged + \" reported.\");\n     }\n     for (Block b : toInvalidate) {\n-      blockLog.info(\"BLOCK* processReport: \"\n-          + b + \" on \" + node + \" size \" + b.getNumBytes()\n-          + \" does not belong to any file\");\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n       markBlockAsCorrupt(b, storageInfo, node);\n     }\n+\n+    return toInvalidate;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Collection\u003cBlock\u003e processReport(\n      final DatanodeStorageInfo storageInfo,\n      final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n   \n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (Block b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for \" + maxNumBlocksToLog\n          + \" of \" + numBlocksLogged + \" reported.\");\n    }\n    for (Block b : toInvalidate) {\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n\n    return toInvalidate;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/14 9:58 AM",
      "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "01/08/14 9:58 AM",
          "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "31/07/14 6:05 PM",
          "commitNameOld": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.66,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,41 @@\n-  private void processReport(final DatanodeDescriptor node,\n-      final DatanodeStorage storage,\n-      final BlockListAsLongs report) throws IOException {\n+  private void processReport(final DatanodeStorageInfo storageInfo,\n+                             final BlockListAsLongs report) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n     Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n     Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n-    reportDiff(node, storage, report,\n+    reportDiff(storageInfo, report,\n         toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n-\n+   \n+    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n-      addStoredBlockUnderConstruction(b, node, storage.getStorageID());\n+      addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (Block b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfo b : toAdd) {\n-      addStoredBlock(b, node, storage.getStorageID(), null, numBlocksLogged \u003c maxNumBlocksToLog);\n+      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for \" + maxNumBlocksToLog\n           + \" of \" + numBlocksLogged + \" reported.\");\n     }\n     for (Block b : toInvalidate) {\n       blockLog.info(\"BLOCK* processReport: \"\n           + b + \" on \" + node + \" size \" + b.getNumBytes()\n           + \" does not belong to any file\");\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n-      markBlockAsCorrupt(b, node, storage.getStorageID());\n+      markBlockAsCorrupt(b, storageInfo, node);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processReport(final DatanodeStorageInfo storageInfo,\n                             final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n   \n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (Block b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for \" + maxNumBlocksToLog\n          + \" of \" + numBlocksLogged + \" reported.\");\n    }\n    for (Block b : toInvalidate) {\n      blockLog.info(\"BLOCK* processReport: \"\n          + b + \" on \" + node + \" size \" + b.getNumBytes()\n          + \" does not belong to any file\");\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[node-DatanodeDescriptor(modifiers-final), storage-DatanodeStorage(modifiers-final), report-BlockListAsLongs(modifiers-final)]",
            "newValue": "[storageInfo-DatanodeStorageInfo(modifiers-final), report-BlockListAsLongs(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "01/08/14 9:58 AM",
          "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "31/07/14 6:05 PM",
          "commitNameOld": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.66,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,41 @@\n-  private void processReport(final DatanodeDescriptor node,\n-      final DatanodeStorage storage,\n-      final BlockListAsLongs report) throws IOException {\n+  private void processReport(final DatanodeStorageInfo storageInfo,\n+                             final BlockListAsLongs report) throws IOException {\n     // Normal case:\n     // Modify the (block--\u003edatanode) map, according to the difference\n     // between the old and new block report.\n     //\n     Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n     Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n     Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n     Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n     Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n-    reportDiff(node, storage, report,\n+    reportDiff(storageInfo, report,\n         toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n-\n+   \n+    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n     // Process the blocks on each queue\n     for (StatefulBlockInfo b : toUC) { \n-      addStoredBlockUnderConstruction(b, node, storage.getStorageID());\n+      addStoredBlockUnderConstruction(b, storageInfo);\n     }\n     for (Block b : toRemove) {\n       removeStoredBlock(b, node);\n     }\n     int numBlocksLogged \u003d 0;\n     for (BlockInfo b : toAdd) {\n-      addStoredBlock(b, node, storage.getStorageID(), null, numBlocksLogged \u003c maxNumBlocksToLog);\n+      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n       numBlocksLogged++;\n     }\n     if (numBlocksLogged \u003e maxNumBlocksToLog) {\n       blockLog.info(\"BLOCK* processReport: logged info for \" + maxNumBlocksToLog\n           + \" of \" + numBlocksLogged + \" reported.\");\n     }\n     for (Block b : toInvalidate) {\n       blockLog.info(\"BLOCK* processReport: \"\n           + b + \" on \" + node + \" size \" + b.getNumBytes()\n           + \" does not belong to any file\");\n       addToInvalidates(b, node);\n     }\n     for (BlockToMarkCorrupt b : toCorrupt) {\n-      markBlockAsCorrupt(b, node, storage.getStorageID());\n+      markBlockAsCorrupt(b, storageInfo, node);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processReport(final DatanodeStorageInfo storageInfo,\n                             final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block--\u003edatanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection\u003cBlockInfo\u003e toAdd \u003d new LinkedList\u003cBlockInfo\u003e();\n    Collection\u003cBlock\u003e toRemove \u003d new TreeSet\u003cBlock\u003e();\n    Collection\u003cBlock\u003e toInvalidate \u003d new LinkedList\u003cBlock\u003e();\n    Collection\u003cBlockToMarkCorrupt\u003e toCorrupt \u003d new LinkedList\u003cBlockToMarkCorrupt\u003e();\n    Collection\u003cStatefulBlockInfo\u003e toUC \u003d new LinkedList\u003cStatefulBlockInfo\u003e();\n    reportDiff(storageInfo, report,\n        toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n   \n    DatanodeDescriptor node \u003d storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) { \n      addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (Block b : toRemove) {\n      removeStoredBlock(b, node);\n    }\n    int numBlocksLogged \u003d 0;\n    for (BlockInfo b : toAdd) {\n      addStoredBlock(b, storageInfo, null, numBlocksLogged \u003c maxNumBlocksToLog);\n      numBlocksLogged++;\n    }\n    if (numBlocksLogged \u003e maxNumBlocksToLog) {\n      blockLog.info(\"BLOCK* processReport: logged info for \" + maxNumBlocksToLog\n          + \" of \" + numBlocksLogged + \" reported.\");\n    }\n    for (Block b : toInvalidate) {\n      blockLog.info(\"BLOCK* processReport: \"\n          + b + \" on \" + node + \" size \" + b.getNumBytes()\n          + \" does not belong to any file\");\n      addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n      markBlockAsCorrupt(b, storageInfo, node);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}