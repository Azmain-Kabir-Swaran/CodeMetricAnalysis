{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FileSystemMultipartUploader.java",
  "functionName": "putPart",
  "functionId": "putPart___filePath-Path__inputStream-InputStream__partNumber-int__uploadId-UploadHandle__lengthInBytes-long",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemMultipartUploader.java",
  "functionStartLine": 72,
  "functionEndLine": 92,
  "numCommitsSeen": 6,
  "timeTaken": 1934,
  "changeHistory": [
    "c1d24f848345f6d34a2ac2d570d49e9787a0df6a",
    "2e6c1109dcdeedb59a3345047e9201271c9a0b27",
    "2ec97abb2e93c1a8127e7a146c08e26454b583fa",
    "980031bb043dd026a6bf42b0e71d304ac89294a5"
  ],
  "changeHistoryShort": {
    "c1d24f848345f6d34a2ac2d570d49e9787a0df6a": "Ybodychange",
    "2e6c1109dcdeedb59a3345047e9201271c9a0b27": "Ybodychange",
    "2ec97abb2e93c1a8127e7a146c08e26454b583fa": "Ybodychange",
    "980031bb043dd026a6bf42b0e71d304ac89294a5": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c1d24f848345f6d34a2ac2d570d49e9787a0df6a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13713. Add specification of Multipart Upload API to FS specification, with contract tests.\n\nContributed by Ewan Higgs and Steve Loughran.\n",
      "commitDate": "29/11/18 7:12 AM",
      "commitName": "c1d24f848345f6d34a2ac2d570d49e9787a0df6a",
      "commitAuthor": "Ewan Higgs",
      "commitDateOld": "02/10/18 5:05 AM",
      "commitNameOld": "6fab6886f652492573734b832ca0375459a82775",
      "commitAuthorOld": "Ewan Higgs",
      "daysBetweenCommits": 58.13,
      "commitsBetweenForRepo": 512,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,21 @@\n   public PartHandle putPart(Path filePath, InputStream inputStream,\n       int partNumber, UploadHandle uploadId, long lengthInBytes)\n       throws IOException {\n-\n+    checkPutArguments(filePath, inputStream, partNumber, uploadId,\n+        lengthInBytes);\n     byte[] uploadIdByteArray \u003d uploadId.toByteArray();\n     checkUploadId(uploadIdByteArray);\n     Path collectorPath \u003d new Path(new String(uploadIdByteArray, 0,\n         uploadIdByteArray.length, Charsets.UTF_8));\n     Path partPath \u003d\n         mergePaths(collectorPath, mergePaths(new Path(Path.SEPARATOR),\n             new Path(Integer.toString(partNumber) + \".part\")));\n     try(FSDataOutputStream fsDataOutputStream \u003d\n             fs.createFile(partPath).build()) {\n       IOUtils.copy(inputStream, fsDataOutputStream, 4096);\n     } finally {\n-      org.apache.hadoop.io.IOUtils.cleanupWithLogger(LOG, inputStream);\n+      cleanupWithLogger(LOG, inputStream);\n     }\n     return BBPartHandle.from(ByteBuffer.wrap(\n         partPath.toString().getBytes(Charsets.UTF_8)));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public PartHandle putPart(Path filePath, InputStream inputStream,\n      int partNumber, UploadHandle uploadId, long lengthInBytes)\n      throws IOException {\n    checkPutArguments(filePath, inputStream, partNumber, uploadId,\n        lengthInBytes);\n    byte[] uploadIdByteArray \u003d uploadId.toByteArray();\n    checkUploadId(uploadIdByteArray);\n    Path collectorPath \u003d new Path(new String(uploadIdByteArray, 0,\n        uploadIdByteArray.length, Charsets.UTF_8));\n    Path partPath \u003d\n        mergePaths(collectorPath, mergePaths(new Path(Path.SEPARATOR),\n            new Path(Integer.toString(partNumber) + \".part\")));\n    try(FSDataOutputStream fsDataOutputStream \u003d\n            fs.createFile(partPath).build()) {\n      IOUtils.copy(inputStream, fsDataOutputStream, 4096);\n    } finally {\n      cleanupWithLogger(LOG, inputStream);\n    }\n    return BBPartHandle.from(ByteBuffer.wrap(\n        partPath.toString().getBytes(Charsets.UTF_8)));\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemMultipartUploader.java",
      "extendedDetails": {}
    },
    "2e6c1109dcdeedb59a3345047e9201271c9a0b27": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15667. FileSystemMultipartUploader should verify that UploadHandle has non-0 length.\nContributed by Ewan Higgs\n",
      "commitDate": "30/08/18 6:33 AM",
      "commitName": "2e6c1109dcdeedb59a3345047e9201271c9a0b27",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "08/08/18 4:50 AM",
      "commitNameOld": "2ec97abb2e93c1a8127e7a146c08e26454b583fa",
      "commitAuthorOld": "Ewan Higgs",
      "daysBetweenCommits": 22.07,
      "commitsBetweenForRepo": 206,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,20 @@\n   public PartHandle putPart(Path filePath, InputStream inputStream,\n       int partNumber, UploadHandle uploadId, long lengthInBytes)\n       throws IOException {\n \n     byte[] uploadIdByteArray \u003d uploadId.toByteArray();\n+    checkUploadId(uploadIdByteArray);\n     Path collectorPath \u003d new Path(new String(uploadIdByteArray, 0,\n         uploadIdByteArray.length, Charsets.UTF_8));\n     Path partPath \u003d\n         mergePaths(collectorPath, mergePaths(new Path(Path.SEPARATOR),\n             new Path(Integer.toString(partNumber) + \".part\")));\n     try(FSDataOutputStream fsDataOutputStream \u003d\n             fs.createFile(partPath).build()) {\n       IOUtils.copy(inputStream, fsDataOutputStream, 4096);\n     } finally {\n       org.apache.hadoop.io.IOUtils.cleanupWithLogger(LOG, inputStream);\n     }\n     return BBPartHandle.from(ByteBuffer.wrap(\n         partPath.toString().getBytes(Charsets.UTF_8)));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public PartHandle putPart(Path filePath, InputStream inputStream,\n      int partNumber, UploadHandle uploadId, long lengthInBytes)\n      throws IOException {\n\n    byte[] uploadIdByteArray \u003d uploadId.toByteArray();\n    checkUploadId(uploadIdByteArray);\n    Path collectorPath \u003d new Path(new String(uploadIdByteArray, 0,\n        uploadIdByteArray.length, Charsets.UTF_8));\n    Path partPath \u003d\n        mergePaths(collectorPath, mergePaths(new Path(Path.SEPARATOR),\n            new Path(Integer.toString(partNumber) + \".part\")));\n    try(FSDataOutputStream fsDataOutputStream \u003d\n            fs.createFile(partPath).build()) {\n      IOUtils.copy(inputStream, fsDataOutputStream, 4096);\n    } finally {\n      org.apache.hadoop.io.IOUtils.cleanupWithLogger(LOG, inputStream);\n    }\n    return BBPartHandle.from(ByteBuffer.wrap(\n        partPath.toString().getBytes(Charsets.UTF_8)));\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemMultipartUploader.java",
      "extendedDetails": {}
    },
    "2ec97abb2e93c1a8127e7a146c08e26454b583fa": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15576. S3A Multipart Uploader to work with S3Guard and encryption Originally contributed by Ewan Higgs with refinements by Steve Loughran.\n",
      "commitDate": "08/08/18 4:50 AM",
      "commitName": "2ec97abb2e93c1a8127e7a146c08e26454b583fa",
      "commitAuthor": "Ewan Higgs",
      "commitDateOld": "17/06/18 11:54 AM",
      "commitNameOld": "980031bb043dd026a6bf42b0e71d304ac89294a5",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 51.71,
      "commitsBetweenForRepo": 308,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,19 @@\n   public PartHandle putPart(Path filePath, InputStream inputStream,\n       int partNumber, UploadHandle uploadId, long lengthInBytes)\n       throws IOException {\n \n     byte[] uploadIdByteArray \u003d uploadId.toByteArray();\n     Path collectorPath \u003d new Path(new String(uploadIdByteArray, 0,\n         uploadIdByteArray.length, Charsets.UTF_8));\n     Path partPath \u003d\n-        Path.mergePaths(collectorPath, Path.mergePaths(new Path(Path.SEPARATOR),\n+        mergePaths(collectorPath, mergePaths(new Path(Path.SEPARATOR),\n             new Path(Integer.toString(partNumber) + \".part\")));\n-    FSDataOutputStreamBuilder outputStream \u003d fs.createFile(partPath);\n-    FSDataOutputStream fsDataOutputStream \u003d outputStream.build();\n-    IOUtils.copy(inputStream, fsDataOutputStream, 4096);\n-    fsDataOutputStream.close();\n+    try(FSDataOutputStream fsDataOutputStream \u003d\n+            fs.createFile(partPath).build()) {\n+      IOUtils.copy(inputStream, fsDataOutputStream, 4096);\n+    } finally {\n+      org.apache.hadoop.io.IOUtils.cleanupWithLogger(LOG, inputStream);\n+    }\n     return BBPartHandle.from(ByteBuffer.wrap(\n         partPath.toString().getBytes(Charsets.UTF_8)));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public PartHandle putPart(Path filePath, InputStream inputStream,\n      int partNumber, UploadHandle uploadId, long lengthInBytes)\n      throws IOException {\n\n    byte[] uploadIdByteArray \u003d uploadId.toByteArray();\n    Path collectorPath \u003d new Path(new String(uploadIdByteArray, 0,\n        uploadIdByteArray.length, Charsets.UTF_8));\n    Path partPath \u003d\n        mergePaths(collectorPath, mergePaths(new Path(Path.SEPARATOR),\n            new Path(Integer.toString(partNumber) + \".part\")));\n    try(FSDataOutputStream fsDataOutputStream \u003d\n            fs.createFile(partPath).build()) {\n      IOUtils.copy(inputStream, fsDataOutputStream, 4096);\n    } finally {\n      org.apache.hadoop.io.IOUtils.cleanupWithLogger(LOG, inputStream);\n    }\n    return BBPartHandle.from(ByteBuffer.wrap(\n        partPath.toString().getBytes(Charsets.UTF_8)));\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemMultipartUploader.java",
      "extendedDetails": {}
    },
    "980031bb043dd026a6bf42b0e71d304ac89294a5": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-13186. Multipart Uploader API. Contributed by Ewan Higgs\n",
      "commitDate": "17/06/18 11:54 AM",
      "commitName": "980031bb043dd026a6bf42b0e71d304ac89294a5",
      "commitAuthor": "Chris Douglas",
      "diff": "@@ -0,0 +1,17 @@\n+  public PartHandle putPart(Path filePath, InputStream inputStream,\n+      int partNumber, UploadHandle uploadId, long lengthInBytes)\n+      throws IOException {\n+\n+    byte[] uploadIdByteArray \u003d uploadId.toByteArray();\n+    Path collectorPath \u003d new Path(new String(uploadIdByteArray, 0,\n+        uploadIdByteArray.length, Charsets.UTF_8));\n+    Path partPath \u003d\n+        Path.mergePaths(collectorPath, Path.mergePaths(new Path(Path.SEPARATOR),\n+            new Path(Integer.toString(partNumber) + \".part\")));\n+    FSDataOutputStreamBuilder outputStream \u003d fs.createFile(partPath);\n+    FSDataOutputStream fsDataOutputStream \u003d outputStream.build();\n+    IOUtils.copy(inputStream, fsDataOutputStream, 4096);\n+    fsDataOutputStream.close();\n+    return BBPartHandle.from(ByteBuffer.wrap(\n+        partPath.toString().getBytes(Charsets.UTF_8)));\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public PartHandle putPart(Path filePath, InputStream inputStream,\n      int partNumber, UploadHandle uploadId, long lengthInBytes)\n      throws IOException {\n\n    byte[] uploadIdByteArray \u003d uploadId.toByteArray();\n    Path collectorPath \u003d new Path(new String(uploadIdByteArray, 0,\n        uploadIdByteArray.length, Charsets.UTF_8));\n    Path partPath \u003d\n        Path.mergePaths(collectorPath, Path.mergePaths(new Path(Path.SEPARATOR),\n            new Path(Integer.toString(partNumber) + \".part\")));\n    FSDataOutputStreamBuilder outputStream \u003d fs.createFile(partPath);\n    FSDataOutputStream fsDataOutputStream \u003d outputStream.build();\n    IOUtils.copy(inputStream, fsDataOutputStream, 4096);\n    fsDataOutputStream.close();\n    return BBPartHandle.from(ByteBuffer.wrap(\n        partPath.toString().getBytes(Charsets.UTF_8)));\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemMultipartUploader.java"
    }
  }
}