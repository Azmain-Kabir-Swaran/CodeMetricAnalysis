{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "MappableBlockLoader.java",
  "functionName": "verifyChecksum",
  "functionId": "verifyChecksum___length-long__metaIn-FileInputStream__blockChannel-FileChannel__blockFileName-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlockLoader.java",
  "functionStartLine": 131,
  "functionEndLine": 178,
  "numCommitsSeen": 21,
  "timeTaken": 6304,
  "changeHistory": [
    "37900c5639f8ba8d41b9fedc3d41ee0fbda7d5db",
    "f3f51284d57ef2e0c7e968b6eea56eab578f7e93",
    "ba50a36a3ead628c3d44d384f7ed4d2b3a55dd07",
    "5c48f379ab359ea7a7c2421df998080f3792a1d9",
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
    "85c203602993a946fb5f41eadf1cf1484a0ce686",
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11"
  ],
  "changeHistoryShort": {
    "37900c5639f8ba8d41b9fedc3d41ee0fbda7d5db": "Ymultichange(Ymovefromfile,Ymodifierchange)",
    "f3f51284d57ef2e0c7e968b6eea56eab578f7e93": "Ymodifierchange",
    "ba50a36a3ead628c3d44d384f7ed4d2b3a55dd07": "Ymultichange(Ymovefromfile,Ymodifierchange,Yexceptionschange)",
    "5c48f379ab359ea7a7c2421df998080f3792a1d9": "Ybodychange",
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "85c203602993a946fb5f41eadf1cf1484a0ce686": "Ybodychange",
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11": "Yintroduced"
  },
  "changeHistoryDetails": {
    "37900c5639f8ba8d41b9fedc3d41ee0fbda7d5db": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange)",
      "commitMessage": "HDFS-14402. Use FileChannel.transferTo() method for transferring block to SCM cache. Contributed by Feilong He.\n",
      "commitDate": "26/05/19 2:00 AM",
      "commitName": "37900c5639f8ba8d41b9fedc3d41ee0fbda7d5db",
      "commitAuthor": "Rakesh Radhakrishnan",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-14402. Use FileChannel.transferTo() method for transferring block to SCM cache. Contributed by Feilong He.\n",
          "commitDate": "26/05/19 2:00 AM",
          "commitName": "37900c5639f8ba8d41b9fedc3d41ee0fbda7d5db",
          "commitAuthor": "Rakesh Radhakrishnan",
          "commitDateOld": "24/05/19 1:12 PM",
          "commitNameOld": "55e0c134f002f74cb4a0360b6682a1b6796d1598",
          "commitAuthorOld": "Erik Krogen",
          "daysBetweenCommits": 1.53,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,49 @@\n-  private void verifyChecksum(long length, FileInputStream metaIn,\n-                             FileChannel blockChannel, String blockFileName)\n+  protected void verifyChecksum(long length, FileInputStream metaIn,\n+                                FileChannel blockChannel, String blockFileName)\n       throws IOException {\n     // Verify the checksum from the block\u0027s meta file\n     // Get the DataChecksum from the meta file header\n     BlockMetadataHeader header \u003d\n         BlockMetadataHeader.readHeader(new DataInputStream(\n             new BufferedInputStream(metaIn, BlockMetadataHeader\n                 .getHeaderSize())));\n     FileChannel metaChannel \u003d null;\n     try {\n       metaChannel \u003d metaIn.getChannel();\n       if (metaChannel \u003d\u003d null) {\n         throw new IOException(\n             \"Block InputStream meta file has no FileChannel.\");\n       }\n       DataChecksum checksum \u003d header.getChecksum();\n       final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       final int checksumSize \u003d checksum.getChecksumSize();\n       final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n       ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n       ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n       // Verify the checksum\n       int bytesVerified \u003d 0;\n       while (bytesVerified \u003c length) {\n         Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n             \"Unexpected partial chunk before EOF\");\n         assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n         int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n         if (bytesRead \u003d\u003d -1) {\n           throw new IOException(\"checksum verification failed: premature EOF\");\n         }\n         blockBuf.flip();\n         // Number of read chunks, including partial chunk at end\n         int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n         checksumBuf.limit(chunks * checksumSize);\n         fillBuffer(metaChannel, checksumBuf);\n         checksumBuf.flip();\n         checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n             bytesVerified);\n         // Success\n         bytesVerified +\u003d bytesRead;\n         blockBuf.clear();\n         checksumBuf.clear();\n       }\n     } finally {\n       IOUtils.closeQuietly(metaChannel);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void verifyChecksum(long length, FileInputStream metaIn,\n                                FileChannel blockChannel, String blockFileName)\n      throws IOException {\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    FileChannel metaChannel \u003d null;\n    try {\n      metaChannel \u003d metaIn.getChannel();\n      if (metaChannel \u003d\u003d null) {\n        throw new IOException(\n            \"Block InputStream meta file has no FileChannel.\");\n      }\n      DataChecksum checksum \u003d header.getChecksum();\n      final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      final int checksumSize \u003d checksum.getChecksumSize();\n      final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n      // Verify the checksum\n      int bytesVerified \u003d 0;\n      while (bytesVerified \u003c length) {\n        Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n            \"Unexpected partial chunk before EOF\");\n        assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n        int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n        if (bytesRead \u003d\u003d -1) {\n          throw new IOException(\"checksum verification failed: premature EOF\");\n        }\n        blockBuf.flip();\n        // Number of read chunks, including partial chunk at end\n        int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n        checksumBuf.limit(chunks * checksumSize);\n        fillBuffer(metaChannel, checksumBuf);\n        checksumBuf.flip();\n        checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n            bytesVerified);\n        // Success\n        bytesVerified +\u003d bytesRead;\n        blockBuf.clear();\n        checksumBuf.clear();\n      }\n    } finally {\n      IOUtils.closeQuietly(metaChannel);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlockLoader.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MemoryMappableBlockLoader.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlockLoader.java",
            "oldMethodName": "verifyChecksum",
            "newMethodName": "verifyChecksum"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-14402. Use FileChannel.transferTo() method for transferring block to SCM cache. Contributed by Feilong He.\n",
          "commitDate": "26/05/19 2:00 AM",
          "commitName": "37900c5639f8ba8d41b9fedc3d41ee0fbda7d5db",
          "commitAuthor": "Rakesh Radhakrishnan",
          "commitDateOld": "24/05/19 1:12 PM",
          "commitNameOld": "55e0c134f002f74cb4a0360b6682a1b6796d1598",
          "commitAuthorOld": "Erik Krogen",
          "daysBetweenCommits": 1.53,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,49 @@\n-  private void verifyChecksum(long length, FileInputStream metaIn,\n-                             FileChannel blockChannel, String blockFileName)\n+  protected void verifyChecksum(long length, FileInputStream metaIn,\n+                                FileChannel blockChannel, String blockFileName)\n       throws IOException {\n     // Verify the checksum from the block\u0027s meta file\n     // Get the DataChecksum from the meta file header\n     BlockMetadataHeader header \u003d\n         BlockMetadataHeader.readHeader(new DataInputStream(\n             new BufferedInputStream(metaIn, BlockMetadataHeader\n                 .getHeaderSize())));\n     FileChannel metaChannel \u003d null;\n     try {\n       metaChannel \u003d metaIn.getChannel();\n       if (metaChannel \u003d\u003d null) {\n         throw new IOException(\n             \"Block InputStream meta file has no FileChannel.\");\n       }\n       DataChecksum checksum \u003d header.getChecksum();\n       final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       final int checksumSize \u003d checksum.getChecksumSize();\n       final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n       ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n       ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n       // Verify the checksum\n       int bytesVerified \u003d 0;\n       while (bytesVerified \u003c length) {\n         Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n             \"Unexpected partial chunk before EOF\");\n         assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n         int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n         if (bytesRead \u003d\u003d -1) {\n           throw new IOException(\"checksum verification failed: premature EOF\");\n         }\n         blockBuf.flip();\n         // Number of read chunks, including partial chunk at end\n         int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n         checksumBuf.limit(chunks * checksumSize);\n         fillBuffer(metaChannel, checksumBuf);\n         checksumBuf.flip();\n         checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n             bytesVerified);\n         // Success\n         bytesVerified +\u003d bytesRead;\n         blockBuf.clear();\n         checksumBuf.clear();\n       }\n     } finally {\n       IOUtils.closeQuietly(metaChannel);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void verifyChecksum(long length, FileInputStream metaIn,\n                                FileChannel blockChannel, String blockFileName)\n      throws IOException {\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    FileChannel metaChannel \u003d null;\n    try {\n      metaChannel \u003d metaIn.getChannel();\n      if (metaChannel \u003d\u003d null) {\n        throw new IOException(\n            \"Block InputStream meta file has no FileChannel.\");\n      }\n      DataChecksum checksum \u003d header.getChecksum();\n      final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      final int checksumSize \u003d checksum.getChecksumSize();\n      final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n      // Verify the checksum\n      int bytesVerified \u003d 0;\n      while (bytesVerified \u003c length) {\n        Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n            \"Unexpected partial chunk before EOF\");\n        assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n        int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n        if (bytesRead \u003d\u003d -1) {\n          throw new IOException(\"checksum verification failed: premature EOF\");\n        }\n        blockBuf.flip();\n        // Number of read chunks, including partial chunk at end\n        int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n        checksumBuf.limit(chunks * checksumSize);\n        fillBuffer(metaChannel, checksumBuf);\n        checksumBuf.flip();\n        checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n            bytesVerified);\n        // Success\n        bytesVerified +\u003d bytesRead;\n        blockBuf.clear();\n        checksumBuf.clear();\n      }\n    } finally {\n      IOUtils.closeQuietly(metaChannel);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlockLoader.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[protected]"
          }
        }
      ]
    },
    "f3f51284d57ef2e0c7e968b6eea56eab578f7e93": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-14393. Refactor FsDatasetCache for SCM cache implementation. Contributed by Rakesh R\n",
      "commitDate": "28/03/19 11:48 AM",
      "commitName": "f3f51284d57ef2e0c7e968b6eea56eab578f7e93",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "14/03/19 10:21 PM",
      "commitNameOld": "ba50a36a3ead628c3d44d384f7ed4d2b3a55dd07",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 13.56,
      "commitsBetweenForRepo": 92,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n-  public void verifyChecksum(long length, FileInputStream metaIn,\n+  private void verifyChecksum(long length, FileInputStream metaIn,\n                              FileChannel blockChannel, String blockFileName)\n       throws IOException {\n     // Verify the checksum from the block\u0027s meta file\n     // Get the DataChecksum from the meta file header\n     BlockMetadataHeader header \u003d\n         BlockMetadataHeader.readHeader(new DataInputStream(\n             new BufferedInputStream(metaIn, BlockMetadataHeader\n                 .getHeaderSize())));\n     FileChannel metaChannel \u003d null;\n     try {\n       metaChannel \u003d metaIn.getChannel();\n       if (metaChannel \u003d\u003d null) {\n         throw new IOException(\n             \"Block InputStream meta file has no FileChannel.\");\n       }\n       DataChecksum checksum \u003d header.getChecksum();\n       final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       final int checksumSize \u003d checksum.getChecksumSize();\n       final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n       ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n       ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n       // Verify the checksum\n       int bytesVerified \u003d 0;\n       while (bytesVerified \u003c length) {\n         Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n             \"Unexpected partial chunk before EOF\");\n         assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n         int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n         if (bytesRead \u003d\u003d -1) {\n           throw new IOException(\"checksum verification failed: premature EOF\");\n         }\n         blockBuf.flip();\n         // Number of read chunks, including partial chunk at end\n         int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n         checksumBuf.limit(chunks * checksumSize);\n         fillBuffer(metaChannel, checksumBuf);\n         checksumBuf.flip();\n         checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n             bytesVerified);\n         // Success\n         bytesVerified +\u003d bytesRead;\n         blockBuf.clear();\n         checksumBuf.clear();\n       }\n     } finally {\n       IOUtils.closeQuietly(metaChannel);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void verifyChecksum(long length, FileInputStream metaIn,\n                             FileChannel blockChannel, String blockFileName)\n      throws IOException {\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    FileChannel metaChannel \u003d null;\n    try {\n      metaChannel \u003d metaIn.getChannel();\n      if (metaChannel \u003d\u003d null) {\n        throw new IOException(\n            \"Block InputStream meta file has no FileChannel.\");\n      }\n      DataChecksum checksum \u003d header.getChecksum();\n      final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      final int checksumSize \u003d checksum.getChecksumSize();\n      final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n      // Verify the checksum\n      int bytesVerified \u003d 0;\n      while (bytesVerified \u003c length) {\n        Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n            \"Unexpected partial chunk before EOF\");\n        assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n        int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n        if (bytesRead \u003d\u003d -1) {\n          throw new IOException(\"checksum verification failed: premature EOF\");\n        }\n        blockBuf.flip();\n        // Number of read chunks, including partial chunk at end\n        int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n        checksumBuf.limit(chunks * checksumSize);\n        fillBuffer(metaChannel, checksumBuf);\n        checksumBuf.flip();\n        checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n            bytesVerified);\n        // Success\n        bytesVerified +\u003d bytesRead;\n        blockBuf.clear();\n        checksumBuf.clear();\n      }\n    } finally {\n      IOUtils.closeQuietly(metaChannel);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MemoryMappableBlockLoader.java",
      "extendedDetails": {
        "oldValue": "[public]",
        "newValue": "[private]"
      }
    },
    "ba50a36a3ead628c3d44d384f7ed4d2b3a55dd07": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Yexceptionschange)",
      "commitMessage": "HDFS-14354: Refactor MappableBlock to align with the implementation of SCM cache. Contributed by Feilong He.\n",
      "commitDate": "14/03/19 10:21 PM",
      "commitName": "ba50a36a3ead628c3d44d384f7ed4d2b3a55dd07",
      "commitAuthor": "Uma Maheswara Rao G",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-14354: Refactor MappableBlock to align with the implementation of SCM cache. Contributed by Feilong He.\n",
          "commitDate": "14/03/19 10:21 PM",
          "commitName": "ba50a36a3ead628c3d44d384f7ed4d2b3a55dd07",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "14/03/19 9:16 PM",
          "commitNameOld": "90015084850298803d8e165a0f6f9719ba724ea3",
          "commitAuthorOld": "Vivek Ratnavel Subramanian",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,49 @@\n-  private static void verifyChecksum(long length,\n-      FileInputStream metaIn, FileChannel blockChannel, String blockFileName)\n-          throws IOException, ChecksumException {\n+  public void verifyChecksum(long length, FileInputStream metaIn,\n+                             FileChannel blockChannel, String blockFileName)\n+      throws IOException {\n     // Verify the checksum from the block\u0027s meta file\n     // Get the DataChecksum from the meta file header\n     BlockMetadataHeader header \u003d\n         BlockMetadataHeader.readHeader(new DataInputStream(\n             new BufferedInputStream(metaIn, BlockMetadataHeader\n                 .getHeaderSize())));\n     FileChannel metaChannel \u003d null;\n     try {\n       metaChannel \u003d metaIn.getChannel();\n       if (metaChannel \u003d\u003d null) {\n-        throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n+        throw new IOException(\n+            \"Block InputStream meta file has no FileChannel.\");\n       }\n       DataChecksum checksum \u003d header.getChecksum();\n       final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       final int checksumSize \u003d checksum.getChecksumSize();\n-      final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n-      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n-      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n+      final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n+      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n+      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n       // Verify the checksum\n       int bytesVerified \u003d 0;\n       while (bytesVerified \u003c length) {\n         Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n             \"Unexpected partial chunk before EOF\");\n         assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n         int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n         if (bytesRead \u003d\u003d -1) {\n           throw new IOException(\"checksum verification failed: premature EOF\");\n         }\n         blockBuf.flip();\n         // Number of read chunks, including partial chunk at end\n-        int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n-        checksumBuf.limit(chunks*checksumSize);\n+        int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n+        checksumBuf.limit(chunks * checksumSize);\n         fillBuffer(metaChannel, checksumBuf);\n         checksumBuf.flip();\n         checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n             bytesVerified);\n         // Success\n         bytesVerified +\u003d bytesRead;\n         blockBuf.clear();\n         checksumBuf.clear();\n       }\n     } finally {\n       IOUtils.closeQuietly(metaChannel);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void verifyChecksum(long length, FileInputStream metaIn,\n                             FileChannel blockChannel, String blockFileName)\n      throws IOException {\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    FileChannel metaChannel \u003d null;\n    try {\n      metaChannel \u003d metaIn.getChannel();\n      if (metaChannel \u003d\u003d null) {\n        throw new IOException(\n            \"Block InputStream meta file has no FileChannel.\");\n      }\n      DataChecksum checksum \u003d header.getChecksum();\n      final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      final int checksumSize \u003d checksum.getChecksumSize();\n      final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n      // Verify the checksum\n      int bytesVerified \u003d 0;\n      while (bytesVerified \u003c length) {\n        Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n            \"Unexpected partial chunk before EOF\");\n        assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n        int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n        if (bytesRead \u003d\u003d -1) {\n          throw new IOException(\"checksum verification failed: premature EOF\");\n        }\n        blockBuf.flip();\n        // Number of read chunks, including partial chunk at end\n        int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n        checksumBuf.limit(chunks * checksumSize);\n        fillBuffer(metaChannel, checksumBuf);\n        checksumBuf.flip();\n        checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n            bytesVerified);\n        // Success\n        bytesVerified +\u003d bytesRead;\n        blockBuf.clear();\n        checksumBuf.clear();\n      }\n    } finally {\n      IOUtils.closeQuietly(metaChannel);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MemoryMappableBlockLoader.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MemoryMappableBlockLoader.java",
            "oldMethodName": "verifyChecksum",
            "newMethodName": "verifyChecksum"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-14354: Refactor MappableBlock to align with the implementation of SCM cache. Contributed by Feilong He.\n",
          "commitDate": "14/03/19 10:21 PM",
          "commitName": "ba50a36a3ead628c3d44d384f7ed4d2b3a55dd07",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "14/03/19 9:16 PM",
          "commitNameOld": "90015084850298803d8e165a0f6f9719ba724ea3",
          "commitAuthorOld": "Vivek Ratnavel Subramanian",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,49 @@\n-  private static void verifyChecksum(long length,\n-      FileInputStream metaIn, FileChannel blockChannel, String blockFileName)\n-          throws IOException, ChecksumException {\n+  public void verifyChecksum(long length, FileInputStream metaIn,\n+                             FileChannel blockChannel, String blockFileName)\n+      throws IOException {\n     // Verify the checksum from the block\u0027s meta file\n     // Get the DataChecksum from the meta file header\n     BlockMetadataHeader header \u003d\n         BlockMetadataHeader.readHeader(new DataInputStream(\n             new BufferedInputStream(metaIn, BlockMetadataHeader\n                 .getHeaderSize())));\n     FileChannel metaChannel \u003d null;\n     try {\n       metaChannel \u003d metaIn.getChannel();\n       if (metaChannel \u003d\u003d null) {\n-        throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n+        throw new IOException(\n+            \"Block InputStream meta file has no FileChannel.\");\n       }\n       DataChecksum checksum \u003d header.getChecksum();\n       final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       final int checksumSize \u003d checksum.getChecksumSize();\n-      final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n-      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n-      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n+      final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n+      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n+      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n       // Verify the checksum\n       int bytesVerified \u003d 0;\n       while (bytesVerified \u003c length) {\n         Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n             \"Unexpected partial chunk before EOF\");\n         assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n         int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n         if (bytesRead \u003d\u003d -1) {\n           throw new IOException(\"checksum verification failed: premature EOF\");\n         }\n         blockBuf.flip();\n         // Number of read chunks, including partial chunk at end\n-        int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n-        checksumBuf.limit(chunks*checksumSize);\n+        int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n+        checksumBuf.limit(chunks * checksumSize);\n         fillBuffer(metaChannel, checksumBuf);\n         checksumBuf.flip();\n         checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n             bytesVerified);\n         // Success\n         bytesVerified +\u003d bytesRead;\n         blockBuf.clear();\n         checksumBuf.clear();\n       }\n     } finally {\n       IOUtils.closeQuietly(metaChannel);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void verifyChecksum(long length, FileInputStream metaIn,\n                             FileChannel blockChannel, String blockFileName)\n      throws IOException {\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    FileChannel metaChannel \u003d null;\n    try {\n      metaChannel \u003d metaIn.getChannel();\n      if (metaChannel \u003d\u003d null) {\n        throw new IOException(\n            \"Block InputStream meta file has no FileChannel.\");\n      }\n      DataChecksum checksum \u003d header.getChecksum();\n      final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      final int checksumSize \u003d checksum.getChecksumSize();\n      final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n      // Verify the checksum\n      int bytesVerified \u003d 0;\n      while (bytesVerified \u003c length) {\n        Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n            \"Unexpected partial chunk before EOF\");\n        assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n        int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n        if (bytesRead \u003d\u003d -1) {\n          throw new IOException(\"checksum verification failed: premature EOF\");\n        }\n        blockBuf.flip();\n        // Number of read chunks, including partial chunk at end\n        int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n        checksumBuf.limit(chunks * checksumSize);\n        fillBuffer(metaChannel, checksumBuf);\n        checksumBuf.flip();\n        checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n            bytesVerified);\n        // Success\n        bytesVerified +\u003d bytesRead;\n        blockBuf.clear();\n        checksumBuf.clear();\n      }\n    } finally {\n      IOUtils.closeQuietly(metaChannel);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MemoryMappableBlockLoader.java",
          "extendedDetails": {
            "oldValue": "[private, static]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-14354: Refactor MappableBlock to align with the implementation of SCM cache. Contributed by Feilong He.\n",
          "commitDate": "14/03/19 10:21 PM",
          "commitName": "ba50a36a3ead628c3d44d384f7ed4d2b3a55dd07",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "14/03/19 9:16 PM",
          "commitNameOld": "90015084850298803d8e165a0f6f9719ba724ea3",
          "commitAuthorOld": "Vivek Ratnavel Subramanian",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,49 @@\n-  private static void verifyChecksum(long length,\n-      FileInputStream metaIn, FileChannel blockChannel, String blockFileName)\n-          throws IOException, ChecksumException {\n+  public void verifyChecksum(long length, FileInputStream metaIn,\n+                             FileChannel blockChannel, String blockFileName)\n+      throws IOException {\n     // Verify the checksum from the block\u0027s meta file\n     // Get the DataChecksum from the meta file header\n     BlockMetadataHeader header \u003d\n         BlockMetadataHeader.readHeader(new DataInputStream(\n             new BufferedInputStream(metaIn, BlockMetadataHeader\n                 .getHeaderSize())));\n     FileChannel metaChannel \u003d null;\n     try {\n       metaChannel \u003d metaIn.getChannel();\n       if (metaChannel \u003d\u003d null) {\n-        throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n+        throw new IOException(\n+            \"Block InputStream meta file has no FileChannel.\");\n       }\n       DataChecksum checksum \u003d header.getChecksum();\n       final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       final int checksumSize \u003d checksum.getChecksumSize();\n-      final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n-      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n-      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n+      final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n+      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n+      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n       // Verify the checksum\n       int bytesVerified \u003d 0;\n       while (bytesVerified \u003c length) {\n         Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n             \"Unexpected partial chunk before EOF\");\n         assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n         int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n         if (bytesRead \u003d\u003d -1) {\n           throw new IOException(\"checksum verification failed: premature EOF\");\n         }\n         blockBuf.flip();\n         // Number of read chunks, including partial chunk at end\n-        int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n-        checksumBuf.limit(chunks*checksumSize);\n+        int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n+        checksumBuf.limit(chunks * checksumSize);\n         fillBuffer(metaChannel, checksumBuf);\n         checksumBuf.flip();\n         checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n             bytesVerified);\n         // Success\n         bytesVerified +\u003d bytesRead;\n         blockBuf.clear();\n         checksumBuf.clear();\n       }\n     } finally {\n       IOUtils.closeQuietly(metaChannel);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void verifyChecksum(long length, FileInputStream metaIn,\n                             FileChannel blockChannel, String blockFileName)\n      throws IOException {\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    FileChannel metaChannel \u003d null;\n    try {\n      metaChannel \u003d metaIn.getChannel();\n      if (metaChannel \u003d\u003d null) {\n        throw new IOException(\n            \"Block InputStream meta file has no FileChannel.\");\n      }\n      DataChecksum checksum \u003d header.getChecksum();\n      final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      final int checksumSize \u003d checksum.getChecksumSize();\n      final int numChunks \u003d (8 * 1024 * 1024) / bytesPerChecksum;\n      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks * bytesPerChecksum);\n      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks * checksumSize);\n      // Verify the checksum\n      int bytesVerified \u003d 0;\n      while (bytesVerified \u003c length) {\n        Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n            \"Unexpected partial chunk before EOF\");\n        assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n        int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n        if (bytesRead \u003d\u003d -1) {\n          throw new IOException(\"checksum verification failed: premature EOF\");\n        }\n        blockBuf.flip();\n        // Number of read chunks, including partial chunk at end\n        int chunks \u003d (bytesRead + bytesPerChecksum - 1) / bytesPerChecksum;\n        checksumBuf.limit(chunks * checksumSize);\n        fillBuffer(metaChannel, checksumBuf);\n        checksumBuf.flip();\n        checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n            bytesVerified);\n        // Success\n        bytesVerified +\u003d bytesRead;\n        blockBuf.clear();\n        checksumBuf.clear();\n      }\n    } finally {\n      IOUtils.closeQuietly(metaChannel);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MemoryMappableBlockLoader.java",
          "extendedDetails": {
            "oldValue": "[IOException, ChecksumException]",
            "newValue": "[IOException]"
          }
        }
      ]
    },
    "5c48f379ab359ea7a7c2421df998080f3792a1d9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6208. DataNode caching can leak file descriptors. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1586154 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/04/14 2:45 PM",
      "commitName": "5c48f379ab359ea7a7c2421df998080f3792a1d9",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "19/11/13 4:48 PM",
      "commitNameOld": "efea68dc3538de9aafae206d64903506e41fc9e1",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 140.87,
      "commitsBetweenForRepo": 990,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,48 @@\n   private static void verifyChecksum(long length,\n       FileInputStream metaIn, FileChannel blockChannel, String blockFileName)\n           throws IOException, ChecksumException {\n     // Verify the checksum from the block\u0027s meta file\n     // Get the DataChecksum from the meta file header\n     BlockMetadataHeader header \u003d\n         BlockMetadataHeader.readHeader(new DataInputStream(\n             new BufferedInputStream(metaIn, BlockMetadataHeader\n                 .getHeaderSize())));\n-    FileChannel metaChannel \u003d metaIn.getChannel();\n-    if (metaChannel \u003d\u003d null) {\n-      throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n-    }\n-    DataChecksum checksum \u003d header.getChecksum();\n-    final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n-    final int checksumSize \u003d checksum.getChecksumSize();\n-    final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n-    ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n-    ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n-    // Verify the checksum\n-    int bytesVerified \u003d 0;\n-    while (bytesVerified \u003c length) {\n-      Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n-          \"Unexpected partial chunk before EOF\");\n-      assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n-      int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n-      if (bytesRead \u003d\u003d -1) {\n-        throw new IOException(\"checksum verification failed: premature EOF\");\n+    FileChannel metaChannel \u003d null;\n+    try {\n+      metaChannel \u003d metaIn.getChannel();\n+      if (metaChannel \u003d\u003d null) {\n+        throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n       }\n-      blockBuf.flip();\n-      // Number of read chunks, including partial chunk at end\n-      int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n-      checksumBuf.limit(chunks*checksumSize);\n-      fillBuffer(metaChannel, checksumBuf);\n-      checksumBuf.flip();\n-      checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n-          bytesVerified);\n-      // Success\n-      bytesVerified +\u003d bytesRead;\n-      blockBuf.clear();\n-      checksumBuf.clear();\n+      DataChecksum checksum \u003d header.getChecksum();\n+      final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n+      final int checksumSize \u003d checksum.getChecksumSize();\n+      final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n+      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n+      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n+      // Verify the checksum\n+      int bytesVerified \u003d 0;\n+      while (bytesVerified \u003c length) {\n+        Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n+            \"Unexpected partial chunk before EOF\");\n+        assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n+        int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n+        if (bytesRead \u003d\u003d -1) {\n+          throw new IOException(\"checksum verification failed: premature EOF\");\n+        }\n+        blockBuf.flip();\n+        // Number of read chunks, including partial chunk at end\n+        int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n+        checksumBuf.limit(chunks*checksumSize);\n+        fillBuffer(metaChannel, checksumBuf);\n+        checksumBuf.flip();\n+        checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n+            bytesVerified);\n+        // Success\n+        bytesVerified +\u003d bytesRead;\n+        blockBuf.clear();\n+        checksumBuf.clear();\n+      }\n+    } finally {\n+      IOUtils.closeQuietly(metaChannel);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void verifyChecksum(long length,\n      FileInputStream metaIn, FileChannel blockChannel, String blockFileName)\n          throws IOException, ChecksumException {\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    FileChannel metaChannel \u003d null;\n    try {\n      metaChannel \u003d metaIn.getChannel();\n      if (metaChannel \u003d\u003d null) {\n        throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n      }\n      DataChecksum checksum \u003d header.getChecksum();\n      final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      final int checksumSize \u003d checksum.getChecksumSize();\n      final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n      ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n      ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n      // Verify the checksum\n      int bytesVerified \u003d 0;\n      while (bytesVerified \u003c length) {\n        Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n            \"Unexpected partial chunk before EOF\");\n        assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n        int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n        if (bytesRead \u003d\u003d -1) {\n          throw new IOException(\"checksum verification failed: premature EOF\");\n        }\n        blockBuf.flip();\n        // Number of read chunks, including partial chunk at end\n        int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n        checksumBuf.limit(chunks*checksumSize);\n        fillBuffer(metaChannel, checksumBuf);\n        checksumBuf.flip();\n        checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n            bytesVerified);\n        // Success\n        bytesVerified +\u003d bytesRead;\n        blockBuf.clear();\n        checksumBuf.clear();\n      }\n    } finally {\n      IOUtils.closeQuietly(metaChannel);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java",
      "extendedDetails": {}
    },
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-5394: Fix race conditions in DN caching and uncaching (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/11/13 7:00 PM",
      "commitName": "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5394: Fix race conditions in DN caching and uncaching (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/11/13 7:00 PM",
          "commitName": "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/09/13 11:41 AM",
          "commitNameOld": "85c203602993a946fb5f41eadf1cf1484a0ce686",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 52.35,
          "commitsBetweenForRepo": 340,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,43 @@\n-  public void verifyChecksum() throws IOException, ChecksumException {\n-    Preconditions.checkArgument(isLocked \u0026\u0026 isMapped,\n-        \"Block must be mapped and locked before checksum verification!\");\n-    // skip if checksum has already been successfully verified\n-    if (isChecksummed) {\n-      return;\n-    }\n+  private static void verifyChecksum(long length,\n+      FileInputStream metaIn, FileChannel blockChannel, String blockFileName)\n+          throws IOException, ChecksumException {\n     // Verify the checksum from the block\u0027s meta file\n     // Get the DataChecksum from the meta file header\n-    metaChannel.position(0);\n     BlockMetadataHeader header \u003d\n         BlockMetadataHeader.readHeader(new DataInputStream(\n             new BufferedInputStream(metaIn, BlockMetadataHeader\n                 .getHeaderSize())));\n+    FileChannel metaChannel \u003d metaIn.getChannel();\n+    if (metaChannel \u003d\u003d null) {\n+      throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n+    }\n     DataChecksum checksum \u003d header.getChecksum();\n     final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n     final int checksumSize \u003d checksum.getChecksumSize();\n     final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n     ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n     ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n     // Verify the checksum\n     int bytesVerified \u003d 0;\n-    while (bytesVerified \u003c blockChannel.size()) {\n+    while (bytesVerified \u003c length) {\n       Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n           \"Unexpected partial chunk before EOF\");\n       assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n       int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n       if (bytesRead \u003d\u003d -1) {\n-        throw new IOException(\"Premature EOF\");\n+        throw new IOException(\"checksum verification failed: premature EOF\");\n       }\n       blockBuf.flip();\n       // Number of read chunks, including partial chunk at end\n       int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n       checksumBuf.limit(chunks*checksumSize);\n       fillBuffer(metaChannel, checksumBuf);\n       checksumBuf.flip();\n-      checksum.verifyChunkedSums(blockBuf, checksumBuf, block.getBlockName(),\n+      checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n           bytesVerified);\n       // Success\n       bytesVerified +\u003d bytesRead;\n       blockBuf.clear();\n       checksumBuf.clear();\n     }\n-    isChecksummed \u003d true;\n-    // Can close the backing file since everything is safely in memory\n-    blockChannel.close();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void verifyChecksum(long length,\n      FileInputStream metaIn, FileChannel blockChannel, String blockFileName)\n          throws IOException, ChecksumException {\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    FileChannel metaChannel \u003d metaIn.getChannel();\n    if (metaChannel \u003d\u003d null) {\n      throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n    }\n    DataChecksum checksum \u003d header.getChecksum();\n    final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n    final int checksumSize \u003d checksum.getChecksumSize();\n    final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n    ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n    ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n    // Verify the checksum\n    int bytesVerified \u003d 0;\n    while (bytesVerified \u003c length) {\n      Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n          \"Unexpected partial chunk before EOF\");\n      assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n      int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n      if (bytesRead \u003d\u003d -1) {\n        throw new IOException(\"checksum verification failed: premature EOF\");\n      }\n      blockBuf.flip();\n      // Number of read chunks, including partial chunk at end\n      int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n      checksumBuf.limit(chunks*checksumSize);\n      fillBuffer(metaChannel, checksumBuf);\n      checksumBuf.flip();\n      checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n          bytesVerified);\n      // Success\n      bytesVerified +\u003d bytesRead;\n      blockBuf.clear();\n      checksumBuf.clear();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[length-long, metaIn-FileInputStream, blockChannel-FileChannel, blockFileName-String]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5394: Fix race conditions in DN caching and uncaching (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/11/13 7:00 PM",
          "commitName": "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/09/13 11:41 AM",
          "commitNameOld": "85c203602993a946fb5f41eadf1cf1484a0ce686",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 52.35,
          "commitsBetweenForRepo": 340,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,43 @@\n-  public void verifyChecksum() throws IOException, ChecksumException {\n-    Preconditions.checkArgument(isLocked \u0026\u0026 isMapped,\n-        \"Block must be mapped and locked before checksum verification!\");\n-    // skip if checksum has already been successfully verified\n-    if (isChecksummed) {\n-      return;\n-    }\n+  private static void verifyChecksum(long length,\n+      FileInputStream metaIn, FileChannel blockChannel, String blockFileName)\n+          throws IOException, ChecksumException {\n     // Verify the checksum from the block\u0027s meta file\n     // Get the DataChecksum from the meta file header\n-    metaChannel.position(0);\n     BlockMetadataHeader header \u003d\n         BlockMetadataHeader.readHeader(new DataInputStream(\n             new BufferedInputStream(metaIn, BlockMetadataHeader\n                 .getHeaderSize())));\n+    FileChannel metaChannel \u003d metaIn.getChannel();\n+    if (metaChannel \u003d\u003d null) {\n+      throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n+    }\n     DataChecksum checksum \u003d header.getChecksum();\n     final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n     final int checksumSize \u003d checksum.getChecksumSize();\n     final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n     ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n     ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n     // Verify the checksum\n     int bytesVerified \u003d 0;\n-    while (bytesVerified \u003c blockChannel.size()) {\n+    while (bytesVerified \u003c length) {\n       Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n           \"Unexpected partial chunk before EOF\");\n       assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n       int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n       if (bytesRead \u003d\u003d -1) {\n-        throw new IOException(\"Premature EOF\");\n+        throw new IOException(\"checksum verification failed: premature EOF\");\n       }\n       blockBuf.flip();\n       // Number of read chunks, including partial chunk at end\n       int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n       checksumBuf.limit(chunks*checksumSize);\n       fillBuffer(metaChannel, checksumBuf);\n       checksumBuf.flip();\n-      checksum.verifyChunkedSums(blockBuf, checksumBuf, block.getBlockName(),\n+      checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n           bytesVerified);\n       // Success\n       bytesVerified +\u003d bytesRead;\n       blockBuf.clear();\n       checksumBuf.clear();\n     }\n-    isChecksummed \u003d true;\n-    // Can close the backing file since everything is safely in memory\n-    blockChannel.close();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void verifyChecksum(long length,\n      FileInputStream metaIn, FileChannel blockChannel, String blockFileName)\n          throws IOException, ChecksumException {\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    FileChannel metaChannel \u003d metaIn.getChannel();\n    if (metaChannel \u003d\u003d null) {\n      throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n    }\n    DataChecksum checksum \u003d header.getChecksum();\n    final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n    final int checksumSize \u003d checksum.getChecksumSize();\n    final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n    ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n    ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n    // Verify the checksum\n    int bytesVerified \u003d 0;\n    while (bytesVerified \u003c length) {\n      Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n          \"Unexpected partial chunk before EOF\");\n      assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n      int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n      if (bytesRead \u003d\u003d -1) {\n        throw new IOException(\"checksum verification failed: premature EOF\");\n      }\n      blockBuf.flip();\n      // Number of read chunks, including partial chunk at end\n      int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n      checksumBuf.limit(chunks*checksumSize);\n      fillBuffer(metaChannel, checksumBuf);\n      checksumBuf.flip();\n      checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n          bytesVerified);\n      // Success\n      bytesVerified +\u003d bytesRead;\n      blockBuf.clear();\n      checksumBuf.clear();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[private, static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5394: Fix race conditions in DN caching and uncaching (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/11/13 7:00 PM",
          "commitName": "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/09/13 11:41 AM",
          "commitNameOld": "85c203602993a946fb5f41eadf1cf1484a0ce686",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 52.35,
          "commitsBetweenForRepo": 340,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,43 @@\n-  public void verifyChecksum() throws IOException, ChecksumException {\n-    Preconditions.checkArgument(isLocked \u0026\u0026 isMapped,\n-        \"Block must be mapped and locked before checksum verification!\");\n-    // skip if checksum has already been successfully verified\n-    if (isChecksummed) {\n-      return;\n-    }\n+  private static void verifyChecksum(long length,\n+      FileInputStream metaIn, FileChannel blockChannel, String blockFileName)\n+          throws IOException, ChecksumException {\n     // Verify the checksum from the block\u0027s meta file\n     // Get the DataChecksum from the meta file header\n-    metaChannel.position(0);\n     BlockMetadataHeader header \u003d\n         BlockMetadataHeader.readHeader(new DataInputStream(\n             new BufferedInputStream(metaIn, BlockMetadataHeader\n                 .getHeaderSize())));\n+    FileChannel metaChannel \u003d metaIn.getChannel();\n+    if (metaChannel \u003d\u003d null) {\n+      throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n+    }\n     DataChecksum checksum \u003d header.getChecksum();\n     final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n     final int checksumSize \u003d checksum.getChecksumSize();\n     final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n     ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n     ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n     // Verify the checksum\n     int bytesVerified \u003d 0;\n-    while (bytesVerified \u003c blockChannel.size()) {\n+    while (bytesVerified \u003c length) {\n       Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n           \"Unexpected partial chunk before EOF\");\n       assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n       int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n       if (bytesRead \u003d\u003d -1) {\n-        throw new IOException(\"Premature EOF\");\n+        throw new IOException(\"checksum verification failed: premature EOF\");\n       }\n       blockBuf.flip();\n       // Number of read chunks, including partial chunk at end\n       int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n       checksumBuf.limit(chunks*checksumSize);\n       fillBuffer(metaChannel, checksumBuf);\n       checksumBuf.flip();\n-      checksum.verifyChunkedSums(blockBuf, checksumBuf, block.getBlockName(),\n+      checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n           bytesVerified);\n       // Success\n       bytesVerified +\u003d bytesRead;\n       blockBuf.clear();\n       checksumBuf.clear();\n     }\n-    isChecksummed \u003d true;\n-    // Can close the backing file since everything is safely in memory\n-    blockChannel.close();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void verifyChecksum(long length,\n      FileInputStream metaIn, FileChannel blockChannel, String blockFileName)\n          throws IOException, ChecksumException {\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    FileChannel metaChannel \u003d metaIn.getChannel();\n    if (metaChannel \u003d\u003d null) {\n      throw new IOException(\"Block InputStream meta file has no FileChannel.\");\n    }\n    DataChecksum checksum \u003d header.getChecksum();\n    final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n    final int checksumSize \u003d checksum.getChecksumSize();\n    final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n    ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n    ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n    // Verify the checksum\n    int bytesVerified \u003d 0;\n    while (bytesVerified \u003c length) {\n      Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n          \"Unexpected partial chunk before EOF\");\n      assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n      int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n      if (bytesRead \u003d\u003d -1) {\n        throw new IOException(\"checksum verification failed: premature EOF\");\n      }\n      blockBuf.flip();\n      // Number of read chunks, including partial chunk at end\n      int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n      checksumBuf.limit(chunks*checksumSize);\n      fillBuffer(metaChannel, checksumBuf);\n      checksumBuf.flip();\n      checksum.verifyChunkedSums(blockBuf, checksumBuf, blockFileName,\n          bytesVerified);\n      // Success\n      bytesVerified +\u003d bytesRead;\n      blockBuf.clear();\n      checksumBuf.clear();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java",
          "extendedDetails": {}
        }
      ]
    },
    "85c203602993a946fb5f41eadf1cf1484a0ce686": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5210. Fix some failing unit tests on HDFS-4949 branch. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1523754 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/09/13 11:41 AM",
      "commitName": "85c203602993a946fb5f41eadf1cf1484a0ce686",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "23/08/13 8:41 PM",
      "commitNameOld": "b992219fa13ccee2b417d91222fd0c3e8c3ffe11",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 23.63,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,47 @@\n   public void verifyChecksum() throws IOException, ChecksumException {\n     Preconditions.checkArgument(isLocked \u0026\u0026 isMapped,\n         \"Block must be mapped and locked before checksum verification!\");\n     // skip if checksum has already been successfully verified\n     if (isChecksummed) {\n       return;\n     }\n     // Verify the checksum from the block\u0027s meta file\n     // Get the DataChecksum from the meta file header\n     metaChannel.position(0);\n     BlockMetadataHeader header \u003d\n         BlockMetadataHeader.readHeader(new DataInputStream(\n             new BufferedInputStream(metaIn, BlockMetadataHeader\n                 .getHeaderSize())));\n     DataChecksum checksum \u003d header.getChecksum();\n     final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n     final int checksumSize \u003d checksum.getChecksumSize();\n     final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n     ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n     ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n     // Verify the checksum\n     int bytesVerified \u003d 0;\n     while (bytesVerified \u003c blockChannel.size()) {\n       Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n           \"Unexpected partial chunk before EOF\");\n       assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n       int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n       if (bytesRead \u003d\u003d -1) {\n         throw new IOException(\"Premature EOF\");\n       }\n       blockBuf.flip();\n       // Number of read chunks, including partial chunk at end\n       int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n-      checksumBuf.limit(chunks*bytesPerChecksum);\n+      checksumBuf.limit(chunks*checksumSize);\n       fillBuffer(metaChannel, checksumBuf);\n       checksumBuf.flip();\n       checksum.verifyChunkedSums(blockBuf, checksumBuf, block.getBlockName(),\n           bytesVerified);\n       // Success\n       bytesVerified +\u003d bytesRead;\n       blockBuf.clear();\n       checksumBuf.clear();\n     }\n     isChecksummed \u003d true;\n     // Can close the backing file since everything is safely in memory\n     blockChannel.close();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void verifyChecksum() throws IOException, ChecksumException {\n    Preconditions.checkArgument(isLocked \u0026\u0026 isMapped,\n        \"Block must be mapped and locked before checksum verification!\");\n    // skip if checksum has already been successfully verified\n    if (isChecksummed) {\n      return;\n    }\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    metaChannel.position(0);\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    DataChecksum checksum \u003d header.getChecksum();\n    final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n    final int checksumSize \u003d checksum.getChecksumSize();\n    final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n    ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n    ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n    // Verify the checksum\n    int bytesVerified \u003d 0;\n    while (bytesVerified \u003c blockChannel.size()) {\n      Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n          \"Unexpected partial chunk before EOF\");\n      assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n      int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n      if (bytesRead \u003d\u003d -1) {\n        throw new IOException(\"Premature EOF\");\n      }\n      blockBuf.flip();\n      // Number of read chunks, including partial chunk at end\n      int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n      checksumBuf.limit(chunks*checksumSize);\n      fillBuffer(metaChannel, checksumBuf);\n      checksumBuf.flip();\n      checksum.verifyChunkedSums(blockBuf, checksumBuf, block.getBlockName(),\n          bytesVerified);\n      // Success\n      bytesVerified +\u003d bytesRead;\n      blockBuf.clear();\n      checksumBuf.clear();\n    }\n    isChecksummed \u003d true;\n    // Can close the backing file since everything is safely in memory\n    blockChannel.close();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java",
      "extendedDetails": {}
    },
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5050.  Add DataNode support for mlock and munlock  (contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1517106 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/08/13 8:41 PM",
      "commitName": "b992219fa13ccee2b417d91222fd0c3e8c3ffe11",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,47 @@\n+  public void verifyChecksum() throws IOException, ChecksumException {\n+    Preconditions.checkArgument(isLocked \u0026\u0026 isMapped,\n+        \"Block must be mapped and locked before checksum verification!\");\n+    // skip if checksum has already been successfully verified\n+    if (isChecksummed) {\n+      return;\n+    }\n+    // Verify the checksum from the block\u0027s meta file\n+    // Get the DataChecksum from the meta file header\n+    metaChannel.position(0);\n+    BlockMetadataHeader header \u003d\n+        BlockMetadataHeader.readHeader(new DataInputStream(\n+            new BufferedInputStream(metaIn, BlockMetadataHeader\n+                .getHeaderSize())));\n+    DataChecksum checksum \u003d header.getChecksum();\n+    final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n+    final int checksumSize \u003d checksum.getChecksumSize();\n+    final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n+    ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n+    ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n+    // Verify the checksum\n+    int bytesVerified \u003d 0;\n+    while (bytesVerified \u003c blockChannel.size()) {\n+      Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n+          \"Unexpected partial chunk before EOF\");\n+      assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n+      int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n+      if (bytesRead \u003d\u003d -1) {\n+        throw new IOException(\"Premature EOF\");\n+      }\n+      blockBuf.flip();\n+      // Number of read chunks, including partial chunk at end\n+      int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n+      checksumBuf.limit(chunks*bytesPerChecksum);\n+      fillBuffer(metaChannel, checksumBuf);\n+      checksumBuf.flip();\n+      checksum.verifyChunkedSums(blockBuf, checksumBuf, block.getBlockName(),\n+          bytesVerified);\n+      // Success\n+      bytesVerified +\u003d bytesRead;\n+      blockBuf.clear();\n+      checksumBuf.clear();\n+    }\n+    isChecksummed \u003d true;\n+    // Can close the backing file since everything is safely in memory\n+    blockChannel.close();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void verifyChecksum() throws IOException, ChecksumException {\n    Preconditions.checkArgument(isLocked \u0026\u0026 isMapped,\n        \"Block must be mapped and locked before checksum verification!\");\n    // skip if checksum has already been successfully verified\n    if (isChecksummed) {\n      return;\n    }\n    // Verify the checksum from the block\u0027s meta file\n    // Get the DataChecksum from the meta file header\n    metaChannel.position(0);\n    BlockMetadataHeader header \u003d\n        BlockMetadataHeader.readHeader(new DataInputStream(\n            new BufferedInputStream(metaIn, BlockMetadataHeader\n                .getHeaderSize())));\n    DataChecksum checksum \u003d header.getChecksum();\n    final int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n    final int checksumSize \u003d checksum.getChecksumSize();\n    final int numChunks \u003d (8*1024*1024) / bytesPerChecksum;\n    ByteBuffer blockBuf \u003d ByteBuffer.allocate(numChunks*bytesPerChecksum);\n    ByteBuffer checksumBuf \u003d ByteBuffer.allocate(numChunks*checksumSize);\n    // Verify the checksum\n    int bytesVerified \u003d 0;\n    while (bytesVerified \u003c blockChannel.size()) {\n      Preconditions.checkState(bytesVerified % bytesPerChecksum \u003d\u003d 0,\n          \"Unexpected partial chunk before EOF\");\n      assert bytesVerified % bytesPerChecksum \u003d\u003d 0;\n      int bytesRead \u003d fillBuffer(blockChannel, blockBuf);\n      if (bytesRead \u003d\u003d -1) {\n        throw new IOException(\"Premature EOF\");\n      }\n      blockBuf.flip();\n      // Number of read chunks, including partial chunk at end\n      int chunks \u003d (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n      checksumBuf.limit(chunks*bytesPerChecksum);\n      fillBuffer(metaChannel, checksumBuf);\n      checksumBuf.flip();\n      checksum.verifyChunkedSums(blockBuf, checksumBuf, block.getBlockName(),\n          bytesVerified);\n      // Success\n      bytesVerified +\u003d bytesRead;\n      blockBuf.clear();\n      checksumBuf.clear();\n    }\n    isChecksummed \u003d true;\n    // Can close the backing file since everything is safely in memory\n    blockChannel.close();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java"
    }
  }
}