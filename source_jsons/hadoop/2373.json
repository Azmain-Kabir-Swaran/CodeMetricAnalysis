{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PBHelperClient.java",
  "functionName": "convert",
  "functionId": "convert___di-DatanodeInfoProto",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
  "functionStartLine": 757,
  "functionEndLine": 787,
  "numCommitsSeen": 234,
  "timeTaken": 9042,
  "changeHistory": [
    "9714fc1dd48edb1c40d96d69ae82ed3b0fab7748",
    "b5adc5c3011f111f86d232cb33ec522547f68a95",
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
    "5f23abfa30ea29a5474513c463b4d462c0e824ee",
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
    "b5229fd19bfecc2e5249db652ad34ec08152334b",
    "3001a172c8868763f8e59e866e36f7f50dee62cc",
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
    "48da033901d3471ef176a94104158546152353e9",
    "7a59150bff64fc81f838de586eacd6d062172605",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9"
  ],
  "changeHistoryShort": {
    "9714fc1dd48edb1c40d96d69ae82ed3b0fab7748": "Ybodychange",
    "b5adc5c3011f111f86d232cb33ec522547f68a95": "Ybodychange",
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d": "Ybodychange",
    "5f23abfa30ea29a5474513c463b4d462c0e824ee": "Ybodychange",
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ymultichange(Ymovefromfile,Ybodychange)",
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501": "Ybodychange",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0": "Ybodychange",
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201": "Ybodychange",
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c": "Ybodychange",
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256": "Ybodychange",
    "b5229fd19bfecc2e5249db652ad34ec08152334b": "Ybodychange",
    "3001a172c8868763f8e59e866e36f7f50dee62cc": "Ybodychange",
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e": "Ybodychange",
    "48da033901d3471ef176a94104158546152353e9": "Ymultichange(Yparameterchange,Ybodychange)",
    "7a59150bff64fc81f838de586eacd6d062172605": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "9714fc1dd48edb1c40d96d69ae82ed3b0fab7748": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-336. dfsadmin -report should report number of blocks from datanode. Contributed by Bharat Viswanadham.\n",
      "commitDate": "13/03/18 4:39 PM",
      "commitName": "9714fc1dd48edb1c40d96d69ae82ed3b0fab7748",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "02/01/18 2:59 PM",
      "commitNameOld": "42a1c98597e6dba2e371510a6b2b6b1fb94e4090",
      "commitAuthorOld": "Manoj Govindassamy",
      "daysBetweenCommits": 70.03,
      "commitsBetweenForRepo": 452,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,31 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) {\n       return null;\n     }\n     DatanodeInfoBuilder dinfo \u003d\n         new DatanodeInfoBuilder().setNodeID(convert(di.getId()))\n             .setNetworkLocation(di.hasLocation() ? di.getLocation() : null)\n             .setCapacity(di.getCapacity()).setDfsUsed(di.getDfsUsed())\n             .setRemaining(di.getRemaining())\n             .setBlockPoolUsed(di.getBlockPoolUsed())\n             .setCacheCapacity(di.getCacheCapacity())\n             .setCacheUsed(di.getCacheUsed()).setLastUpdate(di.getLastUpdate())\n             .setLastUpdateMonotonic(di.getLastUpdateMonotonic())\n             .setXceiverCount(di.getXceiverCount())\n             .setAdminState(convert(di.getAdminState())).setUpgradeDomain(\n             di.hasUpgradeDomain() ? di.getUpgradeDomain() : null)\n             .setLastBlockReportTime(di.hasLastBlockReportTime() ?\n                 di.getLastBlockReportTime() : 0)\n             .setLastBlockReportMonotonic(di.hasLastBlockReportMonotonic() ?\n-                di.getLastBlockReportMonotonic() : 0);\n+                di.getLastBlockReportMonotonic() : 0)\n+            .setNumBlocks(di.getNumBlocks());\n+\n     if (di.hasNonDfsUsed()) {\n       dinfo.setNonDfsUsed(di.getNonDfsUsed());\n     } else {\n       // use the legacy way for older datanodes\n       long nonDFSUsed \u003d di.getCapacity() - di.getDfsUsed() - di.getRemaining();\n       dinfo.setNonDfsUsed(nonDFSUsed \u003c 0 ? 0 : nonDFSUsed);\n     }\n     return dinfo.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) {\n      return null;\n    }\n    DatanodeInfoBuilder dinfo \u003d\n        new DatanodeInfoBuilder().setNodeID(convert(di.getId()))\n            .setNetworkLocation(di.hasLocation() ? di.getLocation() : null)\n            .setCapacity(di.getCapacity()).setDfsUsed(di.getDfsUsed())\n            .setRemaining(di.getRemaining())\n            .setBlockPoolUsed(di.getBlockPoolUsed())\n            .setCacheCapacity(di.getCacheCapacity())\n            .setCacheUsed(di.getCacheUsed()).setLastUpdate(di.getLastUpdate())\n            .setLastUpdateMonotonic(di.getLastUpdateMonotonic())\n            .setXceiverCount(di.getXceiverCount())\n            .setAdminState(convert(di.getAdminState())).setUpgradeDomain(\n            di.hasUpgradeDomain() ? di.getUpgradeDomain() : null)\n            .setLastBlockReportTime(di.hasLastBlockReportTime() ?\n                di.getLastBlockReportTime() : 0)\n            .setLastBlockReportMonotonic(di.hasLastBlockReportMonotonic() ?\n                di.getLastBlockReportMonotonic() : 0)\n            .setNumBlocks(di.getNumBlocks());\n\n    if (di.hasNonDfsUsed()) {\n      dinfo.setNonDfsUsed(di.getNonDfsUsed());\n    } else {\n      // use the legacy way for older datanodes\n      long nonDFSUsed \u003d di.getCapacity() - di.getDfsUsed() - di.getRemaining();\n      dinfo.setNonDfsUsed(nonDFSUsed \u003c 0 ? 0 : nonDFSUsed);\n    }\n    return dinfo.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "b5adc5c3011f111f86d232cb33ec522547f68a95": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10838. Last full block report received time for each DN should be easily discoverable. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "06/03/17 4:39 PM",
      "commitName": "b5adc5c3011f111f86d232cb33ec522547f68a95",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "13/02/17 11:29 AM",
      "commitNameOld": "4ed33e9ca3d85568e3904753a3ef61a85f801838",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 21.22,
      "commitsBetweenForRepo": 133,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,29 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) {\n       return null;\n     }\n     DatanodeInfoBuilder dinfo \u003d\n         new DatanodeInfoBuilder().setNodeID(convert(di.getId()))\n             .setNetworkLocation(di.hasLocation() ? di.getLocation() : null)\n             .setCapacity(di.getCapacity()).setDfsUsed(di.getDfsUsed())\n             .setRemaining(di.getRemaining())\n             .setBlockPoolUsed(di.getBlockPoolUsed())\n             .setCacheCapacity(di.getCacheCapacity())\n             .setCacheUsed(di.getCacheUsed()).setLastUpdate(di.getLastUpdate())\n             .setLastUpdateMonotonic(di.getLastUpdateMonotonic())\n             .setXceiverCount(di.getXceiverCount())\n             .setAdminState(convert(di.getAdminState())).setUpgradeDomain(\n-            di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n+            di.hasUpgradeDomain() ? di.getUpgradeDomain() : null)\n+            .setLastBlockReportTime(di.hasLastBlockReportTime() ?\n+                di.getLastBlockReportTime() : 0)\n+            .setLastBlockReportMonotonic(di.hasLastBlockReportMonotonic() ?\n+                di.getLastBlockReportMonotonic() : 0);\n     if (di.hasNonDfsUsed()) {\n       dinfo.setNonDfsUsed(di.getNonDfsUsed());\n     } else {\n       // use the legacy way for older datanodes\n       long nonDFSUsed \u003d di.getCapacity() - di.getDfsUsed() - di.getRemaining();\n       dinfo.setNonDfsUsed(nonDFSUsed \u003c 0 ? 0 : nonDFSUsed);\n     }\n     return dinfo.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) {\n      return null;\n    }\n    DatanodeInfoBuilder dinfo \u003d\n        new DatanodeInfoBuilder().setNodeID(convert(di.getId()))\n            .setNetworkLocation(di.hasLocation() ? di.getLocation() : null)\n            .setCapacity(di.getCapacity()).setDfsUsed(di.getDfsUsed())\n            .setRemaining(di.getRemaining())\n            .setBlockPoolUsed(di.getBlockPoolUsed())\n            .setCacheCapacity(di.getCacheCapacity())\n            .setCacheUsed(di.getCacheUsed()).setLastUpdate(di.getLastUpdate())\n            .setLastUpdateMonotonic(di.getLastUpdateMonotonic())\n            .setXceiverCount(di.getXceiverCount())\n            .setAdminState(convert(di.getAdminState())).setUpgradeDomain(\n            di.hasUpgradeDomain() ? di.getUpgradeDomain() : null)\n            .setLastBlockReportTime(di.hasLastBlockReportTime() ?\n                di.getLastBlockReportTime() : 0)\n            .setLastBlockReportMonotonic(di.hasLastBlockReportMonotonic() ?\n                di.getLastBlockReportMonotonic() : 0);\n    if (di.hasNonDfsUsed()) {\n      dinfo.setNonDfsUsed(di.getNonDfsUsed());\n    } else {\n      // use the legacy way for older datanodes\n      long nonDFSUsed \u003d di.getCapacity() - di.getDfsUsed() - di.getRemaining();\n      dinfo.setNonDfsUsed(nonDFSUsed \u003c 0 ? 0 : nonDFSUsed);\n    }\n    return dinfo.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9482. Replace DatanodeInfo constructors with a builder pattern. Contributed by Brahma Reddy Battula.\n",
      "commitDate": "08/11/16 6:17 PM",
      "commitName": "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
      "commitAuthor": "Brahma Reddy Battula",
      "commitDateOld": "06/09/16 1:37 PM",
      "commitNameOld": "5f23abfa30ea29a5474513c463b4d462c0e824ee",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 63.24,
      "commitsBetweenForRepo": 511,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,25 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) {\n       return null;\n     }\n-    DatanodeInfo dinfo \u003d new DatanodeInfo(convert(di.getId()),\n-        di.hasLocation() ? di.getLocation() : null, di.getCapacity(),\n-        di.getDfsUsed(), di.getRemaining(), di.getBlockPoolUsed(),\n-        di.getCacheCapacity(), di.getCacheUsed(), di.getLastUpdate(),\n-        di.getLastUpdateMonotonic(), di.getXceiverCount(),\n-        convert(di.getAdminState()),\n-        di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n+    DatanodeInfoBuilder dinfo \u003d\n+        new DatanodeInfoBuilder().setNodeID(convert(di.getId()))\n+            .setNetworkLocation(di.hasLocation() ? di.getLocation() : null)\n+            .setCapacity(di.getCapacity()).setDfsUsed(di.getDfsUsed())\n+            .setRemaining(di.getRemaining())\n+            .setBlockPoolUsed(di.getBlockPoolUsed())\n+            .setCacheCapacity(di.getCacheCapacity())\n+            .setCacheUsed(di.getCacheUsed()).setLastUpdate(di.getLastUpdate())\n+            .setLastUpdateMonotonic(di.getLastUpdateMonotonic())\n+            .setXceiverCount(di.getXceiverCount())\n+            .setAdminState(convert(di.getAdminState())).setUpgradeDomain(\n+            di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n     if (di.hasNonDfsUsed()) {\n       dinfo.setNonDfsUsed(di.getNonDfsUsed());\n     } else {\n       // use the legacy way for older datanodes\n       long nonDFSUsed \u003d di.getCapacity() - di.getDfsUsed() - di.getRemaining();\n       dinfo.setNonDfsUsed(nonDFSUsed \u003c 0 ? 0 : nonDFSUsed);\n     }\n-    return dinfo;\n+    return dinfo.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) {\n      return null;\n    }\n    DatanodeInfoBuilder dinfo \u003d\n        new DatanodeInfoBuilder().setNodeID(convert(di.getId()))\n            .setNetworkLocation(di.hasLocation() ? di.getLocation() : null)\n            .setCapacity(di.getCapacity()).setDfsUsed(di.getDfsUsed())\n            .setRemaining(di.getRemaining())\n            .setBlockPoolUsed(di.getBlockPoolUsed())\n            .setCacheCapacity(di.getCacheCapacity())\n            .setCacheUsed(di.getCacheUsed()).setLastUpdate(di.getLastUpdate())\n            .setLastUpdateMonotonic(di.getLastUpdateMonotonic())\n            .setXceiverCount(di.getXceiverCount())\n            .setAdminState(convert(di.getAdminState())).setUpgradeDomain(\n            di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n    if (di.hasNonDfsUsed()) {\n      dinfo.setNonDfsUsed(di.getNonDfsUsed());\n    } else {\n      // use the legacy way for older datanodes\n      long nonDFSUsed \u003d di.getCapacity() - di.getDfsUsed() - di.getRemaining();\n      dinfo.setNonDfsUsed(nonDFSUsed \u003c 0 ? 0 : nonDFSUsed);\n    }\n    return dinfo.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "5f23abfa30ea29a5474513c463b4d462c0e824ee": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9038. DFS reserved space is erroneously counted towards non-DFS used. (Brahma Reddy Battula)\n",
      "commitDate": "06/09/16 1:37 PM",
      "commitName": "5f23abfa30ea29a5474513c463b4d462c0e824ee",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "23/08/16 4:14 AM",
      "commitNameOld": "f0efea490e5aa9dd629d2199aae9c5b1290a17ee",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 14.39,
      "commitsBetweenForRepo": 82,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,20 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n-    if (di \u003d\u003d null) return null;\n-    return new DatanodeInfo(\n-        convert(di.getId()),\n-        di.hasLocation() ? di.getLocation() : null,\n-        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n-        di.getBlockPoolUsed(), di.getCacheCapacity(), di.getCacheUsed(),\n-        di.getLastUpdate(), di.getLastUpdateMonotonic(),\n-        di.getXceiverCount(), convert(di.getAdminState()),\n+    if (di \u003d\u003d null) {\n+      return null;\n+    }\n+    DatanodeInfo dinfo \u003d new DatanodeInfo(convert(di.getId()),\n+        di.hasLocation() ? di.getLocation() : null, di.getCapacity(),\n+        di.getDfsUsed(), di.getRemaining(), di.getBlockPoolUsed(),\n+        di.getCacheCapacity(), di.getCacheUsed(), di.getLastUpdate(),\n+        di.getLastUpdateMonotonic(), di.getXceiverCount(),\n+        convert(di.getAdminState()),\n         di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n+    if (di.hasNonDfsUsed()) {\n+      dinfo.setNonDfsUsed(di.getNonDfsUsed());\n+    } else {\n+      // use the legacy way for older datanodes\n+      long nonDFSUsed \u003d di.getCapacity() - di.getDfsUsed() - di.getRemaining();\n+      dinfo.setNonDfsUsed(nonDFSUsed \u003c 0 ? 0 : nonDFSUsed);\n+    }\n+    return dinfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) {\n      return null;\n    }\n    DatanodeInfo dinfo \u003d new DatanodeInfo(convert(di.getId()),\n        di.hasLocation() ? di.getLocation() : null, di.getCapacity(),\n        di.getDfsUsed(), di.getRemaining(), di.getBlockPoolUsed(),\n        di.getCacheCapacity(), di.getCacheUsed(), di.getLastUpdate(),\n        di.getLastUpdateMonotonic(), di.getXceiverCount(),\n        convert(di.getAdminState()),\n        di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n    if (di.hasNonDfsUsed()) {\n      dinfo.setNonDfsUsed(di.getNonDfsUsed());\n    } else {\n      // use the legacy way for older datanodes\n      long nonDFSUsed \u003d di.getCapacity() - di.getDfsUsed() - di.getRemaining();\n      dinfo.setNonDfsUsed(nonDFSUsed \u003c 0 ? 0 : nonDFSUsed);\n    }\n    return dinfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
          "commitDate": "21/09/15 6:53 PM",
          "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/09/15 5:51 PM",
          "commitNameOld": "8e01b0d97ac3d74b049a801dfa1cc6e77d8f680a",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,11 +1,11 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) return null;\n     return new DatanodeInfo(\n-        PBHelper.convert(di.getId()),\n-        di.hasLocation() ? di.getLocation() : null , \n+        convert(di.getId()),\n+        di.hasLocation() ? di.getLocation() : null,\n         di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n         di.getBlockPoolUsed(), di.getCacheCapacity(), di.getCacheUsed(),\n         di.getLastUpdate(), di.getLastUpdateMonotonic(),\n-        di.getXceiverCount(), PBHelper.convert(di.getAdminState()),\n+        di.getXceiverCount(), convert(di.getAdminState()),\n         di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        convert(di.getId()),\n        di.hasLocation() ? di.getLocation() : null,\n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed(), di.getCacheCapacity(), di.getCacheUsed(),\n        di.getLastUpdate(), di.getLastUpdateMonotonic(),\n        di.getXceiverCount(), convert(di.getAdminState()),\n        di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
            "oldMethodName": "convert",
            "newMethodName": "convert"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
          "commitDate": "21/09/15 6:53 PM",
          "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/09/15 5:51 PM",
          "commitNameOld": "8e01b0d97ac3d74b049a801dfa1cc6e77d8f680a",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,11 +1,11 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) return null;\n     return new DatanodeInfo(\n-        PBHelper.convert(di.getId()),\n-        di.hasLocation() ? di.getLocation() : null , \n+        convert(di.getId()),\n+        di.hasLocation() ? di.getLocation() : null,\n         di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n         di.getBlockPoolUsed(), di.getCacheCapacity(), di.getCacheUsed(),\n         di.getLastUpdate(), di.getLastUpdateMonotonic(),\n-        di.getXceiverCount(), PBHelper.convert(di.getAdminState()),\n+        di.getXceiverCount(), convert(di.getAdminState()),\n         di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        convert(di.getId()),\n        di.hasLocation() ? di.getLocation() : null,\n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed(), di.getCacheCapacity(), di.getCacheUsed(),\n        di.getLastUpdate(), di.getLastUpdateMonotonic(),\n        di.getXceiverCount(), convert(di.getAdminState()),\n        di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9004. Add upgrade domain to DatanodeInfo. Contributed by Ming Ma (via Lei (Eddy) Xu).\n\nChange-Id: I887c66578eebd61acc34b94f18da6e6851c609f4\n",
      "commitDate": "19/09/15 6:08 PM",
      "commitName": "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "03/09/15 3:32 PM",
      "commitNameOld": "ed78b14ebc9a21bb57ccd088e8b49bfa457a396f",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 16.11,
      "commitsBetweenForRepo": 102,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,11 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) return null;\n     return new DatanodeInfo(\n         PBHelper.convert(di.getId()),\n         di.hasLocation() ? di.getLocation() : null , \n         di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n         di.getBlockPoolUsed(), di.getCacheCapacity(), di.getCacheUsed(),\n         di.getLastUpdate(), di.getLastUpdateMonotonic(),\n-        di.getXceiverCount(), PBHelper.convert(di.getAdminState()));\n+        di.getXceiverCount(), PBHelper.convert(di.getAdminState()),\n+        di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        PBHelper.convert(di.getId()),\n        di.hasLocation() ? di.getLocation() : null , \n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed(), di.getCacheCapacity(), di.getCacheUsed(),\n        di.getLastUpdate(), di.getLastUpdateMonotonic(),\n        di.getXceiverCount(), PBHelper.convert(di.getAdminState()),\n        di.hasUpgradeDomain() ? di.getUpgradeDomain() : null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "13/03/15 12:23 PM",
      "commitNameOld": "d324164a51a43d72c02567248bd9f0f12b244a40",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 6.99,
      "commitsBetweenForRepo": 79,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,10 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) return null;\n     return new DatanodeInfo(\n         PBHelper.convert(di.getId()),\n         di.hasLocation() ? di.getLocation() : null , \n         di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n         di.getBlockPoolUsed(), di.getCacheCapacity(), di.getCacheUsed(),\n-        di.getLastUpdate(), di.getXceiverCount(),\n-        PBHelper.convert(di.getAdminState())); \n+        di.getLastUpdate(), di.getLastUpdateMonotonic(),\n+        di.getXceiverCount(), PBHelper.convert(di.getAdminState()));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        PBHelper.convert(di.getId()),\n        di.hasLocation() ? di.getLocation() : null , \n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed(), di.getCacheCapacity(), di.getCacheUsed(),\n        di.getLastUpdate(), di.getLastUpdateMonotonic(),\n        di.getXceiverCount(), PBHelper.convert(di.getAdminState()));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5141. Add cache status information to datanode heartbeat. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1519101 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/08/13 3:15 PM",
      "commitName": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/07/13 5:49 PM",
      "commitNameOld": "4f68aa060090319b8de5c30f41d193632503ed17",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 30.89,
      "commitsBetweenForRepo": 90,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,10 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) return null;\n     return new DatanodeInfo(\n         PBHelper.convert(di.getId()),\n         di.hasLocation() ? di.getLocation() : null , \n         di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n-        di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n+        di.getBlockPoolUsed(), di.getCacheCapacity(), di.getCacheUsed(),\n+        di.getLastUpdate(), di.getXceiverCount(),\n         PBHelper.convert(di.getAdminState())); \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        PBHelper.convert(di.getId()),\n        di.hasLocation() ? di.getLocation() : null , \n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed(), di.getCacheCapacity(), di.getCacheUsed(),\n        di.getLastUpdate(), di.getXceiverCount(),\n        PBHelper.convert(di.getAdminState())); \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3164. Move DatanodeInfo#hostName to DatanodeID. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1307890 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/03/12 12:58 PM",
      "commitName": "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "17/02/12 5:27 PM",
      "commitNameOld": "ef5d7156dba5fb5f03b3d46e9ecfec5273eca66f",
      "commitAuthorOld": "",
      "daysBetweenCommits": 42.77,
      "commitsBetweenForRepo": 278,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,9 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) return null;\n     return new DatanodeInfo(\n         PBHelper.convert(di.getId()),\n         di.hasLocation() ? di.getLocation() : null , \n-        di.hasHostName() ? di.getHostName() : null,\n         di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n         di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n         PBHelper.convert(di.getAdminState())); \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        PBHelper.convert(di.getId()),\n        di.hasLocation() ? di.getLocation() : null , \n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n        PBHelper.convert(di.getAdminState())); \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Optional protobuf parameters are not handled correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213985 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:31 PM",
      "commitName": "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:27 PM",
      "commitNameOld": "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,10 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) return null;\n     return new DatanodeInfo(\n         PBHelper.convert(di.getId()),\n-        di.getLocation(), di.getHostName(),\n+        di.hasLocation() ? di.getLocation() : null , \n+        di.hasHostName() ? di.getHostName() : null,\n         di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n         di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n         PBHelper.convert(di.getAdminState())); \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        PBHelper.convert(di.getId()),\n        di.hasLocation() ? di.getLocation() : null , \n        di.hasHostName() ? di.getHostName() : null,\n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n        PBHelper.convert(di.getAdminState())); \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256": {
      "type": "Ybodychange",
      "commitMessage": "Reverting the patch r1213981\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213984 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:27 PM",
      "commitName": "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:22 PM",
      "commitNameOld": "b5229fd19bfecc2e5249db652ad34ec08152334b",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,9 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) return null;\n     return new DatanodeInfo(\n         PBHelper.convert(di.getId()),\n-        di.hasLocation() ? di.getLocation() : null , \n-        di.hasHostName() ? di.getHostName() : null,\n+        di.getLocation(), di.getHostName(),\n         di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n         di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n         PBHelper.convert(di.getAdminState())); \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        PBHelper.convert(di.getId()),\n        di.getLocation(), di.getHostName(),\n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n        PBHelper.convert(di.getAdminState())); \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "b5229fd19bfecc2e5249db652ad34ec08152334b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Optional protobuf parameters are not handled correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213981 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:22 PM",
      "commitName": "b5229fd19bfecc2e5249db652ad34ec08152334b",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:17 PM",
      "commitNameOld": "3001a172c8868763f8e59e866e36f7f50dee62cc",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,10 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) return null;\n     return new DatanodeInfo(\n         PBHelper.convert(di.getId()),\n-        di.getLocation(), di.getHostName(),\n+        di.hasLocation() ? di.getLocation() : null , \n+        di.hasHostName() ? di.getHostName() : null,\n         di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n         di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n         PBHelper.convert(di.getAdminState())); \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        PBHelper.convert(di.getId()),\n        di.hasLocation() ? di.getLocation() : null , \n        di.hasHostName() ? di.getHostName() : null,\n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n        PBHelper.convert(di.getAdminState())); \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "3001a172c8868763f8e59e866e36f7f50dee62cc": {
      "type": "Ybodychange",
      "commitMessage": "Reverting r1213512 because it committed changes that were not part of the patch.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213980 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:17 PM",
      "commitName": "3001a172c8868763f8e59e866e36f7f50dee62cc",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 9:59 AM",
      "commitNameOld": "4ec8424e5d8c3f4d802aaacb05cd39d9633eddf8",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.22,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,9 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) return null;\n     return new DatanodeInfo(\n         PBHelper.convert(di.getId()),\n-        di.hasLocation() ? di.getLocation() : null , \n-        di.hasHostName() ? di.getHostName() : null,\n+        di.getLocation(), di.getHostName(),\n         di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n         di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n         PBHelper.convert(di.getAdminState())); \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        PBHelper.convert(di.getId()),\n        di.getLocation(), di.getHostName(),\n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n        PBHelper.convert(di.getAdminState())); \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Handle protobuf optional parameters correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213512 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/12/11 4:21 PM",
      "commitName": "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "11/12/11 9:36 PM",
      "commitNameOld": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthorOld": "Sanjay Radia",
      "daysBetweenCommits": 0.78,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,10 @@\n   static public DatanodeInfo convert(DatanodeInfoProto di) {\n     if (di \u003d\u003d null) return null;\n     return new DatanodeInfo(\n         PBHelper.convert(di.getId()),\n-        di.getLocation(), di.getHostName(),\n+        di.hasLocation() ? di.getLocation() : null , \n+        di.hasHostName() ? di.getHostName() : null,\n         di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n         di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n         PBHelper.convert(di.getAdminState())); \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        PBHelper.convert(di.getId()),\n        di.hasLocation() ? di.getLocation() : null , \n        di.hasHostName() ? di.getHostName() : null,\n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n        PBHelper.convert(di.getAdminState())); \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "48da033901d3471ef176a94104158546152353e9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/11 9:36 PM",
      "commitName": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthor": "Sanjay Radia",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,9 @@\n-  public static DatanodeInfo convert(DatanodeInfoProto info) {\n-    DatanodeIDProto dnId \u003d info.getId();\n-    return new DatanodeInfo(dnId.getName(), dnId.getStorageID(),\n-        dnId.getInfoPort(), dnId.getIpcPort(), info.getCapacity(),\n-        info.getDfsUsed(), info.getRemaining(), info.getBlockPoolUsed(),\n-        info.getLastUpdate(), info.getXceiverCount(), info.getLocation(),\n-        info.getHostName(), convert(info.getAdminState()));\n+  static public DatanodeInfo convert(DatanodeInfoProto di) {\n+    if (di \u003d\u003d null) return null;\n+    return new DatanodeInfo(\n+        PBHelper.convert(di.getId()),\n+        di.getLocation(), di.getHostName(),\n+        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n+        di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n+        PBHelper.convert(di.getAdminState())); \n   }\n\\ No newline at end of file\n",
          "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        PBHelper.convert(di.getId()),\n        di.getLocation(), di.getHostName(),\n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n        PBHelper.convert(di.getAdminState())); \n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[info-DatanodeInfoProto]",
            "newValue": "[di-DatanodeInfoProto]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,9 @@\n-  public static DatanodeInfo convert(DatanodeInfoProto info) {\n-    DatanodeIDProto dnId \u003d info.getId();\n-    return new DatanodeInfo(dnId.getName(), dnId.getStorageID(),\n-        dnId.getInfoPort(), dnId.getIpcPort(), info.getCapacity(),\n-        info.getDfsUsed(), info.getRemaining(), info.getBlockPoolUsed(),\n-        info.getLastUpdate(), info.getXceiverCount(), info.getLocation(),\n-        info.getHostName(), convert(info.getAdminState()));\n+  static public DatanodeInfo convert(DatanodeInfoProto di) {\n+    if (di \u003d\u003d null) return null;\n+    return new DatanodeInfo(\n+        PBHelper.convert(di.getId()),\n+        di.getLocation(), di.getHostName(),\n+        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n+        di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n+        PBHelper.convert(di.getAdminState())); \n   }\n\\ No newline at end of file\n",
          "actualSource": "  static public DatanodeInfo convert(DatanodeInfoProto di) {\n    if (di \u003d\u003d null) return null;\n    return new DatanodeInfo(\n        PBHelper.convert(di.getId()),\n        di.getLocation(), di.getHostName(),\n        di.getCapacity(),  di.getDfsUsed(),  di.getRemaining(),\n        di.getBlockPoolUsed()  ,  di.getLastUpdate() , di.getXceiverCount() ,\n        PBHelper.convert(di.getAdminState())); \n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "7a59150bff64fc81f838de586eacd6d062172605": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/12/11 2:19 PM",
      "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/11 2:19 PM",
          "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "05/12/11 4:25 PM",
          "commitNameOld": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,8 @@\n-  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n-    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n-    int i \u003d 0;\n-    for (BlockWithLocationsProto entry : b) {\n-      ret[i++] \u003d convert(entry);\n-    }\n-    return ret;\n+  public static DatanodeInfo convert(DatanodeInfoProto info) {\n+    DatanodeIDProto dnId \u003d info.getId();\n+    return new DatanodeInfo(dnId.getName(), dnId.getStorageID(),\n+        dnId.getInfoPort(), dnId.getIpcPort(), info.getCapacity(),\n+        info.getDfsUsed(), info.getRemaining(), info.getBlockPoolUsed(),\n+        info.getLastUpdate(), info.getXceiverCount(), info.getLocation(),\n+        info.getHostName(), convert(info.getAdminState()));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfo convert(DatanodeInfoProto info) {\n    DatanodeIDProto dnId \u003d info.getId();\n    return new DatanodeInfo(dnId.getName(), dnId.getStorageID(),\n        dnId.getInfoPort(), dnId.getIpcPort(), info.getCapacity(),\n        info.getDfsUsed(), info.getRemaining(), info.getBlockPoolUsed(),\n        info.getLastUpdate(), info.getXceiverCount(), info.getLocation(),\n        info.getHostName(), convert(info.getAdminState()));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[b-List\u003cBlockWithLocationsProto\u003e]",
            "newValue": "[info-DatanodeInfoProto]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/11 2:19 PM",
          "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "05/12/11 4:25 PM",
          "commitNameOld": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,8 @@\n-  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n-    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n-    int i \u003d 0;\n-    for (BlockWithLocationsProto entry : b) {\n-      ret[i++] \u003d convert(entry);\n-    }\n-    return ret;\n+  public static DatanodeInfo convert(DatanodeInfoProto info) {\n+    DatanodeIDProto dnId \u003d info.getId();\n+    return new DatanodeInfo(dnId.getName(), dnId.getStorageID(),\n+        dnId.getInfoPort(), dnId.getIpcPort(), info.getCapacity(),\n+        info.getDfsUsed(), info.getRemaining(), info.getBlockPoolUsed(),\n+        info.getLastUpdate(), info.getXceiverCount(), info.getLocation(),\n+        info.getHostName(), convert(info.getAdminState()));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfo convert(DatanodeInfoProto info) {\n    DatanodeIDProto dnId \u003d info.getId();\n    return new DatanodeInfo(dnId.getName(), dnId.getStorageID(),\n        dnId.getInfoPort(), dnId.getIpcPort(), info.getCapacity(),\n        info.getDfsUsed(), info.getRemaining(), info.getBlockPoolUsed(),\n        info.getLastUpdate(), info.getXceiverCount(), info.getLocation(),\n        info.getHostName(), convert(info.getAdminState()));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "BlockWithLocations[]",
            "newValue": "DatanodeInfo"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/11 2:19 PM",
          "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "05/12/11 4:25 PM",
          "commitNameOld": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,8 @@\n-  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n-    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n-    int i \u003d 0;\n-    for (BlockWithLocationsProto entry : b) {\n-      ret[i++] \u003d convert(entry);\n-    }\n-    return ret;\n+  public static DatanodeInfo convert(DatanodeInfoProto info) {\n+    DatanodeIDProto dnId \u003d info.getId();\n+    return new DatanodeInfo(dnId.getName(), dnId.getStorageID(),\n+        dnId.getInfoPort(), dnId.getIpcPort(), info.getCapacity(),\n+        info.getDfsUsed(), info.getRemaining(), info.getBlockPoolUsed(),\n+        info.getLastUpdate(), info.getXceiverCount(), info.getLocation(),\n+        info.getHostName(), convert(info.getAdminState()));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfo convert(DatanodeInfoProto info) {\n    DatanodeIDProto dnId \u003d info.getId();\n    return new DatanodeInfo(dnId.getName(), dnId.getStorageID(),\n        dnId.getInfoPort(), dnId.getIpcPort(), info.getCapacity(),\n        info.getDfsUsed(), info.getRemaining(), info.getBlockPoolUsed(),\n        info.getLastUpdate(), info.getXceiverCount(), info.getLocation(),\n        info.getHostName(), convert(info.getAdminState()));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2618. Implement protobuf service for NamenodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210719 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/12/11 4:25 PM",
      "commitName": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
      "commitAuthor": "Suresh Srinivas",
      "diff": "@@ -0,0 +1,8 @@\n+  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n+    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n+    int i \u003d 0;\n+    for (BlockWithLocationsProto entry : b) {\n+      ret[i++] \u003d convert(entry);\n+    }\n+    return ret;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n    int i \u003d 0;\n    for (BlockWithLocationsProto entry : b) {\n      ret[i++] \u003d convert(entry);\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java"
    }
  }
}