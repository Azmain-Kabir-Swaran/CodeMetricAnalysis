{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CombinedHostsFileReader.java",
  "functionName": "readFile",
  "functionId": "readFile___hostsFilePath-String(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileReader.java",
  "functionStartLine": 78,
  "functionEndLine": 121,
  "numCommitsSeen": 12,
  "timeTaken": 3574,
  "changeHistory": [
    "7a3188d054481b9bd563e337901e93476303ce7f",
    "aa1285989092bc253c45b7a83acec2e9bce2c5dc",
    "230b85d5865b7e08fb7aaeab45295b5b966011ef",
    "049e7d27bea13d4254baccf49401daae820b71df",
    "991c946593454b73b56f3c403fe128afe6f46355",
    "4fcfea71bfb16295f3a661e712d66351a1edc55e"
  ],
  "changeHistoryShort": {
    "7a3188d054481b9bd563e337901e93476303ce7f": "Ybodychange",
    "aa1285989092bc253c45b7a83acec2e9bce2c5dc": "Ymultichange(Yparameterchange,Ybodychange)",
    "230b85d5865b7e08fb7aaeab45295b5b966011ef": "Ymultichange(Yreturntypechange,Ybodychange)",
    "049e7d27bea13d4254baccf49401daae820b71df": "Ybodychange",
    "991c946593454b73b56f3c403fe128afe6f46355": "Ybodychange",
    "4fcfea71bfb16295f3a661e712d66351a1edc55e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7a3188d054481b9bd563e337901e93476303ce7f": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16282. Avoid FileStream to improve performance. Contributed by Ayush Saxena.\n",
      "commitDate": "02/05/19 12:58 PM",
      "commitName": "7a3188d054481b9bd563e337901e93476303ce7f",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "14/12/18 4:54 AM",
      "commitNameOld": "aa1285989092bc253c45b7a83acec2e9bce2c5dc",
      "commitAuthorOld": "Zsolt Venczel",
      "daysBetweenCommits": 139.29,
      "commitsBetweenForRepo": 1000,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,44 @@\n       readFile(final String hostsFilePath) throws IOException {\n     DatanodeAdminProperties[] allDNs \u003d new DatanodeAdminProperties[0];\n     ObjectMapper objectMapper \u003d new ObjectMapper();\n     File hostFile \u003d new File(hostsFilePath);\n     boolean tryOldFormat \u003d false;\n \n     if (hostFile.length() \u003e 0) {\n       try (Reader input \u003d\n-                   new InputStreamReader(new FileInputStream(hostFile),\n-                           \"UTF-8\")) {\n+          new InputStreamReader(\n+              Files.newInputStream(hostFile.toPath()), \"UTF-8\")) {\n         allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n       } catch (JsonMappingException jme) {\n         // The old format doesn\u0027t have json top-level token to enclose\n         // the array.\n         // For backward compatibility, try parsing the old format.\n         tryOldFormat \u003d true;\n       }\n     } else {\n       LOG.warn(hostsFilePath + \" is empty.\" + REFER_TO_DOC_MSG);\n     }\n \n     if (tryOldFormat) {\n       ObjectReader objectReader \u003d\n           objectMapper.readerFor(DatanodeAdminProperties.class);\n       JsonFactory jsonFactory \u003d new JsonFactory();\n       List\u003cDatanodeAdminProperties\u003e all \u003d new ArrayList\u003c\u003e();\n       try (Reader input \u003d\n-          new InputStreamReader(new FileInputStream(hostsFilePath),\n+          new InputStreamReader(Files.newInputStream(Paths.get(hostsFilePath)),\n                   \"UTF-8\")) {\n         Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n             objectReader.readValues(jsonFactory.createParser(input));\n         while (iterator.hasNext()) {\n           DatanodeAdminProperties properties \u003d iterator.next();\n           all.add(properties);\n         }\n         LOG.warn(hostsFilePath + \" has legacy JSON format.\" + REFER_TO_DOC_MSG);\n       } catch (Throwable ex) {\n         LOG.warn(hostsFilePath + \" has invalid JSON format.\" + REFER_TO_DOC_MSG,\n                 ex);\n       }\n       allDNs \u003d all.toArray(new DatanodeAdminProperties[all.size()]);\n     }\n     return allDNs;\n   }\n\\ No newline at end of file\n",
      "actualSource": "      readFile(final String hostsFilePath) throws IOException {\n    DatanodeAdminProperties[] allDNs \u003d new DatanodeAdminProperties[0];\n    ObjectMapper objectMapper \u003d new ObjectMapper();\n    File hostFile \u003d new File(hostsFilePath);\n    boolean tryOldFormat \u003d false;\n\n    if (hostFile.length() \u003e 0) {\n      try (Reader input \u003d\n          new InputStreamReader(\n              Files.newInputStream(hostFile.toPath()), \"UTF-8\")) {\n        allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n      } catch (JsonMappingException jme) {\n        // The old format doesn\u0027t have json top-level token to enclose\n        // the array.\n        // For backward compatibility, try parsing the old format.\n        tryOldFormat \u003d true;\n      }\n    } else {\n      LOG.warn(hostsFilePath + \" is empty.\" + REFER_TO_DOC_MSG);\n    }\n\n    if (tryOldFormat) {\n      ObjectReader objectReader \u003d\n          objectMapper.readerFor(DatanodeAdminProperties.class);\n      JsonFactory jsonFactory \u003d new JsonFactory();\n      List\u003cDatanodeAdminProperties\u003e all \u003d new ArrayList\u003c\u003e();\n      try (Reader input \u003d\n          new InputStreamReader(Files.newInputStream(Paths.get(hostsFilePath)),\n                  \"UTF-8\")) {\n        Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n            objectReader.readValues(jsonFactory.createParser(input));\n        while (iterator.hasNext()) {\n          DatanodeAdminProperties properties \u003d iterator.next();\n          all.add(properties);\n        }\n        LOG.warn(hostsFilePath + \" has legacy JSON format.\" + REFER_TO_DOC_MSG);\n      } catch (Throwable ex) {\n        LOG.warn(hostsFilePath + \" has invalid JSON format.\" + REFER_TO_DOC_MSG,\n                ex);\n      }\n      allDNs \u003d all.toArray(new DatanodeAdminProperties[all.size()]);\n    }\n    return allDNs;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileReader.java",
      "extendedDetails": {}
    },
    "aa1285989092bc253c45b7a83acec2e9bce2c5dc": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-14121. Log message about the old hosts file format is misleading\n(Contributed by Zsolt Venczel via Daniel Templeton)\n\nChange-Id: I7ff548f6c82e0aeb08a7a50ca7c2c827db8726bb\n",
      "commitDate": "14/12/18 4:54 AM",
      "commitName": "aa1285989092bc253c45b7a83acec2e9bce2c5dc",
      "commitAuthor": "Zsolt Venczel",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-14121. Log message about the old hosts file format is misleading\n(Contributed by Zsolt Venczel via Daniel Templeton)\n\nChange-Id: I7ff548f6c82e0aeb08a7a50ca7c2c827db8726bb\n",
          "commitDate": "14/12/18 4:54 AM",
          "commitName": "aa1285989092bc253c45b7a83acec2e9bce2c5dc",
          "commitAuthor": "Zsolt Venczel",
          "commitDateOld": "20/09/17 9:03 AM",
          "commitNameOld": "230b85d5865b7e08fb7aaeab45295b5b966011ef",
          "commitAuthorOld": "Ming Ma",
          "daysBetweenCommits": 449.87,
          "commitsBetweenForRepo": 3945,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,44 @@\n-      readFile(final String hostsFile) throws IOException {\n+      readFile(final String hostsFilePath) throws IOException {\n     DatanodeAdminProperties[] allDNs \u003d new DatanodeAdminProperties[0];\n     ObjectMapper objectMapper \u003d new ObjectMapper();\n+    File hostFile \u003d new File(hostsFilePath);\n     boolean tryOldFormat \u003d false;\n-    try (Reader input \u003d\n-        new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n-      allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n-    } catch (JsonMappingException jme) {\n-      // The old format doesn\u0027t have json top-level token to enclose the array.\n-      // For backward compatibility, try parsing the old format.\n-      tryOldFormat \u003d true;\n-      LOG.warn(\"{} has invalid JSON format.\" +\n-          \"Try the old format without top-level token defined.\", hostsFile);\n+\n+    if (hostFile.length() \u003e 0) {\n+      try (Reader input \u003d\n+                   new InputStreamReader(new FileInputStream(hostFile),\n+                           \"UTF-8\")) {\n+        allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n+      } catch (JsonMappingException jme) {\n+        // The old format doesn\u0027t have json top-level token to enclose\n+        // the array.\n+        // For backward compatibility, try parsing the old format.\n+        tryOldFormat \u003d true;\n+      }\n+    } else {\n+      LOG.warn(hostsFilePath + \" is empty.\" + REFER_TO_DOC_MSG);\n     }\n \n     if (tryOldFormat) {\n       ObjectReader objectReader \u003d\n           objectMapper.readerFor(DatanodeAdminProperties.class);\n       JsonFactory jsonFactory \u003d new JsonFactory();\n       List\u003cDatanodeAdminProperties\u003e all \u003d new ArrayList\u003c\u003e();\n       try (Reader input \u003d\n-          new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n+          new InputStreamReader(new FileInputStream(hostsFilePath),\n+                  \"UTF-8\")) {\n         Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n             objectReader.readValues(jsonFactory.createParser(input));\n         while (iterator.hasNext()) {\n           DatanodeAdminProperties properties \u003d iterator.next();\n           all.add(properties);\n         }\n+        LOG.warn(hostsFilePath + \" has legacy JSON format.\" + REFER_TO_DOC_MSG);\n+      } catch (Throwable ex) {\n+        LOG.warn(hostsFilePath + \" has invalid JSON format.\" + REFER_TO_DOC_MSG,\n+                ex);\n       }\n       allDNs \u003d all.toArray(new DatanodeAdminProperties[all.size()]);\n     }\n     return allDNs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "      readFile(final String hostsFilePath) throws IOException {\n    DatanodeAdminProperties[] allDNs \u003d new DatanodeAdminProperties[0];\n    ObjectMapper objectMapper \u003d new ObjectMapper();\n    File hostFile \u003d new File(hostsFilePath);\n    boolean tryOldFormat \u003d false;\n\n    if (hostFile.length() \u003e 0) {\n      try (Reader input \u003d\n                   new InputStreamReader(new FileInputStream(hostFile),\n                           \"UTF-8\")) {\n        allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n      } catch (JsonMappingException jme) {\n        // The old format doesn\u0027t have json top-level token to enclose\n        // the array.\n        // For backward compatibility, try parsing the old format.\n        tryOldFormat \u003d true;\n      }\n    } else {\n      LOG.warn(hostsFilePath + \" is empty.\" + REFER_TO_DOC_MSG);\n    }\n\n    if (tryOldFormat) {\n      ObjectReader objectReader \u003d\n          objectMapper.readerFor(DatanodeAdminProperties.class);\n      JsonFactory jsonFactory \u003d new JsonFactory();\n      List\u003cDatanodeAdminProperties\u003e all \u003d new ArrayList\u003c\u003e();\n      try (Reader input \u003d\n          new InputStreamReader(new FileInputStream(hostsFilePath),\n                  \"UTF-8\")) {\n        Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n            objectReader.readValues(jsonFactory.createParser(input));\n        while (iterator.hasNext()) {\n          DatanodeAdminProperties properties \u003d iterator.next();\n          all.add(properties);\n        }\n        LOG.warn(hostsFilePath + \" has legacy JSON format.\" + REFER_TO_DOC_MSG);\n      } catch (Throwable ex) {\n        LOG.warn(hostsFilePath + \" has invalid JSON format.\" + REFER_TO_DOC_MSG,\n                ex);\n      }\n      allDNs \u003d all.toArray(new DatanodeAdminProperties[all.size()]);\n    }\n    return allDNs;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileReader.java",
          "extendedDetails": {
            "oldValue": "[hostsFile-String(modifiers-final)]",
            "newValue": "[hostsFilePath-String(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-14121. Log message about the old hosts file format is misleading\n(Contributed by Zsolt Venczel via Daniel Templeton)\n\nChange-Id: I7ff548f6c82e0aeb08a7a50ca7c2c827db8726bb\n",
          "commitDate": "14/12/18 4:54 AM",
          "commitName": "aa1285989092bc253c45b7a83acec2e9bce2c5dc",
          "commitAuthor": "Zsolt Venczel",
          "commitDateOld": "20/09/17 9:03 AM",
          "commitNameOld": "230b85d5865b7e08fb7aaeab45295b5b966011ef",
          "commitAuthorOld": "Ming Ma",
          "daysBetweenCommits": 449.87,
          "commitsBetweenForRepo": 3945,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,44 @@\n-      readFile(final String hostsFile) throws IOException {\n+      readFile(final String hostsFilePath) throws IOException {\n     DatanodeAdminProperties[] allDNs \u003d new DatanodeAdminProperties[0];\n     ObjectMapper objectMapper \u003d new ObjectMapper();\n+    File hostFile \u003d new File(hostsFilePath);\n     boolean tryOldFormat \u003d false;\n-    try (Reader input \u003d\n-        new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n-      allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n-    } catch (JsonMappingException jme) {\n-      // The old format doesn\u0027t have json top-level token to enclose the array.\n-      // For backward compatibility, try parsing the old format.\n-      tryOldFormat \u003d true;\n-      LOG.warn(\"{} has invalid JSON format.\" +\n-          \"Try the old format without top-level token defined.\", hostsFile);\n+\n+    if (hostFile.length() \u003e 0) {\n+      try (Reader input \u003d\n+                   new InputStreamReader(new FileInputStream(hostFile),\n+                           \"UTF-8\")) {\n+        allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n+      } catch (JsonMappingException jme) {\n+        // The old format doesn\u0027t have json top-level token to enclose\n+        // the array.\n+        // For backward compatibility, try parsing the old format.\n+        tryOldFormat \u003d true;\n+      }\n+    } else {\n+      LOG.warn(hostsFilePath + \" is empty.\" + REFER_TO_DOC_MSG);\n     }\n \n     if (tryOldFormat) {\n       ObjectReader objectReader \u003d\n           objectMapper.readerFor(DatanodeAdminProperties.class);\n       JsonFactory jsonFactory \u003d new JsonFactory();\n       List\u003cDatanodeAdminProperties\u003e all \u003d new ArrayList\u003c\u003e();\n       try (Reader input \u003d\n-          new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n+          new InputStreamReader(new FileInputStream(hostsFilePath),\n+                  \"UTF-8\")) {\n         Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n             objectReader.readValues(jsonFactory.createParser(input));\n         while (iterator.hasNext()) {\n           DatanodeAdminProperties properties \u003d iterator.next();\n           all.add(properties);\n         }\n+        LOG.warn(hostsFilePath + \" has legacy JSON format.\" + REFER_TO_DOC_MSG);\n+      } catch (Throwable ex) {\n+        LOG.warn(hostsFilePath + \" has invalid JSON format.\" + REFER_TO_DOC_MSG,\n+                ex);\n       }\n       allDNs \u003d all.toArray(new DatanodeAdminProperties[all.size()]);\n     }\n     return allDNs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "      readFile(final String hostsFilePath) throws IOException {\n    DatanodeAdminProperties[] allDNs \u003d new DatanodeAdminProperties[0];\n    ObjectMapper objectMapper \u003d new ObjectMapper();\n    File hostFile \u003d new File(hostsFilePath);\n    boolean tryOldFormat \u003d false;\n\n    if (hostFile.length() \u003e 0) {\n      try (Reader input \u003d\n                   new InputStreamReader(new FileInputStream(hostFile),\n                           \"UTF-8\")) {\n        allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n      } catch (JsonMappingException jme) {\n        // The old format doesn\u0027t have json top-level token to enclose\n        // the array.\n        // For backward compatibility, try parsing the old format.\n        tryOldFormat \u003d true;\n      }\n    } else {\n      LOG.warn(hostsFilePath + \" is empty.\" + REFER_TO_DOC_MSG);\n    }\n\n    if (tryOldFormat) {\n      ObjectReader objectReader \u003d\n          objectMapper.readerFor(DatanodeAdminProperties.class);\n      JsonFactory jsonFactory \u003d new JsonFactory();\n      List\u003cDatanodeAdminProperties\u003e all \u003d new ArrayList\u003c\u003e();\n      try (Reader input \u003d\n          new InputStreamReader(new FileInputStream(hostsFilePath),\n                  \"UTF-8\")) {\n        Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n            objectReader.readValues(jsonFactory.createParser(input));\n        while (iterator.hasNext()) {\n          DatanodeAdminProperties properties \u003d iterator.next();\n          all.add(properties);\n        }\n        LOG.warn(hostsFilePath + \" has legacy JSON format.\" + REFER_TO_DOC_MSG);\n      } catch (Throwable ex) {\n        LOG.warn(hostsFilePath + \" has invalid JSON format.\" + REFER_TO_DOC_MSG,\n                ex);\n      }\n      allDNs \u003d all.toArray(new DatanodeAdminProperties[all.size()]);\n    }\n    return allDNs;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileReader.java",
          "extendedDetails": {}
        }
      ]
    },
    "230b85d5865b7e08fb7aaeab45295b5b966011ef": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-12473. Change hosts JSON file format.\n",
      "commitDate": "20/09/17 9:03 AM",
      "commitName": "230b85d5865b7e08fb7aaeab45295b5b966011ef",
      "commitAuthor": "Ming Ma",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-12473. Change hosts JSON file format.\n",
          "commitDate": "20/09/17 9:03 AM",
          "commitName": "230b85d5865b7e08fb7aaeab45295b5b966011ef",
          "commitAuthor": "Ming Ma",
          "commitDateOld": "12/12/16 6:11 PM",
          "commitNameOld": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 281.58,
          "commitsBetweenForRepo": 1695,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,13 +1,33 @@\n       readFile(final String hostsFile) throws IOException {\n-    HashSet\u003cDatanodeAdminProperties\u003e allDNs \u003d new HashSet\u003c\u003e();\n+    DatanodeAdminProperties[] allDNs \u003d new DatanodeAdminProperties[0];\n+    ObjectMapper objectMapper \u003d new ObjectMapper();\n+    boolean tryOldFormat \u003d false;\n     try (Reader input \u003d\n-         new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n-      Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n-          READER.readValues(JSON_FACTORY.createParser(input));\n-      while (iterator.hasNext()) {\n-        DatanodeAdminProperties properties \u003d iterator.next();\n-        allDNs.add(properties);\n+        new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n+      allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n+    } catch (JsonMappingException jme) {\n+      // The old format doesn\u0027t have json top-level token to enclose the array.\n+      // For backward compatibility, try parsing the old format.\n+      tryOldFormat \u003d true;\n+      LOG.warn(\"{} has invalid JSON format.\" +\n+          \"Try the old format without top-level token defined.\", hostsFile);\n+    }\n+\n+    if (tryOldFormat) {\n+      ObjectReader objectReader \u003d\n+          objectMapper.readerFor(DatanodeAdminProperties.class);\n+      JsonFactory jsonFactory \u003d new JsonFactory();\n+      List\u003cDatanodeAdminProperties\u003e all \u003d new ArrayList\u003c\u003e();\n+      try (Reader input \u003d\n+          new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n+        Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n+            objectReader.readValues(jsonFactory.createParser(input));\n+        while (iterator.hasNext()) {\n+          DatanodeAdminProperties properties \u003d iterator.next();\n+          all.add(properties);\n+        }\n       }\n+      allDNs \u003d all.toArray(new DatanodeAdminProperties[all.size()]);\n     }\n     return allDNs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "      readFile(final String hostsFile) throws IOException {\n    DatanodeAdminProperties[] allDNs \u003d new DatanodeAdminProperties[0];\n    ObjectMapper objectMapper \u003d new ObjectMapper();\n    boolean tryOldFormat \u003d false;\n    try (Reader input \u003d\n        new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n      allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n    } catch (JsonMappingException jme) {\n      // The old format doesn\u0027t have json top-level token to enclose the array.\n      // For backward compatibility, try parsing the old format.\n      tryOldFormat \u003d true;\n      LOG.warn(\"{} has invalid JSON format.\" +\n          \"Try the old format without top-level token defined.\", hostsFile);\n    }\n\n    if (tryOldFormat) {\n      ObjectReader objectReader \u003d\n          objectMapper.readerFor(DatanodeAdminProperties.class);\n      JsonFactory jsonFactory \u003d new JsonFactory();\n      List\u003cDatanodeAdminProperties\u003e all \u003d new ArrayList\u003c\u003e();\n      try (Reader input \u003d\n          new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n        Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n            objectReader.readValues(jsonFactory.createParser(input));\n        while (iterator.hasNext()) {\n          DatanodeAdminProperties properties \u003d iterator.next();\n          all.add(properties);\n        }\n      }\n      allDNs \u003d all.toArray(new DatanodeAdminProperties[all.size()]);\n    }\n    return allDNs;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileReader.java",
          "extendedDetails": {
            "oldValue": "Set\u003cDatanodeAdminProperties\u003e",
            "newValue": "DatanodeAdminProperties[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-12473. Change hosts JSON file format.\n",
          "commitDate": "20/09/17 9:03 AM",
          "commitName": "230b85d5865b7e08fb7aaeab45295b5b966011ef",
          "commitAuthor": "Ming Ma",
          "commitDateOld": "12/12/16 6:11 PM",
          "commitNameOld": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 281.58,
          "commitsBetweenForRepo": 1695,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,13 +1,33 @@\n       readFile(final String hostsFile) throws IOException {\n-    HashSet\u003cDatanodeAdminProperties\u003e allDNs \u003d new HashSet\u003c\u003e();\n+    DatanodeAdminProperties[] allDNs \u003d new DatanodeAdminProperties[0];\n+    ObjectMapper objectMapper \u003d new ObjectMapper();\n+    boolean tryOldFormat \u003d false;\n     try (Reader input \u003d\n-         new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n-      Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n-          READER.readValues(JSON_FACTORY.createParser(input));\n-      while (iterator.hasNext()) {\n-        DatanodeAdminProperties properties \u003d iterator.next();\n-        allDNs.add(properties);\n+        new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n+      allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n+    } catch (JsonMappingException jme) {\n+      // The old format doesn\u0027t have json top-level token to enclose the array.\n+      // For backward compatibility, try parsing the old format.\n+      tryOldFormat \u003d true;\n+      LOG.warn(\"{} has invalid JSON format.\" +\n+          \"Try the old format without top-level token defined.\", hostsFile);\n+    }\n+\n+    if (tryOldFormat) {\n+      ObjectReader objectReader \u003d\n+          objectMapper.readerFor(DatanodeAdminProperties.class);\n+      JsonFactory jsonFactory \u003d new JsonFactory();\n+      List\u003cDatanodeAdminProperties\u003e all \u003d new ArrayList\u003c\u003e();\n+      try (Reader input \u003d\n+          new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n+        Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n+            objectReader.readValues(jsonFactory.createParser(input));\n+        while (iterator.hasNext()) {\n+          DatanodeAdminProperties properties \u003d iterator.next();\n+          all.add(properties);\n+        }\n       }\n+      allDNs \u003d all.toArray(new DatanodeAdminProperties[all.size()]);\n     }\n     return allDNs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "      readFile(final String hostsFile) throws IOException {\n    DatanodeAdminProperties[] allDNs \u003d new DatanodeAdminProperties[0];\n    ObjectMapper objectMapper \u003d new ObjectMapper();\n    boolean tryOldFormat \u003d false;\n    try (Reader input \u003d\n        new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n      allDNs \u003d objectMapper.readValue(input, DatanodeAdminProperties[].class);\n    } catch (JsonMappingException jme) {\n      // The old format doesn\u0027t have json top-level token to enclose the array.\n      // For backward compatibility, try parsing the old format.\n      tryOldFormat \u003d true;\n      LOG.warn(\"{} has invalid JSON format.\" +\n          \"Try the old format without top-level token defined.\", hostsFile);\n    }\n\n    if (tryOldFormat) {\n      ObjectReader objectReader \u003d\n          objectMapper.readerFor(DatanodeAdminProperties.class);\n      JsonFactory jsonFactory \u003d new JsonFactory();\n      List\u003cDatanodeAdminProperties\u003e all \u003d new ArrayList\u003c\u003e();\n      try (Reader input \u003d\n          new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n        Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n            objectReader.readValues(jsonFactory.createParser(input));\n        while (iterator.hasNext()) {\n          DatanodeAdminProperties properties \u003d iterator.next();\n          all.add(properties);\n        }\n      }\n      allDNs \u003d all.toArray(new DatanodeAdminProperties[all.size()]);\n    }\n    return allDNs;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileReader.java",
          "extendedDetails": {}
        }
      ]
    },
    "049e7d27bea13d4254baccf49401daae820b71df": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10970. Update jackson from 1.9.13 to 2.x in hadoop-hdfs.\n",
      "commitDate": "06/11/16 6:16 PM",
      "commitName": "049e7d27bea13d4254baccf49401daae820b71df",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "29/06/16 10:10 AM",
      "commitNameOld": "991c946593454b73b56f3c403fe128afe6f46355",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 130.38,
      "commitsBetweenForRepo": 1044,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n       readFile(final String hostsFile) throws IOException {\n     HashSet\u003cDatanodeAdminProperties\u003e allDNs \u003d new HashSet\u003c\u003e();\n     try (Reader input \u003d\n          new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n       Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n-          READER.readValues(JSON_FACTORY.createJsonParser(input));\n+          READER.readValues(JSON_FACTORY.createParser(input));\n       while (iterator.hasNext()) {\n         DatanodeAdminProperties properties \u003d iterator.next();\n         allDNs.add(properties);\n       }\n     }\n     return allDNs;\n   }\n\\ No newline at end of file\n",
      "actualSource": "      readFile(final String hostsFile) throws IOException {\n    HashSet\u003cDatanodeAdminProperties\u003e allDNs \u003d new HashSet\u003c\u003e();\n    try (Reader input \u003d\n         new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n      Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n          READER.readValues(JSON_FACTORY.createParser(input));\n      while (iterator.hasNext()) {\n        DatanodeAdminProperties properties \u003d iterator.next();\n        allDNs.add(properties);\n      }\n    }\n    return allDNs;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileReader.java",
      "extendedDetails": {}
    },
    "991c946593454b73b56f3c403fe128afe6f46355": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10568. Reuse ObjectMapper instance in CombinedHostsFileReader and CombinedHostsFileWriter. Contributed by Yiqun Lin.\n",
      "commitDate": "29/06/16 10:10 AM",
      "commitName": "991c946593454b73b56f3c403fe128afe6f46355",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "25/03/16 5:09 PM",
      "commitNameOld": "4fcfea71bfb16295f3a661e712d66351a1edc55e",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 95.71,
      "commitsBetweenForRepo": 659,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,13 @@\n       readFile(final String hostsFile) throws IOException {\n     HashSet\u003cDatanodeAdminProperties\u003e allDNs \u003d new HashSet\u003c\u003e();\n-    ObjectMapper mapper \u003d new ObjectMapper();\n     try (Reader input \u003d\n          new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n       Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n-          mapper.readValues(new JsonFactory().createJsonParser(input),\n-              DatanodeAdminProperties.class);\n+          READER.readValues(JSON_FACTORY.createJsonParser(input));\n       while (iterator.hasNext()) {\n         DatanodeAdminProperties properties \u003d iterator.next();\n         allDNs.add(properties);\n       }\n     }\n     return allDNs;\n   }\n\\ No newline at end of file\n",
      "actualSource": "      readFile(final String hostsFile) throws IOException {\n    HashSet\u003cDatanodeAdminProperties\u003e allDNs \u003d new HashSet\u003c\u003e();\n    try (Reader input \u003d\n         new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n      Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n          READER.readValues(JSON_FACTORY.createJsonParser(input));\n      while (iterator.hasNext()) {\n        DatanodeAdminProperties properties \u003d iterator.next();\n        allDNs.add(properties);\n      }\n    }\n    return allDNs;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileReader.java",
      "extendedDetails": {}
    },
    "4fcfea71bfb16295f3a661e712d66351a1edc55e": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9005. Provide support for upgrade domain script. (Ming Ma via Lei Xu)\n",
      "commitDate": "25/03/16 5:09 PM",
      "commitName": "4fcfea71bfb16295f3a661e712d66351a1edc55e",
      "commitAuthor": "Lei Xu",
      "diff": "@@ -0,0 +1,15 @@\n+      readFile(final String hostsFile) throws IOException {\n+    HashSet\u003cDatanodeAdminProperties\u003e allDNs \u003d new HashSet\u003c\u003e();\n+    ObjectMapper mapper \u003d new ObjectMapper();\n+    try (Reader input \u003d\n+         new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n+      Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n+          mapper.readValues(new JsonFactory().createJsonParser(input),\n+              DatanodeAdminProperties.class);\n+      while (iterator.hasNext()) {\n+        DatanodeAdminProperties properties \u003d iterator.next();\n+        allDNs.add(properties);\n+      }\n+    }\n+    return allDNs;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "      readFile(final String hostsFile) throws IOException {\n    HashSet\u003cDatanodeAdminProperties\u003e allDNs \u003d new HashSet\u003c\u003e();\n    ObjectMapper mapper \u003d new ObjectMapper();\n    try (Reader input \u003d\n         new InputStreamReader(new FileInputStream(hostsFile), \"UTF-8\")) {\n      Iterator\u003cDatanodeAdminProperties\u003e iterator \u003d\n          mapper.readValues(new JsonFactory().createJsonParser(input),\n              DatanodeAdminProperties.class);\n      while (iterator.hasNext()) {\n        DatanodeAdminProperties properties \u003d iterator.next();\n        allDNs.add(properties);\n      }\n    }\n    return allDNs;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileReader.java"
    }
  }
}