{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSImageFormatProtobuf.java",
  "functionName": "saveInternal",
  "functionId": "saveInternal___fout-FileOutputStream__compression-FSImageCompression__filePath-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java",
  "functionStartLine": 812,
  "functionEndLine": 883,
  "numCommitsSeen": 38,
  "timeTaken": 5244,
  "changeHistory": [
    "bd7baea5a5d4ff351645e34c0ef09b7ba82f4285",
    "23854443efa62aa70a1c30c32c3816750e5d7a5b",
    "a991e899fb9f98d2089f37ac9ac7c485d3bbb959",
    "ae8f55b93243560bd891962d6c64320ddc62a7d7",
    "71de367c5e80ea76d1e8d21f0216cd6b879dcee5",
    "a795bc42d012bf75872ae412cb2644c2d80177e3",
    "ea0b21af158016651cb77560778834eb95e6b68d",
    "d03acc756094a332f98167426a39db8faf38f450",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305"
  ],
  "changeHistoryShort": {
    "bd7baea5a5d4ff351645e34c0ef09b7ba82f4285": "Ybodychange",
    "23854443efa62aa70a1c30c32c3816750e5d7a5b": "Ybodychange",
    "a991e899fb9f98d2089f37ac9ac7c485d3bbb959": "Ymultichange(Yreturntypechange,Ybodychange)",
    "ae8f55b93243560bd891962d6c64320ddc62a7d7": "Ybodychange",
    "71de367c5e80ea76d1e8d21f0216cd6b879dcee5": "Ybodychange",
    "a795bc42d012bf75872ae412cb2644c2d80177e3": "Ybodychange",
    "ea0b21af158016651cb77560778834eb95e6b68d": "Ybodychange",
    "d03acc756094a332f98167426a39db8faf38f450": "Ybodychange",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Yintroduced"
  },
  "changeHistoryDetails": {
    "bd7baea5a5d4ff351645e34c0ef09b7ba82f4285": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14396. Failed to load image from FSImageFile when downgrade from 3.x to 2.x. Contributed by Fei Hui.\n",
      "commitDate": "22/08/19 11:28 PM",
      "commitName": "bd7baea5a5d4ff351645e34c0ef09b7ba82f4285",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "22/08/19 5:09 PM",
      "commitNameOld": "b67812ea2111fa11bdd76096b923c93e1bdf2923",
      "commitAuthorOld": "Stephen O\u0027Donnell",
      "daysBetweenCommits": 0.26,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,72 @@\n     private long saveInternal(FileOutputStream fout,\n         FSImageCompression compression, String filePath) throws IOException {\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       MessageDigest digester \u003d MD5Hash.getDigester();\n+      int layoutVersion \u003d\n+          context.getSourceNamesystem().getEffectiveLayoutVersion();\n \n       underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n           fout), digester);\n       underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n \n       fileChannel \u003d fout.getChannel();\n \n       FileSummary.Builder b \u003d FileSummary.newBuilder()\n           .setOndiskVersion(FSImageUtil.FILE_VERSION)\n           .setLayoutVersion(\n               context.getSourceNamesystem().getEffectiveLayoutVersion());\n \n       codec \u003d compression.getImageCodec();\n       if (codec !\u003d null) {\n         b.setCodec(codec.getClass().getCanonicalName());\n         sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n       } else {\n         sectionOutputStream \u003d underlyingOutputStream;\n       }\n \n       saveNameSystemSection(b);\n       // Check for cancellation right after serializing the name system section.\n       // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n       // depends on this behavior.\n       context.checkCancelled();\n \n+      Step step;\n+\n       // Erasure coding policies should be saved before inodes\n-      Step step \u003d new Step(StepType.ERASURE_CODING_POLICIES, filePath);\n-      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n-      saveErasureCodingSection(b);\n-      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n+      if (NameNodeLayoutVersion.supports(\n+          NameNodeLayoutVersion.Feature.ERASURE_CODING, layoutVersion)) {\n+        step \u003d new Step(StepType.ERASURE_CODING_POLICIES, filePath);\n+        prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n+        saveErasureCodingSection(b);\n+        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n+      }\n \n       step \u003d new Step(StepType.INODES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       // Count number of non-fatal errors when saving inodes and snapshots.\n       long numErrors \u003d saveInodes(b);\n       numErrors +\u003d saveSnapshots(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveSecretManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.CACHE_POOLS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveCacheManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       saveStringTableSection(b);\n \n       // We use the underlyingOutputStream to write the header. Therefore flush\n       // the buffered stream (which is potentially compressed) first.\n       flushSectionOutputStream();\n \n       FileSummary summary \u003d b.build();\n       saveFileSummary(underlyingOutputStream, summary);\n       underlyingOutputStream.close();\n       savedDigest \u003d new MD5Hash(digester.digest());\n       return numErrors;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long saveInternal(FileOutputStream fout,\n        FSImageCompression compression, String filePath) throws IOException {\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      MessageDigest digester \u003d MD5Hash.getDigester();\n      int layoutVersion \u003d\n          context.getSourceNamesystem().getEffectiveLayoutVersion();\n\n      underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n          fout), digester);\n      underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n\n      fileChannel \u003d fout.getChannel();\n\n      FileSummary.Builder b \u003d FileSummary.newBuilder()\n          .setOndiskVersion(FSImageUtil.FILE_VERSION)\n          .setLayoutVersion(\n              context.getSourceNamesystem().getEffectiveLayoutVersion());\n\n      codec \u003d compression.getImageCodec();\n      if (codec !\u003d null) {\n        b.setCodec(codec.getClass().getCanonicalName());\n        sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n      } else {\n        sectionOutputStream \u003d underlyingOutputStream;\n      }\n\n      saveNameSystemSection(b);\n      // Check for cancellation right after serializing the name system section.\n      // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n      // depends on this behavior.\n      context.checkCancelled();\n\n      Step step;\n\n      // Erasure coding policies should be saved before inodes\n      if (NameNodeLayoutVersion.supports(\n          NameNodeLayoutVersion.Feature.ERASURE_CODING, layoutVersion)) {\n        step \u003d new Step(StepType.ERASURE_CODING_POLICIES, filePath);\n        prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n        saveErasureCodingSection(b);\n        prog.endStep(Phase.SAVING_CHECKPOINT, step);\n      }\n\n      step \u003d new Step(StepType.INODES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      // Count number of non-fatal errors when saving inodes and snapshots.\n      long numErrors \u003d saveInodes(b);\n      numErrors +\u003d saveSnapshots(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveSecretManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.CACHE_POOLS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveCacheManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      saveStringTableSection(b);\n\n      // We use the underlyingOutputStream to write the header. Therefore flush\n      // the buffered stream (which is potentially compressed) first.\n      flushSectionOutputStream();\n\n      FileSummary summary \u003d b.build();\n      saveFileSummary(underlyingOutputStream, summary);\n      underlyingOutputStream.close();\n      savedDigest \u003d new MD5Hash(digester.digest());\n      return numErrors;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java",
      "extendedDetails": {}
    },
    "23854443efa62aa70a1c30c32c3816750e5d7a5b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13813. Exit NameNode if dangling child inode is detected when saving FsImage. Contributed by Siyao Meng.\n",
      "commitDate": "13/08/18 4:12 PM",
      "commitName": "23854443efa62aa70a1c30c32c3816750e5d7a5b",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "28/03/18 12:49 PM",
      "commitNameOld": "a991e899fb9f98d2089f37ac9ac7c485d3bbb959",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 138.14,
      "commitsBetweenForRepo": 1511,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,65 @@\n     private long saveInternal(FileOutputStream fout,\n         FSImageCompression compression, String filePath) throws IOException {\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       MessageDigest digester \u003d MD5Hash.getDigester();\n \n       underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n           fout), digester);\n       underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n \n       fileChannel \u003d fout.getChannel();\n \n       FileSummary.Builder b \u003d FileSummary.newBuilder()\n           .setOndiskVersion(FSImageUtil.FILE_VERSION)\n           .setLayoutVersion(\n               context.getSourceNamesystem().getEffectiveLayoutVersion());\n \n       codec \u003d compression.getImageCodec();\n       if (codec !\u003d null) {\n         b.setCodec(codec.getClass().getCanonicalName());\n         sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n       } else {\n         sectionOutputStream \u003d underlyingOutputStream;\n       }\n \n       saveNameSystemSection(b);\n       // Check for cancellation right after serializing the name system section.\n       // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n       // depends on this behavior.\n       context.checkCancelled();\n \n       // Erasure coding policies should be saved before inodes\n       Step step \u003d new Step(StepType.ERASURE_CODING_POLICIES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveErasureCodingSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.INODES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n-      saveInodes(b);\n-      long numErrors \u003d saveSnapshots(b);\n+      // Count number of non-fatal errors when saving inodes and snapshots.\n+      long numErrors \u003d saveInodes(b);\n+      numErrors +\u003d saveSnapshots(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveSecretManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.CACHE_POOLS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveCacheManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       saveStringTableSection(b);\n \n       // We use the underlyingOutputStream to write the header. Therefore flush\n       // the buffered stream (which is potentially compressed) first.\n       flushSectionOutputStream();\n \n       FileSummary summary \u003d b.build();\n       saveFileSummary(underlyingOutputStream, summary);\n       underlyingOutputStream.close();\n       savedDigest \u003d new MD5Hash(digester.digest());\n       return numErrors;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long saveInternal(FileOutputStream fout,\n        FSImageCompression compression, String filePath) throws IOException {\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      MessageDigest digester \u003d MD5Hash.getDigester();\n\n      underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n          fout), digester);\n      underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n\n      fileChannel \u003d fout.getChannel();\n\n      FileSummary.Builder b \u003d FileSummary.newBuilder()\n          .setOndiskVersion(FSImageUtil.FILE_VERSION)\n          .setLayoutVersion(\n              context.getSourceNamesystem().getEffectiveLayoutVersion());\n\n      codec \u003d compression.getImageCodec();\n      if (codec !\u003d null) {\n        b.setCodec(codec.getClass().getCanonicalName());\n        sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n      } else {\n        sectionOutputStream \u003d underlyingOutputStream;\n      }\n\n      saveNameSystemSection(b);\n      // Check for cancellation right after serializing the name system section.\n      // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n      // depends on this behavior.\n      context.checkCancelled();\n\n      // Erasure coding policies should be saved before inodes\n      Step step \u003d new Step(StepType.ERASURE_CODING_POLICIES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveErasureCodingSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.INODES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      // Count number of non-fatal errors when saving inodes and snapshots.\n      long numErrors \u003d saveInodes(b);\n      numErrors +\u003d saveSnapshots(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveSecretManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.CACHE_POOLS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveCacheManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      saveStringTableSection(b);\n\n      // We use the underlyingOutputStream to write the header. Therefore flush\n      // the buffered stream (which is potentially compressed) first.\n      flushSectionOutputStream();\n\n      FileSummary summary \u003d b.build();\n      saveFileSummary(underlyingOutputStream, summary);\n      underlyingOutputStream.close();\n      savedDigest \u003d new MD5Hash(digester.digest());\n      return numErrors;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java",
      "extendedDetails": {}
    },
    "a991e899fb9f98d2089f37ac9ac7c485d3bbb959": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-13314. NameNode should optionally exit if it detects FsImage corruption. Contributed by Arpit Agarwal.\n",
      "commitDate": "28/03/18 12:49 PM",
      "commitName": "a991e899fb9f98d2089f37ac9ac7c485d3bbb959",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-13314. NameNode should optionally exit if it detects FsImage corruption. Contributed by Arpit Agarwal.\n",
          "commitDate": "28/03/18 12:49 PM",
          "commitName": "a991e899fb9f98d2089f37ac9ac7c485d3bbb959",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "02/11/17 9:27 PM",
          "commitNameOld": "e565b5277d5b890dad107fe85e295a3907e4bfc1",
          "commitAuthorOld": "Xiao Chen",
          "daysBetweenCommits": 145.64,
          "commitsBetweenForRepo": 1105,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,64 @@\n-    private void saveInternal(FileOutputStream fout,\n+    private long saveInternal(FileOutputStream fout,\n         FSImageCompression compression, String filePath) throws IOException {\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       MessageDigest digester \u003d MD5Hash.getDigester();\n \n       underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n           fout), digester);\n       underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n \n       fileChannel \u003d fout.getChannel();\n \n       FileSummary.Builder b \u003d FileSummary.newBuilder()\n           .setOndiskVersion(FSImageUtil.FILE_VERSION)\n           .setLayoutVersion(\n               context.getSourceNamesystem().getEffectiveLayoutVersion());\n \n       codec \u003d compression.getImageCodec();\n       if (codec !\u003d null) {\n         b.setCodec(codec.getClass().getCanonicalName());\n         sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n       } else {\n         sectionOutputStream \u003d underlyingOutputStream;\n       }\n \n       saveNameSystemSection(b);\n       // Check for cancellation right after serializing the name system section.\n       // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n       // depends on this behavior.\n       context.checkCancelled();\n \n       // Erasure coding policies should be saved before inodes\n       Step step \u003d new Step(StepType.ERASURE_CODING_POLICIES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveErasureCodingSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.INODES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveInodes(b);\n-      saveSnapshots(b);\n+      long numErrors \u003d saveSnapshots(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveSecretManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.CACHE_POOLS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveCacheManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       saveStringTableSection(b);\n \n       // We use the underlyingOutputStream to write the header. Therefore flush\n       // the buffered stream (which is potentially compressed) first.\n       flushSectionOutputStream();\n \n       FileSummary summary \u003d b.build();\n       saveFileSummary(underlyingOutputStream, summary);\n       underlyingOutputStream.close();\n       savedDigest \u003d new MD5Hash(digester.digest());\n+      return numErrors;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private long saveInternal(FileOutputStream fout,\n        FSImageCompression compression, String filePath) throws IOException {\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      MessageDigest digester \u003d MD5Hash.getDigester();\n\n      underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n          fout), digester);\n      underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n\n      fileChannel \u003d fout.getChannel();\n\n      FileSummary.Builder b \u003d FileSummary.newBuilder()\n          .setOndiskVersion(FSImageUtil.FILE_VERSION)\n          .setLayoutVersion(\n              context.getSourceNamesystem().getEffectiveLayoutVersion());\n\n      codec \u003d compression.getImageCodec();\n      if (codec !\u003d null) {\n        b.setCodec(codec.getClass().getCanonicalName());\n        sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n      } else {\n        sectionOutputStream \u003d underlyingOutputStream;\n      }\n\n      saveNameSystemSection(b);\n      // Check for cancellation right after serializing the name system section.\n      // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n      // depends on this behavior.\n      context.checkCancelled();\n\n      // Erasure coding policies should be saved before inodes\n      Step step \u003d new Step(StepType.ERASURE_CODING_POLICIES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveErasureCodingSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.INODES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveInodes(b);\n      long numErrors \u003d saveSnapshots(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveSecretManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.CACHE_POOLS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveCacheManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      saveStringTableSection(b);\n\n      // We use the underlyingOutputStream to write the header. Therefore flush\n      // the buffered stream (which is potentially compressed) first.\n      flushSectionOutputStream();\n\n      FileSummary summary \u003d b.build();\n      saveFileSummary(underlyingOutputStream, summary);\n      underlyingOutputStream.close();\n      savedDigest \u003d new MD5Hash(digester.digest());\n      return numErrors;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "long"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13314. NameNode should optionally exit if it detects FsImage corruption. Contributed by Arpit Agarwal.\n",
          "commitDate": "28/03/18 12:49 PM",
          "commitName": "a991e899fb9f98d2089f37ac9ac7c485d3bbb959",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "02/11/17 9:27 PM",
          "commitNameOld": "e565b5277d5b890dad107fe85e295a3907e4bfc1",
          "commitAuthorOld": "Xiao Chen",
          "daysBetweenCommits": 145.64,
          "commitsBetweenForRepo": 1105,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,64 @@\n-    private void saveInternal(FileOutputStream fout,\n+    private long saveInternal(FileOutputStream fout,\n         FSImageCompression compression, String filePath) throws IOException {\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       MessageDigest digester \u003d MD5Hash.getDigester();\n \n       underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n           fout), digester);\n       underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n \n       fileChannel \u003d fout.getChannel();\n \n       FileSummary.Builder b \u003d FileSummary.newBuilder()\n           .setOndiskVersion(FSImageUtil.FILE_VERSION)\n           .setLayoutVersion(\n               context.getSourceNamesystem().getEffectiveLayoutVersion());\n \n       codec \u003d compression.getImageCodec();\n       if (codec !\u003d null) {\n         b.setCodec(codec.getClass().getCanonicalName());\n         sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n       } else {\n         sectionOutputStream \u003d underlyingOutputStream;\n       }\n \n       saveNameSystemSection(b);\n       // Check for cancellation right after serializing the name system section.\n       // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n       // depends on this behavior.\n       context.checkCancelled();\n \n       // Erasure coding policies should be saved before inodes\n       Step step \u003d new Step(StepType.ERASURE_CODING_POLICIES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveErasureCodingSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.INODES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveInodes(b);\n-      saveSnapshots(b);\n+      long numErrors \u003d saveSnapshots(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveSecretManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.CACHE_POOLS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveCacheManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       saveStringTableSection(b);\n \n       // We use the underlyingOutputStream to write the header. Therefore flush\n       // the buffered stream (which is potentially compressed) first.\n       flushSectionOutputStream();\n \n       FileSummary summary \u003d b.build();\n       saveFileSummary(underlyingOutputStream, summary);\n       underlyingOutputStream.close();\n       savedDigest \u003d new MD5Hash(digester.digest());\n+      return numErrors;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private long saveInternal(FileOutputStream fout,\n        FSImageCompression compression, String filePath) throws IOException {\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      MessageDigest digester \u003d MD5Hash.getDigester();\n\n      underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n          fout), digester);\n      underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n\n      fileChannel \u003d fout.getChannel();\n\n      FileSummary.Builder b \u003d FileSummary.newBuilder()\n          .setOndiskVersion(FSImageUtil.FILE_VERSION)\n          .setLayoutVersion(\n              context.getSourceNamesystem().getEffectiveLayoutVersion());\n\n      codec \u003d compression.getImageCodec();\n      if (codec !\u003d null) {\n        b.setCodec(codec.getClass().getCanonicalName());\n        sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n      } else {\n        sectionOutputStream \u003d underlyingOutputStream;\n      }\n\n      saveNameSystemSection(b);\n      // Check for cancellation right after serializing the name system section.\n      // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n      // depends on this behavior.\n      context.checkCancelled();\n\n      // Erasure coding policies should be saved before inodes\n      Step step \u003d new Step(StepType.ERASURE_CODING_POLICIES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveErasureCodingSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.INODES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveInodes(b);\n      long numErrors \u003d saveSnapshots(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveSecretManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.CACHE_POOLS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveCacheManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      saveStringTableSection(b);\n\n      // We use the underlyingOutputStream to write the header. Therefore flush\n      // the buffered stream (which is potentially compressed) first.\n      flushSectionOutputStream();\n\n      FileSummary summary \u003d b.build();\n      saveFileSummary(underlyingOutputStream, summary);\n      underlyingOutputStream.close();\n      savedDigest \u003d new MD5Hash(digester.digest());\n      return numErrors;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java",
          "extendedDetails": {}
        }
      ]
    },
    "ae8f55b93243560bd891962d6c64320ddc62a7d7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7859. Erasure Coding: Persist erasure coding policies in NameNode. Contributed by Sammi Chen\n",
      "commitDate": "14/09/17 6:08 PM",
      "commitName": "ae8f55b93243560bd891962d6c64320ddc62a7d7",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "20/12/16 7:24 AM",
      "commitNameOld": "1b401f6a734df4e23a79b3bd89c816a1fc0de574",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 268.41,
      "commitsBetweenForRepo": 1619,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,63 @@\n     private void saveInternal(FileOutputStream fout,\n         FSImageCompression compression, String filePath) throws IOException {\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       MessageDigest digester \u003d MD5Hash.getDigester();\n \n       underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n           fout), digester);\n       underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n \n       fileChannel \u003d fout.getChannel();\n \n       FileSummary.Builder b \u003d FileSummary.newBuilder()\n           .setOndiskVersion(FSImageUtil.FILE_VERSION)\n           .setLayoutVersion(\n               context.getSourceNamesystem().getEffectiveLayoutVersion());\n \n       codec \u003d compression.getImageCodec();\n       if (codec !\u003d null) {\n         b.setCodec(codec.getClass().getCanonicalName());\n         sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n       } else {\n         sectionOutputStream \u003d underlyingOutputStream;\n       }\n \n       saveNameSystemSection(b);\n       // Check for cancellation right after serializing the name system section.\n       // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n       // depends on this behavior.\n       context.checkCancelled();\n \n-      Step step \u003d new Step(StepType.INODES, filePath);\n+      // Erasure coding policies should be saved before inodes\n+      Step step \u003d new Step(StepType.ERASURE_CODING_POLICIES, filePath);\n+      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n+      saveErasureCodingSection(b);\n+      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n+\n+      step \u003d new Step(StepType.INODES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveInodes(b);\n       saveSnapshots(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveSecretManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.CACHE_POOLS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveCacheManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       saveStringTableSection(b);\n \n       // We use the underlyingOutputStream to write the header. Therefore flush\n       // the buffered stream (which is potentially compressed) first.\n       flushSectionOutputStream();\n \n       FileSummary summary \u003d b.build();\n       saveFileSummary(underlyingOutputStream, summary);\n       underlyingOutputStream.close();\n       savedDigest \u003d new MD5Hash(digester.digest());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void saveInternal(FileOutputStream fout,\n        FSImageCompression compression, String filePath) throws IOException {\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      MessageDigest digester \u003d MD5Hash.getDigester();\n\n      underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n          fout), digester);\n      underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n\n      fileChannel \u003d fout.getChannel();\n\n      FileSummary.Builder b \u003d FileSummary.newBuilder()\n          .setOndiskVersion(FSImageUtil.FILE_VERSION)\n          .setLayoutVersion(\n              context.getSourceNamesystem().getEffectiveLayoutVersion());\n\n      codec \u003d compression.getImageCodec();\n      if (codec !\u003d null) {\n        b.setCodec(codec.getClass().getCanonicalName());\n        sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n      } else {\n        sectionOutputStream \u003d underlyingOutputStream;\n      }\n\n      saveNameSystemSection(b);\n      // Check for cancellation right after serializing the name system section.\n      // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n      // depends on this behavior.\n      context.checkCancelled();\n\n      // Erasure coding policies should be saved before inodes\n      Step step \u003d new Step(StepType.ERASURE_CODING_POLICIES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveErasureCodingSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.INODES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveInodes(b);\n      saveSnapshots(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveSecretManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.CACHE_POOLS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveCacheManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      saveStringTableSection(b);\n\n      // We use the underlyingOutputStream to write the header. Therefore flush\n      // the buffered stream (which is potentially compressed) first.\n      flushSectionOutputStream();\n\n      FileSummary summary \u003d b.build();\n      saveFileSummary(underlyingOutputStream, summary);\n      underlyingOutputStream.close();\n      savedDigest \u003d new MD5Hash(digester.digest());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java",
      "extendedDetails": {}
    },
    "71de367c5e80ea76d1e8d21f0216cd6b879dcee5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8432. Introduce a minimum compatible layout version to allow downgrade in more rolling upgrade use cases. Contributed by Chris Nauroth.\n",
      "commitDate": "06/06/15 9:43 AM",
      "commitName": "71de367c5e80ea76d1e8d21f0216cd6b879dcee5",
      "commitAuthor": "cnauroth",
      "commitDateOld": "03/06/15 1:54 AM",
      "commitNameOld": "e965dcec378cb807856372425307598792977604",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 3.33,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,57 @@\n     private void saveInternal(FileOutputStream fout,\n         FSImageCompression compression, String filePath) throws IOException {\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       MessageDigest digester \u003d MD5Hash.getDigester();\n \n       underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n           fout), digester);\n       underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n \n       fileChannel \u003d fout.getChannel();\n \n       FileSummary.Builder b \u003d FileSummary.newBuilder()\n           .setOndiskVersion(FSImageUtil.FILE_VERSION)\n-          .setLayoutVersion(NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);\n+          .setLayoutVersion(\n+              context.getSourceNamesystem().getEffectiveLayoutVersion());\n \n       codec \u003d compression.getImageCodec();\n       if (codec !\u003d null) {\n         b.setCodec(codec.getClass().getCanonicalName());\n         sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n       } else {\n         sectionOutputStream \u003d underlyingOutputStream;\n       }\n \n       saveNameSystemSection(b);\n       // Check for cancellation right after serializing the name system section.\n       // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n       // depends on this behavior.\n       context.checkCancelled();\n \n       Step step \u003d new Step(StepType.INODES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveInodes(b);\n       saveSnapshots(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveSecretManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.CACHE_POOLS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveCacheManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       saveStringTableSection(b);\n \n       // We use the underlyingOutputStream to write the header. Therefore flush\n       // the buffered stream (which is potentially compressed) first.\n       flushSectionOutputStream();\n \n       FileSummary summary \u003d b.build();\n       saveFileSummary(underlyingOutputStream, summary);\n       underlyingOutputStream.close();\n       savedDigest \u003d new MD5Hash(digester.digest());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void saveInternal(FileOutputStream fout,\n        FSImageCompression compression, String filePath) throws IOException {\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      MessageDigest digester \u003d MD5Hash.getDigester();\n\n      underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n          fout), digester);\n      underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n\n      fileChannel \u003d fout.getChannel();\n\n      FileSummary.Builder b \u003d FileSummary.newBuilder()\n          .setOndiskVersion(FSImageUtil.FILE_VERSION)\n          .setLayoutVersion(\n              context.getSourceNamesystem().getEffectiveLayoutVersion());\n\n      codec \u003d compression.getImageCodec();\n      if (codec !\u003d null) {\n        b.setCodec(codec.getClass().getCanonicalName());\n        sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n      } else {\n        sectionOutputStream \u003d underlyingOutputStream;\n      }\n\n      saveNameSystemSection(b);\n      // Check for cancellation right after serializing the name system section.\n      // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n      // depends on this behavior.\n      context.checkCancelled();\n\n      Step step \u003d new Step(StepType.INODES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveInodes(b);\n      saveSnapshots(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveSecretManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.CACHE_POOLS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveCacheManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      saveStringTableSection(b);\n\n      // We use the underlyingOutputStream to write the header. Therefore flush\n      // the buffered stream (which is potentially compressed) first.\n      flushSectionOutputStream();\n\n      FileSummary summary \u003d b.build();\n      saveFileSummary(underlyingOutputStream, summary);\n      underlyingOutputStream.close();\n      savedDigest \u003d new MD5Hash(digester.digest());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java",
      "extendedDetails": {}
    },
    "a795bc42d012bf75872ae412cb2644c2d80177e3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5494. Merge Protobuf-based-FSImage code from trunk - fix build break after merge. (Contributed by Jing Zhao)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1568517 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/02/14 12:56 PM",
      "commitName": "a795bc42d012bf75872ae412cb2644c2d80177e3",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "12/02/14 4:00 PM",
      "commitNameOld": "2624b20291629b4565ea45590b66f2c38f96df67",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.87,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n     private void saveInternal(FileOutputStream fout,\n         FSImageCompression compression, String filePath) throws IOException {\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       MessageDigest digester \u003d MD5Hash.getDigester();\n \n       underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n           fout), digester);\n       underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n \n       fileChannel \u003d fout.getChannel();\n \n       FileSummary.Builder b \u003d FileSummary.newBuilder()\n           .setOndiskVersion(FSImageUtil.FILE_VERSION)\n-          .setLayoutVersion(LayoutVersion.getCurrentLayoutVersion());\n+          .setLayoutVersion(NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);\n \n       codec \u003d compression.getImageCodec();\n       if (codec !\u003d null) {\n         b.setCodec(codec.getClass().getCanonicalName());\n         sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n       } else {\n         sectionOutputStream \u003d underlyingOutputStream;\n       }\n \n       saveNameSystemSection(b);\n       // Check for cancellation right after serializing the name system section.\n       // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n       // depends on this behavior.\n       context.checkCancelled();\n \n       Step step \u003d new Step(StepType.INODES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveInodes(b);\n       saveSnapshots(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveSecretManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.CACHE_POOLS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveCacheManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       saveStringTableSection(b);\n \n       // We use the underlyingOutputStream to write the header. Therefore flush\n       // the buffered stream (which is potentially compressed) first.\n       flushSectionOutputStream();\n \n       FileSummary summary \u003d b.build();\n       saveFileSummary(underlyingOutputStream, summary);\n       underlyingOutputStream.close();\n       savedDigest \u003d new MD5Hash(digester.digest());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void saveInternal(FileOutputStream fout,\n        FSImageCompression compression, String filePath) throws IOException {\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      MessageDigest digester \u003d MD5Hash.getDigester();\n\n      underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n          fout), digester);\n      underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n\n      fileChannel \u003d fout.getChannel();\n\n      FileSummary.Builder b \u003d FileSummary.newBuilder()\n          .setOndiskVersion(FSImageUtil.FILE_VERSION)\n          .setLayoutVersion(NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);\n\n      codec \u003d compression.getImageCodec();\n      if (codec !\u003d null) {\n        b.setCodec(codec.getClass().getCanonicalName());\n        sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n      } else {\n        sectionOutputStream \u003d underlyingOutputStream;\n      }\n\n      saveNameSystemSection(b);\n      // Check for cancellation right after serializing the name system section.\n      // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n      // depends on this behavior.\n      context.checkCancelled();\n\n      Step step \u003d new Step(StepType.INODES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveInodes(b);\n      saveSnapshots(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveSecretManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.CACHE_POOLS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveCacheManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      saveStringTableSection(b);\n\n      // We use the underlyingOutputStream to write the header. Therefore flush\n      // the buffered stream (which is potentially compressed) first.\n      flushSectionOutputStream();\n\n      FileSummary summary \u003d b.build();\n      saveFileSummary(underlyingOutputStream, summary);\n      underlyingOutputStream.close();\n      savedDigest \u003d new MD5Hash(digester.digest());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java",
      "extendedDetails": {}
    },
    "ea0b21af158016651cb77560778834eb95e6b68d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5933. Optimize the FSImage layout for ACLs. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4685@1567785 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 2:56 PM",
      "commitName": "ea0b21af158016651cb77560778834eb95e6b68d",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "10/02/14 10:25 PM",
      "commitNameOld": "d03acc756094a332f98167426a39db8faf38f450",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 1.69,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,56 @@\n     private void saveInternal(FileOutputStream fout,\n         FSImageCompression compression, String filePath) throws IOException {\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       MessageDigest digester \u003d MD5Hash.getDigester();\n \n       underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n           fout), digester);\n       underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n \n       fileChannel \u003d fout.getChannel();\n \n       FileSummary.Builder b \u003d FileSummary.newBuilder()\n           .setOndiskVersion(FSImageUtil.FILE_VERSION)\n           .setLayoutVersion(LayoutVersion.getCurrentLayoutVersion());\n \n       codec \u003d compression.getImageCodec();\n       if (codec !\u003d null) {\n         b.setCodec(codec.getClass().getCanonicalName());\n         sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n       } else {\n         sectionOutputStream \u003d underlyingOutputStream;\n       }\n \n       saveNameSystemSection(b);\n       // Check for cancellation right after serializing the name system section.\n       // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n       // depends on this behavior.\n       context.checkCancelled();\n \n       Step step \u003d new Step(StepType.INODES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveInodes(b);\n       saveSnapshots(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveSecretManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.CACHE_POOLS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveCacheManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n-      saveExtendedAclSection(b);\n       saveStringTableSection(b);\n \n       // We use the underlyingOutputStream to write the header. Therefore flush\n       // the buffered stream (which is potentially compressed) first.\n       flushSectionOutputStream();\n \n       FileSummary summary \u003d b.build();\n       saveFileSummary(underlyingOutputStream, summary);\n       underlyingOutputStream.close();\n       savedDigest \u003d new MD5Hash(digester.digest());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void saveInternal(FileOutputStream fout,\n        FSImageCompression compression, String filePath) throws IOException {\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      MessageDigest digester \u003d MD5Hash.getDigester();\n\n      underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n          fout), digester);\n      underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n\n      fileChannel \u003d fout.getChannel();\n\n      FileSummary.Builder b \u003d FileSummary.newBuilder()\n          .setOndiskVersion(FSImageUtil.FILE_VERSION)\n          .setLayoutVersion(LayoutVersion.getCurrentLayoutVersion());\n\n      codec \u003d compression.getImageCodec();\n      if (codec !\u003d null) {\n        b.setCodec(codec.getClass().getCanonicalName());\n        sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n      } else {\n        sectionOutputStream \u003d underlyingOutputStream;\n      }\n\n      saveNameSystemSection(b);\n      // Check for cancellation right after serializing the name system section.\n      // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n      // depends on this behavior.\n      context.checkCancelled();\n\n      Step step \u003d new Step(StepType.INODES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveInodes(b);\n      saveSnapshots(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveSecretManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.CACHE_POOLS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveCacheManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      saveStringTableSection(b);\n\n      // We use the underlyingOutputStream to write the header. Therefore flush\n      // the buffered stream (which is potentially compressed) first.\n      flushSectionOutputStream();\n\n      FileSummary summary \u003d b.build();\n      saveFileSummary(underlyingOutputStream, summary);\n      underlyingOutputStream.close();\n      savedDigest \u003d new MD5Hash(digester.digest());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java",
      "extendedDetails": {}
    },
    "d03acc756094a332f98167426a39db8faf38f450": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5914. Incorporate ACLs with the changes from HDFS-5698. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4685@1566991 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/02/14 10:25 PM",
      "commitName": "d03acc756094a332f98167426a39db8faf38f450",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "10/02/14 3:13 PM",
      "commitNameOld": "5c978a43c3052cc1466b23653c354399186b4e10",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 0.3,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,57 @@\n     private void saveInternal(FileOutputStream fout,\n         FSImageCompression compression, String filePath) throws IOException {\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       MessageDigest digester \u003d MD5Hash.getDigester();\n \n       underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n           fout), digester);\n       underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n \n       fileChannel \u003d fout.getChannel();\n \n       FileSummary.Builder b \u003d FileSummary.newBuilder()\n           .setOndiskVersion(FSImageUtil.FILE_VERSION)\n           .setLayoutVersion(LayoutVersion.getCurrentLayoutVersion());\n \n       codec \u003d compression.getImageCodec();\n       if (codec !\u003d null) {\n         b.setCodec(codec.getClass().getCanonicalName());\n         sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n       } else {\n         sectionOutputStream \u003d underlyingOutputStream;\n       }\n \n       saveNameSystemSection(b);\n       // Check for cancellation right after serializing the name system section.\n       // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n       // depends on this behavior.\n       context.checkCancelled();\n \n       Step step \u003d new Step(StepType.INODES, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveInodes(b);\n       saveSnapshots(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveSecretManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n       step \u003d new Step(StepType.CACHE_POOLS, filePath);\n       prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n       saveCacheManagerSection(b);\n       prog.endStep(Phase.SAVING_CHECKPOINT, step);\n \n+      saveExtendedAclSection(b);\n       saveStringTableSection(b);\n \n       // We use the underlyingOutputStream to write the header. Therefore flush\n       // the buffered stream (which is potentially compressed) first.\n       flushSectionOutputStream();\n \n       FileSummary summary \u003d b.build();\n       saveFileSummary(underlyingOutputStream, summary);\n       underlyingOutputStream.close();\n       savedDigest \u003d new MD5Hash(digester.digest());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void saveInternal(FileOutputStream fout,\n        FSImageCompression compression, String filePath) throws IOException {\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      MessageDigest digester \u003d MD5Hash.getDigester();\n\n      underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n          fout), digester);\n      underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n\n      fileChannel \u003d fout.getChannel();\n\n      FileSummary.Builder b \u003d FileSummary.newBuilder()\n          .setOndiskVersion(FSImageUtil.FILE_VERSION)\n          .setLayoutVersion(LayoutVersion.getCurrentLayoutVersion());\n\n      codec \u003d compression.getImageCodec();\n      if (codec !\u003d null) {\n        b.setCodec(codec.getClass().getCanonicalName());\n        sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n      } else {\n        sectionOutputStream \u003d underlyingOutputStream;\n      }\n\n      saveNameSystemSection(b);\n      // Check for cancellation right after serializing the name system section.\n      // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n      // depends on this behavior.\n      context.checkCancelled();\n\n      Step step \u003d new Step(StepType.INODES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveInodes(b);\n      saveSnapshots(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveSecretManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.CACHE_POOLS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveCacheManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      saveExtendedAclSection(b);\n      saveStringTableSection(b);\n\n      // We use the underlyingOutputStream to write the header. Therefore flush\n      // the buffered stream (which is potentially compressed) first.\n      flushSectionOutputStream();\n\n      FileSummary summary \u003d b.build();\n      saveFileSummary(underlyingOutputStream, summary);\n      underlyingOutputStream.close();\n      savedDigest \u003d new MD5Hash(digester.digest());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java",
      "extendedDetails": {}
    },
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,56 @@\n+    private void saveInternal(FileOutputStream fout,\n+        FSImageCompression compression, String filePath) throws IOException {\n+      StartupProgress prog \u003d NameNode.getStartupProgress();\n+      MessageDigest digester \u003d MD5Hash.getDigester();\n+\n+      underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n+          fout), digester);\n+      underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n+\n+      fileChannel \u003d fout.getChannel();\n+\n+      FileSummary.Builder b \u003d FileSummary.newBuilder()\n+          .setOndiskVersion(FSImageUtil.FILE_VERSION)\n+          .setLayoutVersion(LayoutVersion.getCurrentLayoutVersion());\n+\n+      codec \u003d compression.getImageCodec();\n+      if (codec !\u003d null) {\n+        b.setCodec(codec.getClass().getCanonicalName());\n+        sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n+      } else {\n+        sectionOutputStream \u003d underlyingOutputStream;\n+      }\n+\n+      saveNameSystemSection(b);\n+      // Check for cancellation right after serializing the name system section.\n+      // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n+      // depends on this behavior.\n+      context.checkCancelled();\n+\n+      Step step \u003d new Step(StepType.INODES, filePath);\n+      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n+      saveInodes(b);\n+      saveSnapshots(b);\n+      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n+\n+      step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n+      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n+      saveSecretManagerSection(b);\n+      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n+\n+      step \u003d new Step(StepType.CACHE_POOLS, filePath);\n+      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n+      saveCacheManagerSection(b);\n+      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n+\n+      saveStringTableSection(b);\n+\n+      // We use the underlyingOutputStream to write the header. Therefore flush\n+      // the buffered stream (which is potentially compressed) first.\n+      flushSectionOutputStream();\n+\n+      FileSummary summary \u003d b.build();\n+      saveFileSummary(underlyingOutputStream, summary);\n+      underlyingOutputStream.close();\n+      savedDigest \u003d new MD5Hash(digester.digest());\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void saveInternal(FileOutputStream fout,\n        FSImageCompression compression, String filePath) throws IOException {\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      MessageDigest digester \u003d MD5Hash.getDigester();\n\n      underlyingOutputStream \u003d new DigestOutputStream(new BufferedOutputStream(\n          fout), digester);\n      underlyingOutputStream.write(FSImageUtil.MAGIC_HEADER);\n\n      fileChannel \u003d fout.getChannel();\n\n      FileSummary.Builder b \u003d FileSummary.newBuilder()\n          .setOndiskVersion(FSImageUtil.FILE_VERSION)\n          .setLayoutVersion(LayoutVersion.getCurrentLayoutVersion());\n\n      codec \u003d compression.getImageCodec();\n      if (codec !\u003d null) {\n        b.setCodec(codec.getClass().getCanonicalName());\n        sectionOutputStream \u003d codec.createOutputStream(underlyingOutputStream);\n      } else {\n        sectionOutputStream \u003d underlyingOutputStream;\n      }\n\n      saveNameSystemSection(b);\n      // Check for cancellation right after serializing the name system section.\n      // Some unit tests, such as TestSaveNamespace#testCancelSaveNameSpace\n      // depends on this behavior.\n      context.checkCancelled();\n\n      Step step \u003d new Step(StepType.INODES, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveInodes(b);\n      saveSnapshots(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.DELEGATION_TOKENS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveSecretManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      step \u003d new Step(StepType.CACHE_POOLS, filePath);\n      prog.beginStep(Phase.SAVING_CHECKPOINT, step);\n      saveCacheManagerSection(b);\n      prog.endStep(Phase.SAVING_CHECKPOINT, step);\n\n      saveStringTableSection(b);\n\n      // We use the underlyingOutputStream to write the header. Therefore flush\n      // the buffered stream (which is potentially compressed) first.\n      flushSectionOutputStream();\n\n      FileSummary summary \u003d b.build();\n      saveFileSummary(underlyingOutputStream, summary);\n      underlyingOutputStream.close();\n      savedDigest \u003d new MD5Hash(digester.digest());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java"
    }
  }
}