{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeProtocolClientSideTranslatorPB.java",
  "functionName": "blockReport",
  "functionId": "blockReport___registration-DatanodeRegistration__poolId-String__reports-StorageBlockReport[]__context-BlockReportContext",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
  "functionStartLine": 190,
  "functionEndLine": 223,
  "numCommitsSeen": 91,
  "timeTaken": 4273,
  "changeHistory": [
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "50ee8f4e67a66aa77c5359182f61f3e951844db6",
    "d324164a51a43d72c02567248bd9f0f12b244a40",
    "9e108e61fb28244326d7cf4bb31d175eb75d2636",
    "28eadb7cd71e99d563fb5c41aec563ab11e293e5",
    "f88574acdefae2816236bf6180916be96c6a6874",
    "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c",
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
    "b5229fd19bfecc2e5249db652ad34ec08152334b",
    "3001a172c8868763f8e59e866e36f7f50dee62cc",
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
    "38a19bc293dec6221ae96e304fc6ab660d94e706"
  ],
  "changeHistoryShort": {
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ybodychange",
    "50ee8f4e67a66aa77c5359182f61f3e951844db6": "Ymultichange(Yparameterchange,Ybodychange)",
    "d324164a51a43d72c02567248bd9f0f12b244a40": "Ybodychange",
    "9e108e61fb28244326d7cf4bb31d175eb75d2636": "Ybodychange",
    "28eadb7cd71e99d563fb5c41aec563ab11e293e5": "Ymultichange(Yparameterchange,Ybodychange)",
    "f88574acdefae2816236bf6180916be96c6a6874": "Ybodychange",
    "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c": "Ybodychange",
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c": "Ybodychange",
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256": "Ybodychange",
    "b5229fd19bfecc2e5249db652ad34ec08152334b": "Ybodychange",
    "3001a172c8868763f8e59e866e36f7f50dee62cc": "Ybodychange",
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e": "Ybodychange",
    "38a19bc293dec6221ae96e304fc6ab660d94e706": "Yintroduced"
  },
  "changeHistoryDetails": {
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "22/08/15 1:31 PM",
      "commitNameOld": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 30.22,
      "commitsBetweenForRepo": 176,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n       String poolId, StorageBlockReport[] reports, BlockReportContext context)\n         throws IOException {\n     BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n         .newBuilder().setRegistration(PBHelper.convert(registration))\n         .setBlockPoolId(poolId);\n     \n     boolean useBlocksBuffer \u003d registration.getNamespaceInfo()\n         .isCapabilitySupported(Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n \n     for (StorageBlockReport r : reports) {\n       StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n-          .newBuilder().setStorage(PBHelper.convert(r.getStorage()));\n+          .newBuilder().setStorage(PBHelperClient.convert(r.getStorage()));\n       BlockListAsLongs blocks \u003d r.getBlocks();\n       if (useBlocksBuffer) {\n         reportBuilder.setNumberOfBlocks(blocks.getNumberOfBlocks());\n         reportBuilder.addAllBlocksBuffers(blocks.getBlocksBuffers());\n       } else {\n         for (long value : blocks.getBlockListAsLongs()) {\n           reportBuilder.addBlocks(value);\n         }\n       }\n       builder.addReports(reportBuilder.build());\n     }\n     builder.setContext(PBHelper.convert(context));\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n     return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, StorageBlockReport[] reports, BlockReportContext context)\n        throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    \n    boolean useBlocksBuffer \u003d registration.getNamespaceInfo()\n        .isCapabilitySupported(Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n\n    for (StorageBlockReport r : reports) {\n      StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n          .newBuilder().setStorage(PBHelperClient.convert(r.getStorage()));\n      BlockListAsLongs blocks \u003d r.getBlocks();\n      if (useBlocksBuffer) {\n        reportBuilder.setNumberOfBlocks(blocks.getNumberOfBlocks());\n        reportBuilder.addAllBlocksBuffers(blocks.getBlocksBuffers());\n      } else {\n        for (long value : blocks.getBlockListAsLongs()) {\n          reportBuilder.addBlocks(value);\n        }\n      }\n      builder.addReports(reportBuilder.build());\n    }\n    builder.setContext(PBHelper.convert(context));\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "50ee8f4e67a66aa77c5359182f61f3e951844db6": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7960. The full block report should prune zombie storages even if they\u0027re not empty. Contributed by Colin McCabe and Eddy Xu.\n",
      "commitDate": "23/03/15 10:00 PM",
      "commitName": "50ee8f4e67a66aa77c5359182f61f3e951844db6",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7960. The full block report should prune zombie storages even if they\u0027re not empty. Contributed by Colin McCabe and Eddy Xu.\n",
          "commitDate": "23/03/15 10:00 PM",
          "commitName": "50ee8f4e67a66aa77c5359182f61f3e951844db6",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "13/03/15 12:23 PM",
          "commitNameOld": "d324164a51a43d72c02567248bd9f0f12b244a40",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 10.4,
          "commitsBetweenForRepo": 104,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,33 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n-      String poolId, StorageBlockReport[] reports) throws IOException {\n+      String poolId, StorageBlockReport[] reports, BlockReportContext context)\n+        throws IOException {\n     BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n         .newBuilder().setRegistration(PBHelper.convert(registration))\n         .setBlockPoolId(poolId);\n     \n     boolean useBlocksBuffer \u003d registration.getNamespaceInfo()\n         .isCapabilitySupported(Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n \n     for (StorageBlockReport r : reports) {\n       StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n           .newBuilder().setStorage(PBHelper.convert(r.getStorage()));\n       BlockListAsLongs blocks \u003d r.getBlocks();\n       if (useBlocksBuffer) {\n         reportBuilder.setNumberOfBlocks(blocks.getNumberOfBlocks());\n         reportBuilder.addAllBlocksBuffers(blocks.getBlocksBuffers());\n       } else {\n         for (long value : blocks.getBlockListAsLongs()) {\n           reportBuilder.addBlocks(value);\n         }\n       }\n       builder.addReports(reportBuilder.build());\n     }\n+    builder.setContext(PBHelper.convert(context));\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n     return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, StorageBlockReport[] reports, BlockReportContext context)\n        throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    \n    boolean useBlocksBuffer \u003d registration.getNamespaceInfo()\n        .isCapabilitySupported(Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n\n    for (StorageBlockReport r : reports) {\n      StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n          .newBuilder().setStorage(PBHelper.convert(r.getStorage()));\n      BlockListAsLongs blocks \u003d r.getBlocks();\n      if (useBlocksBuffer) {\n        reportBuilder.setNumberOfBlocks(blocks.getNumberOfBlocks());\n        reportBuilder.addAllBlocksBuffers(blocks.getBlocksBuffers());\n      } else {\n        for (long value : blocks.getBlockListAsLongs()) {\n          reportBuilder.addBlocks(value);\n        }\n      }\n      builder.addReports(reportBuilder.build());\n    }\n    builder.setContext(PBHelper.convert(context));\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
          "extendedDetails": {
            "oldValue": "[registration-DatanodeRegistration, poolId-String, reports-StorageBlockReport[]]",
            "newValue": "[registration-DatanodeRegistration, poolId-String, reports-StorageBlockReport[], context-BlockReportContext]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7960. The full block report should prune zombie storages even if they\u0027re not empty. Contributed by Colin McCabe and Eddy Xu.\n",
          "commitDate": "23/03/15 10:00 PM",
          "commitName": "50ee8f4e67a66aa77c5359182f61f3e951844db6",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "13/03/15 12:23 PM",
          "commitNameOld": "d324164a51a43d72c02567248bd9f0f12b244a40",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 10.4,
          "commitsBetweenForRepo": 104,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,33 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n-      String poolId, StorageBlockReport[] reports) throws IOException {\n+      String poolId, StorageBlockReport[] reports, BlockReportContext context)\n+        throws IOException {\n     BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n         .newBuilder().setRegistration(PBHelper.convert(registration))\n         .setBlockPoolId(poolId);\n     \n     boolean useBlocksBuffer \u003d registration.getNamespaceInfo()\n         .isCapabilitySupported(Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n \n     for (StorageBlockReport r : reports) {\n       StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n           .newBuilder().setStorage(PBHelper.convert(r.getStorage()));\n       BlockListAsLongs blocks \u003d r.getBlocks();\n       if (useBlocksBuffer) {\n         reportBuilder.setNumberOfBlocks(blocks.getNumberOfBlocks());\n         reportBuilder.addAllBlocksBuffers(blocks.getBlocksBuffers());\n       } else {\n         for (long value : blocks.getBlockListAsLongs()) {\n           reportBuilder.addBlocks(value);\n         }\n       }\n       builder.addReports(reportBuilder.build());\n     }\n+    builder.setContext(PBHelper.convert(context));\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n     return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, StorageBlockReport[] reports, BlockReportContext context)\n        throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    \n    boolean useBlocksBuffer \u003d registration.getNamespaceInfo()\n        .isCapabilitySupported(Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n\n    for (StorageBlockReport r : reports) {\n      StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n          .newBuilder().setStorage(PBHelper.convert(r.getStorage()));\n      BlockListAsLongs blocks \u003d r.getBlocks();\n      if (useBlocksBuffer) {\n        reportBuilder.setNumberOfBlocks(blocks.getNumberOfBlocks());\n        reportBuilder.addAllBlocksBuffers(blocks.getBlocksBuffers());\n      } else {\n        for (long value : blocks.getBlockListAsLongs()) {\n          reportBuilder.addBlocks(value);\n        }\n      }\n      builder.addReports(reportBuilder.build());\n    }\n    builder.setContext(PBHelper.convert(context));\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
          "extendedDetails": {}
        }
      ]
    },
    "d324164a51a43d72c02567248bd9f0f12b244a40": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7435. PB encoding of block reports is very inefficient. Contributed by Daryn Sharp.\n",
      "commitDate": "13/03/15 12:23 PM",
      "commitName": "d324164a51a43d72c02567248bd9f0f12b244a40",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "16/02/15 2:43 PM",
      "commitNameOld": "9729b244de50322c2cc889c97c2ffb2b4675cf77",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 24.86,
      "commitsBetweenForRepo": 203,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,31 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n       String poolId, StorageBlockReport[] reports) throws IOException {\n     BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n         .newBuilder().setRegistration(PBHelper.convert(registration))\n         .setBlockPoolId(poolId);\n     \n+    boolean useBlocksBuffer \u003d registration.getNamespaceInfo()\n+        .isCapabilitySupported(Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n+\n     for (StorageBlockReport r : reports) {\n       StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n           .newBuilder().setStorage(PBHelper.convert(r.getStorage()));\n-      long[] blocks \u003d r.getBlocks();\n-      for (int i \u003d 0; i \u003c blocks.length; i++) {\n-        reportBuilder.addBlocks(blocks[i]);\n+      BlockListAsLongs blocks \u003d r.getBlocks();\n+      if (useBlocksBuffer) {\n+        reportBuilder.setNumberOfBlocks(blocks.getNumberOfBlocks());\n+        reportBuilder.addAllBlocksBuffers(blocks.getBlocksBuffers());\n+      } else {\n+        for (long value : blocks.getBlockListAsLongs()) {\n+          reportBuilder.addBlocks(value);\n+        }\n       }\n       builder.addReports(reportBuilder.build());\n     }\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n     return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, StorageBlockReport[] reports) throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    \n    boolean useBlocksBuffer \u003d registration.getNamespaceInfo()\n        .isCapabilitySupported(Capability.STORAGE_BLOCK_REPORT_BUFFERS);\n\n    for (StorageBlockReport r : reports) {\n      StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n          .newBuilder().setStorage(PBHelper.convert(r.getStorage()));\n      BlockListAsLongs blocks \u003d r.getBlocks();\n      if (useBlocksBuffer) {\n        reportBuilder.setNumberOfBlocks(blocks.getNumberOfBlocks());\n        reportBuilder.addAllBlocksBuffers(blocks.getBlocksBuffers());\n      } else {\n        for (long value : blocks.getBlockListAsLongs()) {\n          reportBuilder.addBlocks(value);\n        }\n      }\n      builder.addReports(reportBuilder.build());\n    }\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "9e108e61fb28244326d7cf4bb31d175eb75d2636": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3086. Change Datanode not to send storage list in registration.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303318 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/03/12 2:07 AM",
      "commitName": "9e108e61fb28244326d7cf4bb31d175eb75d2636",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "19/03/12 3:09 PM",
      "commitNameOld": "6326605acb5a5bf48d994278c9d3a39733679e81",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.46,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n       String poolId, StorageBlockReport[] reports) throws IOException {\n     BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n         .newBuilder().setRegistration(PBHelper.convert(registration))\n         .setBlockPoolId(poolId);\n     \n     for (StorageBlockReport r : reports) {\n       StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n-          .newBuilder().setStorageID(r.getStorageID());\n+          .newBuilder().setStorage(PBHelper.convert(r.getStorage()));\n       long[] blocks \u003d r.getBlocks();\n       for (int i \u003d 0; i \u003c blocks.length; i++) {\n         reportBuilder.addBlocks(blocks[i]);\n       }\n       builder.addReports(reportBuilder.build());\n     }\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n     return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, StorageBlockReport[] reports) throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    \n    for (StorageBlockReport r : reports) {\n      StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n          .newBuilder().setStorage(PBHelper.convert(r.getStorage()));\n      long[] blocks \u003d r.getBlocks();\n      for (int i \u003d 0; i \u003c blocks.length; i++) {\n        reportBuilder.addBlocks(blocks[i]);\n      }\n      builder.addReports(reportBuilder.build());\n    }\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "28eadb7cd71e99d563fb5c41aec563ab11e293e5": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-2899. Service protocol changes in DatanodeProtocol to add multiple storages. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241519 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/02/12 8:59 AM",
      "commitName": "28eadb7cd71e99d563fb5c41aec563ab11e293e5",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2899. Service protocol changes in DatanodeProtocol to add multiple storages. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241519 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/02/12 8:59 AM",
          "commitName": "28eadb7cd71e99d563fb5c41aec563ab11e293e5",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "06/02/12 5:33 AM",
          "commitNameOld": "2a9e430ff9327ad311db7954400ff664ae66ec45",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 1.14,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,23 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n-      String poolId, long[] blocks) throws IOException {\n-    StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n-        .newBuilder().setStorageID(registration.getStorageID());\n+      String poolId, StorageBlockReport[] reports) throws IOException {\n+    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n+        .newBuilder().setRegistration(PBHelper.convert(registration))\n+        .setBlockPoolId(poolId);\n     \n-    if (blocks !\u003d null) {\n+    for (StorageBlockReport r : reports) {\n+      StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n+          .newBuilder().setStorageID(r.getStorageID());\n+      long[] blocks \u003d r.getBlocks();\n       for (int i \u003d 0; i \u003c blocks.length; i++) {\n         reportBuilder.addBlocks(blocks[i]);\n       }\n+      builder.addReports(reportBuilder.build());\n     }\n-    BlockReportRequestProto req \u003d BlockReportRequestProto\n-        .newBuilder().setRegistration(PBHelper.convert(registration))\n-        .setBlockPoolId(poolId).addReports(reportBuilder.build()).build();\n     BlockReportResponseProto resp;\n     try {\n-      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n+      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n     return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, StorageBlockReport[] reports) throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    \n    for (StorageBlockReport r : reports) {\n      StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n          .newBuilder().setStorageID(r.getStorageID());\n      long[] blocks \u003d r.getBlocks();\n      for (int i \u003d 0; i \u003c blocks.length; i++) {\n        reportBuilder.addBlocks(blocks[i]);\n      }\n      builder.addReports(reportBuilder.build());\n    }\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
          "extendedDetails": {
            "oldValue": "[registration-DatanodeRegistration, poolId-String, blocks-long[]]",
            "newValue": "[registration-DatanodeRegistration, poolId-String, reports-StorageBlockReport[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2899. Service protocol changes in DatanodeProtocol to add multiple storages. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241519 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/02/12 8:59 AM",
          "commitName": "28eadb7cd71e99d563fb5c41aec563ab11e293e5",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "06/02/12 5:33 AM",
          "commitNameOld": "2a9e430ff9327ad311db7954400ff664ae66ec45",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 1.14,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,23 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n-      String poolId, long[] blocks) throws IOException {\n-    StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n-        .newBuilder().setStorageID(registration.getStorageID());\n+      String poolId, StorageBlockReport[] reports) throws IOException {\n+    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n+        .newBuilder().setRegistration(PBHelper.convert(registration))\n+        .setBlockPoolId(poolId);\n     \n-    if (blocks !\u003d null) {\n+    for (StorageBlockReport r : reports) {\n+      StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n+          .newBuilder().setStorageID(r.getStorageID());\n+      long[] blocks \u003d r.getBlocks();\n       for (int i \u003d 0; i \u003c blocks.length; i++) {\n         reportBuilder.addBlocks(blocks[i]);\n       }\n+      builder.addReports(reportBuilder.build());\n     }\n-    BlockReportRequestProto req \u003d BlockReportRequestProto\n-        .newBuilder().setRegistration(PBHelper.convert(registration))\n-        .setBlockPoolId(poolId).addReports(reportBuilder.build()).build();\n     BlockReportResponseProto resp;\n     try {\n-      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n+      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n     return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, StorageBlockReport[] reports) throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    \n    for (StorageBlockReport r : reports) {\n      StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n          .newBuilder().setStorageID(r.getStorageID());\n      long[] blocks \u003d r.getBlocks();\n      for (int i \u003d 0; i \u003c blocks.length; i++) {\n        reportBuilder.addBlocks(blocks[i]);\n      }\n      builder.addReports(reportBuilder.build());\n    }\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, builder.build());\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
          "extendedDetails": {}
        }
      ]
    },
    "f88574acdefae2816236bf6180916be96c6a6874": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2880. Protobuf chagnes in DatanodeProtocol to add multiple storages. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1240653 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/02/12 5:39 PM",
      "commitName": "f88574acdefae2816236bf6180916be96c6a6874",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "28/01/12 6:01 PM",
      "commitNameOld": "98302971c240858c1c8018affb11bac453a83db2",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 6.98,
      "commitsBetweenForRepo": 56,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,21 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n       String poolId, long[] blocks) throws IOException {\n-    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n-        .newBuilder().setRegistration(PBHelper.convert(registration))\n-        .setBlockPoolId(poolId);\n+    StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n+        .newBuilder().setStorageID(registration.getStorageID());\n+    \n     if (blocks !\u003d null) {\n       for (int i \u003d 0; i \u003c blocks.length; i++) {\n-        builder.addBlocks(blocks[i]);\n+        reportBuilder.addBlocks(blocks[i]);\n       }\n     }\n-    BlockReportRequestProto req \u003d builder.build();\n+    BlockReportRequestProto req \u003d BlockReportRequestProto\n+        .newBuilder().setRegistration(PBHelper.convert(registration))\n+        .setBlockPoolId(poolId).addReports(reportBuilder.build()).build();\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n     return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, long[] blocks) throws IOException {\n    StorageBlockReportProto.Builder reportBuilder \u003d StorageBlockReportProto\n        .newBuilder().setStorageID(registration.getStorageID());\n    \n    if (blocks !\u003d null) {\n      for (int i \u003d 0; i \u003c blocks.length; i++) {\n        reportBuilder.addBlocks(blocks[i]);\n      }\n    }\n    BlockReportRequestProto req \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId).addReports(reportBuilder.build()).build();\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2661. Enable protobuf RPC for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1214033 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 6:15 PM",
      "commitName": "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c",
      "commitAuthor": "Jitendra Nath Pandey",
      "commitDateOld": "13/12/11 3:31 PM",
      "commitNameOld": "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.11,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n       String poolId, long[] blocks) throws IOException {\n     BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n         .newBuilder().setRegistration(PBHelper.convert(registration))\n         .setBlockPoolId(poolId);\n     if (blocks !\u003d null) {\n       for (int i \u003d 0; i \u003c blocks.length; i++) {\n-        builder.setBlocks(i, blocks[i]);\n+        builder.addBlocks(blocks[i]);\n       }\n     }\n     BlockReportRequestProto req \u003d builder.build();\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n     return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, long[] blocks) throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    if (blocks !\u003d null) {\n      for (int i \u003d 0; i \u003c blocks.length; i++) {\n        builder.addBlocks(blocks[i]);\n      }\n    }\n    BlockReportRequestProto req \u003d builder.build();\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Optional protobuf parameters are not handled correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213985 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:31 PM",
      "commitName": "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:27 PM",
      "commitNameOld": "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n       String poolId, long[] blocks) throws IOException {\n     BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n         .newBuilder().setRegistration(PBHelper.convert(registration))\n         .setBlockPoolId(poolId);\n     if (blocks !\u003d null) {\n       for (int i \u003d 0; i \u003c blocks.length; i++) {\n         builder.setBlocks(i, blocks[i]);\n       }\n     }\n     BlockReportRequestProto req \u003d builder.build();\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n-    return PBHelper.convert(resp.getCmd());\n+    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, long[] blocks) throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    if (blocks !\u003d null) {\n      for (int i \u003d 0; i \u003c blocks.length; i++) {\n        builder.setBlocks(i, blocks[i]);\n      }\n    }\n    BlockReportRequestProto req \u003d builder.build();\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256": {
      "type": "Ybodychange",
      "commitMessage": "Reverting the patch r1213981\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213984 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:27 PM",
      "commitName": "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:22 PM",
      "commitNameOld": "b5229fd19bfecc2e5249db652ad34ec08152334b",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n       String poolId, long[] blocks) throws IOException {\n     BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n         .newBuilder().setRegistration(PBHelper.convert(registration))\n         .setBlockPoolId(poolId);\n     if (blocks !\u003d null) {\n       for (int i \u003d 0; i \u003c blocks.length; i++) {\n         builder.setBlocks(i, blocks[i]);\n       }\n     }\n     BlockReportRequestProto req \u003d builder.build();\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n-    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n+    return PBHelper.convert(resp.getCmd());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, long[] blocks) throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    if (blocks !\u003d null) {\n      for (int i \u003d 0; i \u003c blocks.length; i++) {\n        builder.setBlocks(i, blocks[i]);\n      }\n    }\n    BlockReportRequestProto req \u003d builder.build();\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return PBHelper.convert(resp.getCmd());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "b5229fd19bfecc2e5249db652ad34ec08152334b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Optional protobuf parameters are not handled correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213981 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:22 PM",
      "commitName": "b5229fd19bfecc2e5249db652ad34ec08152334b",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:17 PM",
      "commitNameOld": "3001a172c8868763f8e59e866e36f7f50dee62cc",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n       String poolId, long[] blocks) throws IOException {\n     BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n         .newBuilder().setRegistration(PBHelper.convert(registration))\n         .setBlockPoolId(poolId);\n     if (blocks !\u003d null) {\n       for (int i \u003d 0; i \u003c blocks.length; i++) {\n         builder.setBlocks(i, blocks[i]);\n       }\n     }\n     BlockReportRequestProto req \u003d builder.build();\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n-    return PBHelper.convert(resp.getCmd());\n+    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, long[] blocks) throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    if (blocks !\u003d null) {\n      for (int i \u003d 0; i \u003c blocks.length; i++) {\n        builder.setBlocks(i, blocks[i]);\n      }\n    }\n    BlockReportRequestProto req \u003d builder.build();\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "3001a172c8868763f8e59e866e36f7f50dee62cc": {
      "type": "Ybodychange",
      "commitMessage": "Reverting r1213512 because it committed changes that were not part of the patch.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213980 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:17 PM",
      "commitName": "3001a172c8868763f8e59e866e36f7f50dee62cc",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "12/12/11 4:21 PM",
      "commitNameOld": "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.96,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n       String poolId, long[] blocks) throws IOException {\n     BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n         .newBuilder().setRegistration(PBHelper.convert(registration))\n         .setBlockPoolId(poolId);\n     if (blocks !\u003d null) {\n       for (int i \u003d 0; i \u003c blocks.length; i++) {\n         builder.setBlocks(i, blocks[i]);\n       }\n     }\n     BlockReportRequestProto req \u003d builder.build();\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n-    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n+    return PBHelper.convert(resp.getCmd());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, long[] blocks) throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    if (blocks !\u003d null) {\n      for (int i \u003d 0; i \u003c blocks.length; i++) {\n        builder.setBlocks(i, blocks[i]);\n      }\n    }\n    BlockReportRequestProto req \u003d builder.build();\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return PBHelper.convert(resp.getCmd());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Handle protobuf optional parameters correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213512 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/12/11 4:21 PM",
      "commitName": "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "11/12/11 9:36 PM",
      "commitNameOld": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthorOld": "Sanjay Radia",
      "daysBetweenCommits": 0.78,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   public DatanodeCommand blockReport(DatanodeRegistration registration,\n       String poolId, long[] blocks) throws IOException {\n     BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n         .newBuilder().setRegistration(PBHelper.convert(registration))\n         .setBlockPoolId(poolId);\n     if (blocks !\u003d null) {\n       for (int i \u003d 0; i \u003c blocks.length; i++) {\n         builder.setBlocks(i, blocks[i]);\n       }\n     }\n     BlockReportRequestProto req \u003d builder.build();\n     BlockReportResponseProto resp;\n     try {\n       resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n     } catch (ServiceException se) {\n       throw ProtobufHelper.getRemoteException(se);\n     }\n-    return PBHelper.convert(resp.getCmd());\n+    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, long[] blocks) throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    if (blocks !\u003d null) {\n      for (int i \u003d 0; i \u003c blocks.length; i++) {\n        builder.setBlocks(i, blocks[i]);\n      }\n    }\n    BlockReportRequestProto req \u003d builder.build();\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return resp.hasCmd() ? PBHelper.convert(resp.getCmd()) : null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "38a19bc293dec6221ae96e304fc6ab660d94e706": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/12/11 12:02 PM",
      "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
      "commitAuthor": "Jitendra Nath Pandey",
      "diff": "@@ -0,0 +1,19 @@\n+  public DatanodeCommand blockReport(DatanodeRegistration registration,\n+      String poolId, long[] blocks) throws IOException {\n+    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n+        .newBuilder().setRegistration(PBHelper.convert(registration))\n+        .setBlockPoolId(poolId);\n+    if (blocks !\u003d null) {\n+      for (int i \u003d 0; i \u003c blocks.length; i++) {\n+        builder.setBlocks(i, blocks[i]);\n+      }\n+    }\n+    BlockReportRequestProto req \u003d builder.build();\n+    BlockReportResponseProto resp;\n+    try {\n+      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n+    } catch (ServiceException se) {\n+      throw ProtobufHelper.getRemoteException(se);\n+    }\n+    return PBHelper.convert(resp.getCmd());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeCommand blockReport(DatanodeRegistration registration,\n      String poolId, long[] blocks) throws IOException {\n    BlockReportRequestProto.Builder builder \u003d BlockReportRequestProto\n        .newBuilder().setRegistration(PBHelper.convert(registration))\n        .setBlockPoolId(poolId);\n    if (blocks !\u003d null) {\n      for (int i \u003d 0; i \u003c blocks.length; i++) {\n        builder.setBlocks(i, blocks[i]);\n      }\n    }\n    BlockReportRequestProto req \u003d builder.build();\n    BlockReportResponseProto resp;\n    try {\n      resp \u003d rpcProxy.blockReport(NULL_CONTROLLER, req);\n    } catch (ServiceException se) {\n      throw ProtobufHelper.getRemoteException(se);\n    }\n    return PBHelper.convert(resp.getCmd());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java"
    }
  }
}