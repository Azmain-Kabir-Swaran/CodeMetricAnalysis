{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FiCaSchedulerApp.java",
  "functionName": "getNodeIdToUnreserve",
  "functionId": "getNodeIdToUnreserve___schedulerKey-SchedulerRequestKey__resourceNeedUnreserve-Resource__resourceCalculator-ResourceCalculator",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
  "functionStartLine": 836,
  "functionEndLine": 862,
  "numCommitsSeen": 228,
  "timeTaken": 8132,
  "changeHistory": [
    "2064ca015d1584263aac0cc20c60b925a3aff612",
    "e81596d06d226f1cfa44b2390ce3095ed4dee621",
    "eac6b4c35c50e555c2f1b5f913bb2c4d839f1ff4",
    "de3b4aac561258ad242a3c5ed1c919428893fd4c",
    "b8a30f2f170ffbd590e7366c3c944ab4919e40df",
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
    "89cab1ba5f0671f8ef30dbe7432079c18362b434",
    "487374b7fe0c92fc7eb1406c568952722b5d5b15",
    "9c22065109a77681bc2534063eabe8692fbcb3cd"
  ],
  "changeHistoryShort": {
    "2064ca015d1584263aac0cc20c60b925a3aff612": "Ybodychange",
    "e81596d06d226f1cfa44b2390ce3095ed4dee621": "Ymultichange(Yparameterchange,Ybodychange)",
    "eac6b4c35c50e555c2f1b5f913bb2c4d839f1ff4": "Ybodychange",
    "de3b4aac561258ad242a3c5ed1c919428893fd4c": "Ybodychange",
    "b8a30f2f170ffbd590e7366c3c944ab4919e40df": "Ymultichange(Ymodifierchange,Ybodychange)",
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828": "Ymultichange(Yparameterchange,Ybodychange)",
    "89cab1ba5f0671f8ef30dbe7432079c18362b434": "Ybodychange",
    "487374b7fe0c92fc7eb1406c568952722b5d5b15": "Ymultichange(Yparameterchange,Ybodychange)",
    "9c22065109a77681bc2534063eabe8692fbcb3cd": "Yintroduced"
  },
  "changeHistoryDetails": {
    "2064ca015d1584263aac0cc20c60b925a3aff612": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9349.  Changed logging to use slf4j api.\n            Contributed by Prabhu Joseph\n",
      "commitDate": "15/03/19 4:20 PM",
      "commitName": "2064ca015d1584263aac0cc20c60b925a3aff612",
      "commitAuthor": "Eric Yang",
      "commitDateOld": "07/03/19 1:47 PM",
      "commitNameOld": "39b4a37e02e929a698fcf9e32f1f71bb6b977635",
      "commitAuthorOld": "Eric Yang",
      "daysBetweenCommits": 8.06,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,27 @@\n   public NodeId getNodeIdToUnreserve(SchedulerRequestKey schedulerKey,\n       Resource resourceNeedUnreserve, ResourceCalculator resourceCalculator) {\n     // first go around make this algorithm simple and just grab first\n     // reservation that has enough resources\n     Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n         schedulerKey);\n \n     if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n       for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n           .entrySet()) {\n         NodeId nodeId \u003d entry.getKey();\n         RMContainer reservedContainer \u003d entry.getValue();\n         Resource reservedResource \u003d reservedContainer.getReservedResource();\n \n         // make sure we unreserve one with at least the same amount of\n         // resources, otherwise could affect capacity limits\n         if (Resources.fitsIn(resourceCalculator, resourceNeedUnreserve,\n             reservedResource)) {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\n-                \"unreserving node with reservation size: \" + reservedResource\n-                    + \" in order to allocate container with size: \"\n-                    + resourceNeedUnreserve);\n-          }\n+          LOG.debug(\"unreserving node with reservation size: {} in order to\"\n+              + \" allocate container with size: {}\", reservedResource,\n+              resourceNeedUnreserve);\n           return nodeId;\n         }\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NodeId getNodeIdToUnreserve(SchedulerRequestKey schedulerKey,\n      Resource resourceNeedUnreserve, ResourceCalculator resourceCalculator) {\n    // first go around make this algorithm simple and just grab first\n    // reservation that has enough resources\n    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n        schedulerKey);\n\n    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n          .entrySet()) {\n        NodeId nodeId \u003d entry.getKey();\n        RMContainer reservedContainer \u003d entry.getValue();\n        Resource reservedResource \u003d reservedContainer.getReservedResource();\n\n        // make sure we unreserve one with at least the same amount of\n        // resources, otherwise could affect capacity limits\n        if (Resources.fitsIn(resourceCalculator, resourceNeedUnreserve,\n            reservedResource)) {\n          LOG.debug(\"unreserving node with reservation size: {} in order to\"\n              + \" allocate container with size: {}\", reservedResource,\n              resourceNeedUnreserve);\n          return nodeId;\n        }\n      }\n    }\n    return null;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
      "extendedDetails": {}
    },
    "e81596d06d226f1cfa44b2390ce3095ed4dee621": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-7172. ResourceCalculator.fitsIn() should not take a cluster resource parameter. (Sen Zhao via wangda)\n\nChange-Id: Icc3670c9381ce7591ca69ec12da5aa52d3612d34\n",
      "commitDate": "17/09/17 9:20 PM",
      "commitName": "e81596d06d226f1cfa44b2390ce3095ed4dee621",
      "commitAuthor": "Wangda Tan",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-7172. ResourceCalculator.fitsIn() should not take a cluster resource parameter. (Sen Zhao via wangda)\n\nChange-Id: Icc3670c9381ce7591ca69ec12da5aa52d3612d34\n",
          "commitDate": "17/09/17 9:20 PM",
          "commitName": "e81596d06d226f1cfa44b2390ce3095ed4dee621",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "03/08/17 6:57 AM",
          "commitNameOld": "f64cfeaf61ec65a465decdd8215f567d4e6677a9",
          "commitAuthorOld": "Sunil G",
          "daysBetweenCommits": 45.6,
          "commitsBetweenForRepo": 408,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,30 @@\n-  public NodeId getNodeIdToUnreserve(\n-      SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n-      ResourceCalculator rc, Resource clusterResource) {\n+  public NodeId getNodeIdToUnreserve(SchedulerRequestKey schedulerKey,\n+      Resource resourceNeedUnreserve, ResourceCalculator resourceCalculator) {\n     // first go around make this algorithm simple and just grab first\n     // reservation that has enough resources\n     Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n         schedulerKey);\n \n     if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n       for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n           .entrySet()) {\n         NodeId nodeId \u003d entry.getKey();\n         RMContainer reservedContainer \u003d entry.getValue();\n         Resource reservedResource \u003d reservedContainer.getReservedResource();\n \n         // make sure we unreserve one with at least the same amount of\n         // resources, otherwise could affect capacity limits\n-        if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n+        if (Resources.fitsIn(resourceCalculator, resourceNeedUnreserve,\n             reservedResource)) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\n                 \"unreserving node with reservation size: \" + reservedResource\n                     + \" in order to allocate container with size: \"\n                     + resourceNeedUnreserve);\n           }\n           return nodeId;\n         }\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public NodeId getNodeIdToUnreserve(SchedulerRequestKey schedulerKey,\n      Resource resourceNeedUnreserve, ResourceCalculator resourceCalculator) {\n    // first go around make this algorithm simple and just grab first\n    // reservation that has enough resources\n    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n        schedulerKey);\n\n    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n          .entrySet()) {\n        NodeId nodeId \u003d entry.getKey();\n        RMContainer reservedContainer \u003d entry.getValue();\n        Resource reservedResource \u003d reservedContainer.getReservedResource();\n\n        // make sure we unreserve one with at least the same amount of\n        // resources, otherwise could affect capacity limits\n        if (Resources.fitsIn(resourceCalculator, resourceNeedUnreserve,\n            reservedResource)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\n                \"unreserving node with reservation size: \" + reservedResource\n                    + \" in order to allocate container with size: \"\n                    + resourceNeedUnreserve);\n          }\n          return nodeId;\n        }\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "[schedulerKey-SchedulerRequestKey, resourceNeedUnreserve-Resource, rc-ResourceCalculator, clusterResource-Resource]",
            "newValue": "[schedulerKey-SchedulerRequestKey, resourceNeedUnreserve-Resource, resourceCalculator-ResourceCalculator]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-7172. ResourceCalculator.fitsIn() should not take a cluster resource parameter. (Sen Zhao via wangda)\n\nChange-Id: Icc3670c9381ce7591ca69ec12da5aa52d3612d34\n",
          "commitDate": "17/09/17 9:20 PM",
          "commitName": "e81596d06d226f1cfa44b2390ce3095ed4dee621",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "03/08/17 6:57 AM",
          "commitNameOld": "f64cfeaf61ec65a465decdd8215f567d4e6677a9",
          "commitAuthorOld": "Sunil G",
          "daysBetweenCommits": 45.6,
          "commitsBetweenForRepo": 408,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,30 @@\n-  public NodeId getNodeIdToUnreserve(\n-      SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n-      ResourceCalculator rc, Resource clusterResource) {\n+  public NodeId getNodeIdToUnreserve(SchedulerRequestKey schedulerKey,\n+      Resource resourceNeedUnreserve, ResourceCalculator resourceCalculator) {\n     // first go around make this algorithm simple and just grab first\n     // reservation that has enough resources\n     Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n         schedulerKey);\n \n     if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n       for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n           .entrySet()) {\n         NodeId nodeId \u003d entry.getKey();\n         RMContainer reservedContainer \u003d entry.getValue();\n         Resource reservedResource \u003d reservedContainer.getReservedResource();\n \n         // make sure we unreserve one with at least the same amount of\n         // resources, otherwise could affect capacity limits\n-        if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n+        if (Resources.fitsIn(resourceCalculator, resourceNeedUnreserve,\n             reservedResource)) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\n                 \"unreserving node with reservation size: \" + reservedResource\n                     + \" in order to allocate container with size: \"\n                     + resourceNeedUnreserve);\n           }\n           return nodeId;\n         }\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public NodeId getNodeIdToUnreserve(SchedulerRequestKey schedulerKey,\n      Resource resourceNeedUnreserve, ResourceCalculator resourceCalculator) {\n    // first go around make this algorithm simple and just grab first\n    // reservation that has enough resources\n    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n        schedulerKey);\n\n    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n          .entrySet()) {\n        NodeId nodeId \u003d entry.getKey();\n        RMContainer reservedContainer \u003d entry.getValue();\n        Resource reservedResource \u003d reservedContainer.getReservedResource();\n\n        // make sure we unreserve one with at least the same amount of\n        // resources, otherwise could affect capacity limits\n        if (Resources.fitsIn(resourceCalculator, resourceNeedUnreserve,\n            reservedResource)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\n                \"unreserving node with reservation size: \" + reservedResource\n                    + \" in order to allocate container with size: \"\n                    + resourceNeedUnreserve);\n          }\n          return nodeId;\n        }\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {}
        }
      ]
    },
    "eac6b4c35c50e555c2f1b5f913bb2c4d839f1ff4": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6216. Unify Container Resizing code paths with Container Updates making it scheduler agnostic. (Arun Suresh via wangda)\n",
      "commitDate": "28/02/17 10:35 AM",
      "commitName": "eac6b4c35c50e555c2f1b5f913bb2c4d839f1ff4",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "09/02/17 10:23 AM",
      "commitNameOld": "5fb723bb77722d41df6959eee23e1b0cfeb5584e",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 19.01,
      "commitsBetweenForRepo": 112,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,31 @@\n   public NodeId getNodeIdToUnreserve(\n       SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n       ResourceCalculator rc, Resource clusterResource) {\n     // first go around make this algorithm simple and just grab first\n     // reservation that has enough resources\n     Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n         schedulerKey);\n \n     if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n       for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n           .entrySet()) {\n         NodeId nodeId \u003d entry.getKey();\n         RMContainer reservedContainer \u003d entry.getValue();\n-        if (reservedContainer.hasIncreaseReservation()) {\n-          // Currently, only regular container allocation supports continuous\n-          // reservation looking, we don\u0027t support canceling increase request\n-          // reservation when allocating regular container.\n-          continue;\n-        }\n-\n         Resource reservedResource \u003d reservedContainer.getReservedResource();\n \n         // make sure we unreserve one with at least the same amount of\n         // resources, otherwise could affect capacity limits\n         if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n             reservedResource)) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\n                 \"unreserving node with reservation size: \" + reservedResource\n                     + \" in order to allocate container with size: \"\n                     + resourceNeedUnreserve);\n           }\n           return nodeId;\n         }\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NodeId getNodeIdToUnreserve(\n      SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n      ResourceCalculator rc, Resource clusterResource) {\n    // first go around make this algorithm simple and just grab first\n    // reservation that has enough resources\n    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n        schedulerKey);\n\n    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n          .entrySet()) {\n        NodeId nodeId \u003d entry.getKey();\n        RMContainer reservedContainer \u003d entry.getValue();\n        Resource reservedResource \u003d reservedContainer.getReservedResource();\n\n        // make sure we unreserve one with at least the same amount of\n        // resources, otherwise could affect capacity limits\n        if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n            reservedResource)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\n                \"unreserving node with reservation size: \" + reservedResource\n                    + \" in order to allocate container with size: \"\n                    + resourceNeedUnreserve);\n          }\n          return nodeId;\n        }\n      }\n    }\n    return null;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
      "extendedDetails": {}
    },
    "de3b4aac561258ad242a3c5ed1c919428893fd4c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
      "commitDate": "07/11/16 10:14 AM",
      "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
      "commitAuthor": "Jian He",
      "commitDateOld": "31/10/16 3:18 PM",
      "commitNameOld": "90dd3a8148468ac37a3f2173ad8d45e38bfcb0c9",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 6.83,
      "commitsBetweenForRepo": 84,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,38 @@\n   public NodeId getNodeIdToUnreserve(\n       SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n       ResourceCalculator rc, Resource clusterResource) {\n-    try {\n-      writeLock.lock();\n-      // first go around make this algorithm simple and just grab first\n-      // reservation that has enough resources\n-      Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n-          schedulerKey);\n+    // first go around make this algorithm simple and just grab first\n+    // reservation that has enough resources\n+    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n+        schedulerKey);\n \n-      if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n-        for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n-            .entrySet()) {\n-          NodeId nodeId \u003d entry.getKey();\n-          RMContainer reservedContainer \u003d entry.getValue();\n-          if (reservedContainer.hasIncreaseReservation()) {\n-            // Currently, only regular container allocation supports continuous\n-            // reservation looking, we don\u0027t support canceling increase request\n-            // reservation when allocating regular container.\n-            continue;\n+    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n+      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n+          .entrySet()) {\n+        NodeId nodeId \u003d entry.getKey();\n+        RMContainer reservedContainer \u003d entry.getValue();\n+        if (reservedContainer.hasIncreaseReservation()) {\n+          // Currently, only regular container allocation supports continuous\n+          // reservation looking, we don\u0027t support canceling increase request\n+          // reservation when allocating regular container.\n+          continue;\n+        }\n+\n+        Resource reservedResource \u003d reservedContainer.getReservedResource();\n+\n+        // make sure we unreserve one with at least the same amount of\n+        // resources, otherwise could affect capacity limits\n+        if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n+            reservedResource)) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\n+                \"unreserving node with reservation size: \" + reservedResource\n+                    + \" in order to allocate container with size: \"\n+                    + resourceNeedUnreserve);\n           }\n-\n-          Resource reservedResource \u003d reservedContainer.getReservedResource();\n-\n-          // make sure we unreserve one with at least the same amount of\n-          // resources, otherwise could affect capacity limits\n-          if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n-              reservedResource)) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\n-                  \"unreserving node with reservation size: \" + reservedResource\n-                      + \" in order to allocate container with size: \"\n-                      + resourceNeedUnreserve);\n-            }\n-            return nodeId;\n-          }\n+          return nodeId;\n         }\n       }\n-      return null;\n-    } finally {\n-      writeLock.unlock();\n     }\n+    return null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NodeId getNodeIdToUnreserve(\n      SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n      ResourceCalculator rc, Resource clusterResource) {\n    // first go around make this algorithm simple and just grab first\n    // reservation that has enough resources\n    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n        schedulerKey);\n\n    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n          .entrySet()) {\n        NodeId nodeId \u003d entry.getKey();\n        RMContainer reservedContainer \u003d entry.getValue();\n        if (reservedContainer.hasIncreaseReservation()) {\n          // Currently, only regular container allocation supports continuous\n          // reservation looking, we don\u0027t support canceling increase request\n          // reservation when allocating regular container.\n          continue;\n        }\n\n        Resource reservedResource \u003d reservedContainer.getReservedResource();\n\n        // make sure we unreserve one with at least the same amount of\n        // resources, otherwise could affect capacity limits\n        if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n            reservedResource)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\n                \"unreserving node with reservation size: \" + reservedResource\n                    + \" in order to allocate container with size: \"\n                    + resourceNeedUnreserve);\n          }\n          return nodeId;\n        }\n      }\n    }\n    return null;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
      "extendedDetails": {}
    },
    "b8a30f2f170ffbd590e7366c3c944ab4919e40df": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "YARN-3141. Improve locks in SchedulerApplicationAttempt/FSAppAttempt/FiCaSchedulerApp. Contributed by Wangda Tan\n",
      "commitDate": "19/09/16 2:08 AM",
      "commitName": "b8a30f2f170ffbd590e7366c3c944ab4919e40df",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-3141. Improve locks in SchedulerApplicationAttempt/FSAppAttempt/FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "19/09/16 2:08 AM",
          "commitName": "b8a30f2f170ffbd590e7366c3c944ab4919e40df",
          "commitAuthor": "Jian He",
          "commitDateOld": "02/09/16 3:32 AM",
          "commitNameOld": "05f5c0f631680cffc36a79550c351620615445db",
          "commitAuthorOld": "Varun Vasudev",
          "daysBetweenCommits": 16.94,
          "commitsBetweenForRepo": 79,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,43 @@\n-  synchronized public NodeId getNodeIdToUnreserve(\n+  public NodeId getNodeIdToUnreserve(\n       SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n       ResourceCalculator rc, Resource clusterResource) {\n+    try {\n+      writeLock.lock();\n+      // first go around make this algorithm simple and just grab first\n+      // reservation that has enough resources\n+      Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n+          schedulerKey);\n \n-    // first go around make this algorithm simple and just grab first\n-    // reservation that has enough resources\n-    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n-        .get(schedulerKey);\n-\n-    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n-      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n-        NodeId nodeId \u003d entry.getKey();\n-        RMContainer reservedContainer \u003d entry.getValue();\n-        if (reservedContainer.hasIncreaseReservation()) {\n-          // Currently, only regular container allocation supports continuous\n-          // reservation looking, we don\u0027t support canceling increase request\n-          // reservation when allocating regular container.\n-          continue;\n-        }\n-        \n-        Resource reservedResource \u003d reservedContainer.getReservedResource();\n-        \n-        // make sure we unreserve one with at least the same amount of\n-        // resources, otherwise could affect capacity limits\n-        if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n-            reservedResource)) {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"unreserving node with reservation size: \"\n-                + reservedResource\n-                + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n+      if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n+        for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n+            .entrySet()) {\n+          NodeId nodeId \u003d entry.getKey();\n+          RMContainer reservedContainer \u003d entry.getValue();\n+          if (reservedContainer.hasIncreaseReservation()) {\n+            // Currently, only regular container allocation supports continuous\n+            // reservation looking, we don\u0027t support canceling increase request\n+            // reservation when allocating regular container.\n+            continue;\n           }\n-          return nodeId;\n+\n+          Resource reservedResource \u003d reservedContainer.getReservedResource();\n+\n+          // make sure we unreserve one with at least the same amount of\n+          // resources, otherwise could affect capacity limits\n+          if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n+              reservedResource)) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\n+                  \"unreserving node with reservation size: \" + reservedResource\n+                      + \" in order to allocate container with size: \"\n+                      + resourceNeedUnreserve);\n+            }\n+            return nodeId;\n+          }\n         }\n       }\n+      return null;\n+    } finally {\n+      writeLock.unlock();\n     }\n-    return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public NodeId getNodeIdToUnreserve(\n      SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n      ResourceCalculator rc, Resource clusterResource) {\n    try {\n      writeLock.lock();\n      // first go around make this algorithm simple and just grab first\n      // reservation that has enough resources\n      Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n          schedulerKey);\n\n      if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n        for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n            .entrySet()) {\n          NodeId nodeId \u003d entry.getKey();\n          RMContainer reservedContainer \u003d entry.getValue();\n          if (reservedContainer.hasIncreaseReservation()) {\n            // Currently, only regular container allocation supports continuous\n            // reservation looking, we don\u0027t support canceling increase request\n            // reservation when allocating regular container.\n            continue;\n          }\n\n          Resource reservedResource \u003d reservedContainer.getReservedResource();\n\n          // make sure we unreserve one with at least the same amount of\n          // resources, otherwise could affect capacity limits\n          if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n              reservedResource)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\n                  \"unreserving node with reservation size: \" + reservedResource\n                      + \" in order to allocate container with size: \"\n                      + resourceNeedUnreserve);\n            }\n            return nodeId;\n          }\n        }\n      }\n      return null;\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3141. Improve locks in SchedulerApplicationAttempt/FSAppAttempt/FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "19/09/16 2:08 AM",
          "commitName": "b8a30f2f170ffbd590e7366c3c944ab4919e40df",
          "commitAuthor": "Jian He",
          "commitDateOld": "02/09/16 3:32 AM",
          "commitNameOld": "05f5c0f631680cffc36a79550c351620615445db",
          "commitAuthorOld": "Varun Vasudev",
          "daysBetweenCommits": 16.94,
          "commitsBetweenForRepo": 79,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,43 @@\n-  synchronized public NodeId getNodeIdToUnreserve(\n+  public NodeId getNodeIdToUnreserve(\n       SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n       ResourceCalculator rc, Resource clusterResource) {\n+    try {\n+      writeLock.lock();\n+      // first go around make this algorithm simple and just grab first\n+      // reservation that has enough resources\n+      Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n+          schedulerKey);\n \n-    // first go around make this algorithm simple and just grab first\n-    // reservation that has enough resources\n-    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n-        .get(schedulerKey);\n-\n-    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n-      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n-        NodeId nodeId \u003d entry.getKey();\n-        RMContainer reservedContainer \u003d entry.getValue();\n-        if (reservedContainer.hasIncreaseReservation()) {\n-          // Currently, only regular container allocation supports continuous\n-          // reservation looking, we don\u0027t support canceling increase request\n-          // reservation when allocating regular container.\n-          continue;\n-        }\n-        \n-        Resource reservedResource \u003d reservedContainer.getReservedResource();\n-        \n-        // make sure we unreserve one with at least the same amount of\n-        // resources, otherwise could affect capacity limits\n-        if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n-            reservedResource)) {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"unreserving node with reservation size: \"\n-                + reservedResource\n-                + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n+      if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n+        for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n+            .entrySet()) {\n+          NodeId nodeId \u003d entry.getKey();\n+          RMContainer reservedContainer \u003d entry.getValue();\n+          if (reservedContainer.hasIncreaseReservation()) {\n+            // Currently, only regular container allocation supports continuous\n+            // reservation looking, we don\u0027t support canceling increase request\n+            // reservation when allocating regular container.\n+            continue;\n           }\n-          return nodeId;\n+\n+          Resource reservedResource \u003d reservedContainer.getReservedResource();\n+\n+          // make sure we unreserve one with at least the same amount of\n+          // resources, otherwise could affect capacity limits\n+          if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n+              reservedResource)) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\n+                  \"unreserving node with reservation size: \" + reservedResource\n+                      + \" in order to allocate container with size: \"\n+                      + resourceNeedUnreserve);\n+            }\n+            return nodeId;\n+          }\n         }\n       }\n+      return null;\n+    } finally {\n+      writeLock.unlock();\n     }\n-    return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public NodeId getNodeIdToUnreserve(\n      SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n      ResourceCalculator rc, Resource clusterResource) {\n    try {\n      writeLock.lock();\n      // first go around make this algorithm simple and just grab first\n      // reservation that has enough resources\n      Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers.get(\n          schedulerKey);\n\n      if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n        for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers\n            .entrySet()) {\n          NodeId nodeId \u003d entry.getKey();\n          RMContainer reservedContainer \u003d entry.getValue();\n          if (reservedContainer.hasIncreaseReservation()) {\n            // Currently, only regular container allocation supports continuous\n            // reservation looking, we don\u0027t support canceling increase request\n            // reservation when allocating regular container.\n            continue;\n          }\n\n          Resource reservedResource \u003d reservedContainer.getReservedResource();\n\n          // make sure we unreserve one with at least the same amount of\n          // resources, otherwise could affect capacity limits\n          if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n              reservedResource)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\n                  \"unreserving node with reservation size: \" + reservedResource\n                      + \" in order to allocate container with size: \"\n                      + resourceNeedUnreserve);\n            }\n            return nodeId;\n          }\n        }\n      }\n      return null;\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {}
        }
      ]
    },
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
      "commitDate": "26/07/16 2:54 PM",
      "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
      "commitAuthor": "Arun Suresh",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
          "commitDate": "26/07/16 2:54 PM",
          "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "05/05/16 12:56 PM",
          "commitNameOld": "bb62e0592566b2fcae7136b30972aad2d3ac55b0",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 82.08,
          "commitsBetweenForRepo": 671,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,37 @@\n-  synchronized public NodeId getNodeIdToUnreserve(Priority priority,\n-      Resource resourceNeedUnreserve, ResourceCalculator rc,\n-      Resource clusterResource) {\n+  synchronized public NodeId getNodeIdToUnreserve(\n+      SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n+      ResourceCalculator rc, Resource clusterResource) {\n \n     // first go around make this algorithm simple and just grab first\n     // reservation that has enough resources\n     Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n-        .get(priority);\n+        .get(schedulerKey);\n \n     if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n       for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n         NodeId nodeId \u003d entry.getKey();\n         RMContainer reservedContainer \u003d entry.getValue();\n         if (reservedContainer.hasIncreaseReservation()) {\n           // Currently, only regular container allocation supports continuous\n           // reservation looking, we don\u0027t support canceling increase request\n           // reservation when allocating regular container.\n           continue;\n         }\n         \n         Resource reservedResource \u003d reservedContainer.getReservedResource();\n         \n         // make sure we unreserve one with at least the same amount of\n         // resources, otherwise could affect capacity limits\n         if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n             reservedResource)) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"unreserving node with reservation size: \"\n                 + reservedResource\n                 + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n           }\n           return nodeId;\n         }\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized public NodeId getNodeIdToUnreserve(\n      SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n      ResourceCalculator rc, Resource clusterResource) {\n\n    // first go around make this algorithm simple and just grab first\n    // reservation that has enough resources\n    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n        .get(schedulerKey);\n\n    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n        NodeId nodeId \u003d entry.getKey();\n        RMContainer reservedContainer \u003d entry.getValue();\n        if (reservedContainer.hasIncreaseReservation()) {\n          // Currently, only regular container allocation supports continuous\n          // reservation looking, we don\u0027t support canceling increase request\n          // reservation when allocating regular container.\n          continue;\n        }\n        \n        Resource reservedResource \u003d reservedContainer.getReservedResource();\n        \n        // make sure we unreserve one with at least the same amount of\n        // resources, otherwise could affect capacity limits\n        if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n            reservedResource)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"unreserving node with reservation size: \"\n                + reservedResource\n                + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n          }\n          return nodeId;\n        }\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "[priority-Priority, resourceNeedUnreserve-Resource, rc-ResourceCalculator, clusterResource-Resource]",
            "newValue": "[schedulerKey-SchedulerRequestKey, resourceNeedUnreserve-Resource, rc-ResourceCalculator, clusterResource-Resource]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
          "commitDate": "26/07/16 2:54 PM",
          "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "05/05/16 12:56 PM",
          "commitNameOld": "bb62e0592566b2fcae7136b30972aad2d3ac55b0",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 82.08,
          "commitsBetweenForRepo": 671,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,37 @@\n-  synchronized public NodeId getNodeIdToUnreserve(Priority priority,\n-      Resource resourceNeedUnreserve, ResourceCalculator rc,\n-      Resource clusterResource) {\n+  synchronized public NodeId getNodeIdToUnreserve(\n+      SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n+      ResourceCalculator rc, Resource clusterResource) {\n \n     // first go around make this algorithm simple and just grab first\n     // reservation that has enough resources\n     Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n-        .get(priority);\n+        .get(schedulerKey);\n \n     if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n       for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n         NodeId nodeId \u003d entry.getKey();\n         RMContainer reservedContainer \u003d entry.getValue();\n         if (reservedContainer.hasIncreaseReservation()) {\n           // Currently, only regular container allocation supports continuous\n           // reservation looking, we don\u0027t support canceling increase request\n           // reservation when allocating regular container.\n           continue;\n         }\n         \n         Resource reservedResource \u003d reservedContainer.getReservedResource();\n         \n         // make sure we unreserve one with at least the same amount of\n         // resources, otherwise could affect capacity limits\n         if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n             reservedResource)) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"unreserving node with reservation size: \"\n                 + reservedResource\n                 + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n           }\n           return nodeId;\n         }\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized public NodeId getNodeIdToUnreserve(\n      SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve,\n      ResourceCalculator rc, Resource clusterResource) {\n\n    // first go around make this algorithm simple and just grab first\n    // reservation that has enough resources\n    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n        .get(schedulerKey);\n\n    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n        NodeId nodeId \u003d entry.getKey();\n        RMContainer reservedContainer \u003d entry.getValue();\n        if (reservedContainer.hasIncreaseReservation()) {\n          // Currently, only regular container allocation supports continuous\n          // reservation looking, we don\u0027t support canceling increase request\n          // reservation when allocating regular container.\n          continue;\n        }\n        \n        Resource reservedResource \u003d reservedContainer.getReservedResource();\n        \n        // make sure we unreserve one with at least the same amount of\n        // resources, otherwise could affect capacity limits\n        if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n            reservedResource)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"unreserving node with reservation size: \"\n                + reservedResource\n                + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n          }\n          return nodeId;\n        }\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {}
        }
      ]
    },
    "89cab1ba5f0671f8ef30dbe7432079c18362b434": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1651. CapacityScheduler side changes to support container resize. Contributed by Wangda Tan\n",
      "commitDate": "23/09/15 1:29 PM",
      "commitName": "89cab1ba5f0671f8ef30dbe7432079c18362b434",
      "commitAuthor": "Jian He",
      "commitDateOld": "01/09/15 1:49 AM",
      "commitNameOld": "bf669b6d9f8ba165e30b8823218d625a49958925",
      "commitAuthorOld": "Varun Vasudev",
      "daysBetweenCommits": 22.49,
      "commitsBetweenForRepo": 147,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,37 @@\n   synchronized public NodeId getNodeIdToUnreserve(Priority priority,\n       Resource resourceNeedUnreserve, ResourceCalculator rc,\n       Resource clusterResource) {\n \n     // first go around make this algorithm simple and just grab first\n     // reservation that has enough resources\n     Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n         .get(priority);\n \n     if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n       for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n         NodeId nodeId \u003d entry.getKey();\n-        Resource containerResource \u003d entry.getValue().getContainer().getResource();\n+        RMContainer reservedContainer \u003d entry.getValue();\n+        if (reservedContainer.hasIncreaseReservation()) {\n+          // Currently, only regular container allocation supports continuous\n+          // reservation looking, we don\u0027t support canceling increase request\n+          // reservation when allocating regular container.\n+          continue;\n+        }\n+        \n+        Resource reservedResource \u003d reservedContainer.getReservedResource();\n         \n         // make sure we unreserve one with at least the same amount of\n         // resources, otherwise could affect capacity limits\n-        if (Resources.lessThanOrEqual(rc, clusterResource,\n-            resourceNeedUnreserve, containerResource)) {\n+        if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n+            reservedResource)) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"unreserving node with reservation size: \"\n-                + containerResource\n+                + reservedResource\n                 + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n           }\n           return nodeId;\n         }\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized public NodeId getNodeIdToUnreserve(Priority priority,\n      Resource resourceNeedUnreserve, ResourceCalculator rc,\n      Resource clusterResource) {\n\n    // first go around make this algorithm simple and just grab first\n    // reservation that has enough resources\n    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n        .get(priority);\n\n    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n        NodeId nodeId \u003d entry.getKey();\n        RMContainer reservedContainer \u003d entry.getValue();\n        if (reservedContainer.hasIncreaseReservation()) {\n          // Currently, only regular container allocation supports continuous\n          // reservation looking, we don\u0027t support canceling increase request\n          // reservation when allocating regular container.\n          continue;\n        }\n        \n        Resource reservedResource \u003d reservedContainer.getReservedResource();\n        \n        // make sure we unreserve one with at least the same amount of\n        // resources, otherwise could affect capacity limits\n        if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve,\n            reservedResource)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"unreserving node with reservation size: \"\n                + reservedResource\n                + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n          }\n          return nodeId;\n        }\n      }\n    }\n    return null;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
      "extendedDetails": {}
    },
    "487374b7fe0c92fc7eb1406c568952722b5d5b15": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
      "commitDate": "17/03/15 10:24 AM",
      "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
          "commitDate": "17/03/15 10:24 AM",
          "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
          "commitAuthor": "Jian He",
          "commitDateOld": "13/01/15 5:32 PM",
          "commitNameOld": "c53420f58364b11fbda1dace7679d45534533382",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 62.66,
          "commitsBetweenForRepo": 562,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,29 @@\n   synchronized public NodeId getNodeIdToUnreserve(Priority priority,\n-      Resource capability) {\n+      Resource resourceNeedUnreserve, ResourceCalculator rc,\n+      Resource clusterResource) {\n \n     // first go around make this algorithm simple and just grab first\n     // reservation that has enough resources\n     Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n         .get(priority);\n \n     if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n       for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n+        NodeId nodeId \u003d entry.getKey();\n+        Resource containerResource \u003d entry.getValue().getContainer().getResource();\n+        \n         // make sure we unreserve one with at least the same amount of\n         // resources, otherwise could affect capacity limits\n-        if (Resources.fitsIn(capability, entry.getValue().getContainer()\n-            .getResource())) {\n+        if (Resources.lessThanOrEqual(rc, clusterResource,\n+            resourceNeedUnreserve, containerResource)) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"unreserving node with reservation size: \"\n-                + entry.getValue().getContainer().getResource()\n-                + \" in order to allocate container with size: \" + capability);\n+                + containerResource\n+                + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n           }\n-          return entry.getKey();\n+          return nodeId;\n         }\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized public NodeId getNodeIdToUnreserve(Priority priority,\n      Resource resourceNeedUnreserve, ResourceCalculator rc,\n      Resource clusterResource) {\n\n    // first go around make this algorithm simple and just grab first\n    // reservation that has enough resources\n    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n        .get(priority);\n\n    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n        NodeId nodeId \u003d entry.getKey();\n        Resource containerResource \u003d entry.getValue().getContainer().getResource();\n        \n        // make sure we unreserve one with at least the same amount of\n        // resources, otherwise could affect capacity limits\n        if (Resources.lessThanOrEqual(rc, clusterResource,\n            resourceNeedUnreserve, containerResource)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"unreserving node with reservation size: \"\n                + containerResource\n                + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n          }\n          return nodeId;\n        }\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "[priority-Priority, capability-Resource]",
            "newValue": "[priority-Priority, resourceNeedUnreserve-Resource, rc-ResourceCalculator, clusterResource-Resource]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
          "commitDate": "17/03/15 10:24 AM",
          "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
          "commitAuthor": "Jian He",
          "commitDateOld": "13/01/15 5:32 PM",
          "commitNameOld": "c53420f58364b11fbda1dace7679d45534533382",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 62.66,
          "commitsBetweenForRepo": 562,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,29 @@\n   synchronized public NodeId getNodeIdToUnreserve(Priority priority,\n-      Resource capability) {\n+      Resource resourceNeedUnreserve, ResourceCalculator rc,\n+      Resource clusterResource) {\n \n     // first go around make this algorithm simple and just grab first\n     // reservation that has enough resources\n     Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n         .get(priority);\n \n     if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n       for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n+        NodeId nodeId \u003d entry.getKey();\n+        Resource containerResource \u003d entry.getValue().getContainer().getResource();\n+        \n         // make sure we unreserve one with at least the same amount of\n         // resources, otherwise could affect capacity limits\n-        if (Resources.fitsIn(capability, entry.getValue().getContainer()\n-            .getResource())) {\n+        if (Resources.lessThanOrEqual(rc, clusterResource,\n+            resourceNeedUnreserve, containerResource)) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"unreserving node with reservation size: \"\n-                + entry.getValue().getContainer().getResource()\n-                + \" in order to allocate container with size: \" + capability);\n+                + containerResource\n+                + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n           }\n-          return entry.getKey();\n+          return nodeId;\n         }\n       }\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized public NodeId getNodeIdToUnreserve(Priority priority,\n      Resource resourceNeedUnreserve, ResourceCalculator rc,\n      Resource clusterResource) {\n\n    // first go around make this algorithm simple and just grab first\n    // reservation that has enough resources\n    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n        .get(priority);\n\n    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n        NodeId nodeId \u003d entry.getKey();\n        Resource containerResource \u003d entry.getValue().getContainer().getResource();\n        \n        // make sure we unreserve one with at least the same amount of\n        // resources, otherwise could affect capacity limits\n        if (Resources.lessThanOrEqual(rc, clusterResource,\n            resourceNeedUnreserve, containerResource)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"unreserving node with reservation size: \"\n                + containerResource\n                + \" in order to allocate container with size: \" + resourceNeedUnreserve);\n          }\n          return nodeId;\n        }\n      }\n    }\n    return null;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {}
        }
      ]
    },
    "9c22065109a77681bc2534063eabe8692fbcb3cd": {
      "type": "Yintroduced",
      "commitMessage": "YARN-1769. CapacityScheduler: Improve reservations. Contributed by Thomas Graves\n",
      "commitDate": "29/09/14 7:12 AM",
      "commitName": "9c22065109a77681bc2534063eabe8692fbcb3cd",
      "commitAuthor": "Jason Lowe",
      "diff": "@@ -0,0 +1,25 @@\n+  synchronized public NodeId getNodeIdToUnreserve(Priority priority,\n+      Resource capability) {\n+\n+    // first go around make this algorithm simple and just grab first\n+    // reservation that has enough resources\n+    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n+        .get(priority);\n+\n+    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n+      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n+        // make sure we unreserve one with at least the same amount of\n+        // resources, otherwise could affect capacity limits\n+        if (Resources.fitsIn(capability, entry.getValue().getContainer()\n+            .getResource())) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"unreserving node with reservation size: \"\n+                + entry.getValue().getContainer().getResource()\n+                + \" in order to allocate container with size: \" + capability);\n+          }\n+          return entry.getKey();\n+        }\n+      }\n+    }\n+    return null;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized public NodeId getNodeIdToUnreserve(Priority priority,\n      Resource capability) {\n\n    // first go around make this algorithm simple and just grab first\n    // reservation that has enough resources\n    Map\u003cNodeId, RMContainer\u003e reservedContainers \u003d this.reservedContainers\n        .get(priority);\n\n    if ((reservedContainers !\u003d null) \u0026\u0026 (!reservedContainers.isEmpty())) {\n      for (Map.Entry\u003cNodeId, RMContainer\u003e entry : reservedContainers.entrySet()) {\n        // make sure we unreserve one with at least the same amount of\n        // resources, otherwise could affect capacity limits\n        if (Resources.fitsIn(capability, entry.getValue().getContainer()\n            .getResource())) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"unreserving node with reservation size: \"\n                + entry.getValue().getContainer().getResource()\n                + \" in order to allocate container with size: \" + capability);\n          }\n          return entry.getKey();\n        }\n      }\n    }\n    return null;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java"
    }
  }
}