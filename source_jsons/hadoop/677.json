{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "sendTransferBlock",
  "functionId": "sendTransferBlock___targets-DatanodeInfo[](modifiers-final)__targetStorageTypes-StorageType[](modifiers-final)__targetStorageIDs-String[](modifiers-final)__blockToken-Token__BlockTokenIdentifier__(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 175,
  "functionEndLine": 191,
  "numCommitsSeen": 58,
  "timeTaken": 2663,
  "changeHistory": [
    "a3954ccab148bddc290cb96528e63ff19799bcc9",
    "627da6f7178e18aa41996969c408b6f344e297d1",
    "3ae652f82110a52bf239f3c1849b48981558eb19"
  ],
  "changeHistoryShort": {
    "a3954ccab148bddc290cb96528e63ff19799bcc9": "Ymultichange(Yparameterchange,Ybodychange)",
    "627da6f7178e18aa41996969c408b6f344e297d1": "Ybodychange",
    "3ae652f82110a52bf239f3c1849b48981558eb19": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a3954ccab148bddc290cb96528e63ff19799bcc9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
      "commitDate": "05/05/17 12:01 PM",
      "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthor": "Chris Douglas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "15/02/17 10:44 AM",
          "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 79.01,
          "commitsBetweenForRepo": 465,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,17 @@\n     void sendTransferBlock(final DatanodeInfo[] targets,\n         final StorageType[] targetStorageTypes,\n+        final String[] targetStorageIDs,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //send the TRANSFER_BLOCK request\n       new Sender(out).transferBlock(block.getCurrentBlock(), blockToken,\n-          dfsClient.clientName, targets, targetStorageTypes);\n+          dfsClient.clientName, targets, targetStorageTypes,\n+          targetStorageIDs);\n       out.flush();\n       //ack\n       BlockOpResponseProto transferResponse \u003d BlockOpResponseProto\n           .parseFrom(PBHelperClient.vintPrefixed(in));\n       if (SUCCESS !\u003d transferResponse.getStatus()) {\n         throw new IOException(\"Failed to add a datanode. Response status: \"\n             + transferResponse.getStatus());\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    void sendTransferBlock(final DatanodeInfo[] targets,\n        final StorageType[] targetStorageTypes,\n        final String[] targetStorageIDs,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //send the TRANSFER_BLOCK request\n      new Sender(out).transferBlock(block.getCurrentBlock(), blockToken,\n          dfsClient.clientName, targets, targetStorageTypes,\n          targetStorageIDs);\n      out.flush();\n      //ack\n      BlockOpResponseProto transferResponse \u003d BlockOpResponseProto\n          .parseFrom(PBHelperClient.vintPrefixed(in));\n      if (SUCCESS !\u003d transferResponse.getStatus()) {\n        throw new IOException(\"Failed to add a datanode. Response status: \"\n            + transferResponse.getStatus());\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldValue": "[targets-DatanodeInfo[](modifiers-final), targetStorageTypes-StorageType[](modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]",
            "newValue": "[targets-DatanodeInfo[](modifiers-final), targetStorageTypes-StorageType[](modifiers-final), targetStorageIDs-String[](modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "15/02/17 10:44 AM",
          "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 79.01,
          "commitsBetweenForRepo": 465,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,17 @@\n     void sendTransferBlock(final DatanodeInfo[] targets,\n         final StorageType[] targetStorageTypes,\n+        final String[] targetStorageIDs,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //send the TRANSFER_BLOCK request\n       new Sender(out).transferBlock(block.getCurrentBlock(), blockToken,\n-          dfsClient.clientName, targets, targetStorageTypes);\n+          dfsClient.clientName, targets, targetStorageTypes,\n+          targetStorageIDs);\n       out.flush();\n       //ack\n       BlockOpResponseProto transferResponse \u003d BlockOpResponseProto\n           .parseFrom(PBHelperClient.vintPrefixed(in));\n       if (SUCCESS !\u003d transferResponse.getStatus()) {\n         throw new IOException(\"Failed to add a datanode. Response status: \"\n             + transferResponse.getStatus());\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    void sendTransferBlock(final DatanodeInfo[] targets,\n        final StorageType[] targetStorageTypes,\n        final String[] targetStorageIDs,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //send the TRANSFER_BLOCK request\n      new Sender(out).transferBlock(block.getCurrentBlock(), blockToken,\n          dfsClient.clientName, targets, targetStorageTypes,\n          targetStorageIDs);\n      out.flush();\n      //ack\n      BlockOpResponseProto transferResponse \u003d BlockOpResponseProto\n          .parseFrom(PBHelperClient.vintPrefixed(in));\n      if (SUCCESS !\u003d transferResponse.getStatus()) {\n        throw new IOException(\"Failed to add a datanode. Response status: \"\n            + transferResponse.getStatus());\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {}
        }
      ]
    },
    "627da6f7178e18aa41996969c408b6f344e297d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8498. Blocks can be committed with wrong size. Contributed by Jing Zhao.\n",
      "commitDate": "15/02/17 10:44 AM",
      "commitName": "627da6f7178e18aa41996969c408b6f344e297d1",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/02/17 10:08 AM",
      "commitNameOld": "0914fcca312b5e9d20bcf1b6633bc13c9034ba46",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 13.03,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,15 @@\n     void sendTransferBlock(final DatanodeInfo[] targets,\n         final StorageType[] targetStorageTypes,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //send the TRANSFER_BLOCK request\n-      new Sender(out)\n-          .transferBlock(block, blockToken, dfsClient.clientName, targets,\n-              targetStorageTypes);\n+      new Sender(out).transferBlock(block.getCurrentBlock(), blockToken,\n+          dfsClient.clientName, targets, targetStorageTypes);\n       out.flush();\n       //ack\n       BlockOpResponseProto transferResponse \u003d BlockOpResponseProto\n           .parseFrom(PBHelperClient.vintPrefixed(in));\n       if (SUCCESS !\u003d transferResponse.getStatus()) {\n         throw new IOException(\"Failed to add a datanode. Response status: \"\n             + transferResponse.getStatus());\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void sendTransferBlock(final DatanodeInfo[] targets,\n        final StorageType[] targetStorageTypes,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //send the TRANSFER_BLOCK request\n      new Sender(out).transferBlock(block.getCurrentBlock(), blockToken,\n          dfsClient.clientName, targets, targetStorageTypes);\n      out.flush();\n      //ack\n      BlockOpResponseProto transferResponse \u003d BlockOpResponseProto\n          .parseFrom(PBHelperClient.vintPrefixed(in));\n      if (SUCCESS !\u003d transferResponse.getStatus()) {\n        throw new IOException(\"Failed to add a datanode. Response status: \"\n            + transferResponse.getStatus());\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "3ae652f82110a52bf239f3c1849b48981558eb19": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-10609. Uncaught InvalidEncryptionKeyException during pipeline recovery may abort downstream applications. Contributed by Wei-Chiu Chuang.\n",
      "commitDate": "26/09/16 1:11 PM",
      "commitName": "3ae652f82110a52bf239f3c1849b48981558eb19",
      "commitAuthor": "Wei-Chiu Chuang",
      "diff": "@@ -0,0 +1,16 @@\n+    void sendTransferBlock(final DatanodeInfo[] targets,\n+        final StorageType[] targetStorageTypes,\n+        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+      //send the TRANSFER_BLOCK request\n+      new Sender(out)\n+          .transferBlock(block, blockToken, dfsClient.clientName, targets,\n+              targetStorageTypes);\n+      out.flush();\n+      //ack\n+      BlockOpResponseProto transferResponse \u003d BlockOpResponseProto\n+          .parseFrom(PBHelperClient.vintPrefixed(in));\n+      if (SUCCESS !\u003d transferResponse.getStatus()) {\n+        throw new IOException(\"Failed to add a datanode. Response status: \"\n+            + transferResponse.getStatus());\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    void sendTransferBlock(final DatanodeInfo[] targets,\n        final StorageType[] targetStorageTypes,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //send the TRANSFER_BLOCK request\n      new Sender(out)\n          .transferBlock(block, blockToken, dfsClient.clientName, targets,\n              targetStorageTypes);\n      out.flush();\n      //ack\n      BlockOpResponseProto transferResponse \u003d BlockOpResponseProto\n          .parseFrom(PBHelperClient.vintPrefixed(in));\n      if (SUCCESS !\u003d transferResponse.getStatus()) {\n        throw new IOException(\"Failed to add a datanode. Response status: \"\n            + transferResponse.getStatus());\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    }
  }
}