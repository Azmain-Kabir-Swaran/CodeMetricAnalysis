{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ReduceTask.java",
  "functionName": "runNewReducer",
  "functionId": "runNewReducer___job-JobConf__umbilical-TaskUmbilicalProtocol(modifiers-final)__reporter-TaskReporter(modifiers-final)__rIter-RawKeyValueIterator__comparator-RawComparator__INKEY____keyClass-Class__INKEY____valueClass-Class__INVALUE__",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
  "functionStartLine": 577,
  "functionEndLine": 632,
  "numCommitsSeen": 16,
  "timeTaken": 8729,
  "changeHistory": [
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "4796e1adcb912005198c9003305c97cf3a8b523e",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymovefromfile",
    "4796e1adcb912005198c9003305c97cf3a8b523e": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4737. Ensure that mapreduce APIs are semantically consistent with mapred API w.r.t Mapper.cleanup and Reducer.cleanup; in the sense that cleanup is now called even if there is an error. The old mapred API already ensures that Mapper.close and Reducer.close are invoked during error handling. Note that it is an incompatible change, however end-users can override Mapper.run and Reducer.run to get the old (inconsistent) behaviour. Contributed by Arun C. Murthy.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1471556 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/04/13 10:38 AM",
      "commitName": "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "06/12/12 6:36 PM",
      "commitNameOld": "b096f61fe25752703785cad5fe0aaae8bf45da2f",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 138.63,
      "commitsBetweenForRepo": 666,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,56 @@\n   void runNewReducer(JobConf job,\n                      final TaskUmbilicalProtocol umbilical,\n                      final TaskReporter reporter,\n                      RawKeyValueIterator rIter,\n                      RawComparator\u003cINKEY\u003e comparator,\n                      Class\u003cINKEY\u003e keyClass,\n                      Class\u003cINVALUE\u003e valueClass\n                      ) throws IOException,InterruptedException, \n                               ClassNotFoundException {\n     // wrap value iterator to report progress.\n     final RawKeyValueIterator rawIter \u003d rIter;\n     rIter \u003d new RawKeyValueIterator() {\n       public void close() throws IOException {\n         rawIter.close();\n       }\n       public DataInputBuffer getKey() throws IOException {\n         return rawIter.getKey();\n       }\n       public Progress getProgress() {\n         return rawIter.getProgress();\n       }\n       public DataInputBuffer getValue() throws IOException {\n         return rawIter.getValue();\n       }\n       public boolean next() throws IOException {\n         boolean ret \u003d rawIter.next();\n         reporter.setProgress(rawIter.getProgress().getProgress());\n         return ret;\n       }\n     };\n     // make a task context so we can get the classes\n     org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n       new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n           getTaskID(), reporter);\n     // make a reducer\n     org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d\n       (org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n         ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n     org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e trackedRW \u003d \n       new NewTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(this, taskContext);\n     job.setBoolean(\"mapred.skip.on\", isSkipping());\n     job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n     org.apache.hadoop.mapreduce.Reducer.Context \n          reducerContext \u003d createReduceContext(reducer, job, getTaskID(),\n                                                rIter, reduceInputKeyCounter, \n                                                reduceInputValueCounter, \n                                                trackedRW,\n                                                committer,\n                                                reporter, comparator, keyClass,\n                                                valueClass);\n-    reducer.run(reducerContext);\n-    trackedRW.close(reducerContext);\n+    try {\n+      reducer.run(reducerContext);\n+    } finally {\n+      trackedRW.close(reducerContext);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runNewReducer(JobConf job,\n                     final TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass\n                     ) throws IOException,InterruptedException, \n                              ClassNotFoundException {\n    // wrap value iterator to report progress.\n    final RawKeyValueIterator rawIter \u003d rIter;\n    rIter \u003d new RawKeyValueIterator() {\n      public void close() throws IOException {\n        rawIter.close();\n      }\n      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }\n      public Progress getProgress() {\n        return rawIter.getProgress();\n      }\n      public DataInputBuffer getValue() throws IOException {\n        return rawIter.getValue();\n      }\n      public boolean next() throws IOException {\n        boolean ret \u003d rawIter.next();\n        reporter.setProgress(rawIter.getProgress().getProgress());\n        return ret;\n      }\n    };\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n          getTaskID(), reporter);\n    // make a reducer\n    org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d\n      (org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n    org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e trackedRW \u003d \n      new NewTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(this, taskContext);\n    job.setBoolean(\"mapred.skip.on\", isSkipping());\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.Reducer.Context \n         reducerContext \u003d createReduceContext(reducer, job, getTaskID(),\n                                               rIter, reduceInputKeyCounter, \n                                               reduceInputValueCounter, \n                                               trackedRW,\n                                               committer,\n                                               reporter, comparator, keyClass,\n                                               valueClass);\n    try {\n      reducer.run(reducerContext);\n    } finally {\n      trackedRW.close(reducerContext);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void runNewReducer(JobConf job,\n                     final TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass\n                     ) throws IOException,InterruptedException, \n                              ClassNotFoundException {\n    // wrap value iterator to report progress.\n    final RawKeyValueIterator rawIter \u003d rIter;\n    rIter \u003d new RawKeyValueIterator() {\n      public void close() throws IOException {\n        rawIter.close();\n      }\n      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }\n      public Progress getProgress() {\n        return rawIter.getProgress();\n      }\n      public DataInputBuffer getValue() throws IOException {\n        return rawIter.getValue();\n      }\n      public boolean next() throws IOException {\n        boolean ret \u003d rawIter.next();\n        reporter.setProgress(rawIter.getProgress().getProgress());\n        return ret;\n      }\n    };\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n          getTaskID(), reporter);\n    // make a reducer\n    org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d\n      (org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n    org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e trackedRW \u003d \n      new NewTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(this, taskContext);\n    job.setBoolean(\"mapred.skip.on\", isSkipping());\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.Reducer.Context \n         reducerContext \u003d createReduceContext(reducer, job, getTaskID(),\n                                               rIter, reduceInputKeyCounter, \n                                               reduceInputValueCounter, \n                                               trackedRW,\n                                               committer,\n                                               reporter, comparator, keyClass,\n                                               valueClass);\n    reducer.run(reducerContext);\n    trackedRW.close(reducerContext);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void runNewReducer(JobConf job,\n                     final TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass\n                     ) throws IOException,InterruptedException, \n                              ClassNotFoundException {\n    // wrap value iterator to report progress.\n    final RawKeyValueIterator rawIter \u003d rIter;\n    rIter \u003d new RawKeyValueIterator() {\n      public void close() throws IOException {\n        rawIter.close();\n      }\n      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }\n      public Progress getProgress() {\n        return rawIter.getProgress();\n      }\n      public DataInputBuffer getValue() throws IOException {\n        return rawIter.getValue();\n      }\n      public boolean next() throws IOException {\n        boolean ret \u003d rawIter.next();\n        reporter.setProgress(rawIter.getProgress().getProgress());\n        return ret;\n      }\n    };\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n          getTaskID(), reporter);\n    // make a reducer\n    org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d\n      (org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n    org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e trackedRW \u003d \n      new NewTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(this, taskContext);\n    job.setBoolean(\"mapred.skip.on\", isSkipping());\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.Reducer.Context \n         reducerContext \u003d createReduceContext(reducer, job, getTaskID(),\n                                               rIter, reduceInputKeyCounter, \n                                               reduceInputValueCounter, \n                                               trackedRW,\n                                               committer,\n                                               reporter, comparator, keyClass,\n                                               valueClass);\n    reducer.run(reducerContext);\n    trackedRW.close(reducerContext);\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/ReduceTask.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
        "oldMethodName": "runNewReducer",
        "newMethodName": "runNewReducer"
      }
    },
    "4796e1adcb912005198c9003305c97cf3a8b523e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2365. Add counters to track bytes (read,written) via File(Input,Output)Format. Contributed by Siddharth Seth. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1146515 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/07/11 4:36 PM",
      "commitName": "4796e1adcb912005198c9003305c97cf3a8b523e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 31.07,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,53 @@\n   void runNewReducer(JobConf job,\n                      final TaskUmbilicalProtocol umbilical,\n                      final TaskReporter reporter,\n                      RawKeyValueIterator rIter,\n                      RawComparator\u003cINKEY\u003e comparator,\n                      Class\u003cINKEY\u003e keyClass,\n                      Class\u003cINVALUE\u003e valueClass\n                      ) throws IOException,InterruptedException, \n                               ClassNotFoundException {\n     // wrap value iterator to report progress.\n     final RawKeyValueIterator rawIter \u003d rIter;\n     rIter \u003d new RawKeyValueIterator() {\n       public void close() throws IOException {\n         rawIter.close();\n       }\n       public DataInputBuffer getKey() throws IOException {\n         return rawIter.getKey();\n       }\n       public Progress getProgress() {\n         return rawIter.getProgress();\n       }\n       public DataInputBuffer getValue() throws IOException {\n         return rawIter.getValue();\n       }\n       public boolean next() throws IOException {\n         boolean ret \u003d rawIter.next();\n         reporter.setProgress(rawIter.getProgress().getProgress());\n         return ret;\n       }\n     };\n     // make a task context so we can get the classes\n     org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n       new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n           getTaskID(), reporter);\n     // make a reducer\n     org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d\n       (org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n         ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n-    org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e output \u003d\n-      (org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e)\n-        outputFormat.getRecordWriter(taskContext);\n     org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e trackedRW \u003d \n-      new NewTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(output, reduceOutputCounter);\n+      new NewTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(this, taskContext);\n     job.setBoolean(\"mapred.skip.on\", isSkipping());\n     job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n     org.apache.hadoop.mapreduce.Reducer.Context \n          reducerContext \u003d createReduceContext(reducer, job, getTaskID(),\n                                                rIter, reduceInputKeyCounter, \n                                                reduceInputValueCounter, \n                                                trackedRW,\n                                                committer,\n                                                reporter, comparator, keyClass,\n                                                valueClass);\n     reducer.run(reducerContext);\n-    output.close(reducerContext);\n+    trackedRW.close(reducerContext);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runNewReducer(JobConf job,\n                     final TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass\n                     ) throws IOException,InterruptedException, \n                              ClassNotFoundException {\n    // wrap value iterator to report progress.\n    final RawKeyValueIterator rawIter \u003d rIter;\n    rIter \u003d new RawKeyValueIterator() {\n      public void close() throws IOException {\n        rawIter.close();\n      }\n      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }\n      public Progress getProgress() {\n        return rawIter.getProgress();\n      }\n      public DataInputBuffer getValue() throws IOException {\n        return rawIter.getValue();\n      }\n      public boolean next() throws IOException {\n        boolean ret \u003d rawIter.next();\n        reporter.setProgress(rawIter.getProgress().getProgress());\n        return ret;\n      }\n    };\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n          getTaskID(), reporter);\n    // make a reducer\n    org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d\n      (org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n    org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e trackedRW \u003d \n      new NewTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(this, taskContext);\n    job.setBoolean(\"mapred.skip.on\", isSkipping());\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.Reducer.Context \n         reducerContext \u003d createReduceContext(reducer, job, getTaskID(),\n                                               rIter, reduceInputKeyCounter, \n                                               reduceInputValueCounter, \n                                               trackedRW,\n                                               committer,\n                                               reporter, comparator, keyClass,\n                                               valueClass);\n    reducer.run(reducerContext);\n    trackedRW.close(reducerContext);\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/ReduceTask.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,56 @@\n+  void runNewReducer(JobConf job,\n+                     final TaskUmbilicalProtocol umbilical,\n+                     final TaskReporter reporter,\n+                     RawKeyValueIterator rIter,\n+                     RawComparator\u003cINKEY\u003e comparator,\n+                     Class\u003cINKEY\u003e keyClass,\n+                     Class\u003cINVALUE\u003e valueClass\n+                     ) throws IOException,InterruptedException, \n+                              ClassNotFoundException {\n+    // wrap value iterator to report progress.\n+    final RawKeyValueIterator rawIter \u003d rIter;\n+    rIter \u003d new RawKeyValueIterator() {\n+      public void close() throws IOException {\n+        rawIter.close();\n+      }\n+      public DataInputBuffer getKey() throws IOException {\n+        return rawIter.getKey();\n+      }\n+      public Progress getProgress() {\n+        return rawIter.getProgress();\n+      }\n+      public DataInputBuffer getValue() throws IOException {\n+        return rawIter.getValue();\n+      }\n+      public boolean next() throws IOException {\n+        boolean ret \u003d rawIter.next();\n+        reporter.setProgress(rawIter.getProgress().getProgress());\n+        return ret;\n+      }\n+    };\n+    // make a task context so we can get the classes\n+    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n+      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n+          getTaskID(), reporter);\n+    // make a reducer\n+    org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d\n+      (org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n+        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n+    org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e output \u003d\n+      (org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e)\n+        outputFormat.getRecordWriter(taskContext);\n+    org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e trackedRW \u003d \n+      new NewTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(output, reduceOutputCounter);\n+    job.setBoolean(\"mapred.skip.on\", isSkipping());\n+    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n+    org.apache.hadoop.mapreduce.Reducer.Context \n+         reducerContext \u003d createReduceContext(reducer, job, getTaskID(),\n+                                               rIter, reduceInputKeyCounter, \n+                                               reduceInputValueCounter, \n+                                               trackedRW,\n+                                               committer,\n+                                               reporter, comparator, keyClass,\n+                                               valueClass);\n+    reducer.run(reducerContext);\n+    output.close(reducerContext);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void runNewReducer(JobConf job,\n                     final TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass\n                     ) throws IOException,InterruptedException, \n                              ClassNotFoundException {\n    // wrap value iterator to report progress.\n    final RawKeyValueIterator rawIter \u003d rIter;\n    rIter \u003d new RawKeyValueIterator() {\n      public void close() throws IOException {\n        rawIter.close();\n      }\n      public DataInputBuffer getKey() throws IOException {\n        return rawIter.getKey();\n      }\n      public Progress getProgress() {\n        return rawIter.getProgress();\n      }\n      public DataInputBuffer getValue() throws IOException {\n        return rawIter.getValue();\n      }\n      public boolean next() throws IOException {\n        boolean ret \u003d rawIter.next();\n        reporter.setProgress(rawIter.getProgress().getProgress());\n        return ret;\n      }\n    };\n    // make a task context so we can get the classes\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext \u003d\n      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,\n          getTaskID(), reporter);\n    // make a reducer\n    org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d\n      (org.apache.hadoop.mapreduce.Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e)\n        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\n    org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e output \u003d\n      (org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e)\n        outputFormat.getRecordWriter(taskContext);\n    org.apache.hadoop.mapreduce.RecordWriter\u003cOUTKEY,OUTVALUE\u003e trackedRW \u003d \n      new NewTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(output, reduceOutputCounter);\n    job.setBoolean(\"mapred.skip.on\", isSkipping());\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n    org.apache.hadoop.mapreduce.Reducer.Context \n         reducerContext \u003d createReduceContext(reducer, job, getTaskID(),\n                                               rIter, reduceInputKeyCounter, \n                                               reduceInputValueCounter, \n                                               trackedRW,\n                                               committer,\n                                               reporter, comparator, keyClass,\n                                               valueClass);\n    reducer.run(reducerContext);\n    output.close(reducerContext);\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/ReduceTask.java"
    }
  }
}