{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FileDistributionCalculator.java",
  "functionName": "visit",
  "functionId": "visit___file-RandomAccessFile",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionCalculator.java",
  "functionStartLine": 96,
  "functionEndLine": 116,
  "numCommitsSeen": 9,
  "timeTaken": 1832,
  "changeHistory": [
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305"
  ],
  "changeHistoryShort": {
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": "Ybodychange",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.\n",
      "commitDate": "11/12/14 12:36 PM",
      "commitName": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "08/11/14 3:39 PM",
      "commitNameOld": "6caa8100d5d2547e34356dc279fd5e65b81a925a",
      "commitAuthorOld": "Arun C. Murthy",
      "daysBetweenCommits": 32.87,
      "commitsBetweenForRepo": 232,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,21 @@\n   void visit(RandomAccessFile file) throws IOException {\n     if (!FSImageUtil.checkFileFormat(file)) {\n       throw new IOException(\"Unrecognized FSImage\");\n     }\n \n     FileSummary summary \u003d FSImageUtil.loadSummary(file);\n-    FileInputStream in \u003d null;\n-    try {\n-      in \u003d new FileInputStream(file.getFD());\n+    try (FileInputStream in \u003d new FileInputStream(file.getFD())) {\n       for (FileSummary.Section s : summary.getSectionsList()) {\n         if (SectionName.fromString(s.getName()) !\u003d SectionName.INODE) {\n           continue;\n         }\n \n         in.getChannel().position(s.getOffset());\n         InputStream is \u003d FSImageUtil.wrapInputStreamForCompression(conf,\n             summary.getCodec(), new BufferedInputStream(new LimitInputStream(\n                 in, s.getLength())));\n         run(is);\n         output();\n       }\n-    } finally {\n-      IOUtils.cleanup(null, in);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void visit(RandomAccessFile file) throws IOException {\n    if (!FSImageUtil.checkFileFormat(file)) {\n      throw new IOException(\"Unrecognized FSImage\");\n    }\n\n    FileSummary summary \u003d FSImageUtil.loadSummary(file);\n    try (FileInputStream in \u003d new FileInputStream(file.getFD())) {\n      for (FileSummary.Section s : summary.getSectionsList()) {\n        if (SectionName.fromString(s.getName()) !\u003d SectionName.INODE) {\n          continue;\n        }\n\n        in.getChannel().position(s.getOffset());\n        InputStream is \u003d FSImageUtil.wrapInputStreamForCompression(conf,\n            summary.getCodec(), new BufferedInputStream(new LimitInputStream(\n                in, s.getLength())));\n        run(is);\n        output();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionCalculator.java",
      "extendedDetails": {}
    },
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,25 @@\n+  void visit(RandomAccessFile file) throws IOException {\n+    if (!FSImageUtil.checkFileFormat(file)) {\n+      throw new IOException(\"Unrecognized FSImage\");\n+    }\n+\n+    FileSummary summary \u003d FSImageUtil.loadSummary(file);\n+    FileInputStream in \u003d null;\n+    try {\n+      in \u003d new FileInputStream(file.getFD());\n+      for (FileSummary.Section s : summary.getSectionsList()) {\n+        if (SectionName.fromString(s.getName()) !\u003d SectionName.INODE) {\n+          continue;\n+        }\n+\n+        in.getChannel().position(s.getOffset());\n+        InputStream is \u003d FSImageUtil.wrapInputStreamForCompression(conf,\n+            summary.getCodec(), new BufferedInputStream(new LimitInputStream(\n+                in, s.getLength())));\n+        run(is);\n+        output();\n+      }\n+    } finally {\n+      IOUtils.cleanup(null, in);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void visit(RandomAccessFile file) throws IOException {\n    if (!FSImageUtil.checkFileFormat(file)) {\n      throw new IOException(\"Unrecognized FSImage\");\n    }\n\n    FileSummary summary \u003d FSImageUtil.loadSummary(file);\n    FileInputStream in \u003d null;\n    try {\n      in \u003d new FileInputStream(file.getFD());\n      for (FileSummary.Section s : summary.getSectionsList()) {\n        if (SectionName.fromString(s.getName()) !\u003d SectionName.INODE) {\n          continue;\n        }\n\n        in.getChannel().position(s.getOffset());\n        InputStream is \u003d FSImageUtil.wrapInputStreamForCompression(conf,\n            summary.getCodec(), new BufferedInputStream(new LimitInputStream(\n                in, s.getLength())));\n        run(is);\n        output();\n      }\n    } finally {\n      IOUtils.cleanup(null, in);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionCalculator.java"
    }
  }
}