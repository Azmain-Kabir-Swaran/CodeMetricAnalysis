{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RegularContainerAllocator.java",
  "functionName": "doAllocation",
  "functionId": "doAllocation___allocationResult-ContainerAllocation__node-FiCaSchedulerNode__schedulerKey-SchedulerRequestKey__reservedContainer-RMContainer",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
  "functionStartLine": 748,
  "functionEndLine": 831,
  "numCommitsSeen": 71,
  "timeTaken": 9088,
  "changeHistory": [
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8",
    "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7",
    "2064ca015d1584263aac0cc20c60b925a3aff612",
    "8598b498bcaf4deffa822f871a26635bdf3d9d5c",
    "0a55bd841ec0f2eb89a0383f4c589526e8b138d4",
    "de3b4aac561258ad242a3c5ed1c919428893fd4c",
    "e0d131f055ee126052ad4d0f7b0d192e6c730188",
    "49969b16cdba0f251b9f8bf3d8df9906e38b5c61",
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
    "ae14e5d07f1b6702a5160637438028bb03d9387e",
    "fa7a43529d529f0006c8033c2003f15b9b93f103",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
    "6cb0af3c39a5d49cb2f7911ee21363a9542ca2d7",
    "796638d9bc86235b9f3e5d1a3a9a25bbf5c04d1c",
    "e5003be907acef87c2770e3f2914953f62017b0e",
    "ba2313d6145a1234777938a747187373f4cd58d9"
  ],
  "changeHistoryShort": {
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8": "Ybodychange",
    "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7": "Ybodychange",
    "2064ca015d1584263aac0cc20c60b925a3aff612": "Ybodychange",
    "8598b498bcaf4deffa822f871a26635bdf3d9d5c": "Ybodychange",
    "0a55bd841ec0f2eb89a0383f4c589526e8b138d4": "Ybodychange",
    "de3b4aac561258ad242a3c5ed1c919428893fd4c": "Ybodychange",
    "e0d131f055ee126052ad4d0f7b0d192e6c730188": "Ybodychange",
    "49969b16cdba0f251b9f8bf3d8df9906e38b5c61": "Ybodychange",
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828": "Ymultichange(Yparameterchange,Ybodychange)",
    "ae14e5d07f1b6702a5160637438028bb03d9387e": "Yparameterchange",
    "fa7a43529d529f0006c8033c2003f15b9b93f103": "Yparameterchange",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": "Yparameterchange",
    "6cb0af3c39a5d49cb2f7911ee21363a9542ca2d7": "Ybodychange",
    "796638d9bc86235b9f3e5d1a3a9a25bbf5c04d1c": "Ybodychange",
    "e5003be907acef87c2770e3f2914953f62017b0e": "Ybodychange",
    "ba2313d6145a1234777938a747187373f4cd58d9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9664. Improve response of scheduler/app activities for better understanding. Contributed by Tao Yang.\n",
      "commitDate": "29/08/19 3:14 AM",
      "commitName": "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "06/06/19 6:59 AM",
      "commitNameOld": "09763925025a3709e6098186348e1afd80cb9f71",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 83.84,
      "commitsBetweenForRepo": 744,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,84 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), schedulerKey);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n           node, application, schedulerKey,\n-          ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n-          ActivityState.REJECTED);\n+          ActivityDiagnosticConstant.APPLICATION_COULD_NOT_GET_CONTAINER,\n+          ActivityState.REJECTED, ActivityLevel.APP);\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n           schedulerKey, container);\n     } else {\n       // When reserving container\n       RMContainer updatedContainer \u003d reservedContainer;\n       if (updatedContainer \u003d\u003d null) {\n         AppPlacementAllocator\u003cFiCaSchedulerNode\u003e ps \u003d\n             application.getAppSchedulingInfo()\n                 .getAppPlacementAllocator(schedulerKey);\n         if (null \u003d\u003d ps) {\n           LOG.warn(\"Failed to get \" + AppPlacementAllocator.class.getName()\n               + \" for application\u003d\" + application.getApplicationId()\n               + \" schedulerRequestKey\u003d\" + schedulerKey);\n           ActivitiesLogger.APP\n               .recordAppActivityWithoutAllocation(activitiesManager, node,\n                   application, schedulerKey,\n                   ActivityDiagnosticConstant.\n-                      PRIORITY_SKIPPED_BECAUSE_NULL_ANY_REQUEST,\n-                  ActivityState.REJECTED);\n+                      REQUEST_SKIPPED_BECAUSE_NULL_ANY_REQUEST,\n+                  ActivityState.REJECTED, ActivityLevel.REQUEST);\n           return ContainerAllocation.PRIORITY_SKIPPED;\n         }\n         updatedContainer \u003d new RMContainerImpl(container, schedulerKey,\n             application.getApplicationAttemptId(), node.getNodeID(),\n             application.getAppSchedulingInfo().getUser(), rmContext,\n             ps.getPrimaryRequestedNodePartition());\n       }\n       allocationResult.updatedContainer \u003d updatedContainer;\n     }\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         LOG.debug(\"Resetting scheduling opportunities\");\n \n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(schedulerKey);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it when:\n       // - It allocated on the default partition\n       //\n       // This is to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       if (StringUtils.equals(node.getPartition(),\n           RMNodeLabelsManager.NO_LABEL)) {\n         application\n             .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n       }\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), schedulerKey);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n          node, application, schedulerKey,\n          ActivityDiagnosticConstant.APPLICATION_COULD_NOT_GET_CONTAINER,\n          ActivityState.REJECTED, ActivityLevel.APP);\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n          schedulerKey, container);\n    } else {\n      // When reserving container\n      RMContainer updatedContainer \u003d reservedContainer;\n      if (updatedContainer \u003d\u003d null) {\n        AppPlacementAllocator\u003cFiCaSchedulerNode\u003e ps \u003d\n            application.getAppSchedulingInfo()\n                .getAppPlacementAllocator(schedulerKey);\n        if (null \u003d\u003d ps) {\n          LOG.warn(\"Failed to get \" + AppPlacementAllocator.class.getName()\n              + \" for application\u003d\" + application.getApplicationId()\n              + \" schedulerRequestKey\u003d\" + schedulerKey);\n          ActivitiesLogger.APP\n              .recordAppActivityWithoutAllocation(activitiesManager, node,\n                  application, schedulerKey,\n                  ActivityDiagnosticConstant.\n                      REQUEST_SKIPPED_BECAUSE_NULL_ANY_REQUEST,\n                  ActivityState.REJECTED, ActivityLevel.REQUEST);\n          return ContainerAllocation.PRIORITY_SKIPPED;\n        }\n        updatedContainer \u003d new RMContainerImpl(container, schedulerKey,\n            application.getApplicationAttemptId(), node.getNodeID(),\n            application.getAppSchedulingInfo().getUser(), rmContext,\n            ps.getPrimaryRequestedNodePartition());\n      }\n      allocationResult.updatedContainer \u003d updatedContainer;\n    }\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        LOG.debug(\"Resetting scheduling opportunities\");\n\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(schedulerKey);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it when:\n      // - It allocated on the default partition\n      //\n      // This is to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      if (StringUtils.equals(node.getPartition(),\n          RMNodeLabelsManager.NO_LABEL)) {\n        application\n            .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n      }\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9440. Improve diagnostics for scheduler and app activities. Contributed by Tao Yang.\n",
      "commitDate": "06/05/19 5:00 AM",
      "commitName": "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "15/03/19 4:20 PM",
      "commitNameOld": "2064ca015d1584263aac0cc20c60b925a3aff612",
      "commitAuthorOld": "Eric Yang",
      "daysBetweenCommits": 51.53,
      "commitsBetweenForRepo": 314,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,84 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), schedulerKey);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n-          node, application, schedulerKey.getPriority(),\n+          node, application, schedulerKey,\n           ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n           ActivityState.REJECTED);\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n           schedulerKey, container);\n     } else {\n       // When reserving container\n       RMContainer updatedContainer \u003d reservedContainer;\n       if (updatedContainer \u003d\u003d null) {\n         AppPlacementAllocator\u003cFiCaSchedulerNode\u003e ps \u003d\n             application.getAppSchedulingInfo()\n                 .getAppPlacementAllocator(schedulerKey);\n         if (null \u003d\u003d ps) {\n           LOG.warn(\"Failed to get \" + AppPlacementAllocator.class.getName()\n               + \" for application\u003d\" + application.getApplicationId()\n               + \" schedulerRequestKey\u003d\" + schedulerKey);\n           ActivitiesLogger.APP\n               .recordAppActivityWithoutAllocation(activitiesManager, node,\n-                  application, schedulerKey.getPriority(),\n+                  application, schedulerKey,\n                   ActivityDiagnosticConstant.\n                       PRIORITY_SKIPPED_BECAUSE_NULL_ANY_REQUEST,\n                   ActivityState.REJECTED);\n           return ContainerAllocation.PRIORITY_SKIPPED;\n         }\n         updatedContainer \u003d new RMContainerImpl(container, schedulerKey,\n             application.getApplicationAttemptId(), node.getNodeID(),\n             application.getAppSchedulingInfo().getUser(), rmContext,\n             ps.getPrimaryRequestedNodePartition());\n       }\n       allocationResult.updatedContainer \u003d updatedContainer;\n     }\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         LOG.debug(\"Resetting scheduling opportunities\");\n \n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(schedulerKey);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it when:\n       // - It allocated on the default partition\n       //\n       // This is to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       if (StringUtils.equals(node.getPartition(),\n           RMNodeLabelsManager.NO_LABEL)) {\n         application\n             .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n       }\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), schedulerKey);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n          node, application, schedulerKey,\n          ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n          ActivityState.REJECTED);\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n          schedulerKey, container);\n    } else {\n      // When reserving container\n      RMContainer updatedContainer \u003d reservedContainer;\n      if (updatedContainer \u003d\u003d null) {\n        AppPlacementAllocator\u003cFiCaSchedulerNode\u003e ps \u003d\n            application.getAppSchedulingInfo()\n                .getAppPlacementAllocator(schedulerKey);\n        if (null \u003d\u003d ps) {\n          LOG.warn(\"Failed to get \" + AppPlacementAllocator.class.getName()\n              + \" for application\u003d\" + application.getApplicationId()\n              + \" schedulerRequestKey\u003d\" + schedulerKey);\n          ActivitiesLogger.APP\n              .recordAppActivityWithoutAllocation(activitiesManager, node,\n                  application, schedulerKey,\n                  ActivityDiagnosticConstant.\n                      PRIORITY_SKIPPED_BECAUSE_NULL_ANY_REQUEST,\n                  ActivityState.REJECTED);\n          return ContainerAllocation.PRIORITY_SKIPPED;\n        }\n        updatedContainer \u003d new RMContainerImpl(container, schedulerKey,\n            application.getApplicationAttemptId(), node.getNodeID(),\n            application.getAppSchedulingInfo().getUser(), rmContext,\n            ps.getPrimaryRequestedNodePartition());\n      }\n      allocationResult.updatedContainer \u003d updatedContainer;\n    }\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        LOG.debug(\"Resetting scheduling opportunities\");\n\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(schedulerKey);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it when:\n      // - It allocated on the default partition\n      //\n      // This is to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      if (StringUtils.equals(node.getPartition(),\n          RMNodeLabelsManager.NO_LABEL)) {\n        application\n            .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n      }\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "2064ca015d1584263aac0cc20c60b925a3aff612": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9349.  Changed logging to use slf4j api.\n            Contributed by Prabhu Joseph\n",
      "commitDate": "15/03/19 4:20 PM",
      "commitName": "2064ca015d1584263aac0cc20c60b925a3aff612",
      "commitAuthor": "Eric Yang",
      "commitDateOld": "04/03/19 9:10 PM",
      "commitNameOld": "e40e2d6ad5cbe782c3a067229270738b501ed27e",
      "commitAuthorOld": "Prabhu Joseph",
      "daysBetweenCommits": 10.76,
      "commitsBetweenForRepo": 108,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,84 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), schedulerKey);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n           node, application, schedulerKey.getPriority(),\n           ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n           ActivityState.REJECTED);\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n           schedulerKey, container);\n     } else {\n       // When reserving container\n       RMContainer updatedContainer \u003d reservedContainer;\n       if (updatedContainer \u003d\u003d null) {\n         AppPlacementAllocator\u003cFiCaSchedulerNode\u003e ps \u003d\n             application.getAppSchedulingInfo()\n                 .getAppPlacementAllocator(schedulerKey);\n         if (null \u003d\u003d ps) {\n           LOG.warn(\"Failed to get \" + AppPlacementAllocator.class.getName()\n               + \" for application\u003d\" + application.getApplicationId()\n               + \" schedulerRequestKey\u003d\" + schedulerKey);\n           ActivitiesLogger.APP\n               .recordAppActivityWithoutAllocation(activitiesManager, node,\n                   application, schedulerKey.getPriority(),\n                   ActivityDiagnosticConstant.\n                       PRIORITY_SKIPPED_BECAUSE_NULL_ANY_REQUEST,\n                   ActivityState.REJECTED);\n           return ContainerAllocation.PRIORITY_SKIPPED;\n         }\n         updatedContainer \u003d new RMContainerImpl(container, schedulerKey,\n             application.getApplicationAttemptId(), node.getNodeID(),\n             application.getAppSchedulingInfo().getUser(), rmContext,\n             ps.getPrimaryRequestedNodePartition());\n       }\n       allocationResult.updatedContainer \u003d updatedContainer;\n     }\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Resetting scheduling opportunities\");\n-        }\n+        LOG.debug(\"Resetting scheduling opportunities\");\n+\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(schedulerKey);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it when:\n       // - It allocated on the default partition\n       //\n       // This is to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       if (StringUtils.equals(node.getPartition(),\n           RMNodeLabelsManager.NO_LABEL)) {\n         application\n             .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n       }\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), schedulerKey);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n          node, application, schedulerKey.getPriority(),\n          ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n          ActivityState.REJECTED);\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n          schedulerKey, container);\n    } else {\n      // When reserving container\n      RMContainer updatedContainer \u003d reservedContainer;\n      if (updatedContainer \u003d\u003d null) {\n        AppPlacementAllocator\u003cFiCaSchedulerNode\u003e ps \u003d\n            application.getAppSchedulingInfo()\n                .getAppPlacementAllocator(schedulerKey);\n        if (null \u003d\u003d ps) {\n          LOG.warn(\"Failed to get \" + AppPlacementAllocator.class.getName()\n              + \" for application\u003d\" + application.getApplicationId()\n              + \" schedulerRequestKey\u003d\" + schedulerKey);\n          ActivitiesLogger.APP\n              .recordAppActivityWithoutAllocation(activitiesManager, node,\n                  application, schedulerKey.getPriority(),\n                  ActivityDiagnosticConstant.\n                      PRIORITY_SKIPPED_BECAUSE_NULL_ANY_REQUEST,\n                  ActivityState.REJECTED);\n          return ContainerAllocation.PRIORITY_SKIPPED;\n        }\n        updatedContainer \u003d new RMContainerImpl(container, schedulerKey,\n            application.getApplicationAttemptId(), node.getNodeID(),\n            application.getAppSchedulingInfo().getUser(), rmContext,\n            ps.getPrimaryRequestedNodePartition());\n      }\n      allocationResult.updatedContainer \u003d updatedContainer;\n    }\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        LOG.debug(\"Resetting scheduling opportunities\");\n\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(schedulerKey);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it when:\n      // - It allocated on the default partition\n      //\n      // This is to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      if (StringUtils.equals(node.getPartition(),\n          RMNodeLabelsManager.NO_LABEL)) {\n        application\n            .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n      }\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "8598b498bcaf4deffa822f871a26635bdf3d9d5c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8774. Memory leak when CapacityScheduler allocates from reserved container with non-default label. Contributed by Tao Yang.\n",
      "commitDate": "28/09/18 8:32 AM",
      "commitName": "8598b498bcaf4deffa822f871a26635bdf3d9d5c",
      "commitAuthor": "Eric E Payne",
      "commitDateOld": "19/09/18 4:31 AM",
      "commitNameOld": "0712537e799bc03855d548d1f4bd690dd478b871",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 9.17,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,85 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), schedulerKey);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n           node, application, schedulerKey.getPriority(),\n           ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n           ActivityState.REJECTED);\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n           schedulerKey, container);\n     } else {\n       // When reserving container\n       RMContainer updatedContainer \u003d reservedContainer;\n       if (updatedContainer \u003d\u003d null) {\n+        AppPlacementAllocator\u003cFiCaSchedulerNode\u003e ps \u003d\n+            application.getAppSchedulingInfo()\n+                .getAppPlacementAllocator(schedulerKey);\n+        if (null \u003d\u003d ps) {\n+          LOG.warn(\"Failed to get \" + AppPlacementAllocator.class.getName()\n+              + \" for application\u003d\" + application.getApplicationId()\n+              + \" schedulerRequestKey\u003d\" + schedulerKey);\n+          ActivitiesLogger.APP\n+              .recordAppActivityWithoutAllocation(activitiesManager, node,\n+                  application, schedulerKey.getPriority(),\n+                  ActivityDiagnosticConstant.\n+                      PRIORITY_SKIPPED_BECAUSE_NULL_ANY_REQUEST,\n+                  ActivityState.REJECTED);\n+          return ContainerAllocation.PRIORITY_SKIPPED;\n+        }\n         updatedContainer \u003d new RMContainerImpl(container, schedulerKey,\n             application.getApplicationAttemptId(), node.getNodeID(),\n-            application.getAppSchedulingInfo().getUser(), rmContext);\n+            application.getAppSchedulingInfo().getUser(), rmContext,\n+            ps.getPrimaryRequestedNodePartition());\n       }\n       allocationResult.updatedContainer \u003d updatedContainer;\n     }\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(schedulerKey);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it when:\n       // - It allocated on the default partition\n       //\n       // This is to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       if (StringUtils.equals(node.getPartition(),\n           RMNodeLabelsManager.NO_LABEL)) {\n         application\n             .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n       }\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), schedulerKey);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n          node, application, schedulerKey.getPriority(),\n          ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n          ActivityState.REJECTED);\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n          schedulerKey, container);\n    } else {\n      // When reserving container\n      RMContainer updatedContainer \u003d reservedContainer;\n      if (updatedContainer \u003d\u003d null) {\n        AppPlacementAllocator\u003cFiCaSchedulerNode\u003e ps \u003d\n            application.getAppSchedulingInfo()\n                .getAppPlacementAllocator(schedulerKey);\n        if (null \u003d\u003d ps) {\n          LOG.warn(\"Failed to get \" + AppPlacementAllocator.class.getName()\n              + \" for application\u003d\" + application.getApplicationId()\n              + \" schedulerRequestKey\u003d\" + schedulerKey);\n          ActivitiesLogger.APP\n              .recordAppActivityWithoutAllocation(activitiesManager, node,\n                  application, schedulerKey.getPriority(),\n                  ActivityDiagnosticConstant.\n                      PRIORITY_SKIPPED_BECAUSE_NULL_ANY_REQUEST,\n                  ActivityState.REJECTED);\n          return ContainerAllocation.PRIORITY_SKIPPED;\n        }\n        updatedContainer \u003d new RMContainerImpl(container, schedulerKey,\n            application.getApplicationAttemptId(), node.getNodeID(),\n            application.getAppSchedulingInfo().getUser(), rmContext,\n            ps.getPrimaryRequestedNodePartition());\n      }\n      allocationResult.updatedContainer \u003d updatedContainer;\n    }\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(schedulerKey);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it when:\n      // - It allocated on the default partition\n      //\n      // This is to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      if (StringUtils.equals(node.getPartition(),\n          RMNodeLabelsManager.NO_LABEL)) {\n        application\n            .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n      }\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "0a55bd841ec0f2eb89a0383f4c589526e8b138d4": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5959. RM changes to support change of container ExecutionType. (Arun Suresh via wangda)\n",
      "commitDate": "05/01/17 10:31 AM",
      "commitName": "0a55bd841ec0f2eb89a0383f4c589526e8b138d4",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "27/12/16 1:35 PM",
      "commitNameOld": "1bbd023275db535ab80fcb60e022151e9679d468",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 8.87,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,69 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), schedulerKey);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n           node, application, schedulerKey.getPriority(),\n           ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n           ActivityState.REJECTED);\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n           schedulerKey, container);\n     } else {\n       // When reserving container\n       RMContainer updatedContainer \u003d reservedContainer;\n       if (updatedContainer \u003d\u003d null) {\n-        updatedContainer \u003d new RMContainerImpl(container,\n+        updatedContainer \u003d new RMContainerImpl(container, schedulerKey,\n             application.getApplicationAttemptId(), node.getNodeID(),\n             application.getAppSchedulingInfo().getUser(), rmContext);\n       }\n       allocationResult.updatedContainer \u003d updatedContainer;\n     }\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(schedulerKey);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it when:\n       // - It allocated on the default partition\n       //\n       // This is to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       if (StringUtils.equals(node.getPartition(),\n           RMNodeLabelsManager.NO_LABEL)) {\n         application\n             .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n       }\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), schedulerKey);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n          node, application, schedulerKey.getPriority(),\n          ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n          ActivityState.REJECTED);\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n          schedulerKey, container);\n    } else {\n      // When reserving container\n      RMContainer updatedContainer \u003d reservedContainer;\n      if (updatedContainer \u003d\u003d null) {\n        updatedContainer \u003d new RMContainerImpl(container, schedulerKey,\n            application.getApplicationAttemptId(), node.getNodeID(),\n            application.getAppSchedulingInfo().getUser(), rmContext);\n      }\n      allocationResult.updatedContainer \u003d updatedContainer;\n    }\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(schedulerKey);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it when:\n      // - It allocated on the default partition\n      //\n      // This is to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      if (StringUtils.equals(node.getPartition(),\n          RMNodeLabelsManager.NO_LABEL)) {\n        application\n            .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n      }\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "de3b4aac561258ad242a3c5ed1c919428893fd4c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
      "commitDate": "07/11/16 10:14 AM",
      "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
      "commitAuthor": "Jian He",
      "commitDateOld": "19/09/16 2:08 AM",
      "commitNameOld": "b8a30f2f170ffbd590e7366c3c944ab4919e40df",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 49.38,
      "commitsBetweenForRepo": 429,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,69 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), schedulerKey);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n           node, application, schedulerKey.getPriority(),\n           ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n           ActivityState.REJECTED);\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n-      allocationResult \u003d\n-          handleNewContainerAllocation(allocationResult, node, schedulerKey,\n-              reservedContainer, container);\n+      allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n+          schedulerKey, container);\n     } else {\n       // When reserving container\n-      application.reserve(schedulerKey, node, reservedContainer, container);\n+      RMContainer updatedContainer \u003d reservedContainer;\n+      if (updatedContainer \u003d\u003d null) {\n+        updatedContainer \u003d new RMContainerImpl(container,\n+            application.getApplicationAttemptId(), node.getNodeID(),\n+            application.getAppSchedulingInfo().getUser(), rmContext);\n+      }\n+      allocationResult.updatedContainer \u003d updatedContainer;\n     }\n-    allocationResult.updatedContainer \u003d container;\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(schedulerKey);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it when:\n       // - It allocated on the default partition\n       //\n       // This is to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       if (StringUtils.equals(node.getPartition(),\n           RMNodeLabelsManager.NO_LABEL)) {\n         application\n             .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n       }\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), schedulerKey);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n          node, application, schedulerKey.getPriority(),\n          ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n          ActivityState.REJECTED);\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d handleNewContainerAllocation(allocationResult, node,\n          schedulerKey, container);\n    } else {\n      // When reserving container\n      RMContainer updatedContainer \u003d reservedContainer;\n      if (updatedContainer \u003d\u003d null) {\n        updatedContainer \u003d new RMContainerImpl(container,\n            application.getApplicationAttemptId(), node.getNodeID(),\n            application.getAppSchedulingInfo().getUser(), rmContext);\n      }\n      allocationResult.updatedContainer \u003d updatedContainer;\n    }\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(schedulerKey);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it when:\n      // - It allocated on the default partition\n      //\n      // This is to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      if (StringUtils.equals(node.getPartition(),\n          RMNodeLabelsManager.NO_LABEL)) {\n        application\n            .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n      }\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "e0d131f055ee126052ad4d0f7b0d192e6c730188": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4091. Add REST API to retrieve scheduler activity. (Chen Ge and Sunil G via wangda)\n",
      "commitDate": "05/08/16 10:27 AM",
      "commitName": "e0d131f055ee126052ad4d0f7b0d192e6c730188",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "26/07/16 6:14 PM",
      "commitNameOld": "49969b16cdba0f251b9f8bf3d8df9906e38b5c61",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 9.68,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,65 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), schedulerKey);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n+      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n+          node, application, schedulerKey.getPriority(),\n+          ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n+          ActivityState.REJECTED);\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d\n           handleNewContainerAllocation(allocationResult, node, schedulerKey,\n               reservedContainer, container);\n     } else {\n       // When reserving container\n       application.reserve(schedulerKey, node, reservedContainer, container);\n     }\n     allocationResult.updatedContainer \u003d container;\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(schedulerKey);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it when:\n       // - It allocated on the default partition\n       //\n       // This is to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       if (StringUtils.equals(node.getPartition(),\n           RMNodeLabelsManager.NO_LABEL)) {\n         application\n             .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n       }\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), schedulerKey);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n          node, application, schedulerKey.getPriority(),\n          ActivityDiagnosticConstant.COULD_NOT_GET_CONTAINER,\n          ActivityState.REJECTED);\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d\n          handleNewContainerAllocation(allocationResult, node, schedulerKey,\n              reservedContainer, container);\n    } else {\n      // When reserving container\n      application.reserve(schedulerKey, node, reservedContainer, container);\n    }\n    allocationResult.updatedContainer \u003d container;\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(schedulerKey);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it when:\n      // - It allocated on the default partition\n      //\n      // This is to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      if (StringUtils.equals(node.getPartition(),\n          RMNodeLabelsManager.NO_LABEL)) {\n        application\n            .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n      }\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "49969b16cdba0f251b9f8bf3d8df9906e38b5c61": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5342. Improve non-exclusive node partition resource allocation in Capacity Scheduler. (Sunil G via wangda)\n",
      "commitDate": "26/07/16 6:14 PM",
      "commitName": "49969b16cdba0f251b9f8bf3d8df9906e38b5c61",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "26/07/16 2:54 PM",
      "commitNameOld": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 0.14,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,61 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), schedulerKey);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d\n           handleNewContainerAllocation(allocationResult, node, schedulerKey,\n               reservedContainer, container);\n     } else {\n       // When reserving container\n       application.reserve(schedulerKey, node, reservedContainer, container);\n     }\n     allocationResult.updatedContainer \u003d container;\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(schedulerKey);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n-      // it every time to make sure non-labeled resource request will be\n+      // it when:\n+      // - It allocated on the default partition\n+      //\n+      // This is to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n-      application.resetMissedNonPartitionedRequestSchedulingOpportunity(\n-          schedulerKey);\n+      if (StringUtils.equals(node.getPartition(),\n+          RMNodeLabelsManager.NO_LABEL)) {\n+        application\n+            .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n+      }\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), schedulerKey);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d\n          handleNewContainerAllocation(allocationResult, node, schedulerKey,\n              reservedContainer, container);\n    } else {\n      // When reserving container\n      application.reserve(schedulerKey, node, reservedContainer, container);\n    }\n    allocationResult.updatedContainer \u003d container;\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(schedulerKey);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it when:\n      // - It allocated on the default partition\n      //\n      // This is to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      if (StringUtils.equals(node.getPartition(),\n          RMNodeLabelsManager.NO_LABEL)) {\n        application\n            .resetMissedNonPartitionedRequestSchedulingOpportunity(schedulerKey);\n      }\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
      "commitDate": "26/07/16 2:54 PM",
      "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
      "commitAuthor": "Arun Suresh",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
          "commitDate": "26/07/16 2:54 PM",
          "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "07/06/16 3:06 PM",
          "commitNameOld": "620325e81696fca140195b74929ed9eda2d5eb16",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 48.99,
          "commitsBetweenForRepo": 441,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,54 +1,55 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n-      FiCaSchedulerNode node, Priority priority,\n+      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n-            allocationResult.getResourceToBeAllocated(), priority);\n+            allocationResult.getResourceToBeAllocated(), schedulerKey);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d\n-          handleNewContainerAllocation(allocationResult, node, priority,\n+          handleNewContainerAllocation(allocationResult, node, schedulerKey,\n               reservedContainer, container);\n     } else {\n       // When reserving container\n-      application.reserve(priority, node, reservedContainer, container);\n+      application.reserve(schedulerKey, node, reservedContainer, container);\n     }\n     allocationResult.updatedContainer \u003d container;\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n-          application.resetSchedulingOpportunities(priority);\n+          application.resetSchedulingOpportunities(schedulerKey);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it every time to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n-      application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+      application.resetMissedNonPartitionedRequestSchedulingOpportunity(\n+          schedulerKey);\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), schedulerKey);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d\n          handleNewContainerAllocation(allocationResult, node, schedulerKey,\n              reservedContainer, container);\n    } else {\n      // When reserving container\n      application.reserve(schedulerKey, node, reservedContainer, container);\n    }\n    allocationResult.updatedContainer \u003d container;\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(schedulerKey);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it every time to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      application.resetMissedNonPartitionedRequestSchedulingOpportunity(\n          schedulerKey);\n    }\n\n    return allocationResult;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[allocationResult-ContainerAllocation, node-FiCaSchedulerNode, priority-Priority, reservedContainer-RMContainer]",
            "newValue": "[allocationResult-ContainerAllocation, node-FiCaSchedulerNode, schedulerKey-SchedulerRequestKey, reservedContainer-RMContainer]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
          "commitDate": "26/07/16 2:54 PM",
          "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "07/06/16 3:06 PM",
          "commitNameOld": "620325e81696fca140195b74929ed9eda2d5eb16",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 48.99,
          "commitsBetweenForRepo": 441,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,54 +1,55 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n-      FiCaSchedulerNode node, Priority priority,\n+      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n-            allocationResult.getResourceToBeAllocated(), priority);\n+            allocationResult.getResourceToBeAllocated(), schedulerKey);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d\n-          handleNewContainerAllocation(allocationResult, node, priority,\n+          handleNewContainerAllocation(allocationResult, node, schedulerKey,\n               reservedContainer, container);\n     } else {\n       // When reserving container\n-      application.reserve(priority, node, reservedContainer, container);\n+      application.reserve(schedulerKey, node, reservedContainer, container);\n     }\n     allocationResult.updatedContainer \u003d container;\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n-          application.resetSchedulingOpportunities(priority);\n+          application.resetSchedulingOpportunities(schedulerKey);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it every time to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n-      application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+      application.resetMissedNonPartitionedRequestSchedulingOpportunity(\n+          schedulerKey);\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), schedulerKey);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d\n          handleNewContainerAllocation(allocationResult, node, schedulerKey,\n              reservedContainer, container);\n    } else {\n      // When reserving container\n      application.reserve(schedulerKey, node, reservedContainer, container);\n    }\n    allocationResult.updatedContainer \u003d container;\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(schedulerKey);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it every time to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      application.resetMissedNonPartitionedRequestSchedulingOpportunity(\n          schedulerKey);\n    }\n\n    return allocationResult;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "ae14e5d07f1b6702a5160637438028bb03d9387e": {
      "type": "Yparameterchange",
      "commitMessage": "YARN-4108. CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n\n(cherry picked from commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913)\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "ae14e5d07f1b6702a5160637438028bb03d9387e",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 5:02 PM",
      "commitNameOld": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,54 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n-      Resource clusterResource, FiCaSchedulerNode node,\n-      SchedulingMode schedulingMode, Priority priority,\n+      FiCaSchedulerNode node, Priority priority,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), priority);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d\n           handleNewContainerAllocation(allocationResult, node, priority,\n               reservedContainer, container);\n     } else {\n       // When reserving container\n       application.reserve(priority, node, reservedContainer, container);\n     }\n     allocationResult.updatedContainer \u003d container;\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(priority);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it every time to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, Priority priority,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), priority);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d\n          handleNewContainerAllocation(allocationResult, node, priority,\n              reservedContainer, container);\n    } else {\n      // When reserving container\n      application.reserve(priority, node, reservedContainer, container);\n    }\n    allocationResult.updatedContainer \u003d container;\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(priority);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it every time to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {
        "oldValue": "[allocationResult-ContainerAllocation, clusterResource-Resource, node-FiCaSchedulerNode, schedulingMode-SchedulingMode, priority-Priority, reservedContainer-RMContainer]",
        "newValue": "[allocationResult-ContainerAllocation, node-FiCaSchedulerNode, priority-Priority, reservedContainer-RMContainer]"
      }
    },
    "fa7a43529d529f0006c8033c2003f15b9b93f103": {
      "type": "Yparameterchange",
      "commitMessage": "Revert \"CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\"\n\nThis reverts commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913.\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 4:59 PM",
      "commitNameOld": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,55 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n-      FiCaSchedulerNode node, Priority priority,\n+      Resource clusterResource, FiCaSchedulerNode node,\n+      SchedulingMode schedulingMode, Priority priority,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), priority);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d\n           handleNewContainerAllocation(allocationResult, node, priority,\n               reservedContainer, container);\n     } else {\n       // When reserving container\n       application.reserve(priority, node, reservedContainer, container);\n     }\n     allocationResult.updatedContainer \u003d container;\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(priority);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it every time to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      Resource clusterResource, FiCaSchedulerNode node,\n      SchedulingMode schedulingMode, Priority priority,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), priority);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d\n          handleNewContainerAllocation(allocationResult, node, priority,\n              reservedContainer, container);\n    } else {\n      // When reserving container\n      application.reserve(priority, node, reservedContainer, container);\n    }\n    allocationResult.updatedContainer \u003d container;\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(priority);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it every time to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {
        "oldValue": "[allocationResult-ContainerAllocation, node-FiCaSchedulerNode, priority-Priority, reservedContainer-RMContainer]",
        "newValue": "[allocationResult-ContainerAllocation, clusterResource-Resource, node-FiCaSchedulerNode, schedulingMode-SchedulingMode, priority-Priority, reservedContainer-RMContainer]"
      }
    },
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": {
      "type": "Yparameterchange",
      "commitMessage": "CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n",
      "commitDate": "16/03/16 4:59 PM",
      "commitName": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "28/02/16 9:35 AM",
      "commitNameOld": "f9692770a58af0ab082eb7f15da9cbdcd177605b",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 17.27,
      "commitsBetweenForRepo": 107,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,54 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n-      Resource clusterResource, FiCaSchedulerNode node,\n-      SchedulingMode schedulingMode, Priority priority,\n+      FiCaSchedulerNode node, Priority priority,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), priority);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       application\n           .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d\n           handleNewContainerAllocation(allocationResult, node, priority,\n               reservedContainer, container);\n     } else {\n       // When reserving container\n       application.reserve(priority, node, reservedContainer, container);\n     }\n     allocationResult.updatedContainer \u003d container;\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(priority);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it every time to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      FiCaSchedulerNode node, Priority priority,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), priority);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d\n          handleNewContainerAllocation(allocationResult, node, priority,\n              reservedContainer, container);\n    } else {\n      // When reserving container\n      application.reserve(priority, node, reservedContainer, container);\n    }\n    allocationResult.updatedContainer \u003d container;\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(priority);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it every time to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {
        "oldValue": "[allocationResult-ContainerAllocation, clusterResource-Resource, node-FiCaSchedulerNode, schedulingMode-SchedulingMode, priority-Priority, reservedContainer-RMContainer]",
        "newValue": "[allocationResult-ContainerAllocation, node-FiCaSchedulerNode, priority-Priority, reservedContainer-RMContainer]"
      }
    },
    "6cb0af3c39a5d49cb2f7911ee21363a9542ca2d7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3946. Update exact reason as to why a submitted app is in ACCEPTED state to app\u0027s diagnostic message. (Naganarasimha G R via wangda)\n",
      "commitDate": "14/12/15 10:52 AM",
      "commitName": "6cb0af3c39a5d49cb2f7911ee21363a9542ca2d7",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "12/11/15 11:09 AM",
      "commitNameOld": "796638d9bc86235b9f3e5d1a3a9a25bbf5c04d1c",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 31.99,
      "commitsBetweenForRepo": 189,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,55 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n       Resource clusterResource, FiCaSchedulerNode node,\n       SchedulingMode schedulingMode, Priority priority,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), priority);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n+      application\n+          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d\n           handleNewContainerAllocation(allocationResult, node, priority,\n               reservedContainer, container);\n     } else {\n       // When reserving container\n       application.reserve(priority, node, reservedContainer, container);\n     }\n     allocationResult.updatedContainer \u003d container;\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         // Only reset scheduling opportunities for RACK_LOCAL if configured\n         // to do so. Not resetting means we will continue to schedule\n         // RACK_LOCAL without delay.\n         if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n             || application.getCSLeafQueue().getRackLocalityFullReset()) {\n           application.resetSchedulingOpportunities(priority);\n         }\n       }\n \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it every time to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      Resource clusterResource, FiCaSchedulerNode node,\n      SchedulingMode schedulingMode, Priority priority,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), priority);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      application\n          .updateAppSkipNodeDiagnostics(\"Scheduling of container failed. \");\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d\n          handleNewContainerAllocation(allocationResult, node, priority,\n              reservedContainer, container);\n    } else {\n      // When reserving container\n      application.reserve(priority, node, reservedContainer, container);\n    }\n    allocationResult.updatedContainer \u003d container;\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(priority);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it every time to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "796638d9bc86235b9f3e5d1a3a9a25bbf5c04d1c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4287. Capacity Scheduler: Rack Locality improvement (Nathan Roberts via wangda)\n",
      "commitDate": "12/11/15 11:09 AM",
      "commitName": "796638d9bc86235b9f3e5d1a3a9a25bbf5c04d1c",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "23/09/15 1:29 PM",
      "commitNameOld": "89cab1ba5f0671f8ef30dbe7432079c18362b434",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 49.94,
      "commitsBetweenForRepo": 405,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,53 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n       Resource clusterResource, FiCaSchedulerNode node,\n       SchedulingMode schedulingMode, Priority priority,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), priority);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d\n           handleNewContainerAllocation(allocationResult, node, priority,\n               reservedContainer, container);\n     } else {\n       // When reserving container\n       application.reserve(priority, node, reservedContainer, container);\n     }\n     allocationResult.updatedContainer \u003d container;\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n-        application.resetSchedulingOpportunities(priority);\n+        // Only reset scheduling opportunities for RACK_LOCAL if configured\n+        // to do so. Not resetting means we will continue to schedule\n+        // RACK_LOCAL without delay.\n+        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n+            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n+          application.resetSchedulingOpportunities(priority);\n+        }\n       }\n-      \n+\n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it every time to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      Resource clusterResource, FiCaSchedulerNode node,\n      SchedulingMode schedulingMode, Priority priority,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), priority);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d\n          handleNewContainerAllocation(allocationResult, node, priority,\n              reservedContainer, container);\n    } else {\n      // When reserving container\n      application.reserve(priority, node, reservedContainer, container);\n    }\n    allocationResult.updatedContainer \u003d container;\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        // Only reset scheduling opportunities for RACK_LOCAL if configured\n        // to do so. Not resetting means we will continue to schedule\n        // RACK_LOCAL without delay.\n        if (allocationResult.containerNodeType \u003d\u003d NodeType.NODE_LOCAL\n            || application.getCSLeafQueue().getRackLocalityFullReset()) {\n          application.resetSchedulingOpportunities(priority);\n        }\n      }\n\n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it every time to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "e5003be907acef87c2770e3f2914953f62017b0e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4026. Refactored ContainerAllocator to accept a list of priorites rather than a single priority. Contributed by Wangda Tan\n",
      "commitDate": "12/08/15 3:07 PM",
      "commitName": "e5003be907acef87c2770e3f2914953f62017b0e",
      "commitAuthor": "Jian He",
      "commitDateOld": "05/08/15 1:47 PM",
      "commitNameOld": "ba2313d6145a1234777938a747187373f4cd58d9",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 7.06,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,47 @@\n   ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n       Resource clusterResource, FiCaSchedulerNode node,\n       SchedulingMode schedulingMode, Priority priority,\n       RMContainer reservedContainer) {\n     // Create the container if necessary\n     Container container \u003d\n         getContainer(reservedContainer, node,\n             allocationResult.getResourceToBeAllocated(), priority);\n \n     // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return ContainerAllocation.QUEUE_SKIPPED;\n+      return ContainerAllocation.APP_SKIPPED;\n     }\n \n     if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n       // When allocating container\n       allocationResult \u003d\n           handleNewContainerAllocation(allocationResult, node, priority,\n               reservedContainer, container);\n     } else {\n       // When reserving container\n       application.reserve(priority, node, reservedContainer, container);\n     }\n     allocationResult.updatedContainer \u003d container;\n \n     // Only reset opportunities when we FIRST allocate the container. (IAW, When\n     // reservedContainer !\u003d null, it\u0027s not the first time)\n     if (reservedContainer \u003d\u003d null) {\n       // Don\u0027t reset scheduling opportunities for off-switch assignments\n       // otherwise the app will be delayed for each non-local assignment.\n       // This helps apps with many off-cluster requests schedule faster.\n       if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resetting scheduling opportunities\");\n         }\n         application.resetSchedulingOpportunities(priority);\n       }\n       \n       // Non-exclusive scheduling opportunity is different: we need reset\n       // it every time to make sure non-labeled resource request will be\n       // most likely allocated on non-labeled nodes first.\n       application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n     }\n \n     return allocationResult;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      Resource clusterResource, FiCaSchedulerNode node,\n      SchedulingMode schedulingMode, Priority priority,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), priority);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return ContainerAllocation.APP_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d\n          handleNewContainerAllocation(allocationResult, node, priority,\n              reservedContainer, container);\n    } else {\n      // When reserving container\n      application.reserve(priority, node, reservedContainer, container);\n    }\n    allocationResult.updatedContainer \u003d container;\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        application.resetSchedulingOpportunities(priority);\n      }\n      \n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it every time to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "ba2313d6145a1234777938a747187373f4cd58d9": {
      "type": "Yintroduced",
      "commitMessage": "YARN-3983. Refactored CapacityScheduleri#FiCaSchedulerApp to easier extend container allocation logic. Contributed by Wangda Tan\n",
      "commitDate": "05/08/15 1:47 PM",
      "commitName": "ba2313d6145a1234777938a747187373f4cd58d9",
      "commitAuthor": "Jian He",
      "diff": "@@ -0,0 +1,47 @@\n+  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n+      Resource clusterResource, FiCaSchedulerNode node,\n+      SchedulingMode schedulingMode, Priority priority,\n+      RMContainer reservedContainer) {\n+    // Create the container if necessary\n+    Container container \u003d\n+        getContainer(reservedContainer, node,\n+            allocationResult.getResourceToBeAllocated(), priority);\n+\n+    // something went wrong getting/creating the container\n+    if (container \u003d\u003d null) {\n+      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n+      return ContainerAllocation.QUEUE_SKIPPED;\n+    }\n+\n+    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n+      // When allocating container\n+      allocationResult \u003d\n+          handleNewContainerAllocation(allocationResult, node, priority,\n+              reservedContainer, container);\n+    } else {\n+      // When reserving container\n+      application.reserve(priority, node, reservedContainer, container);\n+    }\n+    allocationResult.updatedContainer \u003d container;\n+\n+    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n+    // reservedContainer !\u003d null, it\u0027s not the first time)\n+    if (reservedContainer \u003d\u003d null) {\n+      // Don\u0027t reset scheduling opportunities for off-switch assignments\n+      // otherwise the app will be delayed for each non-local assignment.\n+      // This helps apps with many off-cluster requests schedule faster.\n+      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Resetting scheduling opportunities\");\n+        }\n+        application.resetSchedulingOpportunities(priority);\n+      }\n+      \n+      // Non-exclusive scheduling opportunity is different: we need reset\n+      // it every time to make sure non-labeled resource request will be\n+      // most likely allocated on non-labeled nodes first.\n+      application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n+    }\n+\n+    return allocationResult;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  ContainerAllocation doAllocation(ContainerAllocation allocationResult,\n      Resource clusterResource, FiCaSchedulerNode node,\n      SchedulingMode schedulingMode, Priority priority,\n      RMContainer reservedContainer) {\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(reservedContainer, node,\n            allocationResult.getResourceToBeAllocated(), priority);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return ContainerAllocation.QUEUE_SKIPPED;\n    }\n\n    if (allocationResult.getAllocationState() \u003d\u003d AllocationState.ALLOCATED) {\n      // When allocating container\n      allocationResult \u003d\n          handleNewContainerAllocation(allocationResult, node, priority,\n              reservedContainer, container);\n    } else {\n      // When reserving container\n      application.reserve(priority, node, reservedContainer, container);\n    }\n    allocationResult.updatedContainer \u003d container;\n\n    // Only reset opportunities when we FIRST allocate the container. (IAW, When\n    // reservedContainer !\u003d null, it\u0027s not the first time)\n    if (reservedContainer \u003d\u003d null) {\n      // Don\u0027t reset scheduling opportunities for off-switch assignments\n      // otherwise the app will be delayed for each non-local assignment.\n      // This helps apps with many off-cluster requests schedule faster.\n      if (allocationResult.containerNodeType !\u003d NodeType.OFF_SWITCH) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Resetting scheduling opportunities\");\n        }\n        application.resetSchedulingOpportunities(priority);\n      }\n      \n      // Non-exclusive scheduling opportunity is different: we need reset\n      // it every time to make sure non-labeled resource request will be\n      // most likely allocated on non-labeled nodes first.\n      application.resetMissedNonPartitionedRequestSchedulingOpportunity(priority);\n    }\n\n    return allocationResult;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java"
    }
  }
}