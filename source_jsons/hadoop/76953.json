{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HHUtil.java",
  "functionName": "getPiggyBacksFromInput",
  "functionId": "getPiggyBacksFromInput___inputs-ByteBuffer[]__piggyBackIndex-int[]__numParityUnits-int__pgIndex-int__encoder-RawErasureEncoder",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java",
  "functionStartLine": 64,
  "functionEndLine": 120,
  "numCommitsSeen": 2,
  "timeTaken": 931,
  "changeHistory": [
    "31ebccc96238136560f4210bdf6766fe18e0650c",
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743"
  ],
  "changeHistoryShort": {
    "31ebccc96238136560f4210bdf6766fe18e0650c": "Yexceptionschange",
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743": "Yintroduced"
  },
  "changeHistoryDetails": {
    "31ebccc96238136560f4210bdf6766fe18e0650c": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-12613. Native EC coder should implement release() as idempotent function. (Lei (Eddy) Xu)\n",
      "commitDate": "16/10/17 7:44 PM",
      "commitName": "31ebccc96238136560f4210bdf6766fe18e0650c",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "21/01/16 10:30 AM",
      "commitNameOld": "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 634.34,
      "commitsBetweenForRepo": 4248,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,57 @@\n   public static ByteBuffer[] getPiggyBacksFromInput(ByteBuffer[] inputs,\n                                                     int[] piggyBackIndex,\n                                                     int numParityUnits,\n                                                     int pgIndex,\n-                                                    RawErasureEncoder encoder) {\n+                                                    RawErasureEncoder encoder)\n+      throws IOException {\n     ByteBuffer[] emptyInput \u003d new ByteBuffer[inputs.length];\n     ByteBuffer[] tempInput \u003d new ByteBuffer[inputs.length];\n     int[] inputPositions \u003d new int[inputs.length];\n \n     for (int m \u003d 0; m \u003c inputs.length; ++m) {\n       if (inputs[m] !\u003d null) {\n         emptyInput[m] \u003d allocateByteBuffer(inputs[m].isDirect(),\n                 inputs[m].remaining());\n       }\n     }\n \n     ByteBuffer[] tempOutput \u003d new ByteBuffer[numParityUnits];\n     for (int m \u003d 0; m \u003c numParityUnits; ++m) {\n       tempOutput[m] \u003d allocateByteBuffer(inputs[m].isDirect(),\n               inputs[0].remaining());\n     }\n \n     ByteBuffer[] piggyBacks \u003d new ByteBuffer[numParityUnits - 1];\n     assert (piggyBackIndex.length \u003e\u003d numParityUnits);\n \n     // using underlying RS code to create piggybacks\n     for (int i \u003d 0; i \u003c numParityUnits - 1; ++i) {\n       for (int k \u003d piggyBackIndex[i]; k \u003c piggyBackIndex[i + 1]; ++k) {\n         tempInput[k] \u003d inputs[k];\n         inputPositions[k] \u003d inputs[k].position();\n       }\n       for (int n \u003d 0; n \u003c emptyInput.length; ++n) {\n         if (tempInput[n] \u003d\u003d null) {\n           tempInput[n] \u003d emptyInput[n];\n           inputPositions[n] \u003d emptyInput[n].position();\n         }\n       }\n \n       encoder.encode(tempInput, tempOutput);\n \n       piggyBacks[i] \u003d cloneBufferData(tempOutput[pgIndex]);\n \n       for (int j \u003d 0; j \u003c tempInput.length; j++) {\n         if (tempInput[j] !\u003d null) {\n           tempInput[j].position(inputPositions[j]);\n           tempInput[j] \u003d null;\n         }\n       }\n \n       for (int j \u003d 0; j \u003c tempOutput.length; j++) {\n         tempOutput[j].clear();\n       }\n     }\n \n     return piggyBacks;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static ByteBuffer[] getPiggyBacksFromInput(ByteBuffer[] inputs,\n                                                    int[] piggyBackIndex,\n                                                    int numParityUnits,\n                                                    int pgIndex,\n                                                    RawErasureEncoder encoder)\n      throws IOException {\n    ByteBuffer[] emptyInput \u003d new ByteBuffer[inputs.length];\n    ByteBuffer[] tempInput \u003d new ByteBuffer[inputs.length];\n    int[] inputPositions \u003d new int[inputs.length];\n\n    for (int m \u003d 0; m \u003c inputs.length; ++m) {\n      if (inputs[m] !\u003d null) {\n        emptyInput[m] \u003d allocateByteBuffer(inputs[m].isDirect(),\n                inputs[m].remaining());\n      }\n    }\n\n    ByteBuffer[] tempOutput \u003d new ByteBuffer[numParityUnits];\n    for (int m \u003d 0; m \u003c numParityUnits; ++m) {\n      tempOutput[m] \u003d allocateByteBuffer(inputs[m].isDirect(),\n              inputs[0].remaining());\n    }\n\n    ByteBuffer[] piggyBacks \u003d new ByteBuffer[numParityUnits - 1];\n    assert (piggyBackIndex.length \u003e\u003d numParityUnits);\n\n    // using underlying RS code to create piggybacks\n    for (int i \u003d 0; i \u003c numParityUnits - 1; ++i) {\n      for (int k \u003d piggyBackIndex[i]; k \u003c piggyBackIndex[i + 1]; ++k) {\n        tempInput[k] \u003d inputs[k];\n        inputPositions[k] \u003d inputs[k].position();\n      }\n      for (int n \u003d 0; n \u003c emptyInput.length; ++n) {\n        if (tempInput[n] \u003d\u003d null) {\n          tempInput[n] \u003d emptyInput[n];\n          inputPositions[n] \u003d emptyInput[n].position();\n        }\n      }\n\n      encoder.encode(tempInput, tempOutput);\n\n      piggyBacks[i] \u003d cloneBufferData(tempOutput[pgIndex]);\n\n      for (int j \u003d 0; j \u003c tempInput.length; j++) {\n        if (tempInput[j] !\u003d null) {\n          tempInput[j].position(inputPositions[j]);\n          tempInput[j] \u003d null;\n        }\n      }\n\n      for (int j \u003d 0; j \u003c tempOutput.length; j++) {\n        tempOutput[j].clear();\n      }\n    }\n\n    return piggyBacks;\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[IOException]"
      }
    },
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-11828. Implement the Hitchhiker erasure coding algorithm. Contributed by Jack Liu Quan.\n\nChange-Id: If43475ccc2574df60949c947af562722db076251\n",
      "commitDate": "21/01/16 10:30 AM",
      "commitName": "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743",
      "commitAuthor": "Zhe Zhang",
      "diff": "@@ -0,0 +1,56 @@\n+  public static ByteBuffer[] getPiggyBacksFromInput(ByteBuffer[] inputs,\n+                                                    int[] piggyBackIndex,\n+                                                    int numParityUnits,\n+                                                    int pgIndex,\n+                                                    RawErasureEncoder encoder) {\n+    ByteBuffer[] emptyInput \u003d new ByteBuffer[inputs.length];\n+    ByteBuffer[] tempInput \u003d new ByteBuffer[inputs.length];\n+    int[] inputPositions \u003d new int[inputs.length];\n+\n+    for (int m \u003d 0; m \u003c inputs.length; ++m) {\n+      if (inputs[m] !\u003d null) {\n+        emptyInput[m] \u003d allocateByteBuffer(inputs[m].isDirect(),\n+                inputs[m].remaining());\n+      }\n+    }\n+\n+    ByteBuffer[] tempOutput \u003d new ByteBuffer[numParityUnits];\n+    for (int m \u003d 0; m \u003c numParityUnits; ++m) {\n+      tempOutput[m] \u003d allocateByteBuffer(inputs[m].isDirect(),\n+              inputs[0].remaining());\n+    }\n+\n+    ByteBuffer[] piggyBacks \u003d new ByteBuffer[numParityUnits - 1];\n+    assert (piggyBackIndex.length \u003e\u003d numParityUnits);\n+\n+    // using underlying RS code to create piggybacks\n+    for (int i \u003d 0; i \u003c numParityUnits - 1; ++i) {\n+      for (int k \u003d piggyBackIndex[i]; k \u003c piggyBackIndex[i + 1]; ++k) {\n+        tempInput[k] \u003d inputs[k];\n+        inputPositions[k] \u003d inputs[k].position();\n+      }\n+      for (int n \u003d 0; n \u003c emptyInput.length; ++n) {\n+        if (tempInput[n] \u003d\u003d null) {\n+          tempInput[n] \u003d emptyInput[n];\n+          inputPositions[n] \u003d emptyInput[n].position();\n+        }\n+      }\n+\n+      encoder.encode(tempInput, tempOutput);\n+\n+      piggyBacks[i] \u003d cloneBufferData(tempOutput[pgIndex]);\n+\n+      for (int j \u003d 0; j \u003c tempInput.length; j++) {\n+        if (tempInput[j] !\u003d null) {\n+          tempInput[j].position(inputPositions[j]);\n+          tempInput[j] \u003d null;\n+        }\n+      }\n+\n+      for (int j \u003d 0; j \u003c tempOutput.length; j++) {\n+        tempOutput[j].clear();\n+      }\n+    }\n+\n+    return piggyBacks;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static ByteBuffer[] getPiggyBacksFromInput(ByteBuffer[] inputs,\n                                                    int[] piggyBackIndex,\n                                                    int numParityUnits,\n                                                    int pgIndex,\n                                                    RawErasureEncoder encoder) {\n    ByteBuffer[] emptyInput \u003d new ByteBuffer[inputs.length];\n    ByteBuffer[] tempInput \u003d new ByteBuffer[inputs.length];\n    int[] inputPositions \u003d new int[inputs.length];\n\n    for (int m \u003d 0; m \u003c inputs.length; ++m) {\n      if (inputs[m] !\u003d null) {\n        emptyInput[m] \u003d allocateByteBuffer(inputs[m].isDirect(),\n                inputs[m].remaining());\n      }\n    }\n\n    ByteBuffer[] tempOutput \u003d new ByteBuffer[numParityUnits];\n    for (int m \u003d 0; m \u003c numParityUnits; ++m) {\n      tempOutput[m] \u003d allocateByteBuffer(inputs[m].isDirect(),\n              inputs[0].remaining());\n    }\n\n    ByteBuffer[] piggyBacks \u003d new ByteBuffer[numParityUnits - 1];\n    assert (piggyBackIndex.length \u003e\u003d numParityUnits);\n\n    // using underlying RS code to create piggybacks\n    for (int i \u003d 0; i \u003c numParityUnits - 1; ++i) {\n      for (int k \u003d piggyBackIndex[i]; k \u003c piggyBackIndex[i + 1]; ++k) {\n        tempInput[k] \u003d inputs[k];\n        inputPositions[k] \u003d inputs[k].position();\n      }\n      for (int n \u003d 0; n \u003c emptyInput.length; ++n) {\n        if (tempInput[n] \u003d\u003d null) {\n          tempInput[n] \u003d emptyInput[n];\n          inputPositions[n] \u003d emptyInput[n].position();\n        }\n      }\n\n      encoder.encode(tempInput, tempOutput);\n\n      piggyBacks[i] \u003d cloneBufferData(tempOutput[pgIndex]);\n\n      for (int j \u003d 0; j \u003c tempInput.length; j++) {\n        if (tempInput[j] !\u003d null) {\n          tempInput[j].position(inputPositions[j]);\n          tempInput[j] \u003d null;\n        }\n      }\n\n      for (int j \u003d 0; j \u003c tempOutput.length; j++) {\n        tempOutput[j].clear();\n      }\n    }\n\n    return piggyBacks;\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java"
    }
  }
}