{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ReaderStrategy.java",
  "functionName": "readFromBlock",
  "functionId": "readFromBlock___blockReader-BlockReader__length-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReaderStrategy.java",
  "functionStartLine": 176,
  "functionEndLine": 187,
  "numCommitsSeen": 4,
  "timeTaken": 1813,
  "changeHistory": [
    "08bb6c49a5aec32b7d9f29238560f947420405d6",
    "6d5e87aec2f615ed265dc495873bf53ee7d2ace2",
    "f4a21d3abaa7c5a9f0a0d8417e81f7eaf3d1b29a",
    "793447f79924c97c2b562d5e41fa85adf19673fe"
  ],
  "changeHistoryShort": {
    "08bb6c49a5aec32b7d9f29238560f947420405d6": "Ybodychange",
    "6d5e87aec2f615ed265dc495873bf53ee7d2ace2": "Ybodychange",
    "f4a21d3abaa7c5a9f0a0d8417e81f7eaf3d1b29a": "Ybodychange",
    "793447f79924c97c2b562d5e41fa85adf19673fe": "Yintroduced"
  },
  "changeHistoryDetails": {
    "08bb6c49a5aec32b7d9f29238560f947420405d6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13926. ThreadLocal aggregations for FileSystem.Statistics are incorrect with striped reads.\nContributed by Xiao Chen, Hrishikesh Gadre.\n\nSigned-off-by: Xiao Chen \u003cxiao@apache.org\u003e\n",
      "commitDate": "08/10/18 8:31 PM",
      "commitName": "08bb6c49a5aec32b7d9f29238560f947420405d6",
      "commitAuthor": "Hrishikesh Gadre",
      "commitDateOld": "04/06/18 9:13 PM",
      "commitNameOld": "6d5e87aec2f615ed265dc495873bf53ee7d2ace2",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 125.97,
      "commitsBetweenForRepo": 967,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,12 @@\n   public int readFromBlock(BlockReader blockReader,\n                            int length) throws IOException {\n     ByteBuffer tmpBuf \u003d readBuf.duplicate();\n     tmpBuf.limit(tmpBuf.position() + length);\n     int nRead \u003d blockReader.read(tmpBuf);\n     // Only when data are read, update the position\n     if (nRead \u003e 0) {\n       readBuf.position(readBuf.position() + nRead);\n-      updateReadStatistics(readStatistics, nRead, blockReader);\n-      dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n-          nRead);\n-      if (readStatistics.getBlockType() \u003d\u003d BlockType.STRIPED) {\n-        dfsClient.updateFileSystemECReadStats(nRead);\n-      }\n     }\n \n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int readFromBlock(BlockReader blockReader,\n                           int length) throws IOException {\n    ByteBuffer tmpBuf \u003d readBuf.duplicate();\n    tmpBuf.limit(tmpBuf.position() + length);\n    int nRead \u003d blockReader.read(tmpBuf);\n    // Only when data are read, update the position\n    if (nRead \u003e 0) {\n      readBuf.position(readBuf.position() + nRead);\n    }\n\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReaderStrategy.java",
      "extendedDetails": {}
    },
    "6d5e87aec2f615ed265dc495873bf53ee7d2ace2": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15507. Add MapReduce counters about EC bytes read.\n",
      "commitDate": "04/06/18 9:13 PM",
      "commitName": "6d5e87aec2f615ed265dc495873bf53ee7d2ace2",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "26/08/16 7:54 PM",
      "commitNameOld": "f4a21d3abaa7c5a9f0a0d8417e81f7eaf3d1b29a",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 647.06,
      "commitsBetweenForRepo": 4893,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,18 @@\n   public int readFromBlock(BlockReader blockReader,\n                            int length) throws IOException {\n     ByteBuffer tmpBuf \u003d readBuf.duplicate();\n     tmpBuf.limit(tmpBuf.position() + length);\n     int nRead \u003d blockReader.read(tmpBuf);\n     // Only when data are read, update the position\n     if (nRead \u003e 0) {\n       readBuf.position(readBuf.position() + nRead);\n       updateReadStatistics(readStatistics, nRead, blockReader);\n       dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n           nRead);\n+      if (readStatistics.getBlockType() \u003d\u003d BlockType.STRIPED) {\n+        dfsClient.updateFileSystemECReadStats(nRead);\n+      }\n     }\n \n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int readFromBlock(BlockReader blockReader,\n                           int length) throws IOException {\n    ByteBuffer tmpBuf \u003d readBuf.duplicate();\n    tmpBuf.limit(tmpBuf.position() + length);\n    int nRead \u003d blockReader.read(tmpBuf);\n    // Only when data are read, update the position\n    if (nRead \u003e 0) {\n      readBuf.position(readBuf.position() + nRead);\n      updateReadStatistics(readStatistics, nRead, blockReader);\n      dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n          nRead);\n      if (readStatistics.getBlockType() \u003d\u003d BlockType.STRIPED) {\n        dfsClient.updateFileSystemECReadStats(nRead);\n      }\n    }\n\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReaderStrategy.java",
      "extendedDetails": {}
    },
    "f4a21d3abaa7c5a9f0a0d8417e81f7eaf3d1b29a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10795. Fix an error in ReaderStrategy#ByteBufferStrategy. Contributed by Sammi Chen\n",
      "commitDate": "26/08/16 7:54 PM",
      "commitName": "f4a21d3abaa7c5a9f0a0d8417e81f7eaf3d1b29a",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "24/08/16 6:57 AM",
      "commitNameOld": "793447f79924c97c2b562d5e41fa85adf19673fe",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 2.54,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   public int readFromBlock(BlockReader blockReader,\n                            int length) throws IOException {\n     ByteBuffer tmpBuf \u003d readBuf.duplicate();\n     tmpBuf.limit(tmpBuf.position() + length);\n-    int nRead \u003d blockReader.read(readBuf.slice());\n+    int nRead \u003d blockReader.read(tmpBuf);\n     // Only when data are read, update the position\n     if (nRead \u003e 0) {\n       readBuf.position(readBuf.position() + nRead);\n       updateReadStatistics(readStatistics, nRead, blockReader);\n       dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n           nRead);\n     }\n \n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int readFromBlock(BlockReader blockReader,\n                           int length) throws IOException {\n    ByteBuffer tmpBuf \u003d readBuf.duplicate();\n    tmpBuf.limit(tmpBuf.position() + length);\n    int nRead \u003d blockReader.read(tmpBuf);\n    // Only when data are read, update the position\n    if (nRead \u003e 0) {\n      readBuf.position(readBuf.position() + nRead);\n      updateReadStatistics(readStatistics, nRead, blockReader);\n      dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n          nRead);\n    }\n\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReaderStrategy.java",
      "extendedDetails": {}
    },
    "793447f79924c97c2b562d5e41fa85adf19673fe": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8905. Refactor DFSInputStream#ReaderStrategy. Contributed by Kai Zheng and Sammi Chen\n",
      "commitDate": "24/08/16 6:57 AM",
      "commitName": "793447f79924c97c2b562d5e41fa85adf19673fe",
      "commitAuthor": "Kai Zheng",
      "diff": "@@ -0,0 +1,15 @@\n+  public int readFromBlock(BlockReader blockReader,\n+                           int length) throws IOException {\n+    ByteBuffer tmpBuf \u003d readBuf.duplicate();\n+    tmpBuf.limit(tmpBuf.position() + length);\n+    int nRead \u003d blockReader.read(readBuf.slice());\n+    // Only when data are read, update the position\n+    if (nRead \u003e 0) {\n+      readBuf.position(readBuf.position() + nRead);\n+      updateReadStatistics(readStatistics, nRead, blockReader);\n+      dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n+          nRead);\n+    }\n+\n+    return nRead;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public int readFromBlock(BlockReader blockReader,\n                           int length) throws IOException {\n    ByteBuffer tmpBuf \u003d readBuf.duplicate();\n    tmpBuf.limit(tmpBuf.position() + length);\n    int nRead \u003d blockReader.read(readBuf.slice());\n    // Only when data are read, update the position\n    if (nRead \u003e 0) {\n      readBuf.position(readBuf.position() + nRead);\n      updateReadStatistics(readStatistics, nRead, blockReader);\n      dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(),\n          nRead);\n    }\n\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReaderStrategy.java"
    }
  }
}