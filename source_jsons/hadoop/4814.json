{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FileWithSnapshotFeature.java",
  "functionName": "collectBlocksAndClear",
  "functionId": "collectBlocksAndClear___reclaimContext-INode.ReclaimContext__file-INodeFile(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
  "functionStartLine": 218,
  "functionEndLine": 242,
  "numCommitsSeen": 143,
  "timeTaken": 10421,
  "changeHistory": [
    "972782d9568e0849484c027f27c1638ba50ec56e",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "b2c85db86c9a62b0a03ee87547265077f664970a",
    "4536399d47f6c061e149e2504600804a0f1e093d",
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
    "5dae97a584d30cef3e34141edfaca49c4ec57913",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f",
    "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
    "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
    "8df119da214babde03e73243c7ca4cfe6d0ca562",
    "92e0416ced279a910616985bf11fa3f8b1b1de9b",
    "f29fa9e820e25730d00a1a00c51c6f11028fb5a7",
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
    "02e6b72ae148fc8c2ba02ef624536b9e48997b31",
    "afe77ce53d3cf203690aa419e377f26cbd45a96e",
    "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
    "b71d3868908a49c1b2e353afea795a76dfb20f7d",
    "397835acdf66cf48ebdbc256aa15b6660181c339",
    "25116c26fd9b998025fa28666ae45ab03a995d91",
    "8a577a16f96437d87ad764dedbdc67d4c184b8d9",
    "deaf979d4122a0a0e4ae0557abbb7f17d18a9380",
    "719279ea8a510ba8d04174ac85ad42fa991725a2"
  ],
  "changeHistoryShort": {
    "972782d9568e0849484c027f27c1638ba50ec56e": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "b2c85db86c9a62b0a03ee87547265077f664970a": "Ybodychange",
    "4536399d47f6c061e149e2504600804a0f1e093d": "Ymultichange(Yparameterchange,Ybodychange)",
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63": "Ybodychange",
    "5dae97a584d30cef3e34141edfaca49c4ec57913": "Ymultichange(Yparameterchange,Ybodychange)",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": "Ymultichange(Ymodifierchange,Ybodychange)",
    "7e9358feb326d48b8c4f00249e7af5023cebd2e2": "Ybodychange",
    "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a": "Ymultichange(Yfilerename,Ymodifierchange,Ybodychange,Yparameterchange)",
    "8df119da214babde03e73243c7ca4cfe6d0ca562": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "92e0416ced279a910616985bf11fa3f8b1b1de9b": "Ymultichange(Yparameterchange,Ybodychange)",
    "f29fa9e820e25730d00a1a00c51c6f11028fb5a7": "Ybodychange",
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd": "Ybodychange",
    "02e6b72ae148fc8c2ba02ef624536b9e48997b31": "Ybodychange",
    "afe77ce53d3cf203690aa419e377f26cbd45a96e": "Ybodychange",
    "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
    "b71d3868908a49c1b2e353afea795a76dfb20f7d": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "397835acdf66cf48ebdbc256aa15b6660181c339": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "25116c26fd9b998025fa28666ae45ab03a995d91": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "8a577a16f96437d87ad764dedbdc67d4c184b8d9": "Ymultichange(Yparameterchange,Ybodychange)",
    "deaf979d4122a0a0e4ae0557abbb7f17d18a9380": "Ybodychange",
    "719279ea8a510ba8d04174ac85ad42fa991725a2": "Yintroduced"
  },
  "changeHistoryDetails": {
    "972782d9568e0849484c027f27c1638ba50ec56e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9754. Avoid unnecessary getBlockCollection calls in BlockManager. Contributed by Jing Zhao.\n",
      "commitDate": "12/02/16 11:07 AM",
      "commitName": "972782d9568e0849484c027f27c1638ba50ec56e",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "22/08/15 12:09 AM",
      "commitNameOld": "745d04be59accf80feda0ad38efcc74ba362f2ca",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 174.5,
      "commitsBetweenForRepo": 1204,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   public void collectBlocksAndClear(\n       INode.ReclaimContext reclaimContext, final INodeFile file) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n       file.clearFile(reclaimContext);\n       return;\n     }\n     // find max file size.\n     final long max;\n     FileDiff diff \u003d getDiffs().getLast();\n     if (isCurrentFileDeleted()) {\n       max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n     } else {\n       max \u003d file.computeFileSize();\n     }\n \n     // Collect blocks that should be deleted\n     FileDiff last \u003d diffs.getLast();\n     BlockInfo[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n     if(snapshotBlocks \u003d\u003d null)\n-      file.collectBlocksBeyondMax(max, reclaimContext.collectedBlocks());\n+      file.collectBlocksBeyondMax(max, reclaimContext.collectedBlocks(), null);\n     else\n       file.collectBlocksBeyondSnapshot(snapshotBlocks,\n                                        reclaimContext.collectedBlocks());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void collectBlocksAndClear(\n      INode.ReclaimContext reclaimContext, final INodeFile file) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.clearFile(reclaimContext);\n      return;\n    }\n    // find max file size.\n    final long max;\n    FileDiff diff \u003d getDiffs().getLast();\n    if (isCurrentFileDeleted()) {\n      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n    } else {\n      max \u003d file.computeFileSize();\n    }\n\n    // Collect blocks that should be deleted\n    FileDiff last \u003d diffs.getLast();\n    BlockInfo[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n    if(snapshotBlocks \u003d\u003d null)\n      file.collectBlocksBeyondMax(max, reclaimContext.collectedBlocks(), null);\n    else\n      file.collectBlocksBeyondSnapshot(snapshotBlocks,\n                                       reclaimContext.collectedBlocks());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "13/05/15 9:50 PM",
      "commitNameOld": "b2c85db86c9a62b0a03ee87547265077f664970a",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 13.74,
      "commitsBetweenForRepo": 96,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   public void collectBlocksAndClear(\n       INode.ReclaimContext reclaimContext, final INodeFile file) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n       file.clearFile(reclaimContext);\n       return;\n     }\n     // find max file size.\n     final long max;\n     FileDiff diff \u003d getDiffs().getLast();\n     if (isCurrentFileDeleted()) {\n       max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n     } else {\n       max \u003d file.computeFileSize();\n     }\n \n     // Collect blocks that should be deleted\n     FileDiff last \u003d diffs.getLast();\n-    BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n+    BlockInfo[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n     if(snapshotBlocks \u003d\u003d null)\n       file.collectBlocksBeyondMax(max, reclaimContext.collectedBlocks());\n     else\n       file.collectBlocksBeyondSnapshot(snapshotBlocks,\n                                        reclaimContext.collectedBlocks());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void collectBlocksAndClear(\n      INode.ReclaimContext reclaimContext, final INodeFile file) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.clearFile(reclaimContext);\n      return;\n    }\n    // find max file size.\n    final long max;\n    FileDiff diff \u003d getDiffs().getLast();\n    if (isCurrentFileDeleted()) {\n      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n    } else {\n      max \u003d file.computeFileSize();\n    }\n\n    // Collect blocks that should be deleted\n    FileDiff last \u003d diffs.getLast();\n    BlockInfo[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n    if(snapshotBlocks \u003d\u003d null)\n      file.collectBlocksBeyondMax(max, reclaimContext.collectedBlocks());\n    else\n      file.collectBlocksBeyondSnapshot(snapshotBlocks,\n                                       reclaimContext.collectedBlocks());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
      "extendedDetails": {}
    },
    "b2c85db86c9a62b0a03ee87547265077f664970a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7728. Avoid updating quota usage while loading edits. Contributed by Jing Zhao.\n",
      "commitDate": "13/05/15 9:50 PM",
      "commitName": "b2c85db86c9a62b0a03ee87547265077f664970a",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "12/05/15 6:29 AM",
      "commitNameOld": "6d5da9484185ca9f585195d6da069b9cd5be4044",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 1.64,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   public void collectBlocksAndClear(\n       INode.ReclaimContext reclaimContext, final INodeFile file) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n-      file.destroyAndCollectBlocks(reclaimContext);\n+      file.clearFile(reclaimContext);\n       return;\n     }\n     // find max file size.\n     final long max;\n     FileDiff diff \u003d getDiffs().getLast();\n     if (isCurrentFileDeleted()) {\n       max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n-    } else { \n+    } else {\n       max \u003d file.computeFileSize();\n     }\n \n     // Collect blocks that should be deleted\n     FileDiff last \u003d diffs.getLast();\n     BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n     if(snapshotBlocks \u003d\u003d null)\n       file.collectBlocksBeyondMax(max, reclaimContext.collectedBlocks());\n     else\n       file.collectBlocksBeyondSnapshot(snapshotBlocks,\n                                        reclaimContext.collectedBlocks());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void collectBlocksAndClear(\n      INode.ReclaimContext reclaimContext, final INodeFile file) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.clearFile(reclaimContext);\n      return;\n    }\n    // find max file size.\n    final long max;\n    FileDiff diff \u003d getDiffs().getLast();\n    if (isCurrentFileDeleted()) {\n      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n    } else {\n      max \u003d file.computeFileSize();\n    }\n\n    // Collect blocks that should be deleted\n    FileDiff last \u003d diffs.getLast();\n    BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n    if(snapshotBlocks \u003d\u003d null)\n      file.collectBlocksBeyondMax(max, reclaimContext.collectedBlocks());\n    else\n      file.collectBlocksBeyondSnapshot(snapshotBlocks,\n                                       reclaimContext.collectedBlocks());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
      "extendedDetails": {}
    },
    "4536399d47f6c061e149e2504600804a0f1e093d": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8357. Consolidate parameters of INode.CleanSubtree() into a parameter objects. Contributed by Li Lu.\n",
      "commitDate": "09/05/15 10:51 PM",
      "commitName": "4536399d47f6c061e149e2504600804a0f1e093d",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8357. Consolidate parameters of INode.CleanSubtree() into a parameter objects. Contributed by Li Lu.\n",
          "commitDate": "09/05/15 10:51 PM",
          "commitName": "4536399d47f6c061e149e2504600804a0f1e093d",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "08/05/15 11:09 PM",
          "commitNameOld": "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 0.99,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,25 @@\n-  public void collectBlocksAndClear(final BlockStoragePolicySuite bsps, final INodeFile file,\n-      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n+  public void collectBlocksAndClear(\n+      INode.ReclaimContext reclaimContext, final INodeFile file) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n-      file.destroyAndCollectBlocks(bsps, info, removedINodes, null);\n+      file.destroyAndCollectBlocks(reclaimContext);\n       return;\n     }\n     // find max file size.\n     final long max;\n     FileDiff diff \u003d getDiffs().getLast();\n     if (isCurrentFileDeleted()) {\n       max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n     } else { \n       max \u003d file.computeFileSize();\n     }\n \n     // Collect blocks that should be deleted\n     FileDiff last \u003d diffs.getLast();\n     BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n     if(snapshotBlocks \u003d\u003d null)\n-      file.collectBlocksBeyondMax(max, info);\n+      file.collectBlocksBeyondMax(max, reclaimContext.collectedBlocks());\n     else\n-      file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n+      file.collectBlocksBeyondSnapshot(snapshotBlocks,\n+                                       reclaimContext.collectedBlocks());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void collectBlocksAndClear(\n      INode.ReclaimContext reclaimContext, final INodeFile file) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(reclaimContext);\n      return;\n    }\n    // find max file size.\n    final long max;\n    FileDiff diff \u003d getDiffs().getLast();\n    if (isCurrentFileDeleted()) {\n      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    // Collect blocks that should be deleted\n    FileDiff last \u003d diffs.getLast();\n    BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n    if(snapshotBlocks \u003d\u003d null)\n      file.collectBlocksBeyondMax(max, reclaimContext.collectedBlocks());\n    else\n      file.collectBlocksBeyondSnapshot(snapshotBlocks,\n                                       reclaimContext.collectedBlocks());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {
            "oldValue": "[bsps-BlockStoragePolicySuite(modifiers-final), file-INodeFile(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final), removedINodes-List\u003cINode\u003e(modifiers-final)]",
            "newValue": "[reclaimContext-INode.ReclaimContext, file-INodeFile(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8357. Consolidate parameters of INode.CleanSubtree() into a parameter objects. Contributed by Li Lu.\n",
          "commitDate": "09/05/15 10:51 PM",
          "commitName": "4536399d47f6c061e149e2504600804a0f1e093d",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "08/05/15 11:09 PM",
          "commitNameOld": "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 0.99,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,25 @@\n-  public void collectBlocksAndClear(final BlockStoragePolicySuite bsps, final INodeFile file,\n-      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n+  public void collectBlocksAndClear(\n+      INode.ReclaimContext reclaimContext, final INodeFile file) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n-      file.destroyAndCollectBlocks(bsps, info, removedINodes, null);\n+      file.destroyAndCollectBlocks(reclaimContext);\n       return;\n     }\n     // find max file size.\n     final long max;\n     FileDiff diff \u003d getDiffs().getLast();\n     if (isCurrentFileDeleted()) {\n       max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n     } else { \n       max \u003d file.computeFileSize();\n     }\n \n     // Collect blocks that should be deleted\n     FileDiff last \u003d diffs.getLast();\n     BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n     if(snapshotBlocks \u003d\u003d null)\n-      file.collectBlocksBeyondMax(max, info);\n+      file.collectBlocksBeyondMax(max, reclaimContext.collectedBlocks());\n     else\n-      file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n+      file.collectBlocksBeyondSnapshot(snapshotBlocks,\n+                                       reclaimContext.collectedBlocks());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void collectBlocksAndClear(\n      INode.ReclaimContext reclaimContext, final INodeFile file) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(reclaimContext);\n      return;\n    }\n    // find max file size.\n    final long max;\n    FileDiff diff \u003d getDiffs().getLast();\n    if (isCurrentFileDeleted()) {\n      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    // Collect blocks that should be deleted\n    FileDiff last \u003d diffs.getLast();\n    BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n    if(snapshotBlocks \u003d\u003d null)\n      file.collectBlocksBeyondMax(max, reclaimContext.collectedBlocks());\n    else\n      file.collectBlocksBeyondSnapshot(snapshotBlocks,\n                                       reclaimContext.collectedBlocks());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {}
        }
      ]
    },
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6757. Simplify lease manager with INodeID. Contributed by Haohui Mai.\n",
      "commitDate": "08/05/15 11:04 PM",
      "commitName": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.54,
      "commitsBetweenForRepo": 129,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public void collectBlocksAndClear(final BlockStoragePolicySuite bsps, final INodeFile file,\n       final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n-      file.destroyAndCollectBlocks(bsps, info, removedINodes);\n+      file.destroyAndCollectBlocks(bsps, info, removedINodes, null);\n       return;\n     }\n     // find max file size.\n     final long max;\n     FileDiff diff \u003d getDiffs().getLast();\n     if (isCurrentFileDeleted()) {\n       max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n     } else { \n       max \u003d file.computeFileSize();\n     }\n \n     // Collect blocks that should be deleted\n     FileDiff last \u003d diffs.getLast();\n     BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n     if(snapshotBlocks \u003d\u003d null)\n       file.collectBlocksBeyondMax(max, info);\n     else\n       file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void collectBlocksAndClear(final BlockStoragePolicySuite bsps, final INodeFile file,\n      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(bsps, info, removedINodes, null);\n      return;\n    }\n    // find max file size.\n    final long max;\n    FileDiff diff \u003d getDiffs().getLast();\n    if (isCurrentFileDeleted()) {\n      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    // Collect blocks that should be deleted\n    FileDiff last \u003d diffs.getLast();\n    BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n    if(snapshotBlocks \u003d\u003d null)\n      file.collectBlocksBeyondMax(max, info);\n    else\n      file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
      "extendedDetails": {}
    },
    "5dae97a584d30cef3e34141edfaca49c4ec57913": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
      "commitDate": "11/02/15 10:41 AM",
      "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
          "commitDate": "11/02/15 10:41 AM",
          "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "08/02/15 11:51 AM",
          "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 2.95,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,24 @@\n-  public void collectBlocksAndClear(final INodeFile file,\n+  public void collectBlocksAndClear(final BlockStoragePolicySuite bsps, final INodeFile file,\n       final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n-      file.destroyAndCollectBlocks(info, removedINodes);\n+      file.destroyAndCollectBlocks(bsps, info, removedINodes);\n       return;\n     }\n     // find max file size.\n     final long max;\n     FileDiff diff \u003d getDiffs().getLast();\n     if (isCurrentFileDeleted()) {\n       max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n     } else { \n       max \u003d file.computeFileSize();\n     }\n \n     // Collect blocks that should be deleted\n     FileDiff last \u003d diffs.getLast();\n     BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n     if(snapshotBlocks \u003d\u003d null)\n       file.collectBlocksBeyondMax(max, info);\n     else\n       file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void collectBlocksAndClear(final BlockStoragePolicySuite bsps, final INodeFile file,\n      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(bsps, info, removedINodes);\n      return;\n    }\n    // find max file size.\n    final long max;\n    FileDiff diff \u003d getDiffs().getLast();\n    if (isCurrentFileDeleted()) {\n      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    // Collect blocks that should be deleted\n    FileDiff last \u003d diffs.getLast();\n    BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n    if(snapshotBlocks \u003d\u003d null)\n      file.collectBlocksBeyondMax(max, info);\n    else\n      file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {
            "oldValue": "[file-INodeFile(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final), removedINodes-List\u003cINode\u003e(modifiers-final)]",
            "newValue": "[bsps-BlockStoragePolicySuite(modifiers-final), file-INodeFile(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final), removedINodes-List\u003cINode\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
          "commitDate": "11/02/15 10:41 AM",
          "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "08/02/15 11:51 AM",
          "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 2.95,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,24 @@\n-  public void collectBlocksAndClear(final INodeFile file,\n+  public void collectBlocksAndClear(final BlockStoragePolicySuite bsps, final INodeFile file,\n       final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n-      file.destroyAndCollectBlocks(info, removedINodes);\n+      file.destroyAndCollectBlocks(bsps, info, removedINodes);\n       return;\n     }\n     // find max file size.\n     final long max;\n     FileDiff diff \u003d getDiffs().getLast();\n     if (isCurrentFileDeleted()) {\n       max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n     } else { \n       max \u003d file.computeFileSize();\n     }\n \n     // Collect blocks that should be deleted\n     FileDiff last \u003d diffs.getLast();\n     BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n     if(snapshotBlocks \u003d\u003d null)\n       file.collectBlocksBeyondMax(max, info);\n     else\n       file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void collectBlocksAndClear(final BlockStoragePolicySuite bsps, final INodeFile file,\n      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(bsps, info, removedINodes);\n      return;\n    }\n    // find max file size.\n    final long max;\n    FileDiff diff \u003d getDiffs().getLast();\n    if (isCurrentFileDeleted()) {\n      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    // Collect blocks that should be deleted\n    FileDiff last \u003d diffs.getLast();\n    BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n    if(snapshotBlocks \u003d\u003d null)\n      file.collectBlocksBeyondMax(max, info);\n    else\n      file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {}
        }
      ]
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/02/15 4:32 PM",
      "commitNameOld": "8cb473124c1cf1c6f68ead7bde06558ebf7ce47e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 5.8,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public void collectBlocksAndClear(final INodeFile file,\n       final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n       file.destroyAndCollectBlocks(info, removedINodes);\n       return;\n     }\n     // find max file size.\n     final long max;\n     FileDiff diff \u003d getDiffs().getLast();\n     if (isCurrentFileDeleted()) {\n       max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n     } else { \n       max \u003d file.computeFileSize();\n     }\n \n     // Collect blocks that should be deleted\n     FileDiff last \u003d diffs.getLast();\n-    BlockInfo[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n+    BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n     if(snapshotBlocks \u003d\u003d null)\n       file.collectBlocksBeyondMax(max, info);\n     else\n       file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void collectBlocksAndClear(final INodeFile file,\n      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n    // find max file size.\n    final long max;\n    FileDiff diff \u003d getDiffs().getLast();\n    if (isCurrentFileDeleted()) {\n      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    // Collect blocks that should be deleted\n    FileDiff last \u003d diffs.getLast();\n    BlockInfoContiguous[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n    if(snapshotBlocks \u003d\u003d null)\n      file.collectBlocksBeyondMax(max, info);\n    else\n      file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
      "extendedDetails": {}
    },
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-7056. Snapshot support for truncate. Contributed by Konstantin Shvachko and Plamen Jeliazkov.",
      "commitDate": "13/01/15 12:24 AM",
      "commitName": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthor": "Konstantin V Shvachko",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7056. Snapshot support for truncate. Contributed by Konstantin Shvachko and Plamen Jeliazkov.",
          "commitDate": "13/01/15 12:24 AM",
          "commitName": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
          "commitAuthor": "Konstantin V Shvachko",
          "commitDateOld": "12/01/15 10:50 PM",
          "commitNameOld": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
          "commitAuthorOld": "Plamen Jeliazkov",
          "daysBetweenCommits": 0.07,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,24 @@\n-  private void collectBlocksAndClear(final INodeFile file,\n+  public void collectBlocksAndClear(final INodeFile file,\n       final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n       file.destroyAndCollectBlocks(info, removedINodes);\n       return;\n     }\n     // find max file size.\n     final long max;\n+    FileDiff diff \u003d getDiffs().getLast();\n     if (isCurrentFileDeleted()) {\n-      final FileDiff last \u003d getDiffs().getLast();\n-      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n+      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n     } else { \n       max \u003d file.computeFileSize();\n     }\n \n-    file.collectBlocksBeyondMax(max, info);\n+    // Collect blocks that should be deleted\n+    FileDiff last \u003d diffs.getLast();\n+    BlockInfo[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n+    if(snapshotBlocks \u003d\u003d null)\n+      file.collectBlocksBeyondMax(max, info);\n+    else\n+      file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void collectBlocksAndClear(final INodeFile file,\n      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n    // find max file size.\n    final long max;\n    FileDiff diff \u003d getDiffs().getLast();\n    if (isCurrentFileDeleted()) {\n      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    // Collect blocks that should be deleted\n    FileDiff last \u003d diffs.getLast();\n    BlockInfo[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n    if(snapshotBlocks \u003d\u003d null)\n      file.collectBlocksBeyondMax(max, info);\n    else\n      file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7056. Snapshot support for truncate. Contributed by Konstantin Shvachko and Plamen Jeliazkov.",
          "commitDate": "13/01/15 12:24 AM",
          "commitName": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
          "commitAuthor": "Konstantin V Shvachko",
          "commitDateOld": "12/01/15 10:50 PM",
          "commitNameOld": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
          "commitAuthorOld": "Plamen Jeliazkov",
          "daysBetweenCommits": 0.07,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,24 @@\n-  private void collectBlocksAndClear(final INodeFile file,\n+  public void collectBlocksAndClear(final INodeFile file,\n       final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n       file.destroyAndCollectBlocks(info, removedINodes);\n       return;\n     }\n     // find max file size.\n     final long max;\n+    FileDiff diff \u003d getDiffs().getLast();\n     if (isCurrentFileDeleted()) {\n-      final FileDiff last \u003d getDiffs().getLast();\n-      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n+      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n     } else { \n       max \u003d file.computeFileSize();\n     }\n \n-    file.collectBlocksBeyondMax(max, info);\n+    // Collect blocks that should be deleted\n+    FileDiff last \u003d diffs.getLast();\n+    BlockInfo[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n+    if(snapshotBlocks \u003d\u003d null)\n+      file.collectBlocksBeyondMax(max, info);\n+    else\n+      file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void collectBlocksAndClear(final INodeFile file,\n      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n    // find max file size.\n    final long max;\n    FileDiff diff \u003d getDiffs().getLast();\n    if (isCurrentFileDeleted()) {\n      max \u003d diff \u003d\u003d null? 0: diff.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    // Collect blocks that should be deleted\n    FileDiff last \u003d diffs.getLast();\n    BlockInfo[] snapshotBlocks \u003d last \u003d\u003d null ? null : last.getBlocks();\n    if(snapshotBlocks \u003d\u003d null)\n      file.collectBlocksBeyondMax(max, info);\n    else\n      file.collectBlocksBeyondSnapshot(snapshotBlocks, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {}
        }
      ]
    },
    "7e9358feb326d48b8c4f00249e7af5023cebd2e2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3107. Introduce truncate. Contributed by Plamen Jeliazkov.",
      "commitDate": "12/01/15 10:50 PM",
      "commitName": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
      "commitAuthor": "Plamen Jeliazkov",
      "commitDateOld": "22/12/14 11:05 PM",
      "commitNameOld": "50ae1a6664a92619aa683d2a864d0da9fb4af026",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 20.99,
      "commitsBetweenForRepo": 79,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   private void collectBlocksAndClear(final INodeFile file,\n       final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n       file.destroyAndCollectBlocks(info, removedINodes);\n       return;\n     }\n     // find max file size.\n     final long max;\n     if (isCurrentFileDeleted()) {\n       final FileDiff last \u003d getDiffs().getLast();\n       max \u003d last \u003d\u003d null? 0: last.getFileSize();\n     } else { \n       max \u003d file.computeFileSize();\n     }\n \n-    collectBlocksBeyondMax(file, max, info);\n+    file.collectBlocksBeyondMax(max, info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void collectBlocksAndClear(final INodeFile file,\n      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n    // find max file size.\n    final long max;\n    if (isCurrentFileDeleted()) {\n      final FileDiff last \u003d getDiffs().getLast();\n      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    file.collectBlocksBeyondMax(max, info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
      "extendedDetails": {}
    },
    "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-5554. Flatten INodeFile hierarchy: Replace INodeFileWithSnapshot with FileWithSnapshotFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548796 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/12/13 10:17 PM",
      "commitName": "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-5554. Flatten INodeFile hierarchy: Replace INodeFileWithSnapshot with FileWithSnapshotFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548796 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/13 10:17 PM",
          "commitName": "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "06/12/13 4:11 PM",
          "commitNameOld": "7f059104d293614f3250bd1408874e97f659c92b",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.25,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,18 @@\n-  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n-      final List\u003cINode\u003e removedINodes) {\n+  private void collectBlocksAndClear(final INodeFile file,\n+      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n-      destroyAndCollectBlocks(info, removedINodes);\n+      file.destroyAndCollectBlocks(info, removedINodes);\n       return;\n     }\n-\n     // find max file size.\n     final long max;\n     if (isCurrentFileDeleted()) {\n       final FileDiff last \u003d getDiffs().getLast();\n       max \u003d last \u003d\u003d null? 0: last.getFileSize();\n     } else { \n-      max \u003d computeFileSize();\n+      max \u003d file.computeFileSize();\n     }\n \n-    collectBlocksBeyondMax(max, info);\n+    collectBlocksBeyondMax(file, max, info);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksAndClear(final INodeFile file,\n      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n    // find max file size.\n    final long max;\n    if (isCurrentFileDeleted()) {\n      final FileDiff last \u003d getDiffs().getLast();\n      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    collectBlocksBeyondMax(file, max, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5554. Flatten INodeFile hierarchy: Replace INodeFileWithSnapshot with FileWithSnapshotFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548796 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/13 10:17 PM",
          "commitName": "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "06/12/13 4:11 PM",
          "commitNameOld": "7f059104d293614f3250bd1408874e97f659c92b",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.25,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,18 @@\n-  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n-      final List\u003cINode\u003e removedINodes) {\n+  private void collectBlocksAndClear(final INodeFile file,\n+      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n-      destroyAndCollectBlocks(info, removedINodes);\n+      file.destroyAndCollectBlocks(info, removedINodes);\n       return;\n     }\n-\n     // find max file size.\n     final long max;\n     if (isCurrentFileDeleted()) {\n       final FileDiff last \u003d getDiffs().getLast();\n       max \u003d last \u003d\u003d null? 0: last.getFileSize();\n     } else { \n-      max \u003d computeFileSize();\n+      max \u003d file.computeFileSize();\n     }\n \n-    collectBlocksBeyondMax(max, info);\n+    collectBlocksBeyondMax(file, max, info);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksAndClear(final INodeFile file,\n      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n    // find max file size.\n    final long max;\n    if (isCurrentFileDeleted()) {\n      final FileDiff last \u003d getDiffs().getLast();\n      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    collectBlocksBeyondMax(file, max, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5554. Flatten INodeFile hierarchy: Replace INodeFileWithSnapshot with FileWithSnapshotFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548796 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/13 10:17 PM",
          "commitName": "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "06/12/13 4:11 PM",
          "commitNameOld": "7f059104d293614f3250bd1408874e97f659c92b",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.25,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,18 @@\n-  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n-      final List\u003cINode\u003e removedINodes) {\n+  private void collectBlocksAndClear(final INodeFile file,\n+      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n-      destroyAndCollectBlocks(info, removedINodes);\n+      file.destroyAndCollectBlocks(info, removedINodes);\n       return;\n     }\n-\n     // find max file size.\n     final long max;\n     if (isCurrentFileDeleted()) {\n       final FileDiff last \u003d getDiffs().getLast();\n       max \u003d last \u003d\u003d null? 0: last.getFileSize();\n     } else { \n-      max \u003d computeFileSize();\n+      max \u003d file.computeFileSize();\n     }\n \n-    collectBlocksBeyondMax(max, info);\n+    collectBlocksBeyondMax(file, max, info);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksAndClear(final INodeFile file,\n      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n    // find max file size.\n    final long max;\n    if (isCurrentFileDeleted()) {\n      final FileDiff last \u003d getDiffs().getLast();\n      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    collectBlocksBeyondMax(file, max, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5554. Flatten INodeFile hierarchy: Replace INodeFileWithSnapshot with FileWithSnapshotFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548796 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/13 10:17 PM",
          "commitName": "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "06/12/13 4:11 PM",
          "commitNameOld": "7f059104d293614f3250bd1408874e97f659c92b",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.25,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,18 @@\n-  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n-      final List\u003cINode\u003e removedINodes) {\n+  private void collectBlocksAndClear(final INodeFile file,\n+      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n     // check if everything is deleted.\n     if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n-      destroyAndCollectBlocks(info, removedINodes);\n+      file.destroyAndCollectBlocks(info, removedINodes);\n       return;\n     }\n-\n     // find max file size.\n     final long max;\n     if (isCurrentFileDeleted()) {\n       final FileDiff last \u003d getDiffs().getLast();\n       max \u003d last \u003d\u003d null? 0: last.getFileSize();\n     } else { \n-      max \u003d computeFileSize();\n+      max \u003d file.computeFileSize();\n     }\n \n-    collectBlocksBeyondMax(max, info);\n+    collectBlocksBeyondMax(file, max, info);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksAndClear(final INodeFile file,\n      final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      file.destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n    // find max file size.\n    final long max;\n    if (isCurrentFileDeleted()) {\n      final FileDiff last \u003d getDiffs().getLast();\n      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n    } else { \n      max \u003d file.computeFileSize();\n    }\n\n    collectBlocksBeyondMax(file, max, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java",
          "extendedDetails": {
            "oldValue": "[info-BlocksMapUpdateInfo(modifiers-final), removedINodes-List\u003cINode\u003e(modifiers-final)]",
            "newValue": "[file-INodeFile(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final), removedINodes-List\u003cINode\u003e(modifiers-final)]"
          }
        }
      ]
    },
    "8df119da214babde03e73243c7ca4cfe6d0ca562": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-5537. Remove FileWithSnapshot interface.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546184 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 12:32 PM",
      "commitName": "8df119da214babde03e73243c7ca4cfe6d0ca562",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-5537. Remove FileWithSnapshot interface.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546184 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 12:32 PM",
          "commitName": "8df119da214babde03e73243c7ca4cfe6d0ca562",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "27/11/13 10:20 AM",
          "commitNameOld": "2214871d916fdcae62aa51afbb5fd571f2808745",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,19 @@\n-    static void collectBlocksAndClear(final FileWithSnapshot file,\n-        final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n-      // check if everything is deleted.\n-      if (file.isCurrentFileDeleted()\n-          \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n-        file.asINodeFile().destroyAndCollectBlocks(info, removedINodes);\n-        return;\n-      }\n+  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n+      final List\u003cINode\u003e removedINodes) {\n+    // check if everything is deleted.\n+    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n+      destroyAndCollectBlocks(info, removedINodes);\n+      return;\n+    }\n \n-      // find max file size.\n-      final long max;\n-      if (file.isCurrentFileDeleted()) {\n-        final FileDiff last \u003d file.getDiffs().getLast();\n-        max \u003d last \u003d\u003d null? 0: last.fileSize;\n-      } else { \n-        max \u003d file.asINodeFile().computeFileSize();\n-      }\n+    // find max file size.\n+    final long max;\n+    if (isCurrentFileDeleted()) {\n+      final FileDiff last \u003d getDiffs().getLast();\n+      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n+    } else { \n+      max \u003d computeFileSize();\n+    }\n \n-      collectBlocksBeyondMax(file, max, info);\n-    }\n\\ No newline at end of file\n+    collectBlocksBeyondMax(max, info);\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n      final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n\n    // find max file size.\n    final long max;\n    if (isCurrentFileDeleted()) {\n      final FileDiff last \u003d getDiffs().getLast();\n      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n    } else { \n      max \u003d computeFileSize();\n    }\n\n    collectBlocksBeyondMax(max, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
            "oldMethodName": "collectBlocksAndClear",
            "newMethodName": "collectBlocksAndClear"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5537. Remove FileWithSnapshot interface.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546184 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 12:32 PM",
          "commitName": "8df119da214babde03e73243c7ca4cfe6d0ca562",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "27/11/13 10:20 AM",
          "commitNameOld": "2214871d916fdcae62aa51afbb5fd571f2808745",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,19 @@\n-    static void collectBlocksAndClear(final FileWithSnapshot file,\n-        final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n-      // check if everything is deleted.\n-      if (file.isCurrentFileDeleted()\n-          \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n-        file.asINodeFile().destroyAndCollectBlocks(info, removedINodes);\n-        return;\n-      }\n+  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n+      final List\u003cINode\u003e removedINodes) {\n+    // check if everything is deleted.\n+    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n+      destroyAndCollectBlocks(info, removedINodes);\n+      return;\n+    }\n \n-      // find max file size.\n-      final long max;\n-      if (file.isCurrentFileDeleted()) {\n-        final FileDiff last \u003d file.getDiffs().getLast();\n-        max \u003d last \u003d\u003d null? 0: last.fileSize;\n-      } else { \n-        max \u003d file.asINodeFile().computeFileSize();\n-      }\n+    // find max file size.\n+    final long max;\n+    if (isCurrentFileDeleted()) {\n+      final FileDiff last \u003d getDiffs().getLast();\n+      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n+    } else { \n+      max \u003d computeFileSize();\n+    }\n \n-      collectBlocksBeyondMax(file, max, info);\n-    }\n\\ No newline at end of file\n+    collectBlocksBeyondMax(max, info);\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n      final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n\n    // find max file size.\n    final long max;\n    if (isCurrentFileDeleted()) {\n      final FileDiff last \u003d getDiffs().getLast();\n      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n    } else { \n      max \u003d computeFileSize();\n    }\n\n    collectBlocksBeyondMax(max, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[static]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5537. Remove FileWithSnapshot interface.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546184 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 12:32 PM",
          "commitName": "8df119da214babde03e73243c7ca4cfe6d0ca562",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "27/11/13 10:20 AM",
          "commitNameOld": "2214871d916fdcae62aa51afbb5fd571f2808745",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,19 @@\n-    static void collectBlocksAndClear(final FileWithSnapshot file,\n-        final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n-      // check if everything is deleted.\n-      if (file.isCurrentFileDeleted()\n-          \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n-        file.asINodeFile().destroyAndCollectBlocks(info, removedINodes);\n-        return;\n-      }\n+  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n+      final List\u003cINode\u003e removedINodes) {\n+    // check if everything is deleted.\n+    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n+      destroyAndCollectBlocks(info, removedINodes);\n+      return;\n+    }\n \n-      // find max file size.\n-      final long max;\n-      if (file.isCurrentFileDeleted()) {\n-        final FileDiff last \u003d file.getDiffs().getLast();\n-        max \u003d last \u003d\u003d null? 0: last.fileSize;\n-      } else { \n-        max \u003d file.asINodeFile().computeFileSize();\n-      }\n+    // find max file size.\n+    final long max;\n+    if (isCurrentFileDeleted()) {\n+      final FileDiff last \u003d getDiffs().getLast();\n+      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n+    } else { \n+      max \u003d computeFileSize();\n+    }\n \n-      collectBlocksBeyondMax(file, max, info);\n-    }\n\\ No newline at end of file\n+    collectBlocksBeyondMax(max, info);\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n      final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n\n    // find max file size.\n    final long max;\n    if (isCurrentFileDeleted()) {\n      final FileDiff last \u003d getDiffs().getLast();\n      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n    } else { \n      max \u003d computeFileSize();\n    }\n\n    collectBlocksBeyondMax(max, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5537. Remove FileWithSnapshot interface.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546184 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 12:32 PM",
          "commitName": "8df119da214babde03e73243c7ca4cfe6d0ca562",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "27/11/13 10:20 AM",
          "commitNameOld": "2214871d916fdcae62aa51afbb5fd571f2808745",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,19 @@\n-    static void collectBlocksAndClear(final FileWithSnapshot file,\n-        final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n-      // check if everything is deleted.\n-      if (file.isCurrentFileDeleted()\n-          \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n-        file.asINodeFile().destroyAndCollectBlocks(info, removedINodes);\n-        return;\n-      }\n+  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n+      final List\u003cINode\u003e removedINodes) {\n+    // check if everything is deleted.\n+    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n+      destroyAndCollectBlocks(info, removedINodes);\n+      return;\n+    }\n \n-      // find max file size.\n-      final long max;\n-      if (file.isCurrentFileDeleted()) {\n-        final FileDiff last \u003d file.getDiffs().getLast();\n-        max \u003d last \u003d\u003d null? 0: last.fileSize;\n-      } else { \n-        max \u003d file.asINodeFile().computeFileSize();\n-      }\n+    // find max file size.\n+    final long max;\n+    if (isCurrentFileDeleted()) {\n+      final FileDiff last \u003d getDiffs().getLast();\n+      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n+    } else { \n+      max \u003d computeFileSize();\n+    }\n \n-      collectBlocksBeyondMax(file, max, info);\n-    }\n\\ No newline at end of file\n+    collectBlocksBeyondMax(max, info);\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  void collectBlocksAndClear(final BlocksMapUpdateInfo info,\n      final List\u003cINode\u003e removedINodes) {\n    // check if everything is deleted.\n    if (isCurrentFileDeleted() \u0026\u0026 getDiffs().asList().isEmpty()) {\n      destroyAndCollectBlocks(info, removedINodes);\n      return;\n    }\n\n    // find max file size.\n    final long max;\n    if (isCurrentFileDeleted()) {\n      final FileDiff last \u003d getDiffs().getLast();\n      max \u003d last \u003d\u003d null? 0: last.getFileSize();\n    } else { \n      max \u003d computeFileSize();\n    }\n\n    collectBlocksBeyondMax(max, info);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[file-FileWithSnapshot(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final), removedINodes-List\u003cINode\u003e(modifiers-final)]",
            "newValue": "[info-BlocksMapUpdateInfo(modifiers-final), removedINodes-List\u003cINode\u003e(modifiers-final)]"
          }
        }
      ]
    },
    "92e0416ced279a910616985bf11fa3f8b1b1de9b": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4727. Update inodeMap after deleting files/directories/snapshots.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1470756 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/04/13 5:00 PM",
      "commitName": "92e0416ced279a910616985bf11fa3f8b1b1de9b",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4727. Update inodeMap after deleting files/directories/snapshots.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1470756 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/04/13 5:00 PM",
          "commitName": "92e0416ced279a910616985bf11fa3f8b1b1de9b",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/04/13 4:24 PM",
          "commitNameOld": "1096917649fd951be633e5619518764f23cca645",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 21.03,
          "commitsBetweenForRepo": 126,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n     static void collectBlocksAndClear(final FileWithSnapshot file,\n-        final BlocksMapUpdateInfo info) {\n+        final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n       // check if everything is deleted.\n       if (file.isCurrentFileDeleted()\n           \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n-        file.asINodeFile().destroyAndCollectBlocks(info);\n+        file.asINodeFile().destroyAndCollectBlocks(info, removedINodes);\n         return;\n       }\n \n       // find max file size.\n       final long max;\n       if (file.isCurrentFileDeleted()) {\n         final FileDiff last \u003d file.getDiffs().getLast();\n         max \u003d last \u003d\u003d null? 0: last.fileSize;\n       } else { \n         max \u003d file.asINodeFile().computeFileSize();\n       }\n \n       collectBlocksBeyondMax(file, max, info);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksAndClear(final FileWithSnapshot file,\n        final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n      // check if everything is deleted.\n      if (file.isCurrentFileDeleted()\n          \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n        file.asINodeFile().destroyAndCollectBlocks(info, removedINodes);\n        return;\n      }\n\n      // find max file size.\n      final long max;\n      if (file.isCurrentFileDeleted()) {\n        final FileDiff last \u003d file.getDiffs().getLast();\n        max \u003d last \u003d\u003d null? 0: last.fileSize;\n      } else { \n        max \u003d file.asINodeFile().computeFileSize();\n      }\n\n      collectBlocksBeyondMax(file, max, info);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[file-FileWithSnapshot(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[file-FileWithSnapshot(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final), removedINodes-List\u003cINode\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4727. Update inodeMap after deleting files/directories/snapshots.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1470756 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/04/13 5:00 PM",
          "commitName": "92e0416ced279a910616985bf11fa3f8b1b1de9b",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/04/13 4:24 PM",
          "commitNameOld": "1096917649fd951be633e5619518764f23cca645",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 21.03,
          "commitsBetweenForRepo": 126,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n     static void collectBlocksAndClear(final FileWithSnapshot file,\n-        final BlocksMapUpdateInfo info) {\n+        final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n       // check if everything is deleted.\n       if (file.isCurrentFileDeleted()\n           \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n-        file.asINodeFile().destroyAndCollectBlocks(info);\n+        file.asINodeFile().destroyAndCollectBlocks(info, removedINodes);\n         return;\n       }\n \n       // find max file size.\n       final long max;\n       if (file.isCurrentFileDeleted()) {\n         final FileDiff last \u003d file.getDiffs().getLast();\n         max \u003d last \u003d\u003d null? 0: last.fileSize;\n       } else { \n         max \u003d file.asINodeFile().computeFileSize();\n       }\n \n       collectBlocksBeyondMax(file, max, info);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksAndClear(final FileWithSnapshot file,\n        final BlocksMapUpdateInfo info, final List\u003cINode\u003e removedINodes) {\n      // check if everything is deleted.\n      if (file.isCurrentFileDeleted()\n          \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n        file.asINodeFile().destroyAndCollectBlocks(info, removedINodes);\n        return;\n      }\n\n      // find max file size.\n      final long max;\n      if (file.isCurrentFileDeleted()) {\n        final FileDiff last \u003d file.getDiffs().getLast();\n        max \u003d last \u003d\u003d null? 0: last.fileSize;\n      } else { \n        max \u003d file.asINodeFile().computeFileSize();\n      }\n\n      collectBlocksBeyondMax(file, max, info);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {}
        }
      ]
    },
    "f29fa9e820e25730d00a1a00c51c6f11028fb5a7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4499. Fix file/directory/snapshot deletion for file diff.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1448504 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/02/13 7:27 PM",
      "commitName": "f29fa9e820e25730d00a1a00c51c6f11028fb5a7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "20/02/13 12:02 PM",
      "commitNameOld": "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.31,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n     static void collectBlocksAndClear(final FileWithSnapshot file,\n         final BlocksMapUpdateInfo info) {\n       // check if everything is deleted.\n       if (file.isCurrentFileDeleted()\n           \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n-        file.asINodeFile().destroySelfAndCollectBlocks(info);\n+        file.asINodeFile().destroyAndCollectBlocks(info);\n         return;\n       }\n \n       // find max file size.\n       final long max;\n       if (file.isCurrentFileDeleted()) {\n         final FileDiff last \u003d file.getDiffs().getLast();\n         max \u003d last \u003d\u003d null? 0: last.fileSize;\n       } else { \n         max \u003d file.asINodeFile().computeFileSize();\n       }\n \n       collectBlocksBeyondMax(file, max, info);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    static void collectBlocksAndClear(final FileWithSnapshot file,\n        final BlocksMapUpdateInfo info) {\n      // check if everything is deleted.\n      if (file.isCurrentFileDeleted()\n          \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n        file.asINodeFile().destroyAndCollectBlocks(info);\n        return;\n      }\n\n      // find max file size.\n      final long max;\n      if (file.isCurrentFileDeleted()) {\n        final FileDiff last \u003d file.getDiffs().getLast();\n        max \u003d last \u003d\u003d null? 0: last.fileSize;\n      } else { \n        max \u003d file.asINodeFile().computeFileSize();\n      }\n\n      collectBlocksBeyondMax(file, max, info);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
      "extendedDetails": {}
    },
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4503. Update computeContentSummary(..), spaceConsumedInTree(..) and diskspaceConsumed(..) in INode for snapshot.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1448373 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/02/13 12:02 PM",
      "commitName": "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "13/02/13 4:43 PM",
      "commitNameOld": "02e6b72ae148fc8c2ba02ef624536b9e48997b31",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.8,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n     static void collectBlocksAndClear(final FileWithSnapshot file,\n         final BlocksMapUpdateInfo info) {\n       // check if everything is deleted.\n       if (file.isCurrentFileDeleted()\n           \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n         file.asINodeFile().destroySelfAndCollectBlocks(info);\n         return;\n       }\n \n       // find max file size.\n       final long max;\n       if (file.isCurrentFileDeleted()) {\n         final FileDiff last \u003d file.getDiffs().getLast();\n         max \u003d last \u003d\u003d null? 0: last.fileSize;\n       } else { \n-        max \u003d file.asINodeFile().computeFileSize(true, null);\n+        max \u003d file.asINodeFile().computeFileSize();\n       }\n \n       collectBlocksBeyondMax(file, max, info);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    static void collectBlocksAndClear(final FileWithSnapshot file,\n        final BlocksMapUpdateInfo info) {\n      // check if everything is deleted.\n      if (file.isCurrentFileDeleted()\n          \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n        file.asINodeFile().destroySelfAndCollectBlocks(info);\n        return;\n      }\n\n      // find max file size.\n      final long max;\n      if (file.isCurrentFileDeleted()) {\n        final FileDiff last \u003d file.getDiffs().getLast();\n        max \u003d last \u003d\u003d null? 0: last.fileSize;\n      } else { \n        max \u003d file.asINodeFile().computeFileSize();\n      }\n\n      collectBlocksBeyondMax(file, max, info);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
      "extendedDetails": {}
    },
    "02e6b72ae148fc8c2ba02ef624536b9e48997b31": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4481. Change fsimage to support snapshot file diffs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1446000 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/02/13 4:43 PM",
      "commitName": "02e6b72ae148fc8c2ba02ef624536b9e48997b31",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/02/13 3:19 PM",
      "commitNameOld": "afe77ce53d3cf203690aa419e377f26cbd45a96e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.06,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,20 @@\n     static void collectBlocksAndClear(final FileWithSnapshot file,\n         final BlocksMapUpdateInfo info) {\n+      // check if everything is deleted.\n+      if (file.isCurrentFileDeleted()\n+          \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n+        file.asINodeFile().destroySelfAndCollectBlocks(info);\n+        return;\n+      }\n+\n       // find max file size.\n       final long max;\n       if (file.isCurrentFileDeleted()) {\n-        final FileDiff last \u003d file.getFileDiffList().getLast();\n+        final FileDiff last \u003d file.getDiffs().getLast();\n         max \u003d last \u003d\u003d null? 0: last.fileSize;\n       } else { \n         max \u003d file.asINodeFile().computeFileSize(true, null);\n       }\n \n       collectBlocksBeyondMax(file, max, info);\n-\n-      // if everything is deleted, set blocks to null.\n-      if (file.isCurrentFileDeleted()\n-          \u0026\u0026 file.getFileDiffList().asList().isEmpty()) {\n-        file.asINodeFile().setBlocks(null);\n-      }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    static void collectBlocksAndClear(final FileWithSnapshot file,\n        final BlocksMapUpdateInfo info) {\n      // check if everything is deleted.\n      if (file.isCurrentFileDeleted()\n          \u0026\u0026 file.getDiffs().asList().isEmpty()) {\n        file.asINodeFile().destroySelfAndCollectBlocks(info);\n        return;\n      }\n\n      // find max file size.\n      final long max;\n      if (file.isCurrentFileDeleted()) {\n        final FileDiff last \u003d file.getDiffs().getLast();\n        max \u003d last \u003d\u003d null? 0: last.fileSize;\n      } else { \n        max \u003d file.asINodeFile().computeFileSize(true, null);\n      }\n\n      collectBlocksBeyondMax(file, max, info);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
      "extendedDetails": {}
    },
    "afe77ce53d3cf203690aa419e377f26cbd45a96e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4480. Eliminate the file snapshot circular linked list.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1444280 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/02/13 3:19 PM",
      "commitName": "afe77ce53d3cf203690aa419e377f26cbd45a96e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "07/02/13 6:18 PM",
      "commitNameOld": "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.88,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,19 @@\n     static void collectBlocksAndClear(final FileWithSnapshot file,\n         final BlocksMapUpdateInfo info) {\n-      final FileWithSnapshot next \u003d file.getNext();\n-\n-      // find max file size, max replication and the last inode.\n-      long maxFileSize \u003d file.computeMaxFileSize();\n-      short maxReplication \u003d file.getMaxFileReplication();\n-      FileWithSnapshot last \u003d null;\n-      if (next !\u003d null \u0026\u0026 next !\u003d file) {\n-        for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-          final long size \u003d i.computeMaxFileSize();\n-          if (size \u003e maxFileSize) {\n-            maxFileSize \u003d size;\n-          }\n-          final short rep \u003d i.getMaxFileReplication();\n-          if (rep \u003e maxReplication) {\n-            maxReplication \u003d rep;\n-          }\n-          last \u003d i;\n-        }\n+      // find max file size.\n+      final long max;\n+      if (file.isCurrentFileDeleted()) {\n+        final FileDiff last \u003d file.getFileDiffList().getLast();\n+        max \u003d last \u003d\u003d null? 0: last.fileSize;\n+      } else { \n+        max \u003d file.asINodeFile().computeFileSize(true, null);\n       }\n \n-      collectBlocksBeyondMax(file, maxFileSize, info);\n+      collectBlocksBeyondMax(file, max, info);\n \n-      if (file.isEverythingDeleted()) {\n-        // Set the replication of the current INode to the max of all the other\n-        // linked INodes, so that in case the current INode is retrieved from the\n-        // blocksMap before it is removed or updated, the correct replication\n-        // number can be retrieved.\n-        if (maxReplication \u003e 0) {\n-          file.asINodeFile().setFileReplication(maxReplication, null);\n-        }\n-\n-        // remove the file from the circular linked list.\n-        if (last !\u003d null) {\n-          last.setNext(next);\n-        }\n-        file.setNext(null);\n-\n+      // if everything is deleted, set blocks to null.\n+      if (file.isCurrentFileDeleted()\n+          \u0026\u0026 file.getFileDiffList().asList().isEmpty()) {\n         file.asINodeFile().setBlocks(null);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    static void collectBlocksAndClear(final FileWithSnapshot file,\n        final BlocksMapUpdateInfo info) {\n      // find max file size.\n      final long max;\n      if (file.isCurrentFileDeleted()) {\n        final FileDiff last \u003d file.getFileDiffList().getLast();\n        max \u003d last \u003d\u003d null? 0: last.fileSize;\n      } else { \n        max \u003d file.asINodeFile().computeFileSize(true, null);\n      }\n\n      collectBlocksBeyondMax(file, max, info);\n\n      // if everything is deleted, set blocks to null.\n      if (file.isCurrentFileDeleted()\n          \u0026\u0026 file.getFileDiffList().asList().isEmpty()) {\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
      "extendedDetails": {}
    },
    "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4446. Support file snapshots with diff lists.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1443825 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/02/13 6:18 PM",
      "commitName": "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-4446. Support file snapshots with diff lists.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1443825 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/02/13 6:18 PM",
          "commitName": "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "24/01/13 1:33 PM",
          "commitNameOld": "bb80f2fb29d6f58d9c35f4a1fd88c99517f43e16",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 14.2,
          "commitsBetweenForRepo": 46,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,42 @@\n-    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n-            final long max, final BlocksMapUpdateInfo info) {\n-      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n+    static void collectBlocksAndClear(final FileWithSnapshot file,\n+        final BlocksMapUpdateInfo info) {\n+      final FileWithSnapshot next \u003d file.getNext();\n+\n+      // find max file size, max replication and the last inode.\n+      long maxFileSize \u003d file.computeMaxFileSize();\n+      short maxReplication \u003d file.getMaxFileReplication();\n+      FileWithSnapshot last \u003d null;\n+      if (next !\u003d null \u0026\u0026 next !\u003d file) {\n+        for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+          final long size \u003d i.computeMaxFileSize();\n+          if (size \u003e maxFileSize) {\n+            maxFileSize \u003d size;\n+          }\n+          final short rep \u003d i.getMaxFileReplication();\n+          if (rep \u003e maxReplication) {\n+            maxReplication \u003d rep;\n+          }\n+          last \u003d i;\n+        }\n+      }\n+\n+      collectBlocksBeyondMax(file, maxFileSize, info);\n+\n+      if (file.isEverythingDeleted()) {\n+        // Set the replication of the current INode to the max of all the other\n+        // linked INodes, so that in case the current INode is retrieved from the\n+        // blocksMap before it is removed or updated, the correct replication\n+        // number can be retrieved.\n+        if (maxReplication \u003e 0) {\n+          file.asINodeFile().setFileReplication(maxReplication, null);\n         }\n \n-        // Replace the INode for all the remaining blocks in blocksMap\n-        final FileWithSnapshot next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file.asINodeFile(), next.asINodeFile());\n-        if (info !\u003d null) {\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n-          }\n+        // remove the file from the circular linked list.\n+        if (last !\u003d null) {\n+          last.setNext(next);\n         }\n-        \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-            i.asINodeFile().setBlocks(newBlocks);\n-          }\n+        file.setNext(null);\n \n-          // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n-            }\n-          }\n-        }\n         file.asINodeFile().setBlocks(null);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksAndClear(final FileWithSnapshot file,\n        final BlocksMapUpdateInfo info) {\n      final FileWithSnapshot next \u003d file.getNext();\n\n      // find max file size, max replication and the last inode.\n      long maxFileSize \u003d file.computeMaxFileSize();\n      short maxReplication \u003d file.getMaxFileReplication();\n      FileWithSnapshot last \u003d null;\n      if (next !\u003d null \u0026\u0026 next !\u003d file) {\n        for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n          final long size \u003d i.computeMaxFileSize();\n          if (size \u003e maxFileSize) {\n            maxFileSize \u003d size;\n          }\n          final short rep \u003d i.getMaxFileReplication();\n          if (rep \u003e maxReplication) {\n            maxReplication \u003d rep;\n          }\n          last \u003d i;\n        }\n      }\n\n      collectBlocksBeyondMax(file, maxFileSize, info);\n\n      if (file.isEverythingDeleted()) {\n        // Set the replication of the current INode to the max of all the other\n        // linked INodes, so that in case the current INode is retrieved from the\n        // blocksMap before it is removed or updated, the correct replication\n        // number can be retrieved.\n        if (maxReplication \u003e 0) {\n          file.asINodeFile().setFileReplication(maxReplication, null);\n        }\n\n        // remove the file from the circular linked list.\n        if (last !\u003d null) {\n          last.setNext(next);\n        }\n        file.setNext(null);\n\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "collectBlocksBeyondMaxAndClear",
            "newValue": "collectBlocksAndClear"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4446. Support file snapshots with diff lists.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1443825 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/02/13 6:18 PM",
          "commitName": "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "24/01/13 1:33 PM",
          "commitNameOld": "bb80f2fb29d6f58d9c35f4a1fd88c99517f43e16",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 14.2,
          "commitsBetweenForRepo": 46,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,42 @@\n-    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n-            final long max, final BlocksMapUpdateInfo info) {\n-      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n+    static void collectBlocksAndClear(final FileWithSnapshot file,\n+        final BlocksMapUpdateInfo info) {\n+      final FileWithSnapshot next \u003d file.getNext();\n+\n+      // find max file size, max replication and the last inode.\n+      long maxFileSize \u003d file.computeMaxFileSize();\n+      short maxReplication \u003d file.getMaxFileReplication();\n+      FileWithSnapshot last \u003d null;\n+      if (next !\u003d null \u0026\u0026 next !\u003d file) {\n+        for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+          final long size \u003d i.computeMaxFileSize();\n+          if (size \u003e maxFileSize) {\n+            maxFileSize \u003d size;\n+          }\n+          final short rep \u003d i.getMaxFileReplication();\n+          if (rep \u003e maxReplication) {\n+            maxReplication \u003d rep;\n+          }\n+          last \u003d i;\n+        }\n+      }\n+\n+      collectBlocksBeyondMax(file, maxFileSize, info);\n+\n+      if (file.isEverythingDeleted()) {\n+        // Set the replication of the current INode to the max of all the other\n+        // linked INodes, so that in case the current INode is retrieved from the\n+        // blocksMap before it is removed or updated, the correct replication\n+        // number can be retrieved.\n+        if (maxReplication \u003e 0) {\n+          file.asINodeFile().setFileReplication(maxReplication, null);\n         }\n \n-        // Replace the INode for all the remaining blocks in blocksMap\n-        final FileWithSnapshot next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file.asINodeFile(), next.asINodeFile());\n-        if (info !\u003d null) {\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n-          }\n+        // remove the file from the circular linked list.\n+        if (last !\u003d null) {\n+          last.setNext(next);\n         }\n-        \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-            i.asINodeFile().setBlocks(newBlocks);\n-          }\n+        file.setNext(null);\n \n-          // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n-            }\n-          }\n-        }\n         file.asINodeFile().setBlocks(null);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksAndClear(final FileWithSnapshot file,\n        final BlocksMapUpdateInfo info) {\n      final FileWithSnapshot next \u003d file.getNext();\n\n      // find max file size, max replication and the last inode.\n      long maxFileSize \u003d file.computeMaxFileSize();\n      short maxReplication \u003d file.getMaxFileReplication();\n      FileWithSnapshot last \u003d null;\n      if (next !\u003d null \u0026\u0026 next !\u003d file) {\n        for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n          final long size \u003d i.computeMaxFileSize();\n          if (size \u003e maxFileSize) {\n            maxFileSize \u003d size;\n          }\n          final short rep \u003d i.getMaxFileReplication();\n          if (rep \u003e maxReplication) {\n            maxReplication \u003d rep;\n          }\n          last \u003d i;\n        }\n      }\n\n      collectBlocksBeyondMax(file, maxFileSize, info);\n\n      if (file.isEverythingDeleted()) {\n        // Set the replication of the current INode to the max of all the other\n        // linked INodes, so that in case the current INode is retrieved from the\n        // blocksMap before it is removed or updated, the correct replication\n        // number can be retrieved.\n        if (maxReplication \u003e 0) {\n          file.asINodeFile().setFileReplication(maxReplication, null);\n        }\n\n        // remove the file from the circular linked list.\n        if (last !\u003d null) {\n          last.setNext(next);\n        }\n        file.setNext(null);\n\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[file-FileWithSnapshot(modifiers-final), max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[file-FileWithSnapshot(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4446. Support file snapshots with diff lists.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1443825 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/02/13 6:18 PM",
          "commitName": "4f7d921324c7fa9623c34688e3f2aa57fbfcb8b3",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "24/01/13 1:33 PM",
          "commitNameOld": "bb80f2fb29d6f58d9c35f4a1fd88c99517f43e16",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 14.2,
          "commitsBetweenForRepo": 46,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,42 @@\n-    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n-            final long max, final BlocksMapUpdateInfo info) {\n-      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n+    static void collectBlocksAndClear(final FileWithSnapshot file,\n+        final BlocksMapUpdateInfo info) {\n+      final FileWithSnapshot next \u003d file.getNext();\n+\n+      // find max file size, max replication and the last inode.\n+      long maxFileSize \u003d file.computeMaxFileSize();\n+      short maxReplication \u003d file.getMaxFileReplication();\n+      FileWithSnapshot last \u003d null;\n+      if (next !\u003d null \u0026\u0026 next !\u003d file) {\n+        for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+          final long size \u003d i.computeMaxFileSize();\n+          if (size \u003e maxFileSize) {\n+            maxFileSize \u003d size;\n+          }\n+          final short rep \u003d i.getMaxFileReplication();\n+          if (rep \u003e maxReplication) {\n+            maxReplication \u003d rep;\n+          }\n+          last \u003d i;\n+        }\n+      }\n+\n+      collectBlocksBeyondMax(file, maxFileSize, info);\n+\n+      if (file.isEverythingDeleted()) {\n+        // Set the replication of the current INode to the max of all the other\n+        // linked INodes, so that in case the current INode is retrieved from the\n+        // blocksMap before it is removed or updated, the correct replication\n+        // number can be retrieved.\n+        if (maxReplication \u003e 0) {\n+          file.asINodeFile().setFileReplication(maxReplication, null);\n         }\n \n-        // Replace the INode for all the remaining blocks in blocksMap\n-        final FileWithSnapshot next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file.asINodeFile(), next.asINodeFile());\n-        if (info !\u003d null) {\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n-          }\n+        // remove the file from the circular linked list.\n+        if (last !\u003d null) {\n+          last.setNext(next);\n         }\n-        \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-            i.asINodeFile().setBlocks(newBlocks);\n-          }\n+        file.setNext(null);\n \n-          // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n-            }\n-          }\n-        }\n         file.asINodeFile().setBlocks(null);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksAndClear(final FileWithSnapshot file,\n        final BlocksMapUpdateInfo info) {\n      final FileWithSnapshot next \u003d file.getNext();\n\n      // find max file size, max replication and the last inode.\n      long maxFileSize \u003d file.computeMaxFileSize();\n      short maxReplication \u003d file.getMaxFileReplication();\n      FileWithSnapshot last \u003d null;\n      if (next !\u003d null \u0026\u0026 next !\u003d file) {\n        for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n          final long size \u003d i.computeMaxFileSize();\n          if (size \u003e maxFileSize) {\n            maxFileSize \u003d size;\n          }\n          final short rep \u003d i.getMaxFileReplication();\n          if (rep \u003e maxReplication) {\n            maxReplication \u003d rep;\n          }\n          last \u003d i;\n        }\n      }\n\n      collectBlocksBeyondMax(file, maxFileSize, info);\n\n      if (file.isEverythingDeleted()) {\n        // Set the replication of the current INode to the max of all the other\n        // linked INodes, so that in case the current INode is retrieved from the\n        // blocksMap before it is removed or updated, the correct replication\n        // number can be retrieved.\n        if (maxReplication \u003e 0) {\n          file.asINodeFile().setFileReplication(maxReplication, null);\n        }\n\n        // remove the file from the circular linked list.\n        if (last !\u003d null) {\n          last.setNext(next);\n        }\n        file.setNext(null);\n\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {}
        }
      ]
    },
    "b71d3868908a49c1b2e353afea795a76dfb20f7d": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-4098. Add FileWithSnapshot, INodeFileUnderConstructionWithSnapshot and INodeFileUnderConstructionSnapshot for supporting append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1434966 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/01/13 3:38 PM",
      "commitName": "b71d3868908a49c1b2e353afea795a76dfb20f7d",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-4098. Add FileWithSnapshot, INodeFileUnderConstructionWithSnapshot and INodeFileUnderConstructionSnapshot for supporting append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1434966 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/01/13 3:38 PM",
          "commitName": "b71d3868908a49c1b2e353afea795a76dfb20f7d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/01/13 4:43 AM",
          "commitNameOld": "7856221d4a4701565bb21259d839c8c402e72f49",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final FileWithSnapshot next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file.asINodeFile(), next.asINodeFile());\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.asINodeFile().setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.asINodeFile().setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final FileWithSnapshot next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file.asINodeFile(), next.asINodeFile());\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
            "oldMethodName": "collectBlocksBeyondMaxAndClear",
            "newMethodName": "collectBlocksBeyondMaxAndClear"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-4098. Add FileWithSnapshot, INodeFileUnderConstructionWithSnapshot and INodeFileUnderConstructionSnapshot for supporting append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1434966 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/01/13 3:38 PM",
          "commitName": "b71d3868908a49c1b2e353afea795a76dfb20f7d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/01/13 4:43 AM",
          "commitNameOld": "7856221d4a4701565bb21259d839c8c402e72f49",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final FileWithSnapshot next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file.asINodeFile(), next.asINodeFile());\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.asINodeFile().setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.asINodeFile().setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final FileWithSnapshot next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file.asINodeFile(), next.asINodeFile());\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4098. Add FileWithSnapshot, INodeFileUnderConstructionWithSnapshot and INodeFileUnderConstructionSnapshot for supporting append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1434966 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/01/13 3:38 PM",
          "commitName": "b71d3868908a49c1b2e353afea795a76dfb20f7d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/01/13 4:43 AM",
          "commitNameOld": "7856221d4a4701565bb21259d839c8c402e72f49",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final FileWithSnapshot next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file.asINodeFile(), next.asINodeFile());\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.asINodeFile().setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.asINodeFile().setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final FileWithSnapshot next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file.asINodeFile(), next.asINodeFile());\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4098. Add FileWithSnapshot, INodeFileUnderConstructionWithSnapshot and INodeFileUnderConstructionSnapshot for supporting append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1434966 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/01/13 3:38 PM",
          "commitName": "b71d3868908a49c1b2e353afea795a76dfb20f7d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/01/13 4:43 AM",
          "commitNameOld": "7856221d4a4701565bb21259d839c8c402e72f49",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final FileWithSnapshot next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file.asINodeFile(), next.asINodeFile());\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.asINodeFile().setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.asINodeFile().setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "    static void collectBlocksBeyondMaxAndClear(final FileWithSnapshot file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.asINodeFile().getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final FileWithSnapshot next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file.asINodeFile(), next.asINodeFile());\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(FileWithSnapshot i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.asINodeFile().setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.asINodeFile().setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshot.java",
          "extendedDetails": {
            "oldValue": "[max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[file-FileWithSnapshot(modifiers-final), max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]"
          }
        }
      ]
    },
    "397835acdf66cf48ebdbc256aa15b6660181c339": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "svn merge -c -1432788 for reverting HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1433284 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/01/13 8:33 PM",
      "commitName": "397835acdf66cf48ebdbc256aa15b6660181c339",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "svn merge -c -1432788 for reverting HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1433284 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/01/13 8:33 PM",
          "commitName": "397835acdf66cf48ebdbc256aa15b6660181c339",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/01/13 12:40 AM",
          "commitNameOld": "686e13db2fdb1cb7b8d0cc55a677b25df420156d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.83,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,43 @@\n-        void collectBlocksBeyondMaxAndClear(final F file,\n-            final long max, final BlocksMapUpdateInfo info) {\n-      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n-        }\n-\n-        // Replace the INode for all the remaining blocks in blocksMap\n-        final N next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file, next);\n-        if (info !\u003d null) {\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n-          }\n-        }\n-        \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-            i.setBlocks(newBlocks);\n-          }\n-\n-          // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n-            }\n-          }\n-        }\n-        file.setBlocks(null);\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n       }\n-    }\n\\ No newline at end of file\n+\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+        }\n+        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n+          i.setBlocks(newBlocks);\n+        }\n+\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            info.addDeleteBlock(oldBlocks[n]);\n+          }\n+        }\n+      }\n+      setBlocks(null);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
            "oldMethodName": "collectBlocksBeyondMaxAndClear",
            "newMethodName": "collectBlocksBeyondMaxAndClear"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "svn merge -c -1432788 for reverting HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1433284 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/01/13 8:33 PM",
          "commitName": "397835acdf66cf48ebdbc256aa15b6660181c339",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/01/13 12:40 AM",
          "commitNameOld": "686e13db2fdb1cb7b8d0cc55a677b25df420156d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.83,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,43 @@\n-        void collectBlocksBeyondMaxAndClear(final F file,\n-            final long max, final BlocksMapUpdateInfo info) {\n-      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n-        }\n-\n-        // Replace the INode for all the remaining blocks in blocksMap\n-        final N next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file, next);\n-        if (info !\u003d null) {\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n-          }\n-        }\n-        \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-            i.setBlocks(newBlocks);\n-          }\n-\n-          // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n-            }\n-          }\n-        }\n-        file.setBlocks(null);\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n       }\n-    }\n\\ No newline at end of file\n+\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+        }\n+        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n+          i.setBlocks(newBlocks);\n+        }\n+\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            info.addDeleteBlock(oldBlocks[n]);\n+          }\n+        }\n+      }\n+      setBlocks(null);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {
            "oldValue": "[static]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "svn merge -c -1432788 for reverting HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1433284 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/01/13 8:33 PM",
          "commitName": "397835acdf66cf48ebdbc256aa15b6660181c339",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/01/13 12:40 AM",
          "commitNameOld": "686e13db2fdb1cb7b8d0cc55a677b25df420156d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.83,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,43 @@\n-        void collectBlocksBeyondMaxAndClear(final F file,\n-            final long max, final BlocksMapUpdateInfo info) {\n-      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n-        }\n-\n-        // Replace the INode for all the remaining blocks in blocksMap\n-        final N next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file, next);\n-        if (info !\u003d null) {\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n-          }\n-        }\n-        \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-            i.setBlocks(newBlocks);\n-          }\n-\n-          // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n-            }\n-          }\n-        }\n-        file.setBlocks(null);\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n       }\n-    }\n\\ No newline at end of file\n+\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+        }\n+        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n+          i.setBlocks(newBlocks);\n+        }\n+\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            info.addDeleteBlock(oldBlocks[n]);\n+          }\n+        }\n+      }\n+      setBlocks(null);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "svn merge -c -1432788 for reverting HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1433284 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/01/13 8:33 PM",
          "commitName": "397835acdf66cf48ebdbc256aa15b6660181c339",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/01/13 12:40 AM",
          "commitNameOld": "686e13db2fdb1cb7b8d0cc55a677b25df420156d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.83,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,43 @@\n-        void collectBlocksBeyondMaxAndClear(final F file,\n-            final long max, final BlocksMapUpdateInfo info) {\n-      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n-      if (oldBlocks !\u003d null) {\n-        //find the minimum n such that the size of the first n blocks \u003e max\n-        int n \u003d 0;\n-        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-          size +\u003d oldBlocks[n].getNumBytes();\n-        }\n-\n-        // Replace the INode for all the remaining blocks in blocksMap\n-        final N next \u003d file.getNext();\n-        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n-            file, next);\n-        if (info !\u003d null) {\n-          for (int i \u003d 0; i \u003c n; i++) {\n-            info.addUpdateBlock(oldBlocks[i], entry);\n-          }\n-        }\n-        \n-        // starting from block n, the data is beyond max.\n-        if (n \u003c oldBlocks.length) {\n-          // resize the array.  \n-          final BlockInfo[] newBlocks;\n-          if (n \u003d\u003d 0) {\n-            newBlocks \u003d null;\n-          } else {\n-            newBlocks \u003d new BlockInfo[n];\n-            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-          }\n-          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n-            i.setBlocks(newBlocks);\n-          }\n-\n-          // collect the blocks beyond max.  \n-          if (info !\u003d null) {\n-            for(; n \u003c oldBlocks.length; n++) {\n-              info.addDeleteBlock(oldBlocks[n]);\n-            }\n-          }\n-        }\n-        file.setBlocks(null);\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n       }\n-    }\n\\ No newline at end of file\n+\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n+        // resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+        }\n+        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n+          i.setBlocks(newBlocks);\n+        }\n+\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n+          for(; n \u003c oldBlocks.length; n++) {\n+            info.addDeleteBlock(oldBlocks[n]);\n+          }\n+        }\n+      }\n+      setBlocks(null);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {
            "oldValue": "[file-F(modifiers-final), max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]"
          }
        }
      ]
    },
    "25116c26fd9b998025fa28666ae45ab03a995d91": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1432788 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/01/13 6:30 PM",
      "commitName": "25116c26fd9b998025fa28666ae45ab03a995d91",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1432788 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/01/13 6:30 PM",
          "commitName": "25116c26fd9b998025fa28666ae45ab03a995d91",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "13/01/13 3:29 AM",
          "commitNameOld": "bc0aff27a4b781b3af9603251b7e09b43e66368c",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.63,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+        void collectBlocksBeyondMaxAndClear(final F file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final N next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file, next);\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "        void collectBlocksBeyondMaxAndClear(final F file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final N next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file, next);\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
            "oldMethodName": "collectBlocksBeyondMaxAndClear",
            "newMethodName": "collectBlocksBeyondMaxAndClear"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1432788 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/01/13 6:30 PM",
          "commitName": "25116c26fd9b998025fa28666ae45ab03a995d91",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "13/01/13 3:29 AM",
          "commitNameOld": "bc0aff27a4b781b3af9603251b7e09b43e66368c",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.63,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+        void collectBlocksBeyondMaxAndClear(final F file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final N next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file, next);\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "        void collectBlocksBeyondMaxAndClear(final F file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final N next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file, next);\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1432788 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/01/13 6:30 PM",
          "commitName": "25116c26fd9b998025fa28666ae45ab03a995d91",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "13/01/13 3:29 AM",
          "commitNameOld": "bc0aff27a4b781b3af9603251b7e09b43e66368c",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.63,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+        void collectBlocksBeyondMaxAndClear(final F file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final N next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file, next);\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "        void collectBlocksBeyondMaxAndClear(final F file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final N next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file, next);\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4098. Add FileWithLink, INodeFileUnderConstructionWithLink and INodeFileUnderConstructionSnapshot in order to support append to snapshotted files.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1432788 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/01/13 6:30 PM",
          "commitName": "25116c26fd9b998025fa28666ae45ab03a995d91",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "13/01/13 3:29 AM",
          "commitNameOld": "bc0aff27a4b781b3af9603251b7e09b43e66368c",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.63,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max,\n-      final BlocksMapUpdateInfo info) {\n-    final BlockInfo[] oldBlocks \u003d getBlocks();\n-    if (oldBlocks !\u003d null) {\n-      //find the minimum n such that the size of the first n blocks \u003e max\n-      int n \u003d 0;\n-      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d oldBlocks[n].getNumBytes();\n-      }\n-\n-      // Replace the INode for all the remaining blocks in blocksMap\n-      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n-          next);\n-      if (info !\u003d null) {\n-        for (int i \u003d 0; i \u003c n; i++) {\n-          info.addUpdateBlock(oldBlocks[i], entry);\n-        }\n-      }\n-      \n-      // starting from block n, the data is beyond max.\n-      if (n \u003c oldBlocks.length) {\n-        // resize the array.  \n-        final BlockInfo[] newBlocks;\n-        if (n \u003d\u003d 0) {\n-          newBlocks \u003d null;\n-        } else {\n-          newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n-        }\n-        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.setBlocks(newBlocks);\n+        void collectBlocksBeyondMaxAndClear(final F file,\n+            final long max, final BlocksMapUpdateInfo info) {\n+      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n+      if (oldBlocks !\u003d null) {\n+        //find the minimum n such that the size of the first n blocks \u003e max\n+        int n \u003d 0;\n+        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+          size +\u003d oldBlocks[n].getNumBytes();\n         }\n \n-        // collect the blocks beyond max.  \n+        // Replace the INode for all the remaining blocks in blocksMap\n+        final N next \u003d file.getNext();\n+        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n+            file, next);\n         if (info !\u003d null) {\n-          for(; n \u003c oldBlocks.length; n++) {\n-            info.addDeleteBlock(oldBlocks[n]);\n+          for (int i \u003d 0; i \u003c n; i++) {\n+            info.addUpdateBlock(oldBlocks[i], entry);\n           }\n         }\n+        \n+        // starting from block n, the data is beyond max.\n+        if (n \u003c oldBlocks.length) {\n+          // resize the array.  \n+          final BlockInfo[] newBlocks;\n+          if (n \u003d\u003d 0) {\n+            newBlocks \u003d null;\n+          } else {\n+            newBlocks \u003d new BlockInfo[n];\n+            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n+          }\n+          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n+            i.setBlocks(newBlocks);\n+          }\n+\n+          // collect the blocks beyond max.  \n+          if (info !\u003d null) {\n+            for(; n \u003c oldBlocks.length; n++) {\n+              info.addDeleteBlock(oldBlocks[n]);\n+            }\n+          }\n+        }\n+        file.setBlocks(null);\n       }\n-      setBlocks(null);\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "        void collectBlocksBeyondMaxAndClear(final F file,\n            final long max, final BlocksMapUpdateInfo info) {\n      final BlockInfo[] oldBlocks \u003d file.getBlocks();\n      if (oldBlocks !\u003d null) {\n        //find the minimum n such that the size of the first n blocks \u003e max\n        int n \u003d 0;\n        for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n          size +\u003d oldBlocks[n].getNumBytes();\n        }\n\n        // Replace the INode for all the remaining blocks in blocksMap\n        final N next \u003d file.getNext();\n        final BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(\n            file, next);\n        if (info !\u003d null) {\n          for (int i \u003d 0; i \u003c n; i++) {\n            info.addUpdateBlock(oldBlocks[i], entry);\n          }\n        }\n        \n        // starting from block n, the data is beyond max.\n        if (n \u003c oldBlocks.length) {\n          // resize the array.  \n          final BlockInfo[] newBlocks;\n          if (n \u003d\u003d 0) {\n            newBlocks \u003d null;\n          } else {\n            newBlocks \u003d new BlockInfo[n];\n            System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n          }\n          for(N i \u003d next; i !\u003d file; i \u003d i.getNext()) {\n            i.setBlocks(newBlocks);\n          }\n\n          // collect the blocks beyond max.  \n          if (info !\u003d null) {\n            for(; n \u003c oldBlocks.length; n++) {\n              info.addDeleteBlock(oldBlocks[n]);\n            }\n          }\n        }\n        file.setBlocks(null);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithLink.java",
          "extendedDetails": {
            "oldValue": "[max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]",
            "newValue": "[file-F(modifiers-final), max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]"
          }
        }
      ]
    },
    "8a577a16f96437d87ad764dedbdc67d4c184b8d9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4150.  Update the inode in the block map when a snapshotted file or a snapshot file is deleted. Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1406763 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/11/12 11:30 AM",
      "commitName": "8a577a16f96437d87ad764dedbdc67d4c184b8d9",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4150.  Update the inode in the block map when a snapshotted file or a snapshot file is deleted. Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1406763 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/11/12 11:30 AM",
          "commitName": "8a577a16f96437d87ad764dedbdc67d4c184b8d9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "04/11/12 2:00 PM",
          "commitNameOld": "deaf979d4122a0a0e4ae0557abbb7f17d18a9380",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.9,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,43 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n     final BlockInfo[] oldBlocks \u003d getBlocks();\n     if (oldBlocks !\u003d null) {\n       //find the minimum n such that the size of the first n blocks \u003e max\n       int n \u003d 0;\n       for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n         size +\u003d oldBlocks[n].getNumBytes();\n       }\n \n-      //starting from block n, the data is beyond max.\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n       if (n \u003c oldBlocks.length) {\n-        //resize the array.  \n+        // resize the array.  \n         final BlockInfo[] newBlocks;\n         if (n \u003d\u003d 0) {\n           newBlocks \u003d null;\n         } else {\n           newBlocks \u003d new BlockInfo[n];\n           System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n           i.setBlocks(newBlocks);\n         }\n \n-        //collect the blocks beyond max.  \n-        if (v !\u003d null) {\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n           for(; n \u003c oldBlocks.length; n++) {\n-            v.add(oldBlocks[n]);\n+            info.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n       setBlocks(null);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {
            "oldValue": "[max-long(modifiers-final), v-List\u003cBlock\u003e(modifiers-final)]",
            "newValue": "[max-long(modifiers-final), info-BlocksMapUpdateInfo(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4150.  Update the inode in the block map when a snapshotted file or a snapshot file is deleted. Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1406763 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/11/12 11:30 AM",
          "commitName": "8a577a16f96437d87ad764dedbdc67d4c184b8d9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "04/11/12 2:00 PM",
          "commitNameOld": "deaf979d4122a0a0e4ae0557abbb7f17d18a9380",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.9,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,43 @@\n-  private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n+  private void collectBlocksBeyondMaxAndClear(final long max,\n+      final BlocksMapUpdateInfo info) {\n     final BlockInfo[] oldBlocks \u003d getBlocks();\n     if (oldBlocks !\u003d null) {\n       //find the minimum n such that the size of the first n blocks \u003e max\n       int n \u003d 0;\n       for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n         size +\u003d oldBlocks[n].getNumBytes();\n       }\n \n-      //starting from block n, the data is beyond max.\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n+          next);\n+      if (info !\u003d null) {\n+        for (int i \u003d 0; i \u003c n; i++) {\n+          info.addUpdateBlock(oldBlocks[i], entry);\n+        }\n+      }\n+      \n+      // starting from block n, the data is beyond max.\n       if (n \u003c oldBlocks.length) {\n-        //resize the array.  \n+        // resize the array.  \n         final BlockInfo[] newBlocks;\n         if (n \u003d\u003d 0) {\n           newBlocks \u003d null;\n         } else {\n           newBlocks \u003d new BlockInfo[n];\n           System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n           i.setBlocks(newBlocks);\n         }\n \n-        //collect the blocks beyond max.  \n-        if (v !\u003d null) {\n+        // collect the blocks beyond max.  \n+        if (info !\u003d null) {\n           for(; n \u003c oldBlocks.length; n++) {\n-            v.add(oldBlocks[n]);\n+            info.addDeleteBlock(oldBlocks[n]);\n           }\n         }\n       }\n       setBlocks(null);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max,\n      final BlocksMapUpdateInfo info) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      // Replace the INode for all the remaining blocks in blocksMap\n      BlocksMapINodeUpdateEntry entry \u003d new BlocksMapINodeUpdateEntry(this,\n          next);\n      if (info !\u003d null) {\n        for (int i \u003d 0; i \u003c n; i++) {\n          info.addUpdateBlock(oldBlocks[i], entry);\n        }\n      }\n      \n      // starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        // resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        // collect the blocks beyond max.  \n        if (info !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            info.addDeleteBlock(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
          "extendedDetails": {}
        }
      ]
    },
    "deaf979d4122a0a0e4ae0557abbb7f17d18a9380": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4146. Use getter and setter in INodeFileWithLink to access blocks and initialize root directory as snapshottable.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1405648 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/11/12 2:00 PM",
      "commitName": "deaf979d4122a0a0e4ae0557abbb7f17d18a9380",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "02/11/12 6:31 PM",
      "commitNameOld": "e5a7b3d4307951f0574c2341178e3cd95e69f7b7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.85,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,33 @@\n   private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n-    if (blocks !\u003d null) {\n+    final BlockInfo[] oldBlocks \u003d getBlocks();\n+    if (oldBlocks !\u003d null) {\n       //find the minimum n such that the size of the first n blocks \u003e max\n       int n \u003d 0;\n-      for(long size \u003d 0; n \u003c blocks.length \u0026\u0026 max \u003e size; n++) {\n-        size +\u003d blocks[n].getNumBytes();\n+      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d oldBlocks[n].getNumBytes();\n       }\n \n-      //starting from block[n], the data is beyond max.\n-      if (n \u003c blocks.length) {\n+      //starting from block n, the data is beyond max.\n+      if (n \u003c oldBlocks.length) {\n         //resize the array.  \n         final BlockInfo[] newBlocks;\n         if (n \u003d\u003d 0) {\n           newBlocks \u003d null;\n         } else {\n           newBlocks \u003d new BlockInfo[n];\n-          System.arraycopy(blocks, 0, newBlocks, 0, n);\n+          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n         }\n         for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n-          i.blocks \u003d newBlocks;\n+          i.setBlocks(newBlocks);\n         }\n \n         //collect the blocks beyond max.  \n         if (v !\u003d null) {\n-          for(; n \u003c blocks.length; n++) {\n-            v.add(blocks[n]);\n+          for(; n \u003c oldBlocks.length; n++) {\n+            v.add(oldBlocks[n]);\n           }\n         }\n       }\n-      blocks \u003d null;\n+      setBlocks(null);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n    final BlockInfo[] oldBlocks \u003d getBlocks();\n    if (oldBlocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c oldBlocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d oldBlocks[n].getNumBytes();\n      }\n\n      //starting from block n, the data is beyond max.\n      if (n \u003c oldBlocks.length) {\n        //resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(oldBlocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.setBlocks(newBlocks);\n        }\n\n        //collect the blocks beyond max.  \n        if (v !\u003d null) {\n          for(; n \u003c oldBlocks.length; n++) {\n            v.add(oldBlocks[n]);\n          }\n        }\n      }\n      setBlocks(null);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java",
      "extendedDetails": {}
    },
    "719279ea8a510ba8d04174ac85ad42fa991725a2": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4092. Update file deletion logic for snapshot so that the current inode is removed from the circular linked list; and if some blocks at the end of the block list no longer belong to any other inode, collect them and update the block list.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1402287 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/10/12 12:32 PM",
      "commitName": "719279ea8a510ba8d04174ac85ad42fa991725a2",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,32 @@\n+  private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n+    if (blocks !\u003d null) {\n+      //find the minimum n such that the size of the first n blocks \u003e max\n+      int n \u003d 0;\n+      for(long size \u003d 0; n \u003c blocks.length \u0026\u0026 max \u003e size; n++) {\n+        size +\u003d blocks[n].getNumBytes();\n+      }\n+\n+      //starting from block[n], the data is beyond max.\n+      if (n \u003c blocks.length) {\n+        //resize the array.  \n+        final BlockInfo[] newBlocks;\n+        if (n \u003d\u003d 0) {\n+          newBlocks \u003d null;\n+        } else {\n+          newBlocks \u003d new BlockInfo[n];\n+          System.arraycopy(blocks, 0, newBlocks, 0, n);\n+        }\n+        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n+          i.blocks \u003d newBlocks;\n+        }\n+\n+        //collect the blocks beyond max.  \n+        if (v !\u003d null) {\n+          for(; n \u003c blocks.length; n++) {\n+            v.add(blocks[n]);\n+          }\n+        }\n+      }\n+      blocks \u003d null;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void collectBlocksBeyondMaxAndClear(final long max, final List\u003cBlock\u003e v) {\n    if (blocks !\u003d null) {\n      //find the minimum n such that the size of the first n blocks \u003e max\n      int n \u003d 0;\n      for(long size \u003d 0; n \u003c blocks.length \u0026\u0026 max \u003e size; n++) {\n        size +\u003d blocks[n].getNumBytes();\n      }\n\n      //starting from block[n], the data is beyond max.\n      if (n \u003c blocks.length) {\n        //resize the array.  \n        final BlockInfo[] newBlocks;\n        if (n \u003d\u003d 0) {\n          newBlocks \u003d null;\n        } else {\n          newBlocks \u003d new BlockInfo[n];\n          System.arraycopy(blocks, 0, newBlocks, 0, n);\n        }\n        for(INodeFileWithLink i \u003d next; i !\u003d this; i \u003d i.getNext()) {\n          i.blocks \u003d newBlocks;\n        }\n\n        //collect the blocks beyond max.  \n        if (v !\u003d null) {\n          for(; n \u003c blocks.length; n++) {\n            v.add(blocks[n]);\n          }\n        }\n      }\n      blocks \u003d null;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java"
    }
  }
}