{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSNamesystem.java",
  "functionName": "getLiveNodes",
  "functionId": "getLiveNodes",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
  "functionStartLine": 6397,
  "functionEndLine": 6440,
  "numCommitsSeen": 899,
  "timeTaken": 53224,
  "changeHistory": [
    "b5adc5c3011f111f86d232cb33ec522547f68a95",
    "96b12662ea76e3ded4ef13944fc8df206cfb4613",
    "10ab7d595ece59f2d00b406ba8812c6295a4187f",
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
    "9729b244de50322c2cc889c97c2ffb2b4675cf77",
    "45fa7f023532e79dff3cf381056eff717dc4ecc7",
    "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8",
    "88209ce181b5ecc55c0ae2bceff4893ab4817e88",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
    "cf0cf0a6910244d929f40842223e7d0b2c9445e8",
    "39252995c4d734e993e3fa5338e1a7816aee86fc",
    "15fe3ae61b9931bdd24fbc6e4d3181132fcfffce",
    "17d22e6dc441f72cffa4413ec30921d6d4c41cfd",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "371f4a59059322000a40eb4bdf5386b96b626ece",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "b5adc5c3011f111f86d232cb33ec522547f68a95": "Ybodychange",
    "96b12662ea76e3ded4ef13944fc8df206cfb4613": "Ybodychange",
    "10ab7d595ece59f2d00b406ba8812c6295a4187f": "Ybodychange",
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501": "Ybodychange",
    "9729b244de50322c2cc889c97c2ffb2b4675cf77": "Ybodychange",
    "45fa7f023532e79dff3cf381056eff717dc4ecc7": "Ybodychange",
    "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8": "Ybodychange",
    "88209ce181b5ecc55c0ae2bceff4893ab4817e88": "Ybodychange",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": "Ybodychange",
    "cf0cf0a6910244d929f40842223e7d0b2c9445e8": "Ybodychange",
    "39252995c4d734e993e3fa5338e1a7816aee86fc": "Ybodychange",
    "15fe3ae61b9931bdd24fbc6e4d3181132fcfffce": "Ybodychange",
    "17d22e6dc441f72cffa4413ec30921d6d4c41cfd": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "371f4a59059322000a40eb4bdf5386b96b626ece": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b5adc5c3011f111f86d232cb33ec522547f68a95": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10838. Last full block report received time for each DN should be easily discoverable. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "06/03/17 4:39 PM",
      "commitName": "b5adc5c3011f111f86d232cb33ec522547f68a95",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "03/03/17 1:00 PM",
      "commitNameOld": "3085a604300ed76d06a0011bd5555e419897b6cd",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 3.15,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,44 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, false);\n     for (DatanodeDescriptor node : live) {\n       ImmutableMap.Builder\u003cString, Object\u003e innerinfo \u003d\n           ImmutableMap.\u003cString,Object\u003ebuilder();\n       innerinfo\n           .put(\"infoAddr\", node.getInfoAddr())\n           .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n           .put(\"xferaddr\", node.getXferAddr())\n           .put(\"lastContact\", getLastContact(node))\n           .put(\"usedSpace\", getDfsUsed(node))\n           .put(\"adminState\", node.getAdminState().toString())\n           .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n           .put(\"capacity\", node.getCapacity())\n           .put(\"numBlocks\", node.numBlocks())\n           .put(\"version\", node.getSoftwareVersion())\n           .put(\"used\", node.getDfsUsed())\n           .put(\"remaining\", node.getRemaining())\n           .put(\"blockScheduled\", node.getBlocksScheduled())\n           .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n           .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n-          .put(\"volfails\", node.getVolumeFailures());\n+          .put(\"volfails\", node.getVolumeFailures())\n+          // Block report time in minutes\n+          .put(\"lastBlockReport\", getLastBlockReport(node));\n       VolumeFailureSummary volumeFailureSummary \u003d node.getVolumeFailureSummary();\n       if (volumeFailureSummary !\u003d null) {\n         innerinfo\n             .put(\"failedStorageIDs\",\n                 volumeFailureSummary.getFailedStorageLocations())\n             .put(\"lastVolumeFailureDate\",\n                 volumeFailureSummary.getLastVolumeFailureDate())\n             .put(\"estimatedCapacityLostTotal\",\n                 volumeFailureSummary.getEstimatedCapacityLostTotal());\n       }\n       if (node.getUpgradeDomain() !\u003d null) {\n         innerinfo.put(\"upgradeDomain\", node.getUpgradeDomain());\n       }\n       info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo.build());\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, false);\n    for (DatanodeDescriptor node : live) {\n      ImmutableMap.Builder\u003cString, Object\u003e innerinfo \u003d\n          ImmutableMap.\u003cString,Object\u003ebuilder();\n      innerinfo\n          .put(\"infoAddr\", node.getInfoAddr())\n          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n          .put(\"xferaddr\", node.getXferAddr())\n          .put(\"lastContact\", getLastContact(node))\n          .put(\"usedSpace\", getDfsUsed(node))\n          .put(\"adminState\", node.getAdminState().toString())\n          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n          .put(\"capacity\", node.getCapacity())\n          .put(\"numBlocks\", node.numBlocks())\n          .put(\"version\", node.getSoftwareVersion())\n          .put(\"used\", node.getDfsUsed())\n          .put(\"remaining\", node.getRemaining())\n          .put(\"blockScheduled\", node.getBlocksScheduled())\n          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n          .put(\"volfails\", node.getVolumeFailures())\n          // Block report time in minutes\n          .put(\"lastBlockReport\", getLastBlockReport(node));\n      VolumeFailureSummary volumeFailureSummary \u003d node.getVolumeFailureSummary();\n      if (volumeFailureSummary !\u003d null) {\n        innerinfo\n            .put(\"failedStorageIDs\",\n                volumeFailureSummary.getFailedStorageLocations())\n            .put(\"lastVolumeFailureDate\",\n                volumeFailureSummary.getLastVolumeFailureDate())\n            .put(\"estimatedCapacityLostTotal\",\n                volumeFailureSummary.getEstimatedCapacityLostTotal());\n      }\n      if (node.getUpgradeDomain() !\u003d null) {\n        innerinfo.put(\"upgradeDomain\", node.getUpgradeDomain());\n      }\n      info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo.build());\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "96b12662ea76e3ded4ef13944fc8df206cfb4613": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10637. Modifications to remove the assumption that FsVolumes are backed by java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "10/10/16 3:30 PM",
      "commitName": "96b12662ea76e3ded4ef13944fc8df206cfb4613",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "07/10/16 12:15 PM",
      "commitNameOld": "3565c9af17ab05bf9e7f68b71b6c6850df772bb9",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 3.13,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, false);\n     for (DatanodeDescriptor node : live) {\n       ImmutableMap.Builder\u003cString, Object\u003e innerinfo \u003d\n           ImmutableMap.\u003cString,Object\u003ebuilder();\n       innerinfo\n           .put(\"infoAddr\", node.getInfoAddr())\n           .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n           .put(\"xferaddr\", node.getXferAddr())\n           .put(\"lastContact\", getLastContact(node))\n           .put(\"usedSpace\", getDfsUsed(node))\n           .put(\"adminState\", node.getAdminState().toString())\n           .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n           .put(\"capacity\", node.getCapacity())\n           .put(\"numBlocks\", node.numBlocks())\n           .put(\"version\", node.getSoftwareVersion())\n           .put(\"used\", node.getDfsUsed())\n           .put(\"remaining\", node.getRemaining())\n           .put(\"blockScheduled\", node.getBlocksScheduled())\n           .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n           .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n           .put(\"volfails\", node.getVolumeFailures());\n       VolumeFailureSummary volumeFailureSummary \u003d node.getVolumeFailureSummary();\n       if (volumeFailureSummary !\u003d null) {\n         innerinfo\n-            .put(\"failedStorageLocations\",\n+            .put(\"failedStorageIDs\",\n                 volumeFailureSummary.getFailedStorageLocations())\n             .put(\"lastVolumeFailureDate\",\n                 volumeFailureSummary.getLastVolumeFailureDate())\n             .put(\"estimatedCapacityLostTotal\",\n                 volumeFailureSummary.getEstimatedCapacityLostTotal());\n       }\n       if (node.getUpgradeDomain() !\u003d null) {\n         innerinfo.put(\"upgradeDomain\", node.getUpgradeDomain());\n       }\n       info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo.build());\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, false);\n    for (DatanodeDescriptor node : live) {\n      ImmutableMap.Builder\u003cString, Object\u003e innerinfo \u003d\n          ImmutableMap.\u003cString,Object\u003ebuilder();\n      innerinfo\n          .put(\"infoAddr\", node.getInfoAddr())\n          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n          .put(\"xferaddr\", node.getXferAddr())\n          .put(\"lastContact\", getLastContact(node))\n          .put(\"usedSpace\", getDfsUsed(node))\n          .put(\"adminState\", node.getAdminState().toString())\n          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n          .put(\"capacity\", node.getCapacity())\n          .put(\"numBlocks\", node.numBlocks())\n          .put(\"version\", node.getSoftwareVersion())\n          .put(\"used\", node.getDfsUsed())\n          .put(\"remaining\", node.getRemaining())\n          .put(\"blockScheduled\", node.getBlocksScheduled())\n          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n          .put(\"volfails\", node.getVolumeFailures());\n      VolumeFailureSummary volumeFailureSummary \u003d node.getVolumeFailureSummary();\n      if (volumeFailureSummary !\u003d null) {\n        innerinfo\n            .put(\"failedStorageIDs\",\n                volumeFailureSummary.getFailedStorageLocations())\n            .put(\"lastVolumeFailureDate\",\n                volumeFailureSummary.getLastVolumeFailureDate())\n            .put(\"estimatedCapacityLostTotal\",\n                volumeFailureSummary.getEstimatedCapacityLostTotal());\n      }\n      if (node.getUpgradeDomain() !\u003d null) {\n        innerinfo.put(\"upgradeDomain\", node.getUpgradeDomain());\n      }\n      info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo.build());\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "10ab7d595ece59f2d00b406ba8812c6295a4187f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8780. Fetching live/dead datanode list with arg true for removeDecommissionNode,returns list with decom node. (Contributed by J.Andreina)\n",
      "commitDate": "21/09/15 11:55 PM",
      "commitName": "10ab7d595ece59f2d00b406ba8812c6295a4187f",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "19/09/15 6:08 PM",
      "commitNameOld": "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 2.24,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n-    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n+    blockManager.getDatanodeManager().fetchDatanodes(live, null, false);\n     for (DatanodeDescriptor node : live) {\n       ImmutableMap.Builder\u003cString, Object\u003e innerinfo \u003d\n           ImmutableMap.\u003cString,Object\u003ebuilder();\n       innerinfo\n           .put(\"infoAddr\", node.getInfoAddr())\n           .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n           .put(\"xferaddr\", node.getXferAddr())\n           .put(\"lastContact\", getLastContact(node))\n           .put(\"usedSpace\", getDfsUsed(node))\n           .put(\"adminState\", node.getAdminState().toString())\n           .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n           .put(\"capacity\", node.getCapacity())\n           .put(\"numBlocks\", node.numBlocks())\n           .put(\"version\", node.getSoftwareVersion())\n           .put(\"used\", node.getDfsUsed())\n           .put(\"remaining\", node.getRemaining())\n           .put(\"blockScheduled\", node.getBlocksScheduled())\n           .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n           .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n           .put(\"volfails\", node.getVolumeFailures());\n       VolumeFailureSummary volumeFailureSummary \u003d node.getVolumeFailureSummary();\n       if (volumeFailureSummary !\u003d null) {\n         innerinfo\n             .put(\"failedStorageLocations\",\n                 volumeFailureSummary.getFailedStorageLocations())\n             .put(\"lastVolumeFailureDate\",\n                 volumeFailureSummary.getLastVolumeFailureDate())\n             .put(\"estimatedCapacityLostTotal\",\n                 volumeFailureSummary.getEstimatedCapacityLostTotal());\n       }\n       if (node.getUpgradeDomain() !\u003d null) {\n         innerinfo.put(\"upgradeDomain\", node.getUpgradeDomain());\n       }\n       info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo.build());\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, false);\n    for (DatanodeDescriptor node : live) {\n      ImmutableMap.Builder\u003cString, Object\u003e innerinfo \u003d\n          ImmutableMap.\u003cString,Object\u003ebuilder();\n      innerinfo\n          .put(\"infoAddr\", node.getInfoAddr())\n          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n          .put(\"xferaddr\", node.getXferAddr())\n          .put(\"lastContact\", getLastContact(node))\n          .put(\"usedSpace\", getDfsUsed(node))\n          .put(\"adminState\", node.getAdminState().toString())\n          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n          .put(\"capacity\", node.getCapacity())\n          .put(\"numBlocks\", node.numBlocks())\n          .put(\"version\", node.getSoftwareVersion())\n          .put(\"used\", node.getDfsUsed())\n          .put(\"remaining\", node.getRemaining())\n          .put(\"blockScheduled\", node.getBlocksScheduled())\n          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n          .put(\"volfails\", node.getVolumeFailures());\n      VolumeFailureSummary volumeFailureSummary \u003d node.getVolumeFailureSummary();\n      if (volumeFailureSummary !\u003d null) {\n        innerinfo\n            .put(\"failedStorageLocations\",\n                volumeFailureSummary.getFailedStorageLocations())\n            .put(\"lastVolumeFailureDate\",\n                volumeFailureSummary.getLastVolumeFailureDate())\n            .put(\"estimatedCapacityLostTotal\",\n                volumeFailureSummary.getEstimatedCapacityLostTotal());\n      }\n      if (node.getUpgradeDomain() !\u003d null) {\n        innerinfo.put(\"upgradeDomain\", node.getUpgradeDomain());\n      }\n      info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo.build());\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9004. Add upgrade domain to DatanodeInfo. Contributed by Ming Ma (via Lei (Eddy) Xu).\n\nChange-Id: I887c66578eebd61acc34b94f18da6e6851c609f4\n",
      "commitDate": "19/09/15 6:08 PM",
      "commitName": "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "10/09/15 7:16 AM",
      "commitNameOld": "a40342b0dab1f9137ae4b3679a5aca7f2a57d23d",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 9.45,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,42 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (DatanodeDescriptor node : live) {\n       ImmutableMap.Builder\u003cString, Object\u003e innerinfo \u003d\n           ImmutableMap.\u003cString,Object\u003ebuilder();\n       innerinfo\n           .put(\"infoAddr\", node.getInfoAddr())\n           .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n           .put(\"xferaddr\", node.getXferAddr())\n           .put(\"lastContact\", getLastContact(node))\n           .put(\"usedSpace\", getDfsUsed(node))\n           .put(\"adminState\", node.getAdminState().toString())\n           .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n           .put(\"capacity\", node.getCapacity())\n           .put(\"numBlocks\", node.numBlocks())\n           .put(\"version\", node.getSoftwareVersion())\n           .put(\"used\", node.getDfsUsed())\n           .put(\"remaining\", node.getRemaining())\n           .put(\"blockScheduled\", node.getBlocksScheduled())\n           .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n           .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n           .put(\"volfails\", node.getVolumeFailures());\n       VolumeFailureSummary volumeFailureSummary \u003d node.getVolumeFailureSummary();\n       if (volumeFailureSummary !\u003d null) {\n         innerinfo\n             .put(\"failedStorageLocations\",\n                 volumeFailureSummary.getFailedStorageLocations())\n             .put(\"lastVolumeFailureDate\",\n                 volumeFailureSummary.getLastVolumeFailureDate())\n             .put(\"estimatedCapacityLostTotal\",\n                 volumeFailureSummary.getEstimatedCapacityLostTotal());\n       }\n+      if (node.getUpgradeDomain() !\u003d null) {\n+        innerinfo.put(\"upgradeDomain\", node.getUpgradeDomain());\n+      }\n       info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo.build());\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      ImmutableMap.Builder\u003cString, Object\u003e innerinfo \u003d\n          ImmutableMap.\u003cString,Object\u003ebuilder();\n      innerinfo\n          .put(\"infoAddr\", node.getInfoAddr())\n          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n          .put(\"xferaddr\", node.getXferAddr())\n          .put(\"lastContact\", getLastContact(node))\n          .put(\"usedSpace\", getDfsUsed(node))\n          .put(\"adminState\", node.getAdminState().toString())\n          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n          .put(\"capacity\", node.getCapacity())\n          .put(\"numBlocks\", node.numBlocks())\n          .put(\"version\", node.getSoftwareVersion())\n          .put(\"used\", node.getDfsUsed())\n          .put(\"remaining\", node.getRemaining())\n          .put(\"blockScheduled\", node.getBlocksScheduled())\n          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n          .put(\"volfails\", node.getVolumeFailures());\n      VolumeFailureSummary volumeFailureSummary \u003d node.getVolumeFailureSummary();\n      if (volumeFailureSummary !\u003d null) {\n        innerinfo\n            .put(\"failedStorageLocations\",\n                volumeFailureSummary.getFailedStorageLocations())\n            .put(\"lastVolumeFailureDate\",\n                volumeFailureSummary.getLastVolumeFailureDate())\n            .put(\"estimatedCapacityLostTotal\",\n                volumeFailureSummary.getEstimatedCapacityLostTotal());\n      }\n      if (node.getUpgradeDomain() !\u003d null) {\n        innerinfo.put(\"upgradeDomain\", node.getUpgradeDomain());\n      }\n      info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo.build());\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "9729b244de50322c2cc889c97c2ffb2b4675cf77": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7604. Track and display failed DataNode storage locations in NameNode. Contributed by Chris Nauroth.\n",
      "commitDate": "16/02/15 2:43 PM",
      "commitName": "9729b244de50322c2cc889c97c2ffb2b4675cf77",
      "commitAuthor": "cnauroth",
      "commitDateOld": "13/02/15 9:01 PM",
      "commitNameOld": "f2231cebcddc80f0b753c4a7cb45ee4040846951",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.74,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,39 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (DatanodeDescriptor node : live) {\n-      Map\u003cString, Object\u003e innerinfo \u003d ImmutableMap.\u003cString, Object\u003ebuilder()\n+      ImmutableMap.Builder\u003cString, Object\u003e innerinfo \u003d\n+          ImmutableMap.\u003cString,Object\u003ebuilder();\n+      innerinfo\n           .put(\"infoAddr\", node.getInfoAddr())\n           .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n           .put(\"xferaddr\", node.getXferAddr())\n           .put(\"lastContact\", getLastContact(node))\n           .put(\"usedSpace\", getDfsUsed(node))\n           .put(\"adminState\", node.getAdminState().toString())\n           .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n           .put(\"capacity\", node.getCapacity())\n           .put(\"numBlocks\", node.numBlocks())\n           .put(\"version\", node.getSoftwareVersion())\n           .put(\"used\", node.getDfsUsed())\n           .put(\"remaining\", node.getRemaining())\n           .put(\"blockScheduled\", node.getBlocksScheduled())\n           .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n           .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n-          .put(\"volfails\", node.getVolumeFailures())\n-          .build();\n-      info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo);\n+          .put(\"volfails\", node.getVolumeFailures());\n+      VolumeFailureSummary volumeFailureSummary \u003d node.getVolumeFailureSummary();\n+      if (volumeFailureSummary !\u003d null) {\n+        innerinfo\n+            .put(\"failedStorageLocations\",\n+                volumeFailureSummary.getFailedStorageLocations())\n+            .put(\"lastVolumeFailureDate\",\n+                volumeFailureSummary.getLastVolumeFailureDate())\n+            .put(\"estimatedCapacityLostTotal\",\n+                volumeFailureSummary.getEstimatedCapacityLostTotal());\n+      }\n+      info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo.build());\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      ImmutableMap.Builder\u003cString, Object\u003e innerinfo \u003d\n          ImmutableMap.\u003cString,Object\u003ebuilder();\n      innerinfo\n          .put(\"infoAddr\", node.getInfoAddr())\n          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n          .put(\"xferaddr\", node.getXferAddr())\n          .put(\"lastContact\", getLastContact(node))\n          .put(\"usedSpace\", getDfsUsed(node))\n          .put(\"adminState\", node.getAdminState().toString())\n          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n          .put(\"capacity\", node.getCapacity())\n          .put(\"numBlocks\", node.numBlocks())\n          .put(\"version\", node.getSoftwareVersion())\n          .put(\"used\", node.getDfsUsed())\n          .put(\"remaining\", node.getRemaining())\n          .put(\"blockScheduled\", node.getBlocksScheduled())\n          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n          .put(\"volfails\", node.getVolumeFailures());\n      VolumeFailureSummary volumeFailureSummary \u003d node.getVolumeFailureSummary();\n      if (volumeFailureSummary !\u003d null) {\n        innerinfo\n            .put(\"failedStorageLocations\",\n                volumeFailureSummary.getFailedStorageLocations())\n            .put(\"lastVolumeFailureDate\",\n                volumeFailureSummary.getLastVolumeFailureDate())\n            .put(\"estimatedCapacityLostTotal\",\n                volumeFailureSummary.getEstimatedCapacityLostTotal());\n      }\n      info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo.build());\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "45fa7f023532e79dff3cf381056eff717dc4ecc7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7303. NN UI fails to distinguish datanodes on the same host. Contributed by Benoy Antony.\n",
      "commitDate": "24/11/14 5:46 PM",
      "commitName": "45fa7f023532e79dff3cf381056eff717dc4ecc7",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/11/14 3:42 PM",
      "commitNameOld": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.09,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,28 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (DatanodeDescriptor node : live) {\n       Map\u003cString, Object\u003e innerinfo \u003d ImmutableMap.\u003cString, Object\u003ebuilder()\n           .put(\"infoAddr\", node.getInfoAddr())\n           .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n           .put(\"xferaddr\", node.getXferAddr())\n           .put(\"lastContact\", getLastContact(node))\n           .put(\"usedSpace\", getDfsUsed(node))\n           .put(\"adminState\", node.getAdminState().toString())\n           .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n           .put(\"capacity\", node.getCapacity())\n           .put(\"numBlocks\", node.numBlocks())\n           .put(\"version\", node.getSoftwareVersion())\n           .put(\"used\", node.getDfsUsed())\n           .put(\"remaining\", node.getRemaining())\n           .put(\"blockScheduled\", node.getBlocksScheduled())\n           .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n           .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n           .put(\"volfails\", node.getVolumeFailures())\n           .build();\n-\n-      info.put(node.getHostName(), innerinfo);\n+      info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo);\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      Map\u003cString, Object\u003e innerinfo \u003d ImmutableMap.\u003cString, Object\u003ebuilder()\n          .put(\"infoAddr\", node.getInfoAddr())\n          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n          .put(\"xferaddr\", node.getXferAddr())\n          .put(\"lastContact\", getLastContact(node))\n          .put(\"usedSpace\", getDfsUsed(node))\n          .put(\"adminState\", node.getAdminState().toString())\n          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n          .put(\"capacity\", node.getCapacity())\n          .put(\"numBlocks\", node.numBlocks())\n          .put(\"version\", node.getSoftwareVersion())\n          .put(\"used\", node.getDfsUsed())\n          .put(\"remaining\", node.getRemaining())\n          .put(\"blockScheduled\", node.getBlocksScheduled())\n          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n          .put(\"volfails\", node.getVolumeFailures())\n          .build();\n      info.put(node.getHostName() + \":\" + node.getXferPort(), innerinfo);\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8": {
      "type": "Ybodychange",
      "commitMessage": "Revert HDFS-6940.",
      "commitDate": "09/09/14 5:30 PM",
      "commitName": "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "06/09/14 12:07 PM",
      "commitNameOld": "88209ce181b5ecc55c0ae2bceff4893ab4817e88",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 3.22,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,29 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (DatanodeDescriptor node : live) {\n-      info.put(node.getHostName(), getLiveNodeInfo(node));\n+      Map\u003cString, Object\u003e innerinfo \u003d ImmutableMap.\u003cString, Object\u003ebuilder()\n+          .put(\"infoAddr\", node.getInfoAddr())\n+          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n+          .put(\"xferaddr\", node.getXferAddr())\n+          .put(\"lastContact\", getLastContact(node))\n+          .put(\"usedSpace\", getDfsUsed(node))\n+          .put(\"adminState\", node.getAdminState().toString())\n+          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n+          .put(\"capacity\", node.getCapacity())\n+          .put(\"numBlocks\", node.numBlocks())\n+          .put(\"version\", node.getSoftwareVersion())\n+          .put(\"used\", node.getDfsUsed())\n+          .put(\"remaining\", node.getRemaining())\n+          .put(\"blockScheduled\", node.getBlocksScheduled())\n+          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n+          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n+          .put(\"volfails\", node.getVolumeFailures())\n+          .build();\n+\n+      info.put(node.getHostName(), innerinfo);\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      Map\u003cString, Object\u003e innerinfo \u003d ImmutableMap.\u003cString, Object\u003ebuilder()\n          .put(\"infoAddr\", node.getInfoAddr())\n          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n          .put(\"xferaddr\", node.getXferAddr())\n          .put(\"lastContact\", getLastContact(node))\n          .put(\"usedSpace\", getDfsUsed(node))\n          .put(\"adminState\", node.getAdminState().toString())\n          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n          .put(\"capacity\", node.getCapacity())\n          .put(\"numBlocks\", node.numBlocks())\n          .put(\"version\", node.getSoftwareVersion())\n          .put(\"used\", node.getDfsUsed())\n          .put(\"remaining\", node.getRemaining())\n          .put(\"blockScheduled\", node.getBlocksScheduled())\n          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n          .put(\"volfails\", node.getVolumeFailures())\n          .build();\n\n      info.put(node.getHostName(), innerinfo);\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "88209ce181b5ecc55c0ae2bceff4893ab4817e88": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6940. Refactoring to allow ConsensusNode implementation.\nContributed by Konstantin Shvachko.",
      "commitDate": "06/09/14 12:07 PM",
      "commitName": "88209ce181b5ecc55c0ae2bceff4893ab4817e88",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "04/09/14 6:54 PM",
      "commitNameOld": "6104520369045dfaa4b543cbad21236ed322249b",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.72,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,10 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (DatanodeDescriptor node : live) {\n-      Map\u003cString, Object\u003e innerinfo \u003d ImmutableMap.\u003cString, Object\u003ebuilder()\n-          .put(\"infoAddr\", node.getInfoAddr())\n-          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n-          .put(\"xferaddr\", node.getXferAddr())\n-          .put(\"lastContact\", getLastContact(node))\n-          .put(\"usedSpace\", getDfsUsed(node))\n-          .put(\"adminState\", node.getAdminState().toString())\n-          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n-          .put(\"capacity\", node.getCapacity())\n-          .put(\"numBlocks\", node.numBlocks())\n-          .put(\"version\", node.getSoftwareVersion())\n-          .put(\"used\", node.getDfsUsed())\n-          .put(\"remaining\", node.getRemaining())\n-          .put(\"blockScheduled\", node.getBlocksScheduled())\n-          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n-          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n-          .put(\"volfails\", node.getVolumeFailures())\n-          .build();\n-\n-      info.put(node.getHostName(), innerinfo);\n+      info.put(node.getHostName(), getLiveNodeInfo(node));\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      info.put(node.getHostName(), getLiveNodeInfo(node));\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": {
      "type": "Ybodychange",
      "commitMessage": "merge trunk to branch HDFS-4949\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532952 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 7:14 PM",
      "commitName": "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "16/10/13 3:15 PM",
      "commitNameOld": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 0.17,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,29 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (DatanodeDescriptor node : live) {\n-      final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n-      innerinfo.put(\"lastContact\", getLastContact(node));\n-      innerinfo.put(\"usedSpace\", getDfsUsed(node));\n-      innerinfo.put(\"adminState\", node.getAdminState().toString());\n-      innerinfo.put(\"nonDfsUsedSpace\", node.getNonDfsUsed());\n-      innerinfo.put(\"capacity\", node.getCapacity());\n-      innerinfo.put(\"numBlocks\", node.numBlocks());\n-      innerinfo.put(\"version\", node.getSoftwareVersion());\n+      Map\u003cString, Object\u003e innerinfo \u003d ImmutableMap.\u003cString, Object\u003ebuilder()\n+          .put(\"infoAddr\", node.getInfoAddr())\n+          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n+          .put(\"xferaddr\", node.getXferAddr())\n+          .put(\"lastContact\", getLastContact(node))\n+          .put(\"usedSpace\", getDfsUsed(node))\n+          .put(\"adminState\", node.getAdminState().toString())\n+          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n+          .put(\"capacity\", node.getCapacity())\n+          .put(\"numBlocks\", node.numBlocks())\n+          .put(\"version\", node.getSoftwareVersion())\n+          .put(\"used\", node.getDfsUsed())\n+          .put(\"remaining\", node.getRemaining())\n+          .put(\"blockScheduled\", node.getBlocksScheduled())\n+          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n+          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n+          .put(\"volfails\", node.getVolumeFailures())\n+          .build();\n+\n       info.put(node.getHostName(), innerinfo);\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      Map\u003cString, Object\u003e innerinfo \u003d ImmutableMap.\u003cString, Object\u003ebuilder()\n          .put(\"infoAddr\", node.getInfoAddr())\n          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n          .put(\"xferaddr\", node.getXferAddr())\n          .put(\"lastContact\", getLastContact(node))\n          .put(\"usedSpace\", getDfsUsed(node))\n          .put(\"adminState\", node.getAdminState().toString())\n          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n          .put(\"capacity\", node.getCapacity())\n          .put(\"numBlocks\", node.numBlocks())\n          .put(\"version\", node.getSoftwareVersion())\n          .put(\"used\", node.getDfsUsed())\n          .put(\"remaining\", node.getRemaining())\n          .put(\"blockScheduled\", node.getBlocksScheduled())\n          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n          .put(\"volfails\", node.getVolumeFailures())\n          .build();\n\n      info.put(node.getHostName(), innerinfo);\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cf0cf0a6910244d929f40842223e7d0b2c9445e8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5342. Provide more information in the FSNamesystem JMX interfaces. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1532090 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/10/13 1:56 PM",
      "commitName": "cf0cf0a6910244d929f40842223e7d0b2c9445e8",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/10/13 9:28 PM",
      "commitNameOld": "7cd044b1aa03745180d6cc56f2da09bc6e901e15",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 1.69,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,29 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (DatanodeDescriptor node : live) {\n-      final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n-      innerinfo.put(\"lastContact\", getLastContact(node));\n-      innerinfo.put(\"usedSpace\", getDfsUsed(node));\n-      innerinfo.put(\"adminState\", node.getAdminState().toString());\n-      innerinfo.put(\"nonDfsUsedSpace\", node.getNonDfsUsed());\n-      innerinfo.put(\"capacity\", node.getCapacity());\n-      innerinfo.put(\"numBlocks\", node.numBlocks());\n-      innerinfo.put(\"version\", node.getSoftwareVersion());\n+      Map\u003cString, Object\u003e innerinfo \u003d ImmutableMap.\u003cString, Object\u003ebuilder()\n+          .put(\"infoAddr\", node.getInfoAddr())\n+          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n+          .put(\"xferaddr\", node.getXferAddr())\n+          .put(\"lastContact\", getLastContact(node))\n+          .put(\"usedSpace\", getDfsUsed(node))\n+          .put(\"adminState\", node.getAdminState().toString())\n+          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n+          .put(\"capacity\", node.getCapacity())\n+          .put(\"numBlocks\", node.numBlocks())\n+          .put(\"version\", node.getSoftwareVersion())\n+          .put(\"used\", node.getDfsUsed())\n+          .put(\"remaining\", node.getRemaining())\n+          .put(\"blockScheduled\", node.getBlocksScheduled())\n+          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n+          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n+          .put(\"volfails\", node.getVolumeFailures())\n+          .build();\n+\n       info.put(node.getHostName(), innerinfo);\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      Map\u003cString, Object\u003e innerinfo \u003d ImmutableMap.\u003cString, Object\u003ebuilder()\n          .put(\"infoAddr\", node.getInfoAddr())\n          .put(\"infoSecureAddr\", node.getInfoSecureAddr())\n          .put(\"xferaddr\", node.getXferAddr())\n          .put(\"lastContact\", getLastContact(node))\n          .put(\"usedSpace\", getDfsUsed(node))\n          .put(\"adminState\", node.getAdminState().toString())\n          .put(\"nonDfsUsedSpace\", node.getNonDfsUsed())\n          .put(\"capacity\", node.getCapacity())\n          .put(\"numBlocks\", node.numBlocks())\n          .put(\"version\", node.getSoftwareVersion())\n          .put(\"used\", node.getDfsUsed())\n          .put(\"remaining\", node.getRemaining())\n          .put(\"blockScheduled\", node.getBlocksScheduled())\n          .put(\"blockPoolUsed\", node.getBlockPoolUsed())\n          .put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent())\n          .put(\"volfails\", node.getVolumeFailures())\n          .build();\n\n      info.put(node.getHostName(), innerinfo);\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "39252995c4d734e993e3fa5338e1a7816aee86fc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3245. Add metrics and web UI for cluster version summary. Contributed by Ravi Prakash.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1517937 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/08/13 12:21 PM",
      "commitName": "39252995c4d734e993e3fa5338e1a7816aee86fc",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "21/08/13 11:15 AM",
      "commitNameOld": "2499a86664103eb2d16bd53bf424446599b61820",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 6.05,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,18 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (DatanodeDescriptor node : live) {\n       final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n       innerinfo.put(\"lastContact\", getLastContact(node));\n       innerinfo.put(\"usedSpace\", getDfsUsed(node));\n       innerinfo.put(\"adminState\", node.getAdminState().toString());\n       innerinfo.put(\"nonDfsUsedSpace\", node.getNonDfsUsed());\n       innerinfo.put(\"capacity\", node.getCapacity());\n       innerinfo.put(\"numBlocks\", node.numBlocks());\n+      innerinfo.put(\"version\", node.getSoftwareVersion());\n       info.put(node.getHostName(), innerinfo);\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n      innerinfo.put(\"lastContact\", getLastContact(node));\n      innerinfo.put(\"usedSpace\", getDfsUsed(node));\n      innerinfo.put(\"adminState\", node.getAdminState().toString());\n      innerinfo.put(\"nonDfsUsedSpace\", node.getNonDfsUsed());\n      innerinfo.put(\"capacity\", node.getCapacity());\n      innerinfo.put(\"numBlocks\", node.numBlocks());\n      innerinfo.put(\"version\", node.getSoftwareVersion());\n      info.put(node.getHostName(), innerinfo);\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "15fe3ae61b9931bdd24fbc6e4d3181132fcfffce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2708. Stats for the # of blocks per DN. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1326039 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/04/12 6:51 PM",
      "commitName": "15fe3ae61b9931bdd24fbc6e4d3181132fcfffce",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "09/04/12 12:39 PM",
      "commitNameOld": "706394d03992b394e9f907aff2155df493e4ea4e",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 4.26,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,17 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (DatanodeDescriptor node : live) {\n       final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n       innerinfo.put(\"lastContact\", getLastContact(node));\n       innerinfo.put(\"usedSpace\", getDfsUsed(node));\n       innerinfo.put(\"adminState\", node.getAdminState().toString());\n       innerinfo.put(\"nonDfsUsedSpace\", node.getNonDfsUsed());\n       innerinfo.put(\"capacity\", node.getCapacity());\n+      innerinfo.put(\"numBlocks\", node.numBlocks());\n       info.put(node.getHostName(), innerinfo);\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n      innerinfo.put(\"lastContact\", getLastContact(node));\n      innerinfo.put(\"usedSpace\", getDfsUsed(node));\n      innerinfo.put(\"adminState\", node.getAdminState().toString());\n      innerinfo.put(\"nonDfsUsedSpace\", node.getNonDfsUsed());\n      innerinfo.put(\"capacity\", node.getCapacity());\n      innerinfo.put(\"numBlocks\", node.numBlocks());\n      info.put(node.getHostName(), innerinfo);\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "17d22e6dc441f72cffa4413ec30921d6d4c41cfd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3158. LiveNodes member of NameNodeMXBean should list non-DFS used space and capacity per DN. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1306635 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/03/12 3:59 PM",
      "commitName": "17d22e6dc441f72cffa4413ec30921d6d4c41cfd",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "20/03/12 12:00 PM",
      "commitNameOld": "4ceca97606e7258e807867bbcd8cc60d0cce7439",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 8.17,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,16 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n     final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n     blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n     for (DatanodeDescriptor node : live) {\n       final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n       innerinfo.put(\"lastContact\", getLastContact(node));\n       innerinfo.put(\"usedSpace\", getDfsUsed(node));\n       innerinfo.put(\"adminState\", node.getAdminState().toString());\n+      innerinfo.put(\"nonDfsUsedSpace\", node.getNonDfsUsed());\n+      innerinfo.put(\"capacity\", node.getCapacity());\n       info.put(node.getHostName(), innerinfo);\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n      innerinfo.put(\"lastContact\", getLastContact(node));\n      innerinfo.put(\"usedSpace\", getDfsUsed(node));\n      innerinfo.put(\"adminState\", node.getAdminState().toString());\n      innerinfo.put(\"nonDfsUsedSpace\", node.getNonDfsUsed());\n      innerinfo.put(\"capacity\", node.getCapacity());\n      info.put(node.getHostName(), innerinfo);\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n      innerinfo.put(\"lastContact\", getLastContact(node));\n      innerinfo.put(\"usedSpace\", getDfsUsed(node));\n      innerinfo.put(\"adminState\", node.getAdminState().toString());\n      info.put(node.getHostName(), innerinfo);\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n      innerinfo.put(\"lastContact\", getLastContact(node));\n      innerinfo.put(\"usedSpace\", getDfsUsed(node));\n      innerinfo.put(\"adminState\", node.getAdminState().toString());\n      info.put(node.getHostName(), innerinfo);\n    }\n    return JSON.toString(info);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "371f4a59059322000a40eb4bdf5386b96b626ece": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2228. Move block and datanode code from FSNamesystem to BlockManager and DatanodeManager.  (szetszwo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154899 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/08/11 3:06 AM",
      "commitName": "371f4a59059322000a40eb4bdf5386b96b626ece",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "04/08/11 3:55 PM",
      "commitNameOld": "7fac946ac983e31613fd62836c8ac9c4a579210a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.47,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,14 @@\n   public String getLiveNodes() {\n     final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n       new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n-    final ArrayList\u003cDatanodeDescriptor\u003e liveNodeList \u003d \n-      new ArrayList\u003cDatanodeDescriptor\u003e();\n-    final ArrayList\u003cDatanodeDescriptor\u003e deadNodeList \u003d\n-      new ArrayList\u003cDatanodeDescriptor\u003e();\n-    DFSNodesStatus(liveNodeList, deadNodeList);\n-    removeDecomNodeFromList(liveNodeList);\n-    for (DatanodeDescriptor node : liveNodeList) {\n+    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n+    for (DatanodeDescriptor node : live) {\n       final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n       innerinfo.put(\"lastContact\", getLastContact(node));\n       innerinfo.put(\"usedSpace\", getDfsUsed(node));\n       innerinfo.put(\"adminState\", node.getAdminState().toString());\n       info.put(node.getHostName(), innerinfo);\n     }\n     return JSON.toString(info);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n    blockManager.getDatanodeManager().fetchDatanodes(live, null, true);\n    for (DatanodeDescriptor node : live) {\n      final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n      innerinfo.put(\"lastContact\", getLastContact(node));\n      innerinfo.put(\"usedSpace\", getDfsUsed(node));\n      innerinfo.put(\"adminState\", node.getAdminState().toString());\n      info.put(node.getHostName(), innerinfo);\n    }\n    return JSON.toString(info);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,18 @@\n+  public String getLiveNodes() {\n+    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n+      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n+    final ArrayList\u003cDatanodeDescriptor\u003e liveNodeList \u003d \n+      new ArrayList\u003cDatanodeDescriptor\u003e();\n+    final ArrayList\u003cDatanodeDescriptor\u003e deadNodeList \u003d\n+      new ArrayList\u003cDatanodeDescriptor\u003e();\n+    DFSNodesStatus(liveNodeList, deadNodeList);\n+    removeDecomNodeFromList(liveNodeList);\n+    for (DatanodeDescriptor node : liveNodeList) {\n+      final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n+      innerinfo.put(\"lastContact\", getLastContact(node));\n+      innerinfo.put(\"usedSpace\", getDfsUsed(node));\n+      innerinfo.put(\"adminState\", node.getAdminState().toString());\n+      info.put(node.getHostName(), innerinfo);\n+    }\n+    return JSON.toString(info);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public String getLiveNodes() {\n    final Map\u003cString, Map\u003cString,Object\u003e\u003e info \u003d \n      new HashMap\u003cString, Map\u003cString,Object\u003e\u003e();\n    final ArrayList\u003cDatanodeDescriptor\u003e liveNodeList \u003d \n      new ArrayList\u003cDatanodeDescriptor\u003e();\n    final ArrayList\u003cDatanodeDescriptor\u003e deadNodeList \u003d\n      new ArrayList\u003cDatanodeDescriptor\u003e();\n    DFSNodesStatus(liveNodeList, deadNodeList);\n    removeDecomNodeFromList(liveNodeList);\n    for (DatanodeDescriptor node : liveNodeList) {\n      final Map\u003cString, Object\u003e innerinfo \u003d new HashMap\u003cString, Object\u003e();\n      innerinfo.put(\"lastContact\", getLastContact(node));\n      innerinfo.put(\"usedSpace\", getDfsUsed(node));\n      innerinfo.put(\"adminState\", node.getAdminState().toString());\n      info.put(node.getHostName(), innerinfo);\n    }\n    return JSON.toString(info);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}