{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DiskBalancerCluster.java",
  "functionName": "computePlan",
  "functionId": "computePlan___thresholdPercent-double",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java",
  "functionStartLine": 302,
  "functionEndLine": 338,
  "numCommitsSeen": 14,
  "timeTaken": 1138,
  "changeHistory": [
    "75882ec0b096da862b8b373b70a091c19f281b2a",
    "5724a103161424f4b293ba937f0d0540179f36ac"
  ],
  "changeHistoryShort": {
    "75882ec0b096da862b8b373b70a091c19f281b2a": "Yparameterchange",
    "5724a103161424f4b293ba937f0d0540179f36ac": "Yintroduced"
  },
  "changeHistoryDetails": {
    "75882ec0b096da862b8b373b70a091c19f281b2a": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-9545: DiskBalancer: Add Plan Command. Contributed by Anu Engineer.\n",
      "commitDate": "23/06/16 6:21 PM",
      "commitName": "75882ec0b096da862b8b373b70a091c19f281b2a",
      "commitAuthor": "Anu Engineer",
      "commitDateOld": "23/06/16 6:18 PM",
      "commitNameOld": "747227e9dea10ac6b5f601b7cf4dcc418b10d9c8",
      "commitAuthorOld": "Anu Engineer",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n-  public List\u003cNodePlan\u003e computePlan(float thresholdPercent) {\n+  public List\u003cNodePlan\u003e computePlan(double thresholdPercent) {\n     List\u003cNodePlan\u003e planList \u003d new LinkedList\u003c\u003e();\n \n     if (nodesToProcess \u003d\u003d null) {\n       LOG.warn(\"Nodes to process is null. No nodes processed.\");\n       return planList;\n     }\n \n     int poolSize \u003d computePoolSize(nodesToProcess.size());\n \n     ExecutorService executorService \u003d Executors.newFixedThreadPool(poolSize);\n     List\u003cFuture\u003cNodePlan\u003e\u003e futureList \u003d new LinkedList\u003c\u003e();\n     for (int x \u003d 0; x \u003c nodesToProcess.size(); x++) {\n       final DiskBalancerDataNode node \u003d nodesToProcess.get(x);\n       final Planner planner \u003d PlannerFactory\n           .getPlanner(PlannerFactory.GREEDY_PLANNER, node,\n               thresholdPercent);\n       futureList.add(executorService.submit(new Callable\u003cNodePlan\u003e() {\n         @Override\n         public NodePlan call() throws Exception {\n           assert planner !\u003d null;\n           return planner.plan(node);\n         }\n       }));\n     }\n \n     for (Future\u003cNodePlan\u003e f : futureList) {\n       try {\n         planList.add(f.get());\n       } catch (InterruptedException e) {\n         LOG.error(\"Compute Node plan was cancelled or interrupted : \", e);\n       } catch (ExecutionException e) {\n         LOG.error(\"Unable to compute plan : \", e);\n       }\n     }\n     return planList;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cNodePlan\u003e computePlan(double thresholdPercent) {\n    List\u003cNodePlan\u003e planList \u003d new LinkedList\u003c\u003e();\n\n    if (nodesToProcess \u003d\u003d null) {\n      LOG.warn(\"Nodes to process is null. No nodes processed.\");\n      return planList;\n    }\n\n    int poolSize \u003d computePoolSize(nodesToProcess.size());\n\n    ExecutorService executorService \u003d Executors.newFixedThreadPool(poolSize);\n    List\u003cFuture\u003cNodePlan\u003e\u003e futureList \u003d new LinkedList\u003c\u003e();\n    for (int x \u003d 0; x \u003c nodesToProcess.size(); x++) {\n      final DiskBalancerDataNode node \u003d nodesToProcess.get(x);\n      final Planner planner \u003d PlannerFactory\n          .getPlanner(PlannerFactory.GREEDY_PLANNER, node,\n              thresholdPercent);\n      futureList.add(executorService.submit(new Callable\u003cNodePlan\u003e() {\n        @Override\n        public NodePlan call() throws Exception {\n          assert planner !\u003d null;\n          return planner.plan(node);\n        }\n      }));\n    }\n\n    for (Future\u003cNodePlan\u003e f : futureList) {\n      try {\n        planList.add(f.get());\n      } catch (InterruptedException e) {\n        LOG.error(\"Compute Node plan was cancelled or interrupted : \", e);\n      } catch (ExecutionException e) {\n        LOG.error(\"Unable to compute plan : \", e);\n      }\n    }\n    return planList;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java",
      "extendedDetails": {
        "oldValue": "[thresholdPercent-float]",
        "newValue": "[thresholdPercent-double]"
      }
    },
    "5724a103161424f4b293ba937f0d0540179f36ac": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9469. DiskBalancer: Add Planner. (Contributed by Anu Engineer)\n",
      "commitDate": "23/06/16 6:18 PM",
      "commitName": "5724a103161424f4b293ba937f0d0540179f36ac",
      "commitAuthor": "Arpit Agarwal",
      "diff": "@@ -0,0 +1,37 @@\n+  public List\u003cNodePlan\u003e computePlan(float thresholdPercent) {\n+    List\u003cNodePlan\u003e planList \u003d new LinkedList\u003c\u003e();\n+\n+    if (nodesToProcess \u003d\u003d null) {\n+      LOG.warn(\"Nodes to process is null. No nodes processed.\");\n+      return planList;\n+    }\n+\n+    int poolSize \u003d computePoolSize(nodesToProcess.size());\n+\n+    ExecutorService executorService \u003d Executors.newFixedThreadPool(poolSize);\n+    List\u003cFuture\u003cNodePlan\u003e\u003e futureList \u003d new LinkedList\u003c\u003e();\n+    for (int x \u003d 0; x \u003c nodesToProcess.size(); x++) {\n+      final DiskBalancerDataNode node \u003d nodesToProcess.get(x);\n+      final Planner planner \u003d PlannerFactory\n+          .getPlanner(PlannerFactory.GREEDY_PLANNER, node,\n+              thresholdPercent);\n+      futureList.add(executorService.submit(new Callable\u003cNodePlan\u003e() {\n+        @Override\n+        public NodePlan call() throws Exception {\n+          assert planner !\u003d null;\n+          return planner.plan(node);\n+        }\n+      }));\n+    }\n+\n+    for (Future\u003cNodePlan\u003e f : futureList) {\n+      try {\n+        planList.add(f.get());\n+      } catch (InterruptedException e) {\n+        LOG.error(\"Compute Node plan was cancelled or interrupted : \", e);\n+      } catch (ExecutionException e) {\n+        LOG.error(\"Unable to compute plan : \", e);\n+      }\n+    }\n+    return planList;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cNodePlan\u003e computePlan(float thresholdPercent) {\n    List\u003cNodePlan\u003e planList \u003d new LinkedList\u003c\u003e();\n\n    if (nodesToProcess \u003d\u003d null) {\n      LOG.warn(\"Nodes to process is null. No nodes processed.\");\n      return planList;\n    }\n\n    int poolSize \u003d computePoolSize(nodesToProcess.size());\n\n    ExecutorService executorService \u003d Executors.newFixedThreadPool(poolSize);\n    List\u003cFuture\u003cNodePlan\u003e\u003e futureList \u003d new LinkedList\u003c\u003e();\n    for (int x \u003d 0; x \u003c nodesToProcess.size(); x++) {\n      final DiskBalancerDataNode node \u003d nodesToProcess.get(x);\n      final Planner planner \u003d PlannerFactory\n          .getPlanner(PlannerFactory.GREEDY_PLANNER, node,\n              thresholdPercent);\n      futureList.add(executorService.submit(new Callable\u003cNodePlan\u003e() {\n        @Override\n        public NodePlan call() throws Exception {\n          assert planner !\u003d null;\n          return planner.plan(node);\n        }\n      }));\n    }\n\n    for (Future\u003cNodePlan\u003e f : futureList) {\n      try {\n        planList.add(f.get());\n      } catch (InterruptedException e) {\n        LOG.error(\"Compute Node plan was cancelled or interrupted : \", e);\n      } catch (ExecutionException e) {\n        LOG.error(\"Unable to compute plan : \", e);\n      }\n    }\n    return planList;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java"
    }
  }
}