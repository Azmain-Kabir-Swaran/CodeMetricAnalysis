{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "processPendingReconstructions",
  "functionId": "processPendingReconstructions",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 2509,
  "functionEndLine": 2537,
  "numCommitsSeen": 837,
  "timeTaken": 25277,
  "changeHistory": [
    "42a1c98597e6dba2e371510a6b2b6b1fb94e4090",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "5411dc559d5f73e4153e76fdff94a26869c17a37",
    "745d04be59accf80feda0ad38efcc74ba362f2ca",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75",
    "d62b63d297bff12d93de560dd50ddd48743b851d",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
    "6e3fcffe291faec40fa9214f4880a35a952836c4",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "c9e0268216584f1df1a7c6cd25d2cfb2bc6d1d3c",
    "a38a37c63417a3b19dcdf98251af196c9d7b8c31",
    "8860e352c394372e4eb3ebdf82ea899567f34e4e",
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
    "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8",
    "88209ce181b5ecc55c0ae2bceff4893ab4817e88",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "969a263188f7015261719fe45fa1505121ebb80e",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "42a1c98597e6dba2e371510a6b2b6b1fb94e4090": "Ymodifierchange",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": "Ybodychange",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": "Ymultichange(Yrename,Ybodychange)",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ybodychange",
    "5411dc559d5f73e4153e76fdff94a26869c17a37": "Ybodychange",
    "745d04be59accf80feda0ad38efcc74ba362f2ca": "Ybodychange",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": "Ybodychange",
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75": "Ybodychange",
    "d62b63d297bff12d93de560dd50ddd48743b851d": "Ybodychange",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": "Ybodychange",
    "6e3fcffe291faec40fa9214f4880a35a952836c4": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "c9e0268216584f1df1a7c6cd25d2cfb2bc6d1d3c": "Ybodychange",
    "a38a37c63417a3b19dcdf98251af196c9d7b8c31": "Ybodychange",
    "8860e352c394372e4eb3ebdf82ea899567f34e4e": "Ybodychange",
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b": "Ybodychange",
    "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8": "Ymodifierchange",
    "88209ce181b5ecc55c0ae2bceff4893ab4817e88": "Ymodifierchange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "969a263188f7015261719fe45fa1505121ebb80e": "Ymodifierchange",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": "Ymultichange(Yfilerename,Ymodifierchange)",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": "Ymultichange(Yfilerename,Ymodifierchange)",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": "Ymultichange(Yfilerename,Ymodifierchange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "42a1c98597e6dba2e371510a6b2b6b1fb94e4090": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-11847. Enhance dfsadmin listOpenFiles command to list files blocking datanode decommissioning.\n",
      "commitDate": "02/01/18 2:59 PM",
      "commitName": "42a1c98597e6dba2e371510a6b2b6b1fb94e4090",
      "commitAuthor": "Manoj Govindassamy",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "8239e3afb31d3c4485817d4b8b8b195b554acbe7",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 17.88,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n-  private void processPendingReconstructions() {\n+  void processPendingReconstructions() {\n     BlockInfo[] timedOutItems \u003d pendingReconstruction.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n           BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReconstruction(bi, num)) {\n             neededReconstruction.add(bi, num.liveReplicas(),\n                 num.readOnlyReplicas(), num.outOfServiceReplicas(),\n                 getExpectedRedundancyNum(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processPendingReconstructions() {\n    BlockInfo[] timedOutItems \u003d pendingReconstruction.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReconstruction(bi, num)) {\n            neededReconstruction.add(bi, num.liveReplicas(),\n                num.readOnlyReplicas(), num.outOfServiceReplicas(),\n                getExpectedRedundancyNum(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[]"
      }
    },
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "14/10/16 6:13 PM",
      "commitNameOld": "391ce535a739dc92cb90017d759217265a4fd969",
      "commitAuthorOld": "Vinitha Reddy Gankidi",
      "daysBetweenCommits": 2.98,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   private void processPendingReconstructions() {\n     BlockInfo[] timedOutItems \u003d pendingReconstruction.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n           BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n-          if (isNeededReconstruction(bi, num.liveReplicas())) {\n+          if (isNeededReconstruction(bi, num)) {\n             neededReconstruction.add(bi, num.liveReplicas(),\n-                num.readOnlyReplicas(), num.decommissionedAndDecommissioning(),\n-                getRedundancy(bi));\n+                num.readOnlyReplicas(), num.outOfServiceReplicas(),\n+                getExpectedRedundancyNum(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReconstructions() {\n    BlockInfo[] timedOutItems \u003d pendingReconstruction.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReconstruction(bi, num)) {\n            neededReconstruction.add(bi, num.liveReplicas(),\n                num.readOnlyReplicas(), num.outOfServiceReplicas(),\n                getExpectedRedundancyNum(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10236. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-3]. Contributed by Rakesh R.\n",
      "commitDate": "26/05/16 4:50 PM",
      "commitName": "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "28/04/16 10:44 AM",
      "commitNameOld": "6243eabb48390fffada2418ade5adf9e0766afbe",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 28.25,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   private void processPendingReconstructions() {\n     BlockInfo[] timedOutItems \u003d pendingReconstruction.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n           BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReconstruction(bi, num.liveReplicas())) {\n             neededReconstruction.add(bi, num.liveReplicas(),\n                 num.readOnlyReplicas(), num.decommissionedAndDecommissioning(),\n-                getReplication(bi));\n+                getRedundancy(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReconstructions() {\n    BlockInfo[] timedOutItems \u003d pendingReconstruction.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReconstruction(bi, num.liveReplicas())) {\n            neededReconstruction.add(bi, num.liveReplicas(),\n                num.readOnlyReplicas(), num.decommissionedAndDecommissioning(),\n                getRedundancy(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": {
      "type": "Ymultichange(Yrename,Ybodychange)",
      "commitMessage": "HDFS-9869. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-2]. Contributed by Rakesh R.\n",
      "commitDate": "25/04/16 10:01 PM",
      "commitName": "5865fe2bf01284993572ea60b3ec3bf8b4492818",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-9869. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-2]. Contributed by Rakesh R.\n",
          "commitDate": "25/04/16 10:01 PM",
          "commitName": "5865fe2bf01284993572ea60b3ec3bf8b4492818",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "17/04/16 6:28 PM",
          "commitNameOld": "67523ffcf491f4f2db5335899c00a174d0caaa9b",
          "commitAuthorOld": "Walter Su",
          "daysBetweenCommits": 8.15,
          "commitsBetweenForRepo": 47,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,29 @@\n-  private void processPendingReplications() {\n-    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n+  private void processPendingReconstructions() {\n+    BlockInfo[] timedOutItems \u003d pendingReconstruction.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n           BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReconstruction(bi, num.liveReplicas())) {\n             neededReconstruction.add(bi, num.liveReplicas(),\n                 num.readOnlyReplicas(), num.decommissionedAndDecommissioning(),\n                 getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processPendingReconstructions() {\n    BlockInfo[] timedOutItems \u003d pendingReconstruction.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReconstruction(bi, num.liveReplicas())) {\n            neededReconstruction.add(bi, num.liveReplicas(),\n                num.readOnlyReplicas(), num.decommissionedAndDecommissioning(),\n                getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "processPendingReplications",
            "newValue": "processPendingReconstructions"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9869. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-2]. Contributed by Rakesh R.\n",
          "commitDate": "25/04/16 10:01 PM",
          "commitName": "5865fe2bf01284993572ea60b3ec3bf8b4492818",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "17/04/16 6:28 PM",
          "commitNameOld": "67523ffcf491f4f2db5335899c00a174d0caaa9b",
          "commitAuthorOld": "Walter Su",
          "daysBetweenCommits": 8.15,
          "commitsBetweenForRepo": 47,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,29 @@\n-  private void processPendingReplications() {\n-    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n+  private void processPendingReconstructions() {\n+    BlockInfo[] timedOutItems \u003d pendingReconstruction.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n           BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReconstruction(bi, num.liveReplicas())) {\n             neededReconstruction.add(bi, num.liveReplicas(),\n                 num.readOnlyReplicas(), num.decommissionedAndDecommissioning(),\n                 getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processPendingReconstructions() {\n    BlockInfo[] timedOutItems \u003d pendingReconstruction.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReconstruction(bi, num.liveReplicas())) {\n            neededReconstruction.add(bi, num.liveReplicas(),\n                num.readOnlyReplicas(), num.decommissionedAndDecommissioning(),\n                getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,29 @@\n   private void processPendingReplications() {\n     BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n           BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n-          if (isNeededReplication(bi, num.liveReplicas())) {\n-            neededReplications.add(bi, num.liveReplicas(), num.readOnlyReplicas(),\n-                num.decommissionedAndDecommissioning(), getReplication(bi));\n+          if (isNeededReconstruction(bi, num.liveReplicas())) {\n+            neededReconstruction.add(bi, num.liveReplicas(),\n+                num.readOnlyReplicas(), num.decommissionedAndDecommissioning(),\n+                getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReconstruction(bi, num.liveReplicas())) {\n            neededReconstruction.add(bi, num.liveReplicas(),\n                num.readOnlyReplicas(), num.decommissionedAndDecommissioning(),\n                getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "5411dc559d5f73e4153e76fdff94a26869c17a37": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9205. Do not schedule corrupt blocks for replication.  (szetszwo)\n",
      "commitDate": "15/10/15 3:07 AM",
      "commitName": "5411dc559d5f73e4153e76fdff94a26869c17a37",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "14/10/15 4:17 PM",
      "commitNameOld": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.45,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void processPendingReplications() {\n     BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n           BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(bi, num.liveReplicas())) {\n-            neededReplications.add(bi, num.liveReplicas(),\n+            neededReplications.add(bi, num.liveReplicas(), num.readOnlyReplicas(),\n                 num.decommissionedAndDecommissioning(), getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(bi, num.liveReplicas())) {\n            neededReplications.add(bi, num.liveReplicas(), num.readOnlyReplicas(),\n                num.decommissionedAndDecommissioning(), getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "745d04be59accf80feda0ad38efcc74ba362f2ca": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8823. Move replication factor into individual blocks. Contributed by Haohui Mai.\n",
      "commitDate": "22/08/15 12:09 AM",
      "commitName": "745d04be59accf80feda0ad38efcc74ba362f2ca",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/08/15 3:11 PM",
      "commitNameOld": "4e14f7982a6e57bf08deb3b266806c2b779a157d",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.37,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void processPendingReplications() {\n     BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n           BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n-          if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n+          if (isNeededReplication(bi, num.liveReplicas())) {\n             neededReplications.add(bi, num.liveReplicas(),\n                 num.decommissionedAndDecommissioning(), getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(bi, num.liveReplicas())) {\n            neededReplications.add(bi, num.liveReplicas(),\n                num.decommissionedAndDecommissioning(), getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\"\n\nThis reverts commit de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "31/07/15 4:15 PM",
      "commitNameOld": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 5.75,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void processPendingReplications() {\n     BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n-          BlockInfo bi \u003d getStoredBlock(timedOutItems[i]);\n+          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n             neededReplications.add(bi, num.liveReplicas(),\n                 num.decommissionedAndDecommissioning(), getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n            neededReplications.add(bi, num.liveReplicas(),\n                num.decommissionedAndDecommissioning(), getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\"\n\nThis reverts commit d62b63d297bff12d93de560dd50ddd48743b851d.\n",
      "commitDate": "07/07/15 10:13 AM",
      "commitName": "bc99aaffe7b0ed13b1efc37b6a32cdbd344c2d75",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "06/07/15 3:54 PM",
      "commitNameOld": "d62b63d297bff12d93de560dd50ddd48743b851d",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.76,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void processPendingReplications() {\n     BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n-        for (BlockInfo timedOutItem : timedOutItems) {\n+        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n-          BlockInfo bi \u003d getStoredBlock(timedOutItem);\n+          BlockInfo bi \u003d getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n-          NumberReplicas num \u003d countNodes(timedOutItem);\n+          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n             neededReplications.add(bi, num.liveReplicas(),\n                 num.decommissionedAndDecommissioning(), getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n            neededReplications.add(bi, num.liveReplicas(),\n                num.decommissionedAndDecommissioning(), getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "d62b63d297bff12d93de560dd50ddd48743b851d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8652. Track BlockInfo instead of Block in CorruptReplicasMap. Contributed by Jing Zhao.\n",
      "commitDate": "06/07/15 3:54 PM",
      "commitName": "d62b63d297bff12d93de560dd50ddd48743b851d",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "29/06/15 11:00 AM",
      "commitNameOld": "d3fed8e653ed9e18d3a29a11c4b24a628ac770bb",
      "commitAuthorOld": "Benoy Antony",
      "daysBetweenCommits": 7.2,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void processPendingReplications() {\n     BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n-        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n+        for (BlockInfo timedOutItem : timedOutItems) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n-          BlockInfo bi \u003d getStoredBlock(timedOutItems[i]);\n+          BlockInfo bi \u003d getStoredBlock(timedOutItem);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n-          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n+          NumberReplicas num \u003d countNodes(timedOutItem);\n           if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n             neededReplications.add(bi, num.liveReplicas(),\n                 num.decommissionedAndDecommissioning(), getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (BlockInfo timedOutItem : timedOutItems) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d getStoredBlock(timedOutItem);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItem);\n          if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n            neededReplications.add(bi, num.liveReplicas(),\n                num.decommissionedAndDecommissioning(), getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\n",
      "commitDate": "26/06/15 10:49 AM",
      "commitName": "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/06/15 2:42 PM",
      "commitNameOld": "afe9ea3c12e1f5a71922400eadb642960bc87ca1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void processPendingReplications() {\n     BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n-          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n+          BlockInfo bi \u003d getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n             neededReplications.add(bi, num.liveReplicas(),\n                 num.decommissionedAndDecommissioning(), getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n            neededReplications.add(bi, num.liveReplicas(),\n                num.decommissionedAndDecommissioning(), getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "6e3fcffe291faec40fa9214f4880a35a952836c4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8608. Merge HDFS-7912 to trunk and branch-2 (track BlockInfo instead of Block in UnderReplicatedBlocks and PendingReplicationBlocks). Contributed by Zhe Zhang.\n",
      "commitDate": "17/06/15 8:05 AM",
      "commitName": "6e3fcffe291faec40fa9214f4880a35a952836c4",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "12/06/15 11:38 AM",
      "commitNameOld": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.85,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void processPendingReplications() {\n-    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n+    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n           BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n             neededReplications.add(bi, num.liveReplicas(),\n                 num.decommissionedAndDecommissioning(), getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n            neededReplications.add(bi, num.liveReplicas(),\n                num.decommissionedAndDecommissioning(), getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "19/05/15 11:05 AM",
      "commitNameOld": "8860e352c394372e4eb3ebdf82ea899567f34e4e",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 8.19,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n-          BlockInfoContiguous bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n+          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n             neededReplications.add(bi, num.liveReplicas(),\n                 num.decommissionedAndDecommissioning(), getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n            neededReplications.add(bi, num.liveReplicas(),\n                num.decommissionedAndDecommissioning(), getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "c9e0268216584f1df1a7c6cd25d2cfb2bc6d1d3c": {
      "type": "Ybodychange",
      "commitMessage": "Addendum fix for HDFS-7912.\n",
      "commitDate": "26/05/15 12:32 PM",
      "commitName": "c9e0268216584f1df1a7c6cd25d2cfb2bc6d1d3c",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 12:07 PM",
      "commitNameOld": "9a18598e2da8e699ed852ffa30fd7f503902190c",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void processPendingReplications() {\n     BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n-          BlockInfoContiguous bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n+          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n             neededReplications.add(bi, num.liveReplicas(),\n                 num.decommissionedAndDecommissioning(), getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfo bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n            neededReplications.add(bi, num.liveReplicas(),\n                num.decommissionedAndDecommissioning(), getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "a38a37c63417a3b19dcdf98251af196c9d7b8c31": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7912. Erasure Coding: track BlockInfo instead of Block in UnderReplicatedBlocks and PendingReplicationBlocks. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:41 AM",
      "commitName": "a38a37c63417a3b19dcdf98251af196c9d7b8c31",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:32 AM",
      "commitNameOld": "f05c21285ef23b6a973d69f045b1cb46c5abc039",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void processPendingReplications() {\n-    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n+    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           /*\n            * Use the blockinfo from the blocksmap to be certain we\u0027re working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n           BlockInfoContiguous bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi \u003d\u003d null) {\n             continue;\n           }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n             neededReplications.add(bi, num.liveReplicas(),\n                 num.decommissionedAndDecommissioning(), getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    BlockInfo[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfoContiguous bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n            neededReplications.add(bi, num.liveReplicas(),\n                num.decommissionedAndDecommissioning(), getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8860e352c394372e4eb3ebdf82ea899567f34e4e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8404. Pending block replication can get stuck using older genstamp. Contributed by Nathan Roberts.\n",
      "commitDate": "19/05/15 11:05 AM",
      "commitName": "8860e352c394372e4eb3ebdf82ea899567f34e4e",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "19/05/15 10:50 AM",
      "commitNameOld": "470c87dbc6c24dd3b370f1ad9e7ab1f6dabd2080",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,28 @@\n   private void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n+          /*\n+           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n+           * with the most up-to-date block information (e.g. genstamp).\n+           */\n+          BlockInfoContiguous bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n+          if (bi \u003d\u003d null) {\n+            continue;\n+          }\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n-          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n-                                 num.liveReplicas())) {\n-            neededReplications.add(timedOutItems[i],\n-                                   num.liveReplicas(),\n-                                   num.decommissionedAndDecommissioning(),\n-                                   getReplication(timedOutItems[i]));\n+          if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n+            neededReplications.add(bi, num.liveReplicas(),\n+                num.decommissionedAndDecommissioning(), getReplication(bi));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          /*\n           * Use the blockinfo from the blocksmap to be certain we\u0027re working\n           * with the most up-to-date block information (e.g. genstamp).\n           */\n          BlockInfoContiguous bi \u003d blocksMap.getStoredBlock(timedOutItems[i]);\n          if (bi \u003d\u003d null) {\n            continue;\n          }\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(bi, getReplication(bi), num.liveReplicas())) {\n            neededReplications.add(bi, num.liveReplicas(),\n                num.decommissionedAndDecommissioning(), getReplication(bi));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7933. fsck should also report decommissioning replicas. Contributed by Xiaoyu Yao.\n",
      "commitDate": "11/04/15 1:23 PM",
      "commitName": "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
      "commitAuthor": "cnauroth",
      "commitDateOld": "10/04/15 4:36 PM",
      "commitNameOld": "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.87,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   private void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                  num.liveReplicas())) {\n             neededReplications.add(timedOutItems[i],\n                                    num.liveReplicas(),\n-                                   num.decommissionedReplicas(),\n+                                   num.decommissionedAndDecommissioning(),\n                                    getReplication(timedOutItems[i]));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedAndDecommissioning(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8": {
      "type": "Ymodifierchange",
      "commitMessage": "Revert HDFS-6940.",
      "commitDate": "09/09/14 5:30 PM",
      "commitName": "05af0ff4be871ddbb4c4cb4f0b5b506ecee36fb8",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "06/09/14 12:07 PM",
      "commitNameOld": "88209ce181b5ecc55c0ae2bceff4893ab4817e88",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 3.22,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n-  void processPendingReplications() {\n+  private void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                  num.liveReplicas())) {\n             neededReplications.add(timedOutItems[i],\n                                    num.liveReplicas(),\n                                    num.decommissionedReplicas(),\n                                    getReplication(timedOutItems[i]));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[private]"
      }
    },
    "88209ce181b5ecc55c0ae2bceff4893ab4817e88": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-6940. Refactoring to allow ConsensusNode implementation.\nContributed by Konstantin Shvachko.",
      "commitDate": "06/09/14 12:07 PM",
      "commitName": "88209ce181b5ecc55c0ae2bceff4893ab4817e88",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "18/08/14 11:41 AM",
      "commitNameOld": "0cc08f6da4d299141b19b07147e39caef050faf6",
      "commitAuthorOld": "",
      "daysBetweenCommits": 19.02,
      "commitsBetweenForRepo": 132,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n-  private void processPendingReplications() {\n+  void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                  num.liveReplicas())) {\n             neededReplications.add(timedOutItems[i],\n                                    num.liveReplicas(),\n                                    num.decommissionedReplicas(),\n                                    getReplication(timedOutItems[i]));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[]"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "969a263188f7015261719fe45fa1505121ebb80e": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-2191.  Move datanodeMap from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151339 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/11 10:46 PM",
      "commitName": "969a263188f7015261719fe45fa1505121ebb80e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "22/07/11 6:01 PM",
      "commitNameOld": "89537b7710b23db7abcd2a77f03818c06a5f5fa7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.2,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n-  public void processPendingReplications() {\n+  private void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                  num.liveReplicas())) {\n             neededReplications.add(timedOutItems[i],\n                                    num.liveReplicas(),\n                                    num.decommissionedReplicas(),\n                                    getReplication(timedOutItems[i]));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "[public]",
        "newValue": "[private]"
      }
    },
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 6:31 PM",
      "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 6:31 PM",
          "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 5:26 PM",
          "commitNameOld": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  void processPendingReplications() {\n+  public void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                  num.liveReplicas())) {\n             neededReplications.add(timedOutItems[i],\n                                    num.liveReplicas(),\n                                    num.decommissionedReplicas(),\n                                    getReplication(timedOutItems[i]));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 6:31 PM",
          "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 5:26 PM",
          "commitNameOld": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  void processPendingReplications() {\n+  public void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                  num.liveReplicas())) {\n             neededReplications.add(timedOutItems[i],\n                                    num.liveReplicas(),\n                                    num.decommissionedReplicas(),\n                                    getReplication(timedOutItems[i]));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[public]"
          }
        }
      ]
    },
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 5:26 PM",
      "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 5:26 PM",
          "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 4:57 PM",
          "commitNameOld": "d58e3efe9269efe00c309ed0e9726d2f94bcd03a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  public void processPendingReplications() {\n+  void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                  num.liveReplicas())) {\n             neededReplications.add(timedOutItems[i],\n                                    num.liveReplicas(),\n                                    num.decommissionedReplicas(),\n                                    getReplication(timedOutItems[i]));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 5:26 PM",
          "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 4:57 PM",
          "commitNameOld": "d58e3efe9269efe00c309ed0e9726d2f94bcd03a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  public void processPendingReplications() {\n+  void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                  num.liveReplicas())) {\n             neededReplications.add(timedOutItems[i],\n                                    num.liveReplicas(),\n                                    num.decommissionedReplicas(),\n                                    getReplication(timedOutItems[i]));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        }
      ]
    },
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 4:43 PM",
      "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 4:43 PM",
          "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 9:21 AM",
          "commitNameOld": "1834fb99f516b2f2cd5e0ab1f89d407f98a7237a",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.31,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  void processPendingReplications() {\n+  public void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                  num.liveReplicas())) {\n             neededReplications.add(timedOutItems[i],\n                                    num.liveReplicas(),\n                                    num.decommissionedReplicas(),\n                                    getReplication(timedOutItems[i]));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 4:43 PM",
          "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 9:21 AM",
          "commitNameOld": "1834fb99f516b2f2cd5e0ab1f89d407f98a7237a",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.31,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  void processPendingReplications() {\n+  public void processPendingReplications() {\n     Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n     if (timedOutItems !\u003d null) {\n       namesystem.writeLock();\n       try {\n         for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n           NumberReplicas num \u003d countNodes(timedOutItems[i]);\n           if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                  num.liveReplicas())) {\n             neededReplications.add(timedOutItems[i],\n                                    num.liveReplicas(),\n                                    num.decommissionedReplicas(),\n                                    getReplication(timedOutItems[i]));\n           }\n         }\n       } finally {\n         namesystem.writeUnlock();\n       }\n       /* If we know the target datanodes where the replication timedout,\n        * we could invoke decBlocksScheduled() on it. Its ok for now.\n        */\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[public]"
          }
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,23 @@\n+  void processPendingReplications() {\n+    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n+    if (timedOutItems !\u003d null) {\n+      namesystem.writeLock();\n+      try {\n+        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n+          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n+          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n+                                 num.liveReplicas())) {\n+            neededReplications.add(timedOutItems[i],\n+                                   num.liveReplicas(),\n+                                   num.decommissionedReplicas(),\n+                                   getReplication(timedOutItems[i]));\n+          }\n+        }\n+      } finally {\n+        namesystem.writeUnlock();\n+      }\n+      /* If we know the target datanodes where the replication timedout,\n+       * we could invoke decBlocksScheduled() on it. Its ok for now.\n+       */\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void processPendingReplications() {\n    Block[] timedOutItems \u003d pendingReplications.getTimedOutBlocks();\n    if (timedOutItems !\u003d null) {\n      namesystem.writeLock();\n      try {\n        for (int i \u003d 0; i \u003c timedOutItems.length; i++) {\n          NumberReplicas num \u003d countNodes(timedOutItems[i]);\n          if (isNeededReplication(timedOutItems[i], getReplication(timedOutItems[i]),\n                                 num.liveReplicas())) {\n            neededReplications.add(timedOutItems[i],\n                                   num.liveReplicas(),\n                                   num.decommissionedReplicas(),\n                                   getReplication(timedOutItems[i]));\n          }\n        }\n      } finally {\n        namesystem.writeUnlock();\n      }\n      /* If we know the target datanodes where the replication timedout,\n       * we could invoke decBlocksScheduled() on it. Its ok for now.\n       */\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
    }
  }
}