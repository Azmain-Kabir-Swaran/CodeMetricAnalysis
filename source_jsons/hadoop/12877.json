{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ProvidedStorageMap.java",
  "functionName": "chooseRandom",
  "functionId": "chooseRandom___excludedStorages-DatanodeStorageInfo",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java",
  "functionStartLine": 391,
  "functionEndLine": 401,
  "numCommitsSeen": 25,
  "timeTaken": 2212,
  "changeHistory": [
    "4d59dabb7f6ef1d8565bf2bb2d38aeb91bf7f7cc",
    "d65df0f27395792c6e25f5e03b6ba1765e2ba925"
  ],
  "changeHistoryShort": {
    "4d59dabb7f6ef1d8565bf2bb2d38aeb91bf7f7cc": "Ymultichange(Yparameterchange,Ybodychange)",
    "d65df0f27395792c6e25f5e03b6ba1765e2ba925": "Yintroduced"
  },
  "changeHistoryDetails": {
    "4d59dabb7f6ef1d8565bf2bb2d38aeb91bf7f7cc": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-12809. [READ] Fix the randomized selection of locations in {{ProvidedBlocksBuilder}}.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "4d59dabb7f6ef1d8565bf2bb2d38aeb91bf7f7cc",
      "commitAuthor": "Virajith Jalaparti",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-12809. [READ] Fix the randomized selection of locations in {{ProvidedBlocksBuilder}}.\n",
          "commitDate": "15/12/17 5:51 PM",
          "commitName": "4d59dabb7f6ef1d8565bf2bb2d38aeb91bf7f7cc",
          "commitAuthor": "Virajith Jalaparti",
          "commitDateOld": "15/12/17 5:51 PM",
          "commitNameOld": "3d3be87e301d9f8ab1a220bc5dbeae0f032c5a86",
          "commitAuthorOld": "Virajith Jalaparti",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,11 @@\n-    DatanodeDescriptor chooseRandom(DatanodeStorageInfo[] excludedStorages) {\n-      // TODO: Currently this is not uniformly random;\n-      // skewed toward sparse sections of the ids\n-      Set\u003cDatanodeDescriptor\u003e excludedNodes \u003d\n-          new HashSet\u003cDatanodeDescriptor\u003e();\n+    DatanodeDescriptor chooseRandom(DatanodeStorageInfo... excludedStorages) {\n+      Set\u003cString\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n       if (excludedStorages !\u003d null) {\n-        for (int i\u003d 0; i \u003c excludedStorages.length; i++) {\n-          LOG.info(\"Excluded: \" + excludedStorages[i].getDatanodeDescriptor());\n-          excludedNodes.add(excludedStorages[i].getDatanodeDescriptor());\n+        for (int i \u003d 0; i \u003c excludedStorages.length; i++) {\n+          DatanodeDescriptor dn \u003d excludedStorages[i].getDatanodeDescriptor();\n+          String uuid \u003d dn.getDatanodeUuid();\n+          excludedNodes.add(uuid);\n         }\n       }\n-      Set\u003cDatanodeDescriptor\u003e exploredNodes \u003d new HashSet\u003cDatanodeDescriptor\u003e();\n-\n-      while(exploredNodes.size() \u003c dns.size()) {\n-        Map.Entry\u003cString, DatanodeDescriptor\u003e d \u003d\n-            dns.ceilingEntry(UUID.randomUUID().toString());\n-        if (null \u003d\u003d d) {\n-          d \u003d dns.firstEntry();\n-        }\n-        DatanodeDescriptor node \u003d d.getValue();\n-        //this node has already been explored, and was not selected earlier\n-        if (exploredNodes.contains(node)) {\n-          continue;\n-        }\n-        exploredNodes.add(node);\n-        //this node has been excluded\n-        if (excludedNodes.contains(node)) {\n-          continue;\n-        }\n-        return node;\n-      }\n-      return null;\n+      return choose(null, excludedNodes);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    DatanodeDescriptor chooseRandom(DatanodeStorageInfo... excludedStorages) {\n      Set\u003cString\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n      if (excludedStorages !\u003d null) {\n        for (int i \u003d 0; i \u003c excludedStorages.length; i++) {\n          DatanodeDescriptor dn \u003d excludedStorages[i].getDatanodeDescriptor();\n          String uuid \u003d dn.getDatanodeUuid();\n          excludedNodes.add(uuid);\n        }\n      }\n      return choose(null, excludedNodes);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java",
          "extendedDetails": {
            "oldValue": "[excludedStorages-DatanodeStorageInfo[]]",
            "newValue": "[excludedStorages-DatanodeStorageInfo]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-12809. [READ] Fix the randomized selection of locations in {{ProvidedBlocksBuilder}}.\n",
          "commitDate": "15/12/17 5:51 PM",
          "commitName": "4d59dabb7f6ef1d8565bf2bb2d38aeb91bf7f7cc",
          "commitAuthor": "Virajith Jalaparti",
          "commitDateOld": "15/12/17 5:51 PM",
          "commitNameOld": "3d3be87e301d9f8ab1a220bc5dbeae0f032c5a86",
          "commitAuthorOld": "Virajith Jalaparti",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,11 @@\n-    DatanodeDescriptor chooseRandom(DatanodeStorageInfo[] excludedStorages) {\n-      // TODO: Currently this is not uniformly random;\n-      // skewed toward sparse sections of the ids\n-      Set\u003cDatanodeDescriptor\u003e excludedNodes \u003d\n-          new HashSet\u003cDatanodeDescriptor\u003e();\n+    DatanodeDescriptor chooseRandom(DatanodeStorageInfo... excludedStorages) {\n+      Set\u003cString\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n       if (excludedStorages !\u003d null) {\n-        for (int i\u003d 0; i \u003c excludedStorages.length; i++) {\n-          LOG.info(\"Excluded: \" + excludedStorages[i].getDatanodeDescriptor());\n-          excludedNodes.add(excludedStorages[i].getDatanodeDescriptor());\n+        for (int i \u003d 0; i \u003c excludedStorages.length; i++) {\n+          DatanodeDescriptor dn \u003d excludedStorages[i].getDatanodeDescriptor();\n+          String uuid \u003d dn.getDatanodeUuid();\n+          excludedNodes.add(uuid);\n         }\n       }\n-      Set\u003cDatanodeDescriptor\u003e exploredNodes \u003d new HashSet\u003cDatanodeDescriptor\u003e();\n-\n-      while(exploredNodes.size() \u003c dns.size()) {\n-        Map.Entry\u003cString, DatanodeDescriptor\u003e d \u003d\n-            dns.ceilingEntry(UUID.randomUUID().toString());\n-        if (null \u003d\u003d d) {\n-          d \u003d dns.firstEntry();\n-        }\n-        DatanodeDescriptor node \u003d d.getValue();\n-        //this node has already been explored, and was not selected earlier\n-        if (exploredNodes.contains(node)) {\n-          continue;\n-        }\n-        exploredNodes.add(node);\n-        //this node has been excluded\n-        if (excludedNodes.contains(node)) {\n-          continue;\n-        }\n-        return node;\n-      }\n-      return null;\n+      return choose(null, excludedNodes);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    DatanodeDescriptor chooseRandom(DatanodeStorageInfo... excludedStorages) {\n      Set\u003cString\u003e excludedNodes \u003d new HashSet\u003c\u003e();\n      if (excludedStorages !\u003d null) {\n        for (int i \u003d 0; i \u003c excludedStorages.length; i++) {\n          DatanodeDescriptor dn \u003d excludedStorages[i].getDatanodeDescriptor();\n          String uuid \u003d dn.getDatanodeUuid();\n          excludedNodes.add(uuid);\n        }\n      }\n      return choose(null, excludedNodes);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java",
          "extendedDetails": {}
        }
      ]
    },
    "d65df0f27395792c6e25f5e03b6ba1765e2ba925": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11190. [READ] Namenode support for data stored in external stores.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "d65df0f27395792c6e25f5e03b6ba1765e2ba925",
      "commitAuthor": "Virajith Jalaparti",
      "diff": "@@ -0,0 +1,33 @@\n+    DatanodeDescriptor chooseRandom(DatanodeStorageInfo[] excludedStorages) {\n+      // TODO: Currently this is not uniformly random;\n+      // skewed toward sparse sections of the ids\n+      Set\u003cDatanodeDescriptor\u003e excludedNodes \u003d\n+          new HashSet\u003cDatanodeDescriptor\u003e();\n+      if (excludedStorages !\u003d null) {\n+        for (int i\u003d 0; i \u003c excludedStorages.length; i++) {\n+          LOG.info(\"Excluded: \" + excludedStorages[i].getDatanodeDescriptor());\n+          excludedNodes.add(excludedStorages[i].getDatanodeDescriptor());\n+        }\n+      }\n+      Set\u003cDatanodeDescriptor\u003e exploredNodes \u003d new HashSet\u003cDatanodeDescriptor\u003e();\n+\n+      while(exploredNodes.size() \u003c dns.size()) {\n+        Map.Entry\u003cString, DatanodeDescriptor\u003e d \u003d\n+            dns.ceilingEntry(UUID.randomUUID().toString());\n+        if (null \u003d\u003d d) {\n+          d \u003d dns.firstEntry();\n+        }\n+        DatanodeDescriptor node \u003d d.getValue();\n+        //this node has already been explored, and was not selected earlier\n+        if (exploredNodes.contains(node)) {\n+          continue;\n+        }\n+        exploredNodes.add(node);\n+        //this node has been excluded\n+        if (excludedNodes.contains(node)) {\n+          continue;\n+        }\n+        return node;\n+      }\n+      return null;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    DatanodeDescriptor chooseRandom(DatanodeStorageInfo[] excludedStorages) {\n      // TODO: Currently this is not uniformly random;\n      // skewed toward sparse sections of the ids\n      Set\u003cDatanodeDescriptor\u003e excludedNodes \u003d\n          new HashSet\u003cDatanodeDescriptor\u003e();\n      if (excludedStorages !\u003d null) {\n        for (int i\u003d 0; i \u003c excludedStorages.length; i++) {\n          LOG.info(\"Excluded: \" + excludedStorages[i].getDatanodeDescriptor());\n          excludedNodes.add(excludedStorages[i].getDatanodeDescriptor());\n        }\n      }\n      Set\u003cDatanodeDescriptor\u003e exploredNodes \u003d new HashSet\u003cDatanodeDescriptor\u003e();\n\n      while(exploredNodes.size() \u003c dns.size()) {\n        Map.Entry\u003cString, DatanodeDescriptor\u003e d \u003d\n            dns.ceilingEntry(UUID.randomUUID().toString());\n        if (null \u003d\u003d d) {\n          d \u003d dns.firstEntry();\n        }\n        DatanodeDescriptor node \u003d d.getValue();\n        //this node has already been explored, and was not selected earlier\n        if (exploredNodes.contains(node)) {\n          continue;\n        }\n        exploredNodes.add(node);\n        //this node has been excluded\n        if (excludedNodes.contains(node)) {\n          continue;\n        }\n        return node;\n      }\n      return null;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java"
    }
  }
}