{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FrameworkUploader.java",
  "functionName": "getSmallestReplicatedBlockCount",
  "functionId": "getSmallestReplicatedBlockCount",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-uploader/src/main/java/org/apache/hadoop/mapred/uploader/FrameworkUploader.java",
  "functionStartLine": 261,
  "functionEndLine": 291,
  "numCommitsSeen": 12,
  "timeTaken": 1133,
  "changeHistory": [
    "836643d793c68bf1bee883abece84f024591da7c",
    "d716084f4503bf826ef10424d7025ea1ff4ee104"
  ],
  "changeHistoryShort": {
    "836643d793c68bf1bee883abece84f024591da7c": "Ybodychange",
    "d716084f4503bf826ef10424d7025ea1ff4ee104": "Yintroduced"
  },
  "changeHistoryDetails": {
    "836643d793c68bf1bee883abece84f024591da7c": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6995. Uploader tool for Distributed Cache Deploy documentation (miklos.szegedi@cloudera.com via rkanter)\n",
      "commitDate": "19/01/18 5:57 PM",
      "commitName": "836643d793c68bf1bee883abece84f024591da7c",
      "commitAuthor": "Robert Kanter",
      "commitDateOld": "16/01/18 10:45 AM",
      "commitNameOld": "d716084f4503bf826ef10424d7025ea1ff4ee104",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 3.3,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,31 @@\n   private long getSmallestReplicatedBlockCount()\n       throws IOException {\n-    FileSystem fileSystem \u003d targetPath.getFileSystem(new Configuration());\n+    FileSystem fileSystem \u003d targetPath.getFileSystem(conf);\n     FileStatus status \u003d fileSystem.getFileStatus(targetPath);\n     long length \u003d status.getLen();\n     HashMap\u003cLong, Integer\u003e blockCount \u003d new HashMap\u003c\u003e();\n \n     // Start with 0s for each offset\n     for (long offset \u003d 0; offset \u003c length; offset +\u003dstatus.getBlockSize()) {\n       blockCount.put(offset, 0);\n     }\n \n     // Count blocks\n     BlockLocation[] locations \u003d fileSystem.getFileBlockLocations(\n         targetPath, 0, length);\n     for(BlockLocation location: locations) {\n       final int replicas \u003d location.getHosts().length;\n       blockCount.compute(\n-          location.getOffset(), (key, value) -\u003e value + replicas);\n+          location.getOffset(),\n+          (key, value) -\u003e value \u003d\u003d null ? 0 : value + replicas);\n     }\n \n     // Print out the results\n     for (long offset \u003d 0; offset \u003c length; offset +\u003dstatus.getBlockSize()) {\n       LOG.info(String.format(\n           \"Replication counts offset:%d blocks:%d\",\n           offset, blockCount.get(offset)));\n     }\n \n     return Collections.min(blockCount.values());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long getSmallestReplicatedBlockCount()\n      throws IOException {\n    FileSystem fileSystem \u003d targetPath.getFileSystem(conf);\n    FileStatus status \u003d fileSystem.getFileStatus(targetPath);\n    long length \u003d status.getLen();\n    HashMap\u003cLong, Integer\u003e blockCount \u003d new HashMap\u003c\u003e();\n\n    // Start with 0s for each offset\n    for (long offset \u003d 0; offset \u003c length; offset +\u003dstatus.getBlockSize()) {\n      blockCount.put(offset, 0);\n    }\n\n    // Count blocks\n    BlockLocation[] locations \u003d fileSystem.getFileBlockLocations(\n        targetPath, 0, length);\n    for(BlockLocation location: locations) {\n      final int replicas \u003d location.getHosts().length;\n      blockCount.compute(\n          location.getOffset(),\n          (key, value) -\u003e value \u003d\u003d null ? 0 : value + replicas);\n    }\n\n    // Print out the results\n    for (long offset \u003d 0; offset \u003c length; offset +\u003dstatus.getBlockSize()) {\n      LOG.info(String.format(\n          \"Replication counts offset:%d blocks:%d\",\n          offset, blockCount.get(offset)));\n    }\n\n    return Collections.min(blockCount.values());\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-uploader/src/main/java/org/apache/hadoop/mapred/uploader/FrameworkUploader.java",
      "extendedDetails": {}
    },
    "d716084f4503bf826ef10424d7025ea1ff4ee104": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-7032. Add the ability to specify a delayed replication count (miklos.szegedi@cloudera.com via rkanter)\n",
      "commitDate": "16/01/18 10:45 AM",
      "commitName": "d716084f4503bf826ef10424d7025ea1ff4ee104",
      "commitAuthor": "Robert Kanter",
      "diff": "@@ -0,0 +1,30 @@\n+  private long getSmallestReplicatedBlockCount()\n+      throws IOException {\n+    FileSystem fileSystem \u003d targetPath.getFileSystem(new Configuration());\n+    FileStatus status \u003d fileSystem.getFileStatus(targetPath);\n+    long length \u003d status.getLen();\n+    HashMap\u003cLong, Integer\u003e blockCount \u003d new HashMap\u003c\u003e();\n+\n+    // Start with 0s for each offset\n+    for (long offset \u003d 0; offset \u003c length; offset +\u003dstatus.getBlockSize()) {\n+      blockCount.put(offset, 0);\n+    }\n+\n+    // Count blocks\n+    BlockLocation[] locations \u003d fileSystem.getFileBlockLocations(\n+        targetPath, 0, length);\n+    for(BlockLocation location: locations) {\n+      final int replicas \u003d location.getHosts().length;\n+      blockCount.compute(\n+          location.getOffset(), (key, value) -\u003e value + replicas);\n+    }\n+\n+    // Print out the results\n+    for (long offset \u003d 0; offset \u003c length; offset +\u003dstatus.getBlockSize()) {\n+      LOG.info(String.format(\n+          \"Replication counts offset:%d blocks:%d\",\n+          offset, blockCount.get(offset)));\n+    }\n+\n+    return Collections.min(blockCount.values());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private long getSmallestReplicatedBlockCount()\n      throws IOException {\n    FileSystem fileSystem \u003d targetPath.getFileSystem(new Configuration());\n    FileStatus status \u003d fileSystem.getFileStatus(targetPath);\n    long length \u003d status.getLen();\n    HashMap\u003cLong, Integer\u003e blockCount \u003d new HashMap\u003c\u003e();\n\n    // Start with 0s for each offset\n    for (long offset \u003d 0; offset \u003c length; offset +\u003dstatus.getBlockSize()) {\n      blockCount.put(offset, 0);\n    }\n\n    // Count blocks\n    BlockLocation[] locations \u003d fileSystem.getFileBlockLocations(\n        targetPath, 0, length);\n    for(BlockLocation location: locations) {\n      final int replicas \u003d location.getHosts().length;\n      blockCount.compute(\n          location.getOffset(), (key, value) -\u003e value + replicas);\n    }\n\n    // Print out the results\n    for (long offset \u003d 0; offset \u003c length; offset +\u003dstatus.getBlockSize()) {\n      LOG.info(String.format(\n          \"Replication counts offset:%d blocks:%d\",\n          offset, blockCount.get(offset)));\n    }\n\n    return Collections.min(blockCount.values());\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-uploader/src/main/java/org/apache/hadoop/mapred/uploader/FrameworkUploader.java"
    }
  }
}