{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockListAsLongs.java",
  "functionName": "readFrom",
  "functionId": "readFrom___is-InputStream__maxDataLength-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java",
  "functionStartLine": 148,
  "functionEndLine": 177,
  "numCommitsSeen": 25,
  "timeTaken": 2352,
  "changeHistory": [
    "63ac2db59af2b50e74dc892cae1dbc4d2e061423",
    "fc1031af749435dc95efea6745b1b2300ce29446"
  ],
  "changeHistoryShort": {
    "63ac2db59af2b50e74dc892cae1dbc4d2e061423": "Ymultichange(Yparameterchange,Ybodychange)",
    "fc1031af749435dc95efea6745b1b2300ce29446": "Yintroduced"
  },
  "changeHistoryDetails": {
    "63ac2db59af2b50e74dc892cae1dbc4d2e061423": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10312. Large block reports may fail to decode at NameNode due to 64 MB protobuf maximum length restriction. Contributed by Chris Nauroth.\n",
      "commitDate": "20/04/16 1:39 PM",
      "commitName": "63ac2db59af2b50e74dc892cae1dbc4d2e061423",
      "commitAuthor": "Chris Nauroth",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10312. Large block reports may fail to decode at NameNode due to 64 MB protobuf maximum length restriction. Contributed by Chris Nauroth.\n",
          "commitDate": "20/04/16 1:39 PM",
          "commitName": "63ac2db59af2b50e74dc892cae1dbc4d2e061423",
          "commitAuthor": "Chris Nauroth",
          "commitDateOld": "25/03/15 12:42 PM",
          "commitNameOld": "fc1031af749435dc95efea6745b1b2300ce29446",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 392.04,
          "commitsBetweenForRepo": 2943,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,30 @@\n-  public static BlockListAsLongs readFrom(InputStream is) throws IOException {\n+  public static BlockListAsLongs readFrom(InputStream is, int maxDataLength)\n+      throws IOException {\n     CodedInputStream cis \u003d CodedInputStream.newInstance(is);\n+    if (maxDataLength !\u003d IPC_MAXIMUM_DATA_LENGTH_DEFAULT) {\n+      cis.setSizeLimit(maxDataLength);\n+    }\n     int numBlocks \u003d -1;\n     ByteString blocksBuf \u003d null;\n     while (!cis.isAtEnd()) {\n       int tag \u003d cis.readTag();\n       int field \u003d WireFormat.getTagFieldNumber(tag);\n       switch(field) {\n         case 0:\n           break;\n         case 1:\n           numBlocks \u003d (int)cis.readInt32();\n           break;\n         case 2:\n           blocksBuf \u003d cis.readBytes();\n           break;\n         default:\n           cis.skipField(tag);\n           break;\n       }\n     }\n     if (numBlocks !\u003d -1 \u0026\u0026 blocksBuf !\u003d null) {\n-      return decodeBuffer(numBlocks, blocksBuf);\n+      return decodeBuffer(numBlocks, blocksBuf, maxDataLength);\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockListAsLongs readFrom(InputStream is, int maxDataLength)\n      throws IOException {\n    CodedInputStream cis \u003d CodedInputStream.newInstance(is);\n    if (maxDataLength !\u003d IPC_MAXIMUM_DATA_LENGTH_DEFAULT) {\n      cis.setSizeLimit(maxDataLength);\n    }\n    int numBlocks \u003d -1;\n    ByteString blocksBuf \u003d null;\n    while (!cis.isAtEnd()) {\n      int tag \u003d cis.readTag();\n      int field \u003d WireFormat.getTagFieldNumber(tag);\n      switch(field) {\n        case 0:\n          break;\n        case 1:\n          numBlocks \u003d (int)cis.readInt32();\n          break;\n        case 2:\n          blocksBuf \u003d cis.readBytes();\n          break;\n        default:\n          cis.skipField(tag);\n          break;\n      }\n    }\n    if (numBlocks !\u003d -1 \u0026\u0026 blocksBuf !\u003d null) {\n      return decodeBuffer(numBlocks, blocksBuf, maxDataLength);\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java",
          "extendedDetails": {
            "oldValue": "[is-InputStream]",
            "newValue": "[is-InputStream, maxDataLength-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10312. Large block reports may fail to decode at NameNode due to 64 MB protobuf maximum length restriction. Contributed by Chris Nauroth.\n",
          "commitDate": "20/04/16 1:39 PM",
          "commitName": "63ac2db59af2b50e74dc892cae1dbc4d2e061423",
          "commitAuthor": "Chris Nauroth",
          "commitDateOld": "25/03/15 12:42 PM",
          "commitNameOld": "fc1031af749435dc95efea6745b1b2300ce29446",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 392.04,
          "commitsBetweenForRepo": 2943,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,30 @@\n-  public static BlockListAsLongs readFrom(InputStream is) throws IOException {\n+  public static BlockListAsLongs readFrom(InputStream is, int maxDataLength)\n+      throws IOException {\n     CodedInputStream cis \u003d CodedInputStream.newInstance(is);\n+    if (maxDataLength !\u003d IPC_MAXIMUM_DATA_LENGTH_DEFAULT) {\n+      cis.setSizeLimit(maxDataLength);\n+    }\n     int numBlocks \u003d -1;\n     ByteString blocksBuf \u003d null;\n     while (!cis.isAtEnd()) {\n       int tag \u003d cis.readTag();\n       int field \u003d WireFormat.getTagFieldNumber(tag);\n       switch(field) {\n         case 0:\n           break;\n         case 1:\n           numBlocks \u003d (int)cis.readInt32();\n           break;\n         case 2:\n           blocksBuf \u003d cis.readBytes();\n           break;\n         default:\n           cis.skipField(tag);\n           break;\n       }\n     }\n     if (numBlocks !\u003d -1 \u0026\u0026 blocksBuf !\u003d null) {\n-      return decodeBuffer(numBlocks, blocksBuf);\n+      return decodeBuffer(numBlocks, blocksBuf, maxDataLength);\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockListAsLongs readFrom(InputStream is, int maxDataLength)\n      throws IOException {\n    CodedInputStream cis \u003d CodedInputStream.newInstance(is);\n    if (maxDataLength !\u003d IPC_MAXIMUM_DATA_LENGTH_DEFAULT) {\n      cis.setSizeLimit(maxDataLength);\n    }\n    int numBlocks \u003d -1;\n    ByteString blocksBuf \u003d null;\n    while (!cis.isAtEnd()) {\n      int tag \u003d cis.readTag();\n      int field \u003d WireFormat.getTagFieldNumber(tag);\n      switch(field) {\n        case 0:\n          break;\n        case 1:\n          numBlocks \u003d (int)cis.readInt32();\n          break;\n        case 2:\n          blocksBuf \u003d cis.readBytes();\n          break;\n        default:\n          cis.skipField(tag);\n          break;\n      }\n    }\n    if (numBlocks !\u003d -1 \u0026\u0026 blocksBuf !\u003d null) {\n      return decodeBuffer(numBlocks, blocksBuf, maxDataLength);\n    }\n    return null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java",
          "extendedDetails": {}
        }
      ]
    },
    "fc1031af749435dc95efea6745b1b2300ce29446": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7928. Scanning blocks from disk during rolling upgrade startup takes a lot of time if disks are busy. Contributed by Rushabh Shah.\n",
      "commitDate": "25/03/15 12:42 PM",
      "commitName": "fc1031af749435dc95efea6745b1b2300ce29446",
      "commitAuthor": "Kihwal Lee",
      "diff": "@@ -0,0 +1,26 @@\n+  public static BlockListAsLongs readFrom(InputStream is) throws IOException {\n+    CodedInputStream cis \u003d CodedInputStream.newInstance(is);\n+    int numBlocks \u003d -1;\n+    ByteString blocksBuf \u003d null;\n+    while (!cis.isAtEnd()) {\n+      int tag \u003d cis.readTag();\n+      int field \u003d WireFormat.getTagFieldNumber(tag);\n+      switch(field) {\n+        case 0:\n+          break;\n+        case 1:\n+          numBlocks \u003d (int)cis.readInt32();\n+          break;\n+        case 2:\n+          blocksBuf \u003d cis.readBytes();\n+          break;\n+        default:\n+          cis.skipField(tag);\n+          break;\n+      }\n+    }\n+    if (numBlocks !\u003d -1 \u0026\u0026 blocksBuf !\u003d null) {\n+      return decodeBuffer(numBlocks, blocksBuf);\n+    }\n+    return null;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockListAsLongs readFrom(InputStream is) throws IOException {\n    CodedInputStream cis \u003d CodedInputStream.newInstance(is);\n    int numBlocks \u003d -1;\n    ByteString blocksBuf \u003d null;\n    while (!cis.isAtEnd()) {\n      int tag \u003d cis.readTag();\n      int field \u003d WireFormat.getTagFieldNumber(tag);\n      switch(field) {\n        case 0:\n          break;\n        case 1:\n          numBlocks \u003d (int)cis.readInt32();\n          break;\n        case 2:\n          blocksBuf \u003d cis.readBytes();\n          break;\n        default:\n          cis.skipField(tag);\n          break;\n      }\n    }\n    if (numBlocks !\u003d -1 \u0026\u0026 blocksBuf !\u003d null) {\n      return decodeBuffer(numBlocks, blocksBuf);\n    }\n    return null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java"
    }
  }
}