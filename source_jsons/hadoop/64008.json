{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataJoinJob.java",
  "functionName": "createDataJoinJob",
  "functionId": "createDataJoinJob___args-String[]",
  "sourceFilePath": "hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java",
  "functionStartLine": 59,
  "functionEndLine": 116,
  "numCommitsSeen": 4,
  "timeTaken": 4281,
  "changeHistory": [
    "f35294eb95b67e62cd323703595c7f18e4c9281f",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "f35294eb95b67e62cd323703595c7f18e4c9281f": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f35294eb95b67e62cd323703595c7f18e4c9281f": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-4238. mavenize data_join. (tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1338835 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/05/12 12:09 PM",
      "commitName": "f35294eb95b67e62cd323703595c7f18e4c9281f",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "15/05/12 12:03 PM",
      "commitNameOld": "7428aeca8666aeaf5f6682efbdb5349f44d1753e",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static JobConf createDataJoinJob(String args[]) throws IOException {\n\n    String inputDir \u003d args[0];\n    String outputDir \u003d args[1];\n    Class inputFormat \u003d SequenceFileInputFormat.class;\n    if (args[2].compareToIgnoreCase(\"text\") !\u003d 0) {\n      System.out.println(\"Using SequenceFileInputFormat: \" + args[2]);\n    } else {\n      System.out.println(\"Using TextInputFormat: \" + args[2]);\n      inputFormat \u003d TextInputFormat.class;\n    }\n    int numOfReducers \u003d Integer.parseInt(args[3]);\n    Class mapper \u003d getClassByName(args[4]);\n    Class reducer \u003d getClassByName(args[5]);\n    Class mapoutputValueClass \u003d getClassByName(args[6]);\n    Class outputFormat \u003d TextOutputFormat.class;\n    Class outputValueClass \u003d Text.class;\n    if (args[7].compareToIgnoreCase(\"text\") !\u003d 0) {\n      System.out.println(\"Using SequenceFileOutputFormat: \" + args[7]);\n      outputFormat \u003d SequenceFileOutputFormat.class;\n      outputValueClass \u003d getClassByName(args[7]);\n    } else {\n      System.out.println(\"Using TextOutputFormat: \" + args[7]);\n    }\n    long maxNumOfValuesPerGroup \u003d 100;\n    String jobName \u003d \"\";\n    if (args.length \u003e 8) {\n      maxNumOfValuesPerGroup \u003d Long.parseLong(args[8]);\n    }\n    if (args.length \u003e 9) {\n      jobName \u003d args[9];\n    }\n    Configuration defaults \u003d new Configuration();\n    JobConf job \u003d new JobConf(defaults, DataJoinJob.class);\n    job.setJobName(\"DataJoinJob: \" + jobName);\n\n    FileSystem fs \u003d FileSystem.get(defaults);\n    fs.delete(new Path(outputDir), true);\n    FileInputFormat.setInputPaths(job, inputDir);\n\n    job.setInputFormat(inputFormat);\n\n    job.setMapperClass(mapper);\n    FileOutputFormat.setOutputPath(job, new Path(outputDir));\n    job.setOutputFormat(outputFormat);\n    SequenceFileOutputFormat.setOutputCompressionType(job,\n            SequenceFile.CompressionType.BLOCK);\n    job.setMapOutputKeyClass(Text.class);\n    job.setMapOutputValueClass(mapoutputValueClass);\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(outputValueClass);\n    job.setReducerClass(reducer);\n\n    job.setNumMapTasks(1);\n    job.setNumReduceTasks(numOfReducers);\n    job.setLong(\"datajoin.maxNumOfValuesPerGroup\", maxNumOfValuesPerGroup);\n    return job;\n  }",
      "path": "hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java",
        "newPath": "hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static JobConf createDataJoinJob(String args[]) throws IOException {\n\n    String inputDir \u003d args[0];\n    String outputDir \u003d args[1];\n    Class inputFormat \u003d SequenceFileInputFormat.class;\n    if (args[2].compareToIgnoreCase(\"text\") !\u003d 0) {\n      System.out.println(\"Using SequenceFileInputFormat: \" + args[2]);\n    } else {\n      System.out.println(\"Using TextInputFormat: \" + args[2]);\n      inputFormat \u003d TextInputFormat.class;\n    }\n    int numOfReducers \u003d Integer.parseInt(args[3]);\n    Class mapper \u003d getClassByName(args[4]);\n    Class reducer \u003d getClassByName(args[5]);\n    Class mapoutputValueClass \u003d getClassByName(args[6]);\n    Class outputFormat \u003d TextOutputFormat.class;\n    Class outputValueClass \u003d Text.class;\n    if (args[7].compareToIgnoreCase(\"text\") !\u003d 0) {\n      System.out.println(\"Using SequenceFileOutputFormat: \" + args[7]);\n      outputFormat \u003d SequenceFileOutputFormat.class;\n      outputValueClass \u003d getClassByName(args[7]);\n    } else {\n      System.out.println(\"Using TextOutputFormat: \" + args[7]);\n    }\n    long maxNumOfValuesPerGroup \u003d 100;\n    String jobName \u003d \"\";\n    if (args.length \u003e 8) {\n      maxNumOfValuesPerGroup \u003d Long.parseLong(args[8]);\n    }\n    if (args.length \u003e 9) {\n      jobName \u003d args[9];\n    }\n    Configuration defaults \u003d new Configuration();\n    JobConf job \u003d new JobConf(defaults, DataJoinJob.class);\n    job.setJobName(\"DataJoinJob: \" + jobName);\n\n    FileSystem fs \u003d FileSystem.get(defaults);\n    fs.delete(new Path(outputDir), true);\n    FileInputFormat.setInputPaths(job, inputDir);\n\n    job.setInputFormat(inputFormat);\n\n    job.setMapperClass(mapper);\n    FileOutputFormat.setOutputPath(job, new Path(outputDir));\n    job.setOutputFormat(outputFormat);\n    SequenceFileOutputFormat.setOutputCompressionType(job,\n            SequenceFile.CompressionType.BLOCK);\n    job.setMapOutputKeyClass(Text.class);\n    job.setMapOutputValueClass(mapoutputValueClass);\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(outputValueClass);\n    job.setReducerClass(reducer);\n\n    job.setNumMapTasks(1);\n    job.setNumReduceTasks(numOfReducers);\n    job.setLong(\"datajoin.maxNumOfValuesPerGroup\", maxNumOfValuesPerGroup);\n    return job;\n  }",
      "path": "hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java",
        "newPath": "hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static JobConf createDataJoinJob(String args[]) throws IOException {\n\n    String inputDir \u003d args[0];\n    String outputDir \u003d args[1];\n    Class inputFormat \u003d SequenceFileInputFormat.class;\n    if (args[2].compareToIgnoreCase(\"text\") !\u003d 0) {\n      System.out.println(\"Using SequenceFileInputFormat: \" + args[2]);\n    } else {\n      System.out.println(\"Using TextInputFormat: \" + args[2]);\n      inputFormat \u003d TextInputFormat.class;\n    }\n    int numOfReducers \u003d Integer.parseInt(args[3]);\n    Class mapper \u003d getClassByName(args[4]);\n    Class reducer \u003d getClassByName(args[5]);\n    Class mapoutputValueClass \u003d getClassByName(args[6]);\n    Class outputFormat \u003d TextOutputFormat.class;\n    Class outputValueClass \u003d Text.class;\n    if (args[7].compareToIgnoreCase(\"text\") !\u003d 0) {\n      System.out.println(\"Using SequenceFileOutputFormat: \" + args[7]);\n      outputFormat \u003d SequenceFileOutputFormat.class;\n      outputValueClass \u003d getClassByName(args[7]);\n    } else {\n      System.out.println(\"Using TextOutputFormat: \" + args[7]);\n    }\n    long maxNumOfValuesPerGroup \u003d 100;\n    String jobName \u003d \"\";\n    if (args.length \u003e 8) {\n      maxNumOfValuesPerGroup \u003d Long.parseLong(args[8]);\n    }\n    if (args.length \u003e 9) {\n      jobName \u003d args[9];\n    }\n    Configuration defaults \u003d new Configuration();\n    JobConf job \u003d new JobConf(defaults, DataJoinJob.class);\n    job.setJobName(\"DataJoinJob: \" + jobName);\n\n    FileSystem fs \u003d FileSystem.get(defaults);\n    fs.delete(new Path(outputDir), true);\n    FileInputFormat.setInputPaths(job, inputDir);\n\n    job.setInputFormat(inputFormat);\n\n    job.setMapperClass(mapper);\n    FileOutputFormat.setOutputPath(job, new Path(outputDir));\n    job.setOutputFormat(outputFormat);\n    SequenceFileOutputFormat.setOutputCompressionType(job,\n            SequenceFile.CompressionType.BLOCK);\n    job.setMapOutputKeyClass(Text.class);\n    job.setMapOutputValueClass(mapoutputValueClass);\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(outputValueClass);\n    job.setReducerClass(reducer);\n\n    job.setNumMapTasks(1);\n    job.setNumReduceTasks(numOfReducers);\n    job.setLong(\"datajoin.maxNumOfValuesPerGroup\", maxNumOfValuesPerGroup);\n    return job;\n  }",
      "path": "hadoop-mapreduce/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java",
        "newPath": "hadoop-mapreduce/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,58 @@\n+  public static JobConf createDataJoinJob(String args[]) throws IOException {\n+\n+    String inputDir \u003d args[0];\n+    String outputDir \u003d args[1];\n+    Class inputFormat \u003d SequenceFileInputFormat.class;\n+    if (args[2].compareToIgnoreCase(\"text\") !\u003d 0) {\n+      System.out.println(\"Using SequenceFileInputFormat: \" + args[2]);\n+    } else {\n+      System.out.println(\"Using TextInputFormat: \" + args[2]);\n+      inputFormat \u003d TextInputFormat.class;\n+    }\n+    int numOfReducers \u003d Integer.parseInt(args[3]);\n+    Class mapper \u003d getClassByName(args[4]);\n+    Class reducer \u003d getClassByName(args[5]);\n+    Class mapoutputValueClass \u003d getClassByName(args[6]);\n+    Class outputFormat \u003d TextOutputFormat.class;\n+    Class outputValueClass \u003d Text.class;\n+    if (args[7].compareToIgnoreCase(\"text\") !\u003d 0) {\n+      System.out.println(\"Using SequenceFileOutputFormat: \" + args[7]);\n+      outputFormat \u003d SequenceFileOutputFormat.class;\n+      outputValueClass \u003d getClassByName(args[7]);\n+    } else {\n+      System.out.println(\"Using TextOutputFormat: \" + args[7]);\n+    }\n+    long maxNumOfValuesPerGroup \u003d 100;\n+    String jobName \u003d \"\";\n+    if (args.length \u003e 8) {\n+      maxNumOfValuesPerGroup \u003d Long.parseLong(args[8]);\n+    }\n+    if (args.length \u003e 9) {\n+      jobName \u003d args[9];\n+    }\n+    Configuration defaults \u003d new Configuration();\n+    JobConf job \u003d new JobConf(defaults, DataJoinJob.class);\n+    job.setJobName(\"DataJoinJob: \" + jobName);\n+\n+    FileSystem fs \u003d FileSystem.get(defaults);\n+    fs.delete(new Path(outputDir), true);\n+    FileInputFormat.setInputPaths(job, inputDir);\n+\n+    job.setInputFormat(inputFormat);\n+\n+    job.setMapperClass(mapper);\n+    FileOutputFormat.setOutputPath(job, new Path(outputDir));\n+    job.setOutputFormat(outputFormat);\n+    SequenceFileOutputFormat.setOutputCompressionType(job,\n+            SequenceFile.CompressionType.BLOCK);\n+    job.setMapOutputKeyClass(Text.class);\n+    job.setMapOutputValueClass(mapoutputValueClass);\n+    job.setOutputKeyClass(Text.class);\n+    job.setOutputValueClass(outputValueClass);\n+    job.setReducerClass(reducer);\n+\n+    job.setNumMapTasks(1);\n+    job.setNumReduceTasks(numOfReducers);\n+    job.setLong(\"datajoin.maxNumOfValuesPerGroup\", maxNumOfValuesPerGroup);\n+    return job;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static JobConf createDataJoinJob(String args[]) throws IOException {\n\n    String inputDir \u003d args[0];\n    String outputDir \u003d args[1];\n    Class inputFormat \u003d SequenceFileInputFormat.class;\n    if (args[2].compareToIgnoreCase(\"text\") !\u003d 0) {\n      System.out.println(\"Using SequenceFileInputFormat: \" + args[2]);\n    } else {\n      System.out.println(\"Using TextInputFormat: \" + args[2]);\n      inputFormat \u003d TextInputFormat.class;\n    }\n    int numOfReducers \u003d Integer.parseInt(args[3]);\n    Class mapper \u003d getClassByName(args[4]);\n    Class reducer \u003d getClassByName(args[5]);\n    Class mapoutputValueClass \u003d getClassByName(args[6]);\n    Class outputFormat \u003d TextOutputFormat.class;\n    Class outputValueClass \u003d Text.class;\n    if (args[7].compareToIgnoreCase(\"text\") !\u003d 0) {\n      System.out.println(\"Using SequenceFileOutputFormat: \" + args[7]);\n      outputFormat \u003d SequenceFileOutputFormat.class;\n      outputValueClass \u003d getClassByName(args[7]);\n    } else {\n      System.out.println(\"Using TextOutputFormat: \" + args[7]);\n    }\n    long maxNumOfValuesPerGroup \u003d 100;\n    String jobName \u003d \"\";\n    if (args.length \u003e 8) {\n      maxNumOfValuesPerGroup \u003d Long.parseLong(args[8]);\n    }\n    if (args.length \u003e 9) {\n      jobName \u003d args[9];\n    }\n    Configuration defaults \u003d new Configuration();\n    JobConf job \u003d new JobConf(defaults, DataJoinJob.class);\n    job.setJobName(\"DataJoinJob: \" + jobName);\n\n    FileSystem fs \u003d FileSystem.get(defaults);\n    fs.delete(new Path(outputDir), true);\n    FileInputFormat.setInputPaths(job, inputDir);\n\n    job.setInputFormat(inputFormat);\n\n    job.setMapperClass(mapper);\n    FileOutputFormat.setOutputPath(job, new Path(outputDir));\n    job.setOutputFormat(outputFormat);\n    SequenceFileOutputFormat.setOutputCompressionType(job,\n            SequenceFile.CompressionType.BLOCK);\n    job.setMapOutputKeyClass(Text.class);\n    job.setMapOutputValueClass(mapoutputValueClass);\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(outputValueClass);\n    job.setReducerClass(reducer);\n\n    job.setNumMapTasks(1);\n    job.setNumReduceTasks(numOfReducers);\n    job.setLong(\"datajoin.maxNumOfValuesPerGroup\", maxNumOfValuesPerGroup);\n    return job;\n  }",
      "path": "mapreduce/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java"
    }
  }
}