{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PBImageXmlWriter.java",
  "functionName": "dumpINodeDirectorySection",
  "functionId": "dumpINodeDirectorySection___in-InputStream",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
  "functionStartLine": 475,
  "functionEndLine": 495,
  "numCommitsSeen": 27,
  "timeTaken": 2627,
  "changeHistory": [
    "680716f31e120f4d3ee70b095e4db46c05b891d9",
    "700b0e4019cf483f7532609711812150b8c44742",
    "2624b20291629b4565ea45590b66f2c38f96df67",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305"
  ],
  "changeHistoryShort": {
    "680716f31e120f4d3ee70b095e4db46c05b891d9": "Ybodychange",
    "700b0e4019cf483f7532609711812150b8c44742": "Ybodychange",
    "2624b20291629b4565ea45590b66f2c38f96df67": "Ybodychange",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Yintroduced"
  },
  "changeHistoryDetails": {
    "680716f31e120f4d3ee70b095e4db46c05b891d9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9951. Use string constants for XML tags in OfflineImageReconstructor (Lin Yiqun via cmccabe)\n",
      "commitDate": "21/03/16 11:40 AM",
      "commitName": "680716f31e120f4d3ee70b095e4db46c05b891d9",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "02/03/16 5:56 PM",
      "commitNameOld": "700b0e4019cf483f7532609711812150b8c44742",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 18.7,
      "commitsBetweenForRepo": 94,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   private void dumpINodeDirectorySection(InputStream in) throws IOException {\n-    out.print(\"\u003cINodeDirectorySection\u003e\");\n+    out.print(\"\u003c\" + INODE_DIRECTORY_SECTION_NAME + \"\u003e\");\n     while (true) {\n       INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n           .parseDelimitedFrom(in);\n       // note that in is a LimitedInputStream\n       if (e \u003d\u003d null) {\n         break;\n       }\n-      out.print(\"\u003cdirectory\u003e\");\n-      o(\"parent\", e.getParent());\n+      out.print(\"\u003c\" + INODE_DIRECTORY_SECTION_DIRECTORY + \"\u003e\");\n+      o(INODE_DIRECTORY_SECTION_PARENT, e.getParent());\n       for (long id : e.getChildrenList()) {\n-        o(\"child\", id);\n+        o(INODE_DIRECTORY_SECTION_CHILD, id);\n       }\n       for (int refId : e.getRefChildrenList()) {\n-        o(\"refChild\", refId);\n+        o(INODE_DIRECTORY_SECTION_REF_CHILD, refId);\n       }\n-      out.print(\"\u003c/directory\u003e\\n\");\n+      out.print(\"\u003c/\" + INODE_DIRECTORY_SECTION_DIRECTORY + \"\u003e\\n\");\n     }\n-    out.print(\"\u003c/INodeDirectorySection\u003e\\n\");\n+    out.print(\"\u003c/\" + INODE_DIRECTORY_SECTION_NAME + \"\u003e\\n\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeDirectorySection(InputStream in) throws IOException {\n    out.print(\"\u003c\" + INODE_DIRECTORY_SECTION_NAME + \"\u003e\");\n    while (true) {\n      INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n          .parseDelimitedFrom(in);\n      // note that in is a LimitedInputStream\n      if (e \u003d\u003d null) {\n        break;\n      }\n      out.print(\"\u003c\" + INODE_DIRECTORY_SECTION_DIRECTORY + \"\u003e\");\n      o(INODE_DIRECTORY_SECTION_PARENT, e.getParent());\n      for (long id : e.getChildrenList()) {\n        o(INODE_DIRECTORY_SECTION_CHILD, id);\n      }\n      for (int refId : e.getRefChildrenList()) {\n        o(INODE_DIRECTORY_SECTION_REF_CHILD, refId);\n      }\n      out.print(\"\u003c/\" + INODE_DIRECTORY_SECTION_DIRECTORY + \"\u003e\\n\");\n    }\n    out.print(\"\u003c/\" + INODE_DIRECTORY_SECTION_NAME + \"\u003e\\n\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "700b0e4019cf483f7532609711812150b8c44742": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9835. OIV: add ReverseXML processor which reconstructs an fsimage from an XML file (cmccabe)\n",
      "commitDate": "02/03/16 5:56 PM",
      "commitName": "700b0e4019cf483f7532609711812150b8c44742",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "21/10/15 2:58 PM",
      "commitNameOld": "a24c6e84205c684ef864b0fc5301dc07b3578351",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 133.17,
      "commitsBetweenForRepo": 905,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   private void dumpINodeDirectorySection(InputStream in) throws IOException {\n     out.print(\"\u003cINodeDirectorySection\u003e\");\n     while (true) {\n       INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n           .parseDelimitedFrom(in);\n       // note that in is a LimitedInputStream\n       if (e \u003d\u003d null) {\n         break;\n       }\n       out.print(\"\u003cdirectory\u003e\");\n       o(\"parent\", e.getParent());\n       for (long id : e.getChildrenList()) {\n-        o(\"inode\", id);\n+        o(\"child\", id);\n       }\n       for (int refId : e.getRefChildrenList()) {\n-        o(\"inodereference-index\", refId);\n+        o(\"refChild\", refId);\n       }\n       out.print(\"\u003c/directory\u003e\\n\");\n     }\n     out.print(\"\u003c/INodeDirectorySection\u003e\\n\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeDirectorySection(InputStream in) throws IOException {\n    out.print(\"\u003cINodeDirectorySection\u003e\");\n    while (true) {\n      INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n          .parseDelimitedFrom(in);\n      // note that in is a LimitedInputStream\n      if (e \u003d\u003d null) {\n        break;\n      }\n      out.print(\"\u003cdirectory\u003e\");\n      o(\"parent\", e.getParent());\n      for (long id : e.getChildrenList()) {\n        o(\"child\", id);\n      }\n      for (int refId : e.getRefChildrenList()) {\n        o(\"refChild\", refId);\n      }\n      out.print(\"\u003c/directory\u003e\\n\");\n    }\n    out.print(\"\u003c/INodeDirectorySection\u003e\\n\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "2624b20291629b4565ea45590b66f2c38f96df67": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5847. Consolidate INodeReference into a separate section. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567812 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 4:00 PM",
      "commitName": "2624b20291629b4565ea45590b66f2c38f96df67",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "09/02/14 11:18 AM",
      "commitNameOld": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 3.2,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,21 @@\n   private void dumpINodeDirectorySection(InputStream in) throws IOException {\n     out.print(\"\u003cINodeDirectorySection\u003e\");\n     while (true) {\n       INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n           .parseDelimitedFrom(in);\n       // note that in is a LimitedInputStream\n       if (e \u003d\u003d null) {\n         break;\n       }\n       out.print(\"\u003cdirectory\u003e\");\n       o(\"parent\", e.getParent());\n       for (long id : e.getChildrenList()) {\n         o(\"inode\", id);\n       }\n-      for (int i \u003d 0; i \u003c e.getNumOfRef(); i++) {\n-        INodeSection.INodeReference r \u003d INodeSection.INodeReference\n-            .parseDelimitedFrom(in);\n-        dumpINodeReference(r);\n-\n+      for (int refId : e.getRefChildrenList()) {\n+        o(\"inodereference-index\", refId);\n       }\n       out.print(\"\u003c/directory\u003e\\n\");\n     }\n     out.print(\"\u003c/INodeDirectorySection\u003e\\n\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeDirectorySection(InputStream in) throws IOException {\n    out.print(\"\u003cINodeDirectorySection\u003e\");\n    while (true) {\n      INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n          .parseDelimitedFrom(in);\n      // note that in is a LimitedInputStream\n      if (e \u003d\u003d null) {\n        break;\n      }\n      out.print(\"\u003cdirectory\u003e\");\n      o(\"parent\", e.getParent());\n      for (long id : e.getChildrenList()) {\n        o(\"inode\", id);\n      }\n      for (int refId : e.getRefChildrenList()) {\n        o(\"inodereference-index\", refId);\n      }\n      out.print(\"\u003c/directory\u003e\\n\");\n    }\n    out.print(\"\u003c/INodeDirectorySection\u003e\\n\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java",
      "extendedDetails": {}
    },
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,24 @@\n+  private void dumpINodeDirectorySection(InputStream in) throws IOException {\n+    out.print(\"\u003cINodeDirectorySection\u003e\");\n+    while (true) {\n+      INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n+          .parseDelimitedFrom(in);\n+      // note that in is a LimitedInputStream\n+      if (e \u003d\u003d null) {\n+        break;\n+      }\n+      out.print(\"\u003cdirectory\u003e\");\n+      o(\"parent\", e.getParent());\n+      for (long id : e.getChildrenList()) {\n+        o(\"inode\", id);\n+      }\n+      for (int i \u003d 0; i \u003c e.getNumOfRef(); i++) {\n+        INodeSection.INodeReference r \u003d INodeSection.INodeReference\n+            .parseDelimitedFrom(in);\n+        dumpINodeReference(r);\n+\n+      }\n+      out.print(\"\u003c/directory\u003e\\n\");\n+    }\n+    out.print(\"\u003c/INodeDirectorySection\u003e\\n\");\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void dumpINodeDirectorySection(InputStream in) throws IOException {\n    out.print(\"\u003cINodeDirectorySection\u003e\");\n    while (true) {\n      INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n          .parseDelimitedFrom(in);\n      // note that in is a LimitedInputStream\n      if (e \u003d\u003d null) {\n        break;\n      }\n      out.print(\"\u003cdirectory\u003e\");\n      o(\"parent\", e.getParent());\n      for (long id : e.getChildrenList()) {\n        o(\"inode\", id);\n      }\n      for (int i \u003d 0; i \u003c e.getNumOfRef(); i++) {\n        INodeSection.INodeReference r \u003d INodeSection.INodeReference\n            .parseDelimitedFrom(in);\n        dumpINodeReference(r);\n\n      }\n      out.print(\"\u003c/directory\u003e\\n\");\n    }\n    out.print(\"\u003c/INodeDirectorySection\u003e\\n\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java"
    }
  }
}