{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "fetchBlockByteRange",
  "functionId": "fetchBlockByteRange___block-LocatedBlock__start-long__end-long__buf-ByteBuffer__corruptedBlocks-CorruptedBlocks",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 1138,
  "functionEndLine": 1155,
  "numCommitsSeen": 332,
  "timeTaken": 11556,
  "changeHistory": [
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11",
    "401db4fc65140979fe7665983e36905e886df971",
    "be34e85e682880f46eee0310bf00ecc7d39cd5bd",
    "8808779db351fe444388d4acb3094766b5980718",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "bff5999d07e9416a22846c849487e509ede55040",
    "a42bb1cd915abe5dc33eda3c01e8c74c64f35748",
    "caa42adf208bfb5625d1b3ef665fbf334ffcccd9",
    "17db74a1c1972392a5aba48a3e0334dcd6c76487",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
    "c3b236ce5d2ca0905d03d63efe74b9f2c8e27436",
    "90122f25e142ff5ae9e2610b6b8968ac5fee8f79",
    "a31a3a68ead2773f8daa5b3ac1041e271b442789",
    "8373c4134afef0b7fb08a63b44ef0131faacec00",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
    "bbb24fbf5d220fbe137d43651ba3802a9806b1a3",
    "9a4030e0e84a688c12daa21fe9a165808c3eca70",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
    "837e17b2eac1471d93e2eff395272063b265fee7",
    "239b2742d0e80d13c970fd062af4930e672fe903",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "be7dd8333a7e56e732171db0781786987de03195",
    "2ab10e29d9cca5018064be46a40e3c74423615a8",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11": "Ybodychange",
    "401db4fc65140979fe7665983e36905e886df971": "Ymultichange(Yparameterchange,Ybodychange)",
    "be34e85e682880f46eee0310bf00ecc7d39cd5bd": "Ybodychange",
    "8808779db351fe444388d4acb3094766b5980718": "Ymultichange(Yparameterchange,Ybodychange)",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "bff5999d07e9416a22846c849487e509ede55040": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "a42bb1cd915abe5dc33eda3c01e8c74c64f35748": "Ymultichange(Yparameterchange,Ybodychange)",
    "caa42adf208bfb5625d1b3ef665fbf334ffcccd9": "Ybodychange",
    "17db74a1c1972392a5aba48a3e0334dcd6c76487": "Ybodychange",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": "Ybodychange",
    "c3b236ce5d2ca0905d03d63efe74b9f2c8e27436": "Ybodychange",
    "90122f25e142ff5ae9e2610b6b8968ac5fee8f79": "Ybodychange",
    "a31a3a68ead2773f8daa5b3ac1041e271b442789": "Ybodychange",
    "8373c4134afef0b7fb08a63b44ef0131faacec00": "Ybodychange",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": "Ybodychange",
    "bbb24fbf5d220fbe137d43651ba3802a9806b1a3": "Ybodychange",
    "9a4030e0e84a688c12daa21fe9a165808c3eca70": "Ybodychange",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": "Ybodychange",
    "837e17b2eac1471d93e2eff395272063b265fee7": "Ybodychange",
    "239b2742d0e80d13c970fd062af4930e672fe903": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "2ab10e29d9cca5018064be46a40e3c74423615a8": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11708. Positional read will fail if replicas moved to different DNs after stream is opened. Contributed by Vinayakumar B.\n",
      "commitDate": "06/06/17 10:25 PM",
      "commitName": "70fc6746b326b9a913e8bebca5f5afaf01ab9e11",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "03/04/17 8:13 PM",
      "commitNameOld": "6eba79232f36b36e0196163adc8fe4219a6b6bf9",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 64.09,
      "commitsBetweenForRepo": 349,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,18 @@\n   protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n-    block \u003d refreshLocatedBlock(block);\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n+      // Latest block, if refreshed internally\n+      block \u003d addressPair.block;\n       try {\n-        actualGetFromOneDataNode(addressPair, block, start, end,\n-            buf, corruptedBlocks);\n+        actualGetFromOneDataNode(addressPair, start, end, buf,\n+            corruptedBlocks);\n         return;\n       } catch (IOException e) {\n         checkInterrupted(e); // check if the read has been interrupted\n         // Ignore other IOException. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      // Latest block, if refreshed internally\n      block \u003d addressPair.block;\n      try {\n        actualGetFromOneDataNode(addressPair, start, end, buf,\n            corruptedBlocks);\n        return;\n      } catch (IOException e) {\n        checkInterrupted(e); // check if the read has been interrupted\n        // Ignore other IOException. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "401db4fc65140979fe7665983e36905e886df971": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
      "commitDate": "08/09/16 11:54 AM",
      "commitName": "401db4fc65140979fe7665983e36905e886df971",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
          "commitDate": "08/09/16 11:54 AM",
          "commitName": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "24/08/16 6:57 AM",
          "commitNameOld": "793447f79924c97c2b562d5e41fa85adf19673fe",
          "commitAuthorOld": "Kai Zheng",
          "daysBetweenCommits": 15.21,
          "commitsBetweenForRepo": 83,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,17 @@\n   protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n-      byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n+      ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n         actualGetFromOneDataNode(addressPair, block, start, end,\n-            buf, offset, corruptedBlocks);\n+            buf, corruptedBlocks);\n         return;\n       } catch (IOException e) {\n         checkInterrupted(e); // check if the read has been interrupted\n         // Ignore other IOException. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, block, start, end,\n            buf, corruptedBlocks);\n        return;\n      } catch (IOException e) {\n        checkInterrupted(e); // check if the read has been interrupted\n        // Ignore other IOException. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[block-LocatedBlock, start-long, end-long, buf-byte[], offset-int, corruptedBlocks-CorruptedBlocks]",
            "newValue": "[block-LocatedBlock, start-long, end-long, buf-ByteBuffer, corruptedBlocks-CorruptedBlocks]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
          "commitDate": "08/09/16 11:54 AM",
          "commitName": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "24/08/16 6:57 AM",
          "commitNameOld": "793447f79924c97c2b562d5e41fa85adf19673fe",
          "commitAuthorOld": "Kai Zheng",
          "daysBetweenCommits": 15.21,
          "commitsBetweenForRepo": 83,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,17 @@\n   protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n-      byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n+      ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n         actualGetFromOneDataNode(addressPair, block, start, end,\n-            buf, offset, corruptedBlocks);\n+            buf, corruptedBlocks);\n         return;\n       } catch (IOException e) {\n         checkInterrupted(e); // check if the read has been interrupted\n         // Ignore other IOException. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, block, start, end,\n            buf, corruptedBlocks);\n        return;\n      } catch (IOException e) {\n        checkInterrupted(e); // check if the read has been interrupted\n        // Ignore other IOException. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "be34e85e682880f46eee0310bf00ecc7d39cd5bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10468. HDFS read ends up ignoring an interrupt. Contributed by Jing Zhao\n",
      "commitDate": "07/06/16 10:48 AM",
      "commitName": "be34e85e682880f46eee0310bf00ecc7d39cd5bd",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "25/04/16 12:01 PM",
      "commitNameOld": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 42.95,
      "commitsBetweenForRepo": 291,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,17 @@\n   protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n         actualGetFromOneDataNode(addressPair, block, start, end,\n             buf, offset, corruptedBlocks);\n         return;\n       } catch (IOException e) {\n-        // Ignore. Already processed inside the function.\n+        checkInterrupted(e); // check if the read has been interrupted\n+        // Ignore other IOException. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, block, start, end,\n            buf, offset, corruptedBlocks);\n        return;\n      } catch (IOException e) {\n        checkInterrupted(e); // check if the read has been interrupted\n        // Ignore other IOException. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "8808779db351fe444388d4acb3094766b5980718": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9734. Refactoring of checksum failure report related codes. Contributed by Kai Zheng.\n\nChange-Id: Ie69a77e3498a360959f8e213c51fb2b17c28b64a\n",
      "commitDate": "25/02/16 9:55 AM",
      "commitName": "8808779db351fe444388d4acb3094766b5980718",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9734. Refactoring of checksum failure report related codes. Contributed by Kai Zheng.\n\nChange-Id: Ie69a77e3498a360959f8e213c51fb2b17c28b64a\n",
          "commitDate": "25/02/16 9:55 AM",
          "commitName": "8808779db351fe444388d4acb3094766b5980718",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "22/01/16 9:46 AM",
          "commitNameOld": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 34.01,
          "commitsBetweenForRepo": 234,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,16 @@\n   protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n-      byte[] buf, int offset,\n-      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n+      byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n         actualGetFromOneDataNode(addressPair, block, start, end,\n-            buf, offset, corruptedBlockMap);\n+            buf, offset, corruptedBlocks);\n         return;\n       } catch (IOException e) {\n         // Ignore. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, block, start, end,\n            buf, offset, corruptedBlocks);\n        return;\n      } catch (IOException e) {\n        // Ignore. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[block-LocatedBlock, start-long, end-long, buf-byte[], offset-int, corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e]",
            "newValue": "[block-LocatedBlock, start-long, end-long, buf-byte[], offset-int, corruptedBlocks-CorruptedBlocks]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9734. Refactoring of checksum failure report related codes. Contributed by Kai Zheng.\n\nChange-Id: Ie69a77e3498a360959f8e213c51fb2b17c28b64a\n",
          "commitDate": "25/02/16 9:55 AM",
          "commitName": "8808779db351fe444388d4acb3094766b5980718",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "22/01/16 9:46 AM",
          "commitNameOld": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 34.01,
          "commitsBetweenForRepo": 234,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,16 @@\n   protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n-      byte[] buf, int offset,\n-      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n+      byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n         actualGetFromOneDataNode(addressPair, block, start, end,\n-            buf, offset, corruptedBlockMap);\n+            buf, offset, corruptedBlocks);\n         return;\n       } catch (IOException e) {\n         // Ignore. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, block, start, end,\n            buf, offset, corruptedBlocks);\n        return;\n      } catch (IOException e) {\n        // Ignore. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, block, start, end,\n            buf, offset, corruptedBlockMap);\n        return;\n      } catch (IOException e) {\n        // Ignore. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "bff5999d07e9416a22846c849487e509ede55040": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
      "commitDate": "02/07/15 3:41 AM",
      "commitName": "bff5999d07e9416a22846c849487e509ede55040",
      "commitAuthor": "Vinayakumar B",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
          "commitDate": "02/07/15 3:41 AM",
          "commitName": "bff5999d07e9416a22846c849487e509ede55040",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "04/06/15 10:51 AM",
          "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 27.7,
          "commitsBetweenForRepo": 196,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,17 @@\n-  private void fetchBlockByteRange(long blockStartOffset, long start, long end,\n+  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n-    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n+    block \u003d refreshLocatedBlock(block);\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n-        actualGetFromOneDataNode(addressPair, blockStartOffset, start, end,\n+        actualGetFromOneDataNode(addressPair, block, start, end,\n             buf, offset, corruptedBlockMap);\n         return;\n       } catch (IOException e) {\n         // Ignore. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, block, start, end,\n            buf, offset, corruptedBlockMap);\n        return;\n      } catch (IOException e) {\n        // Ignore. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[blockStartOffset-long, start-long, end-long, buf-byte[], offset-int, corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e]",
            "newValue": "[block-LocatedBlock, start-long, end-long, buf-byte[], offset-int, corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
          "commitDate": "02/07/15 3:41 AM",
          "commitName": "bff5999d07e9416a22846c849487e509ede55040",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "04/06/15 10:51 AM",
          "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 27.7,
          "commitsBetweenForRepo": 196,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,17 @@\n-  private void fetchBlockByteRange(long blockStartOffset, long start, long end,\n+  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n-    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n+    block \u003d refreshLocatedBlock(block);\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n-        actualGetFromOneDataNode(addressPair, blockStartOffset, start, end,\n+        actualGetFromOneDataNode(addressPair, block, start, end,\n             buf, offset, corruptedBlockMap);\n         return;\n       } catch (IOException e) {\n         // Ignore. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, block, start, end,\n            buf, offset, corruptedBlockMap);\n        return;\n      } catch (IOException e) {\n        // Ignore. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[protected]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
          "commitDate": "02/07/15 3:41 AM",
          "commitName": "bff5999d07e9416a22846c849487e509ede55040",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "04/06/15 10:51 AM",
          "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 27.7,
          "commitsBetweenForRepo": 196,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,17 @@\n-  private void fetchBlockByteRange(long blockStartOffset, long start, long end,\n+  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n-    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n+    block \u003d refreshLocatedBlock(block);\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n-        actualGetFromOneDataNode(addressPair, blockStartOffset, start, end,\n+        actualGetFromOneDataNode(addressPair, block, start, end,\n             buf, offset, corruptedBlockMap);\n         return;\n       } catch (IOException e) {\n         // Ignore. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, block, start, end,\n            buf, offset, corruptedBlockMap);\n        return;\n      } catch (IOException e) {\n        // Ignore. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "a42bb1cd915abe5dc33eda3c01e8c74c64f35748": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8076. Code cleanup for DFSInputStream: use offset instead of LocatedBlock when possible. Contributed by Zhe Zhang.\n",
      "commitDate": "08/04/15 3:41 PM",
      "commitName": "a42bb1cd915abe5dc33eda3c01e8c74c64f35748",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8076. Code cleanup for DFSInputStream: use offset instead of LocatedBlock when possible. Contributed by Zhe Zhang.\n",
          "commitDate": "08/04/15 3:41 PM",
          "commitName": "a42bb1cd915abe5dc33eda3c01e8c74c64f35748",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "25/02/15 1:30 PM",
          "commitNameOld": "caa42adf208bfb5625d1b3ef665fbf334ffcccd9",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 42.05,
          "commitsBetweenForRepo": 367,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,17 @@\n-  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n+  private void fetchBlockByteRange(long blockStartOffset, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n-    block \u003d getBlockAt(block.getStartOffset());\n+    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n-        actualGetFromOneDataNode(addressPair, block, start, end, buf, offset,\n-            corruptedBlockMap);\n+        actualGetFromOneDataNode(addressPair, blockStartOffset, start, end,\n+            buf, offset, corruptedBlockMap);\n         return;\n       } catch (IOException e) {\n         // Ignore. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void fetchBlockByteRange(long blockStartOffset, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, blockStartOffset, start, end,\n            buf, offset, corruptedBlockMap);\n        return;\n      } catch (IOException e) {\n        // Ignore. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[block-LocatedBlock, start-long, end-long, buf-byte[], offset-int, corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e]",
            "newValue": "[blockStartOffset-long, start-long, end-long, buf-byte[], offset-int, corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8076. Code cleanup for DFSInputStream: use offset instead of LocatedBlock when possible. Contributed by Zhe Zhang.\n",
          "commitDate": "08/04/15 3:41 PM",
          "commitName": "a42bb1cd915abe5dc33eda3c01e8c74c64f35748",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "25/02/15 1:30 PM",
          "commitNameOld": "caa42adf208bfb5625d1b3ef665fbf334ffcccd9",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 42.05,
          "commitsBetweenForRepo": 367,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,17 @@\n-  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n+  private void fetchBlockByteRange(long blockStartOffset, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n-    block \u003d getBlockAt(block.getStartOffset());\n+    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n-        actualGetFromOneDataNode(addressPair, block, start, end, buf, offset,\n-            corruptedBlockMap);\n+        actualGetFromOneDataNode(addressPair, blockStartOffset, start, end,\n+            buf, offset, corruptedBlockMap);\n         return;\n       } catch (IOException e) {\n         // Ignore. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void fetchBlockByteRange(long blockStartOffset, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, blockStartOffset, start, end,\n            buf, offset, corruptedBlockMap);\n        return;\n      } catch (IOException e) {\n        // Ignore. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "caa42adf208bfb5625d1b3ef665fbf334ffcccd9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7495. Remove updatePosition argument from DFSInputStream#getBlockAt() (cmccabe)\n",
      "commitDate": "25/02/15 1:30 PM",
      "commitName": "caa42adf208bfb5625d1b3ef665fbf334ffcccd9",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "21/02/15 3:38 PM",
      "commitNameOld": "8b465b4b8caed31ca9daeaae108f9a868a30a455",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 3.91,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,17 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n-    block \u003d getBlockAt(block.getStartOffset(), false);\n+    block \u003d getBlockAt(block.getStartOffset());\n     while (true) {\n       DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n         actualGetFromOneDataNode(addressPair, block, start, end, buf, offset,\n             corruptedBlockMap);\n         return;\n       } catch (IOException e) {\n         // Ignore. Already processed inside the function.\n         // Loop through to try the next node.\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    block \u003d getBlockAt(block.getStartOffset());\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, block, start, end, buf, offset,\n            corruptedBlockMap);\n        return;\n      } catch (IOException e) {\n        // Ignore. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "17db74a1c1972392a5aba48a3e0334dcd6c76487": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5776 Support \u0027hedged\u0027 reads in DFSClient\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1571466 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/02/14 2:34 PM",
      "commitName": "17db74a1c1972392a5aba48a3e0334dcd6c76487",
      "commitAuthor": "Michael Stack",
      "commitDateOld": "12/02/14 11:08 AM",
      "commitNameOld": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 12.14,
      "commitsBetweenForRepo": 95,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,17 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n-    //\n-    // Connect to best DataNode for desired Block, with potential offset\n-    //\n-    int refetchToken \u003d 1; // only need to get a new access token once\n-    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n-    \n+    block \u003d getBlockAt(block.getStartOffset(), false);\n     while (true) {\n-      // cached block locations may have been updated by chooseDataNode()\n-      // or fetchBlockAt(). Always get the latest list of locations at the \n-      // start of the loop.\n-      CachingStrategy curCachingStrategy;\n-      boolean allowShortCircuitLocalReads;\n-      synchronized (this) {\n-        block \u003d getBlockAt(block.getStartOffset(), false);\n-        curCachingStrategy \u003d cachingStrategy;\n-        allowShortCircuitLocalReads \u003d !shortCircuitForbidden();\n-      }\n-      DNAddrPair retval \u003d chooseDataNode(block);\n-      DatanodeInfo chosenNode \u003d retval.info;\n-      InetSocketAddress targetAddr \u003d retval.addr;\n-      BlockReader reader \u003d null;\n-          \n+      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n       try {\n-        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n-        int len \u003d (int) (end - start + 1);\n-        reader \u003d new BlockReaderFactory(dfsClient.getConf()).\n-            setInetSocketAddress(targetAddr).\n-            setRemotePeerFactory(dfsClient).\n-            setDatanodeInfo(chosenNode).\n-            setFileName(src).\n-            setBlock(block.getBlock()).\n-            setBlockToken(blockToken).\n-            setStartOffset(start).\n-            setVerifyChecksum(verifyChecksum).\n-            setClientName(dfsClient.clientName).\n-            setLength(len).\n-            setCachingStrategy(curCachingStrategy).\n-            setAllowShortCircuitLocalReads(allowShortCircuitLocalReads).\n-            setClientCacheContext(dfsClient.getClientContext()).\n-            setUserGroupInformation(dfsClient.ugi).\n-            setConfiguration(dfsClient.getConfiguration()).\n-            build();\n-        int nread \u003d reader.readAll(buf, offset, len);\n-        if (nread !\u003d len) {\n-          throw new IOException(\"truncated return from reader.read(): \" +\n-                                \"excpected \" + len + \", got \" + nread);\n-        }\n+        actualGetFromOneDataNode(addressPair, block, start, end, buf, offset,\n+            corruptedBlockMap);\n         return;\n-      } catch (ChecksumException e) {\n-        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n-                 src + \" at \" + block.getBlock() + \":\" + \n-                 e.getPos() + \" from \" + chosenNode);\n-        // we want to remember what we have tried\n-        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n       } catch (IOException e) {\n-        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n-          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n-              + \"encryption key was invalid when connecting to \" + targetAddr\n-              + \" : \" + e);\n-          // The encryption key used is invalid.\n-          refetchEncryptionKey--;\n-          dfsClient.clearDataEncryptionKey();\n-          continue;\n-        } else if (refetchToken \u003e 0 \u0026\u0026 tokenRefetchNeeded(e, targetAddr)) {\n-          refetchToken--;\n-          fetchBlockAt(block.getStartOffset());\n-          continue;\n-        } else {\n-          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n-              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n-          if (DFSClient.LOG.isDebugEnabled()) {\n-            DFSClient.LOG.debug(\"Connection failure \", e);\n-          }\n-        }\n-      } finally {\n-        if (reader !\u003d null) {\n-          reader.close();\n-        }\n+        // Ignore. Already processed inside the function.\n+        // Loop through to try the next node.\n       }\n-      // Put chosen node into dead list, continue\n-      addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    block \u003d getBlockAt(block.getStartOffset(), false);\n    while (true) {\n      DNAddrPair addressPair \u003d chooseDataNode(block, null);\n      try {\n        actualGetFromOneDataNode(addressPair, block, start, end, buf, offset,\n            corruptedBlockMap);\n        return;\n      } catch (IOException e) {\n        // Ignore. Already processed inside the function.\n        // Loop through to try the next node.\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 11:08 AM",
      "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "06/02/14 7:45 AM",
      "commitNameOld": "ab96a0838dafbfea77382135914feadbfd03cf53",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 6.14,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,87 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       CachingStrategy curCachingStrategy;\n+      boolean allowShortCircuitLocalReads;\n       synchronized (this) {\n         block \u003d getBlockAt(block.getStartOffset(), false);\n         curCachingStrategy \u003d cachingStrategy;\n+        allowShortCircuitLocalReads \u003d !shortCircuitForbidden();\n       }\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n-            \n         int len \u003d (int) (end - start + 1);\n-        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n-            blockToken, start, len, buffersize, verifyChecksum,\n-            dfsClient.clientName, curCachingStrategy);\n+        reader \u003d new BlockReaderFactory(dfsClient.getConf()).\n+            setInetSocketAddress(targetAddr).\n+            setRemotePeerFactory(dfsClient).\n+            setDatanodeInfo(chosenNode).\n+            setFileName(src).\n+            setBlock(block.getBlock()).\n+            setBlockToken(blockToken).\n+            setStartOffset(start).\n+            setVerifyChecksum(verifyChecksum).\n+            setClientName(dfsClient.clientName).\n+            setLength(len).\n+            setCachingStrategy(curCachingStrategy).\n+            setAllowShortCircuitLocalReads(allowShortCircuitLocalReads).\n+            setClientCacheContext(dfsClient.getClientContext()).\n+            setUserGroupInformation(dfsClient.ugi).\n+            setConfiguration(dfsClient.getConfiguration()).\n+            build();\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n-      } catch (AccessControlException ex) {\n-        DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n-        dfsClient.disableLegacyBlockReaderLocal();\n-        continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n               + \"encryption key was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           continue;\n         } else if (refetchToken \u003e 0 \u0026\u0026 tokenRefetchNeeded(e, targetAddr)) {\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n           reader.close();\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      CachingStrategy curCachingStrategy;\n      boolean allowShortCircuitLocalReads;\n      synchronized (this) {\n        block \u003d getBlockAt(block.getStartOffset(), false);\n        curCachingStrategy \u003d cachingStrategy;\n        allowShortCircuitLocalReads \u003d !shortCircuitForbidden();\n      }\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n        int len \u003d (int) (end - start + 1);\n        reader \u003d new BlockReaderFactory(dfsClient.getConf()).\n            setInetSocketAddress(targetAddr).\n            setRemotePeerFactory(dfsClient).\n            setDatanodeInfo(chosenNode).\n            setFileName(src).\n            setBlock(block.getBlock()).\n            setBlockToken(blockToken).\n            setStartOffset(start).\n            setVerifyChecksum(verifyChecksum).\n            setClientName(dfsClient.clientName).\n            setLength(len).\n            setCachingStrategy(curCachingStrategy).\n            setAllowShortCircuitLocalReads(allowShortCircuitLocalReads).\n            setClientCacheContext(dfsClient.getClientContext()).\n            setUserGroupInformation(dfsClient.ugi).\n            setConfiguration(dfsClient.getConfiguration()).\n            build();\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          continue;\n        } else if (refetchToken \u003e 0 \u0026\u0026 tokenRefetchNeeded(e, targetAddr)) {\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close();\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "c3b236ce5d2ca0905d03d63efe74b9f2c8e27436": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5766. In DFSInputStream, do not add datanode to deadNodes after InvalidEncryptionKeyException in fetchBlockByteRange (Liang Xie via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1558536 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/01/14 11:23 AM",
      "commitName": "c3b236ce5d2ca0905d03d63efe74b9f2c8e27436",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "31/12/13 8:24 AM",
      "commitNameOld": "97e881b955b91b07ac7b6fbc0718f0ecf009dc84",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 15.12,
      "commitsBetweenForRepo": 68,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,76 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       CachingStrategy curCachingStrategy;\n       synchronized (this) {\n         block \u003d getBlockAt(block.getStartOffset(), false);\n         curCachingStrategy \u003d cachingStrategy;\n       }\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n             dfsClient.clientName, curCachingStrategy);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n       } catch (AccessControlException ex) {\n         DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n         dfsClient.disableLegacyBlockReaderLocal();\n         continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n               + \"encryption key was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n+          continue;\n         } else if (refetchToken \u003e 0 \u0026\u0026 tokenRefetchNeeded(e, targetAddr)) {\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n           reader.close();\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      CachingStrategy curCachingStrategy;\n      synchronized (this) {\n        block \u003d getBlockAt(block.getStartOffset(), false);\n        curCachingStrategy \u003d cachingStrategy;\n      }\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName, curCachingStrategy);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n        dfsClient.disableLegacyBlockReaderLocal();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          continue;\n        } else if (refetchToken \u003e 0 \u0026\u0026 tokenRefetchNeeded(e, targetAddr)) {\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close();\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "90122f25e142ff5ae9e2610b6b8968ac5fee8f79": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5676. fix inconsistent synchronization of CachingStrategy (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1552162 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/12/13 3:29 PM",
      "commitName": "90122f25e142ff5ae9e2610b6b8968ac5fee8f79",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "17/12/13 12:57 PM",
      "commitNameOld": "124e507674c0d396f8494585e64226957199097b",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 1.11,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,75 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n-      block \u003d getBlockAt(block.getStartOffset(), false);\n+      CachingStrategy curCachingStrategy;\n+      synchronized (this) {\n+        block \u003d getBlockAt(block.getStartOffset(), false);\n+        curCachingStrategy \u003d cachingStrategy;\n+      }\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n-            dfsClient.clientName);\n+            dfsClient.clientName, curCachingStrategy);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n       } catch (AccessControlException ex) {\n         DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n         dfsClient.disableLegacyBlockReaderLocal();\n         continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n               + \"encryption key was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n         } else if (refetchToken \u003e 0 \u0026\u0026 tokenRefetchNeeded(e, targetAddr)) {\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n           reader.close();\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      CachingStrategy curCachingStrategy;\n      synchronized (this) {\n        block \u003d getBlockAt(block.getStartOffset(), false);\n        curCachingStrategy \u003d cachingStrategy;\n      }\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName, curCachingStrategy);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n        dfsClient.disableLegacyBlockReaderLocal();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n        } else if (refetchToken \u003e 0 \u0026\u0026 tokenRefetchNeeded(e, targetAddr)) {\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close();\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "a31a3a68ead2773f8daa5b3ac1041e271b442789": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5652. Refactor invalid block token exception handling in DFSInputStream. (Liang Xie via junping_du)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1550620 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/12/13 6:47 PM",
      "commitName": "a31a3a68ead2773f8daa5b3ac1041e271b442789",
      "commitAuthor": "Junping Du",
      "commitDateOld": "11/12/13 10:51 PM",
      "commitNameOld": "c656562556d98323396043421b30cef800c850fd",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.83,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,71 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       block \u003d getBlockAt(block.getStartOffset(), false);\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n             dfsClient.clientName);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n       } catch (AccessControlException ex) {\n         DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n         dfsClient.disableLegacyBlockReaderLocal();\n         continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n               + \"encryption key was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n-        } else if ((e instanceof InvalidBlockTokenException || e instanceof InvalidToken)\n-            \u0026\u0026 refetchToken \u003e 0) {\n-          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n-              + \"access token was invalid when connecting to \" + targetAddr\n-              + \" : \" + e);\n+        } else if (refetchToken \u003e 0 \u0026\u0026 tokenRefetchNeeded(e, targetAddr)) {\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n           reader.close();\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n        dfsClient.disableLegacyBlockReaderLocal();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n        } else if (refetchToken \u003e 0 \u0026\u0026 tokenRefetchNeeded(e, targetAddr)) {\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close();\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "8373c4134afef0b7fb08a63b44ef0131faacec00": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5637. Try to refeatchToken while local read InvalidToken occurred. (Liang Xie via junping_du)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1550335 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/13 7:04 PM",
      "commitName": "8373c4134afef0b7fb08a63b44ef0131faacec00",
      "commitAuthor": "Junping Du",
      "commitDateOld": "20/11/13 6:43 AM",
      "commitNameOld": "f26d2adbf98890cfe350c17241f5049b89a11e3c",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 21.51,
      "commitsBetweenForRepo": 113,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,75 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       block \u003d getBlockAt(block.getStartOffset(), false);\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n             dfsClient.clientName);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n       } catch (AccessControlException ex) {\n         DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n         dfsClient.disableLegacyBlockReaderLocal();\n         continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n               + \"encryption key was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n-        } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n+        } else if ((e instanceof InvalidBlockTokenException || e instanceof InvalidToken)\n+            \u0026\u0026 refetchToken \u003e 0) {\n           DFSClient.LOG.info(\"Will get a new access token and retry, \"\n               + \"access token was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n           reader.close();\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n        dfsClient.disableLegacyBlockReaderLocal();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n        } else if ((e instanceof InvalidBlockTokenException || e instanceof InvalidToken)\n            \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close();\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4661. A few little code cleanups of some HDFS-347-related code. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1480839 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/13 5:03 PM",
      "commitName": "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "29/03/13 2:33 PM",
      "commitNameOld": "bbb24fbf5d220fbe137d43651ba3802a9806b1a3",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 41.1,
      "commitsBetweenForRepo": 266,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,74 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       block \u003d getBlockAt(block.getStartOffset(), false);\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n             dfsClient.clientName);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n       } catch (AccessControlException ex) {\n         DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n         dfsClient.disableLegacyBlockReaderLocal();\n         continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n               + \"encryption key was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n         } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n           DFSClient.LOG.info(\"Will get a new access token and retry, \"\n               + \"access token was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n-          reader.close(peerCache, fileInputStreamCache);\n+          reader.close();\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n        dfsClient.disableLegacyBlockReaderLocal();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n        } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close();\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "bbb24fbf5d220fbe137d43651ba3802a9806b1a3": {
      "type": "Ybodychange",
      "commitMessage": "Merge trunk into branch.\n\nConflicts resolved:\nC       hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestShortCircuitLocalRead.java\n!     C hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java\n\n(thanks to Colin for help resolving)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1462652 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/03/13 2:33 PM",
      "commitName": "bbb24fbf5d220fbe137d43651ba3802a9806b1a3",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "27/03/13 12:28 PM",
      "commitNameOld": "694a6721316aea14c1244447974231abc8dff0cb",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 2.09,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,74 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       block \u003d getBlockAt(block.getStartOffset(), false);\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n             dfsClient.clientName);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n+      } catch (AccessControlException ex) {\n+        DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n+        dfsClient.disableLegacyBlockReaderLocal();\n+        continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n               + \"encryption key was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n         } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n           DFSClient.LOG.info(\"Will get a new access token and retry, \"\n               + \"access token was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n           reader.close(peerCache, fileInputStreamCache);\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \" + ex);\n        dfsClient.disableLegacyBlockReaderLocal();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n        } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close(peerCache, fileInputStreamCache);\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "9a4030e0e84a688c12daa21fe9a165808c3eca70": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4356. BlockReaderLocal should use passed file descriptors rather than paths. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1432335 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/01/13 3:52 PM",
      "commitName": "9a4030e0e84a688c12daa21fe9a165808c3eca70",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "09/01/13 1:34 PM",
      "commitNameOld": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 2.1,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,70 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       block \u003d getBlockAt(block.getStartOffset(), false);\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n             dfsClient.clientName);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n-      } catch (AccessControlException ex) {\n-        DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n-        dfsClient.disableShortCircuit();\n-        continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n               + \"encryption key was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n         } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n           DFSClient.LOG.info(\"Will get a new access token and retry, \"\n               + \"access token was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n-          reader.close(peerCache);\n+          reader.close(peerCache, fileInputStreamCache);\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n        } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close(peerCache, fileInputStreamCache);\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:34 PM",
      "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "08/01/13 6:41 PM",
      "commitNameOld": "fab2cbc2c1fa7b592e27a186411dcc4a67ea2bc2",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,74 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       block \u003d getBlockAt(block.getStartOffset(), false);\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n             dfsClient.clientName);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n       } catch (AccessControlException ex) {\n         DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n         dfsClient.disableShortCircuit();\n         continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n               + \"encryption key was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n         } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n           DFSClient.LOG.info(\"Will get a new access token and retry, \"\n               + \"access token was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n-          closeBlockReader(reader);\n+          reader.close(peerCache);\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n        dfsClient.disableShortCircuit();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n        } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close(peerCache);\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "837e17b2eac1471d93e2eff395272063b265fee7": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 6:39 PM",
      "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/01/13 12:44 PM",
      "commitNameOld": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.25,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,74 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       block \u003d getBlockAt(block.getStartOffset(), false);\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n             dfsClient.clientName);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n       } catch (AccessControlException ex) {\n         DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n         dfsClient.disableShortCircuit();\n         continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n               + \"encryption key was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n         } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n           DFSClient.LOG.info(\"Will get a new access token and retry, \"\n               + \"access token was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n-          reader.close(peerCache);\n+          closeBlockReader(reader);\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n        dfsClient.disableShortCircuit();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n        } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          closeBlockReader(reader);\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "239b2742d0e80d13c970fd062af4930e672fe903": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 12:44 PM",
      "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "03/01/13 10:59 PM",
      "commitNameOld": "32052a1e3a8007b5348dc42415861aeb859ebc5a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.57,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,74 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       block \u003d getBlockAt(block.getStartOffset(), false);\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n             dfsClient.clientName);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n       } catch (AccessControlException ex) {\n         DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n         dfsClient.disableShortCircuit();\n         continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n               + \"encryption key was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n         } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n           DFSClient.LOG.info(\"Will get a new access token and retry, \"\n               + \"access token was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n-          closeBlockReader(reader);\n+          reader.close(peerCache);\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n        dfsClient.disableShortCircuit();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n        } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          reader.close(peerCache);\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "06/07/12 11:59 AM",
      "commitNameOld": "fb95fce24056c0b0aa5b77683c684fe1b68c4f76",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 31.9,
      "commitsBetweenForRepo": 179,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,74 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n+    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       block \u003d getBlockAt(block.getStartOffset(), false);\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n             dfsClient.clientName);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n       } catch (AccessControlException ex) {\n         DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n         dfsClient.disableShortCircuit();\n         continue;\n       } catch (IOException e) {\n-        if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n+        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n+          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n+              + \"encryption key was invalid when connecting to \" + targetAddr\n+              + \" : \" + e);\n+          // The encryption key used is invalid.\n+          refetchEncryptionKey--;\n+          dfsClient.clearDataEncryptionKey();\n+        } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n           DFSClient.LOG.info(\"Will get a new access token and retry, \"\n               + \"access token was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n           closeBlockReader(reader);\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    int refetchEncryptionKey \u003d 1; // only need to get a new encryption key once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n        dfsClient.disableShortCircuit();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n              + \"encryption key was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n        } else if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          closeBlockReader(reader);\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "21/03/12 10:30 AM",
      "commitNameOld": "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 11.2,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,66 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       block \u003d getBlockAt(block.getStartOffset(), false);\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n         reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n             blockToken, start, len, buffersize, verifyChecksum,\n             dfsClient.clientName);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n-                 e.getPos() + \" from \" + chosenNode.getName());\n+                 e.getPos() + \" from \" + chosenNode);\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n       } catch (AccessControlException ex) {\n         DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n         dfsClient.disableShortCircuit();\n         continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n           DFSClient.LOG.info(\"Will get a new access token and retry, \"\n               + \"access token was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n           closeBlockReader(reader);\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode);\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n        dfsClient.disableShortCircuit();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          closeBlockReader(reader);\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "2ab10e29d9cca5018064be46a40e3c74423615a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2246. Enable reading a block directly from local file system for a client on the same node as the block file.  Contributed by Andrew Purtell, Suresh Srinivas and Jitendra Nath Pandey\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1204792 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/11 6:57 PM",
      "commitName": "2ab10e29d9cca5018064be46a40e3c74423615a8",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "18/11/11 6:34 PM",
      "commitNameOld": "b7cd8c0f865e88e40eee75fd2690b1fdc4155071",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 3.02,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,66 @@\n   private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n       byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     //\n     // Connect to best DataNode for desired Block, with potential offset\n     //\n     int refetchToken \u003d 1; // only need to get a new access token once\n     \n     while (true) {\n       // cached block locations may have been updated by chooseDataNode()\n       // or fetchBlockAt(). Always get the latest list of locations at the \n       // start of the loop.\n       block \u003d getBlockAt(block.getStartOffset(), false);\n       DNAddrPair retval \u003d chooseDataNode(block);\n       DatanodeInfo chosenNode \u003d retval.info;\n       InetSocketAddress targetAddr \u003d retval.addr;\n       BlockReader reader \u003d null;\n           \n       try {\n         Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n             \n         int len \u003d (int) (end - start + 1);\n-\n-        reader \u003d getBlockReader(targetAddr, src,\n-                                block.getBlock(),\n-                                blockToken,\n-                                start, len, buffersize,\n-                                verifyChecksum, dfsClient.clientName);\n+        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n+            blockToken, start, len, buffersize, verifyChecksum,\n+            dfsClient.clientName);\n         int nread \u003d reader.readAll(buf, offset, len);\n         if (nread !\u003d len) {\n           throw new IOException(\"truncated return from reader.read(): \" +\n                                 \"excpected \" + len + \", got \" + nread);\n         }\n         return;\n       } catch (ChecksumException e) {\n         DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                  src + \" at \" + block.getBlock() + \":\" + \n                  e.getPos() + \" from \" + chosenNode.getName());\n         // we want to remember what we have tried\n         addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n+      } catch (AccessControlException ex) {\n+        DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n+        dfsClient.disableShortCircuit();\n+        continue;\n       } catch (IOException e) {\n         if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n           DFSClient.LOG.info(\"Will get a new access token and retry, \"\n               + \"access token was invalid when connecting to \" + targetAddr\n               + \" : \" + e);\n           refetchToken--;\n           fetchBlockAt(block.getStartOffset());\n           continue;\n         } else {\n           DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n               \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Connection failure \", e);\n           }\n         }\n       } finally {\n         if (reader !\u003d null) {\n           closeBlockReader(reader);\n         }\n       }\n       // Put chosen node into dead list, continue\n       addToDeadNodes(chosenNode);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n        reader \u003d getBlockReader(targetAddr, chosenNode, src, block.getBlock(),\n            blockToken, start, len, buffersize, verifyChecksum,\n            dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode.getName());\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (AccessControlException ex) {\n        DFSClient.LOG.warn(\"Short circuit access failed \", ex);\n        dfsClient.disableShortCircuit();\n        continue;\n      } catch (IOException e) {\n        if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          closeBlockReader(reader);\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n\n        reader \u003d getBlockReader(targetAddr, src,\n                                block.getBlock(),\n                                blockToken,\n                                start, len, buffersize,\n                                verifyChecksum, dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode.getName());\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (IOException e) {\n        if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          closeBlockReader(reader);\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n\n        reader \u003d getBlockReader(targetAddr, src,\n                                block.getBlock(),\n                                blockToken,\n                                start, len, buffersize,\n                                verifyChecksum, dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode.getName());\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (IOException e) {\n        if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          closeBlockReader(reader);\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,65 @@\n+  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n+      byte[] buf, int offset,\n+      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n+      throws IOException {\n+    //\n+    // Connect to best DataNode for desired Block, with potential offset\n+    //\n+    int refetchToken \u003d 1; // only need to get a new access token once\n+    \n+    while (true) {\n+      // cached block locations may have been updated by chooseDataNode()\n+      // or fetchBlockAt(). Always get the latest list of locations at the \n+      // start of the loop.\n+      block \u003d getBlockAt(block.getStartOffset(), false);\n+      DNAddrPair retval \u003d chooseDataNode(block);\n+      DatanodeInfo chosenNode \u003d retval.info;\n+      InetSocketAddress targetAddr \u003d retval.addr;\n+      BlockReader reader \u003d null;\n+          \n+      try {\n+        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n+            \n+        int len \u003d (int) (end - start + 1);\n+\n+        reader \u003d getBlockReader(targetAddr, src,\n+                                block.getBlock(),\n+                                blockToken,\n+                                start, len, buffersize,\n+                                verifyChecksum, dfsClient.clientName);\n+        int nread \u003d reader.readAll(buf, offset, len);\n+        if (nread !\u003d len) {\n+          throw new IOException(\"truncated return from reader.read(): \" +\n+                                \"excpected \" + len + \", got \" + nread);\n+        }\n+        return;\n+      } catch (ChecksumException e) {\n+        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n+                 src + \" at \" + block.getBlock() + \":\" + \n+                 e.getPos() + \" from \" + chosenNode.getName());\n+        // we want to remember what we have tried\n+        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n+      } catch (IOException e) {\n+        if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n+          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n+              + \"access token was invalid when connecting to \" + targetAddr\n+              + \" : \" + e);\n+          refetchToken--;\n+          fetchBlockAt(block.getStartOffset());\n+          continue;\n+        } else {\n+          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n+              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n+          if (DFSClient.LOG.isDebugEnabled()) {\n+            DFSClient.LOG.debug(\"Connection failure \", e);\n+          }\n+        }\n+      } finally {\n+        if (reader !\u003d null) {\n+          closeBlockReader(reader);\n+        }\n+      }\n+      // Put chosen node into dead list, continue\n+      addToDeadNodes(chosenNode);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void fetchBlockByteRange(LocatedBlock block, long start, long end,\n      byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    //\n    // Connect to best DataNode for desired Block, with potential offset\n    //\n    int refetchToken \u003d 1; // only need to get a new access token once\n    \n    while (true) {\n      // cached block locations may have been updated by chooseDataNode()\n      // or fetchBlockAt(). Always get the latest list of locations at the \n      // start of the loop.\n      block \u003d getBlockAt(block.getStartOffset(), false);\n      DNAddrPair retval \u003d chooseDataNode(block);\n      DatanodeInfo chosenNode \u003d retval.info;\n      InetSocketAddress targetAddr \u003d retval.addr;\n      BlockReader reader \u003d null;\n          \n      try {\n        Token\u003cBlockTokenIdentifier\u003e blockToken \u003d block.getBlockToken();\n            \n        int len \u003d (int) (end - start + 1);\n\n        reader \u003d getBlockReader(targetAddr, src,\n                                block.getBlock(),\n                                blockToken,\n                                start, len, buffersize,\n                                verifyChecksum, dfsClient.clientName);\n        int nread \u003d reader.readAll(buf, offset, len);\n        if (nread !\u003d len) {\n          throw new IOException(\"truncated return from reader.read(): \" +\n                                \"excpected \" + len + \", got \" + nread);\n        }\n        return;\n      } catch (ChecksumException e) {\n        DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +\n                 src + \" at \" + block.getBlock() + \":\" + \n                 e.getPos() + \" from \" + chosenNode.getName());\n        // we want to remember what we have tried\n        addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);\n      } catch (IOException e) {\n        if (e instanceof InvalidBlockTokenException \u0026\u0026 refetchToken \u003e 0) {\n          DFSClient.LOG.info(\"Will get a new access token and retry, \"\n              + \"access token was invalid when connecting to \" + targetAddr\n              + \" : \" + e);\n          refetchToken--;\n          fetchBlockAt(block.getStartOffset());\n          continue;\n        } else {\n          DFSClient.LOG.warn(\"Failed to connect to \" + targetAddr + \n              \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e);\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Connection failure \", e);\n          }\n        }\n      } finally {\n        if (reader !\u003d null) {\n          closeBlockReader(reader);\n        }\n      }\n      // Put chosen node into dead list, continue\n      addToDeadNodes(chosenNode);\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}