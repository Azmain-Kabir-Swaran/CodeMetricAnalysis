{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "refetchLocations",
  "functionId": "refetchLocations___block-LocatedBlock__ignoredNodes-Collection__DatanodeInfo__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 994,
  "functionEndLine": 1043,
  "numCommitsSeen": 49,
  "timeTaken": 1670,
  "changeHistory": [
    "b3119b9ab60a19d624db476c4e1c53410870c7a6",
    "b6bfb2fcb2391d51b8de97c01c1290880779132e"
  ],
  "changeHistoryShort": {
    "b3119b9ab60a19d624db476c4e1c53410870c7a6": "Ybodychange",
    "b6bfb2fcb2391d51b8de97c01c1290880779132e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b3119b9ab60a19d624db476c4e1c53410870c7a6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14648. Implement DeadNodeDetector basic model. Contributed by Lisheng Sun.\n",
      "commitDate": "15/11/19 7:32 PM",
      "commitName": "b3119b9ab60a19d624db476c4e1c53410870c7a6",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "06/11/19 5:58 AM",
      "commitNameOld": "c36014165c212b26d75268ee3659aa2cadcff349",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 9.57,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,50 @@\n   private LocatedBlock refetchLocations(LocatedBlock block,\n       Collection\u003cDatanodeInfo\u003e ignoredNodes) throws IOException {\n     String errMsg \u003d getBestNodeDNAddrPairErrorString(block.getLocations(),\n-        deadNodes, ignoredNodes);\n+            dfsClient.getDeadNodes(this), ignoredNodes);\n     String blockInfo \u003d block.getBlock() + \" file\u003d\" + src;\n     if (failures \u003e\u003d dfsClient.getConf().getMaxBlockAcquireFailures()) {\n       String description \u003d \"Could not obtain block: \" + blockInfo;\n       DFSClient.LOG.warn(description + errMsg\n           + \". Throwing a BlockMissingException\");\n       throw new BlockMissingException(src, description,\n           block.getStartOffset());\n     }\n \n     DatanodeInfo[] nodes \u003d block.getLocations();\n     if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n       DFSClient.LOG.info(\"No node available for \" + blockInfo);\n     }\n     DFSClient.LOG.info(\"Could not obtain \" + block.getBlock()\n         + \" from any node: \" + errMsg\n         + \". Will get new block locations from namenode and retry...\");\n     try {\n       // Introducing a random factor to the wait time before another retry.\n       // The wait time is dependent on # of failures and a random factor.\n       // At the first time of getting a BlockMissingException, the wait time\n       // is a random number between 0..3000 ms. If the first retry\n       // still fails, we will wait 3000 ms grace period before the 2nd retry.\n       // Also at the second retry, the waiting window is expanded to 6000 ms\n       // alleviating the request rate from the server. Similarly the 3rd retry\n       // will wait 6000ms grace period before retry and the waiting window is\n       // expanded to 9000ms.\n       final int timeWindow \u003d dfsClient.getConf().getTimeWindow();\n       // grace period for the last round of attempt\n       double waitTime \u003d timeWindow * failures +\n           // expanding time window for each failure\n           timeWindow * (failures + 1) *\n           ThreadLocalRandom.current().nextDouble();\n       DFSClient.LOG.warn(\"DFS chooseDataNode: got # \" + (failures + 1) +\n           \" IOException, will wait for \" + waitTime + \" msec.\");\n       Thread.sleep((long)waitTime);\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       throw new InterruptedIOException(\n           \"Interrupted while choosing DataNode for read.\");\n     }\n-    deadNodes.clear(); //2nd option is to remove only nodes[blockId]\n+    clearLocalDeadNodes(); //2nd option is to remove only nodes[blockId]\n     openInfo(true);\n     block \u003d refreshLocatedBlock(block);\n     failures++;\n     return block;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private LocatedBlock refetchLocations(LocatedBlock block,\n      Collection\u003cDatanodeInfo\u003e ignoredNodes) throws IOException {\n    String errMsg \u003d getBestNodeDNAddrPairErrorString(block.getLocations(),\n            dfsClient.getDeadNodes(this), ignoredNodes);\n    String blockInfo \u003d block.getBlock() + \" file\u003d\" + src;\n    if (failures \u003e\u003d dfsClient.getConf().getMaxBlockAcquireFailures()) {\n      String description \u003d \"Could not obtain block: \" + blockInfo;\n      DFSClient.LOG.warn(description + errMsg\n          + \". Throwing a BlockMissingException\");\n      throw new BlockMissingException(src, description,\n          block.getStartOffset());\n    }\n\n    DatanodeInfo[] nodes \u003d block.getLocations();\n    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n      DFSClient.LOG.info(\"No node available for \" + blockInfo);\n    }\n    DFSClient.LOG.info(\"Could not obtain \" + block.getBlock()\n        + \" from any node: \" + errMsg\n        + \". Will get new block locations from namenode and retry...\");\n    try {\n      // Introducing a random factor to the wait time before another retry.\n      // The wait time is dependent on # of failures and a random factor.\n      // At the first time of getting a BlockMissingException, the wait time\n      // is a random number between 0..3000 ms. If the first retry\n      // still fails, we will wait 3000 ms grace period before the 2nd retry.\n      // Also at the second retry, the waiting window is expanded to 6000 ms\n      // alleviating the request rate from the server. Similarly the 3rd retry\n      // will wait 6000ms grace period before retry and the waiting window is\n      // expanded to 9000ms.\n      final int timeWindow \u003d dfsClient.getConf().getTimeWindow();\n      // grace period for the last round of attempt\n      double waitTime \u003d timeWindow * failures +\n          // expanding time window for each failure\n          timeWindow * (failures + 1) *\n          ThreadLocalRandom.current().nextDouble();\n      DFSClient.LOG.warn(\"DFS chooseDataNode: got # \" + (failures + 1) +\n          \" IOException, will wait for \" + waitTime + \" msec.\");\n      Thread.sleep((long)waitTime);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\n          \"Interrupted while choosing DataNode for read.\");\n    }\n    clearLocalDeadNodes(); //2nd option is to remove only nodes[blockId]\n    openInfo(true);\n    block \u003d refreshLocatedBlock(block);\n    failures++;\n    return block;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "b6bfb2fcb2391d51b8de97c01c1290880779132e": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11738. Hedged pread takes more time when block moved from initial locations. Contributed by Vinayakumar B.\n",
      "commitDate": "21/08/17 1:45 PM",
      "commitName": "b6bfb2fcb2391d51b8de97c01c1290880779132e",
      "commitAuthor": "John Zhuge",
      "diff": "@@ -0,0 +1,50 @@\n+  private LocatedBlock refetchLocations(LocatedBlock block,\n+      Collection\u003cDatanodeInfo\u003e ignoredNodes) throws IOException {\n+    String errMsg \u003d getBestNodeDNAddrPairErrorString(block.getLocations(),\n+        deadNodes, ignoredNodes);\n+    String blockInfo \u003d block.getBlock() + \" file\u003d\" + src;\n+    if (failures \u003e\u003d dfsClient.getConf().getMaxBlockAcquireFailures()) {\n+      String description \u003d \"Could not obtain block: \" + blockInfo;\n+      DFSClient.LOG.warn(description + errMsg\n+          + \". Throwing a BlockMissingException\");\n+      throw new BlockMissingException(src, description,\n+          block.getStartOffset());\n+    }\n+\n+    DatanodeInfo[] nodes \u003d block.getLocations();\n+    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n+      DFSClient.LOG.info(\"No node available for \" + blockInfo);\n+    }\n+    DFSClient.LOG.info(\"Could not obtain \" + block.getBlock()\n+        + \" from any node: \" + errMsg\n+        + \". Will get new block locations from namenode and retry...\");\n+    try {\n+      // Introducing a random factor to the wait time before another retry.\n+      // The wait time is dependent on # of failures and a random factor.\n+      // At the first time of getting a BlockMissingException, the wait time\n+      // is a random number between 0..3000 ms. If the first retry\n+      // still fails, we will wait 3000 ms grace period before the 2nd retry.\n+      // Also at the second retry, the waiting window is expanded to 6000 ms\n+      // alleviating the request rate from the server. Similarly the 3rd retry\n+      // will wait 6000ms grace period before retry and the waiting window is\n+      // expanded to 9000ms.\n+      final int timeWindow \u003d dfsClient.getConf().getTimeWindow();\n+      // grace period for the last round of attempt\n+      double waitTime \u003d timeWindow * failures +\n+          // expanding time window for each failure\n+          timeWindow * (failures + 1) *\n+          ThreadLocalRandom.current().nextDouble();\n+      DFSClient.LOG.warn(\"DFS chooseDataNode: got # \" + (failures + 1) +\n+          \" IOException, will wait for \" + waitTime + \" msec.\");\n+      Thread.sleep((long)waitTime);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      throw new InterruptedIOException(\n+          \"Interrupted while choosing DataNode for read.\");\n+    }\n+    deadNodes.clear(); //2nd option is to remove only nodes[blockId]\n+    openInfo(true);\n+    block \u003d refreshLocatedBlock(block);\n+    failures++;\n+    return block;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private LocatedBlock refetchLocations(LocatedBlock block,\n      Collection\u003cDatanodeInfo\u003e ignoredNodes) throws IOException {\n    String errMsg \u003d getBestNodeDNAddrPairErrorString(block.getLocations(),\n        deadNodes, ignoredNodes);\n    String blockInfo \u003d block.getBlock() + \" file\u003d\" + src;\n    if (failures \u003e\u003d dfsClient.getConf().getMaxBlockAcquireFailures()) {\n      String description \u003d \"Could not obtain block: \" + blockInfo;\n      DFSClient.LOG.warn(description + errMsg\n          + \". Throwing a BlockMissingException\");\n      throw new BlockMissingException(src, description,\n          block.getStartOffset());\n    }\n\n    DatanodeInfo[] nodes \u003d block.getLocations();\n    if (nodes \u003d\u003d null || nodes.length \u003d\u003d 0) {\n      DFSClient.LOG.info(\"No node available for \" + blockInfo);\n    }\n    DFSClient.LOG.info(\"Could not obtain \" + block.getBlock()\n        + \" from any node: \" + errMsg\n        + \". Will get new block locations from namenode and retry...\");\n    try {\n      // Introducing a random factor to the wait time before another retry.\n      // The wait time is dependent on # of failures and a random factor.\n      // At the first time of getting a BlockMissingException, the wait time\n      // is a random number between 0..3000 ms. If the first retry\n      // still fails, we will wait 3000 ms grace period before the 2nd retry.\n      // Also at the second retry, the waiting window is expanded to 6000 ms\n      // alleviating the request rate from the server. Similarly the 3rd retry\n      // will wait 6000ms grace period before retry and the waiting window is\n      // expanded to 9000ms.\n      final int timeWindow \u003d dfsClient.getConf().getTimeWindow();\n      // grace period for the last round of attempt\n      double waitTime \u003d timeWindow * failures +\n          // expanding time window for each failure\n          timeWindow * (failures + 1) *\n          ThreadLocalRandom.current().nextDouble();\n      DFSClient.LOG.warn(\"DFS chooseDataNode: got # \" + (failures + 1) +\n          \" IOException, will wait for \" + waitTime + \" msec.\");\n      Thread.sleep((long)waitTime);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new InterruptedIOException(\n          \"Interrupted while choosing DataNode for read.\");\n    }\n    deadNodes.clear(); //2nd option is to remove only nodes[blockId]\n    openInfo(true);\n    block \u003d refreshLocatedBlock(block);\n    failures++;\n    return block;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}