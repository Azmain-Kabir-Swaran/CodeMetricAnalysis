{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HAUtil.java",
  "functionName": "getProxiesForAllNameNodesInNameservice",
  "functionId": "getProxiesForAllNameNodesInNameservice___conf-Configuration__nsId-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java",
  "functionStartLine": 308,
  "functionEndLine": 319,
  "numCommitsSeen": 39,
  "timeTaken": 2021,
  "changeHistory": [
    "e8ca6480050e38d2fe4859baf4f9a8d22e7f9b85",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de"
  ],
  "changeHistoryShort": {
    "e8ca6480050e38d2fe4859baf4f9a8d22e7f9b85": "Ybodychange",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": "Yintroduced"
  },
  "changeHistoryDetails": {
    "e8ca6480050e38d2fe4859baf4f9a8d22e7f9b85": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6507. Improve DFSAdmin to support HA cluster better. (Contributd by Zesheng Wu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1604692 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/06/14 10:16 PM",
      "commitName": "e8ca6480050e38d2fe4859baf4f9a8d22e7f9b85",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "13/05/14 9:19 AM",
      "commitNameOld": "33ade356b35223654a077103ed7fbed89f3f2321",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 40.54,
      "commitsBetweenForRepo": 250,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,12 @@\n   public static List\u003cClientProtocol\u003e getProxiesForAllNameNodesInNameservice(\n       Configuration conf, String nsId) throws IOException {\n-    Map\u003cString, InetSocketAddress\u003e nnAddresses \u003d\n-        DFSUtil.getRpcAddressesForNameserviceId(conf, nsId, null);\n-    \n-    List\u003cClientProtocol\u003e namenodes \u003d new ArrayList\u003cClientProtocol\u003e();\n-    for (InetSocketAddress nnAddress : nnAddresses.values()) {\n-      NameNodeProxies.ProxyAndInfo\u003cClientProtocol\u003e proxyInfo \u003d null;\n-      proxyInfo \u003d NameNodeProxies.createNonHAProxy(conf,\n-          nnAddress, ClientProtocol.class,\n-          UserGroupInformation.getCurrentUser(), false);\n-      namenodes.add(proxyInfo.getProxy());\n+    List\u003cProxyAndInfo\u003cClientProtocol\u003e\u003e proxies \u003d\n+        getProxiesForAllNameNodesInNameservice(conf, nsId, ClientProtocol.class);\n+\n+    List\u003cClientProtocol\u003e namenodes \u003d new ArrayList\u003cClientProtocol\u003e(\n+        proxies.size());\n+    for (ProxyAndInfo\u003cClientProtocol\u003e proxy : proxies) {\n+      namenodes.add(proxy.getProxy());\n     }\n     return namenodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static List\u003cClientProtocol\u003e getProxiesForAllNameNodesInNameservice(\n      Configuration conf, String nsId) throws IOException {\n    List\u003cProxyAndInfo\u003cClientProtocol\u003e\u003e proxies \u003d\n        getProxiesForAllNameNodesInNameservice(conf, nsId, ClientProtocol.class);\n\n    List\u003cClientProtocol\u003e namenodes \u003d new ArrayList\u003cClientProtocol\u003e(\n        proxies.size());\n    for (ProxyAndInfo\u003cClientProtocol\u003e proxy : proxies) {\n      namenodes.add(proxy.getProxy());\n    }\n    return namenodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java",
      "extendedDetails": {}
    },
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5138. Support HDFS upgrade in HA. Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1561381 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/14 12:01 PM",
      "commitName": "edb6dc5f303093c2604cd07b0c0dacf12dbce5de",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,15 @@\n+  public static List\u003cClientProtocol\u003e getProxiesForAllNameNodesInNameservice(\n+      Configuration conf, String nsId) throws IOException {\n+    Map\u003cString, InetSocketAddress\u003e nnAddresses \u003d\n+        DFSUtil.getRpcAddressesForNameserviceId(conf, nsId, null);\n+    \n+    List\u003cClientProtocol\u003e namenodes \u003d new ArrayList\u003cClientProtocol\u003e();\n+    for (InetSocketAddress nnAddress : nnAddresses.values()) {\n+      NameNodeProxies.ProxyAndInfo\u003cClientProtocol\u003e proxyInfo \u003d null;\n+      proxyInfo \u003d NameNodeProxies.createNonHAProxy(conf,\n+          nnAddress, ClientProtocol.class,\n+          UserGroupInformation.getCurrentUser(), false);\n+      namenodes.add(proxyInfo.getProxy());\n+    }\n+    return namenodes;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static List\u003cClientProtocol\u003e getProxiesForAllNameNodesInNameservice(\n      Configuration conf, String nsId) throws IOException {\n    Map\u003cString, InetSocketAddress\u003e nnAddresses \u003d\n        DFSUtil.getRpcAddressesForNameserviceId(conf, nsId, null);\n    \n    List\u003cClientProtocol\u003e namenodes \u003d new ArrayList\u003cClientProtocol\u003e();\n    for (InetSocketAddress nnAddress : nnAddresses.values()) {\n      NameNodeProxies.ProxyAndInfo\u003cClientProtocol\u003e proxyInfo \u003d null;\n      proxyInfo \u003d NameNodeProxies.createNonHAProxy(conf,\n          nnAddress, ClientProtocol.class,\n          UserGroupInformation.getCurrentUser(), false);\n      namenodes.add(proxyInfo.getProxy());\n    }\n    return namenodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java"
    }
  }
}