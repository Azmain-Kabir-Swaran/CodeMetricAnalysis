{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "setupPipelineInternal",
  "functionId": "setupPipelineInternal___datanodes-DatanodeInfo[]__nodeStorageTypes-StorageType[]__nodeStorageIDs-String[]",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 1484,
  "functionEndLine": 1518,
  "numCommitsSeen": 58,
  "timeTaken": 2542,
  "changeHistory": [
    "a3954ccab148bddc290cb96528e63ff19799bcc9",
    "627da6f7178e18aa41996969c408b6f344e297d1"
  ],
  "changeHistoryShort": {
    "a3954ccab148bddc290cb96528e63ff19799bcc9": "Ymultichange(Yparameterchange,Ybodychange)",
    "627da6f7178e18aa41996969c408b6f344e297d1": "Ybodychange"
  },
  "changeHistoryDetails": {
    "a3954ccab148bddc290cb96528e63ff19799bcc9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
      "commitDate": "05/05/17 12:01 PM",
      "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthor": "Chris Douglas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "15/02/17 10:44 AM",
          "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 79.01,
          "commitsBetweenForRepo": 465,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,35 @@\n   protected void setupPipelineInternal(DatanodeInfo[] datanodes,\n-      StorageType[] nodeStorageTypes) throws IOException {\n+      StorageType[] nodeStorageTypes, String[] nodeStorageIDs)\n+      throws IOException {\n     boolean success \u003d false;\n     long newGS \u003d 0L;\n     while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n       if (!handleRestartingDatanode()) {\n         return;\n       }\n \n       final boolean isRecovery \u003d errorState.hasInternalError();\n       if (!handleBadDatanode()) {\n         return;\n       }\n \n       handleDatanodeReplacement();\n \n       // get a new generation stamp and an access token\n       final LocatedBlock lb \u003d updateBlockForPipeline();\n       newGS \u003d lb.getBlock().getGenerationStamp();\n       accessToken \u003d lb.getBlockToken();\n \n       // set up the pipeline again with the remaining nodes\n-      success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n+      success \u003d createBlockOutputStream(nodes, storageTypes, storageIDs, newGS,\n+          isRecovery);\n \n       failPacket4Testing();\n \n       errorState.checkRestartingNodeDeadline(nodes);\n     } // while\n \n     if (success) {\n       updatePipeline(newGS);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void setupPipelineInternal(DatanodeInfo[] datanodes,\n      StorageType[] nodeStorageTypes, String[] nodeStorageIDs)\n      throws IOException {\n    boolean success \u003d false;\n    long newGS \u003d 0L;\n    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      if (!handleRestartingDatanode()) {\n        return;\n      }\n\n      final boolean isRecovery \u003d errorState.hasInternalError();\n      if (!handleBadDatanode()) {\n        return;\n      }\n\n      handleDatanodeReplacement();\n\n      // get a new generation stamp and an access token\n      final LocatedBlock lb \u003d updateBlockForPipeline();\n      newGS \u003d lb.getBlock().getGenerationStamp();\n      accessToken \u003d lb.getBlockToken();\n\n      // set up the pipeline again with the remaining nodes\n      success \u003d createBlockOutputStream(nodes, storageTypes, storageIDs, newGS,\n          isRecovery);\n\n      failPacket4Testing();\n\n      errorState.checkRestartingNodeDeadline(nodes);\n    } // while\n\n    if (success) {\n      updatePipeline(newGS);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldValue": "[datanodes-DatanodeInfo[], nodeStorageTypes-StorageType[]]",
            "newValue": "[datanodes-DatanodeInfo[], nodeStorageTypes-StorageType[], nodeStorageIDs-String[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "15/02/17 10:44 AM",
          "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 79.01,
          "commitsBetweenForRepo": 465,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,35 @@\n   protected void setupPipelineInternal(DatanodeInfo[] datanodes,\n-      StorageType[] nodeStorageTypes) throws IOException {\n+      StorageType[] nodeStorageTypes, String[] nodeStorageIDs)\n+      throws IOException {\n     boolean success \u003d false;\n     long newGS \u003d 0L;\n     while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n       if (!handleRestartingDatanode()) {\n         return;\n       }\n \n       final boolean isRecovery \u003d errorState.hasInternalError();\n       if (!handleBadDatanode()) {\n         return;\n       }\n \n       handleDatanodeReplacement();\n \n       // get a new generation stamp and an access token\n       final LocatedBlock lb \u003d updateBlockForPipeline();\n       newGS \u003d lb.getBlock().getGenerationStamp();\n       accessToken \u003d lb.getBlockToken();\n \n       // set up the pipeline again with the remaining nodes\n-      success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n+      success \u003d createBlockOutputStream(nodes, storageTypes, storageIDs, newGS,\n+          isRecovery);\n \n       failPacket4Testing();\n \n       errorState.checkRestartingNodeDeadline(nodes);\n     } // while\n \n     if (success) {\n       updatePipeline(newGS);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void setupPipelineInternal(DatanodeInfo[] datanodes,\n      StorageType[] nodeStorageTypes, String[] nodeStorageIDs)\n      throws IOException {\n    boolean success \u003d false;\n    long newGS \u003d 0L;\n    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      if (!handleRestartingDatanode()) {\n        return;\n      }\n\n      final boolean isRecovery \u003d errorState.hasInternalError();\n      if (!handleBadDatanode()) {\n        return;\n      }\n\n      handleDatanodeReplacement();\n\n      // get a new generation stamp and an access token\n      final LocatedBlock lb \u003d updateBlockForPipeline();\n      newGS \u003d lb.getBlock().getGenerationStamp();\n      accessToken \u003d lb.getBlockToken();\n\n      // set up the pipeline again with the remaining nodes\n      success \u003d createBlockOutputStream(nodes, storageTypes, storageIDs, newGS,\n          isRecovery);\n\n      failPacket4Testing();\n\n      errorState.checkRestartingNodeDeadline(nodes);\n    } // while\n\n    if (success) {\n      updatePipeline(newGS);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {}
        }
      ]
    },
    "627da6f7178e18aa41996969c408b6f344e297d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8498. Blocks can be committed with wrong size. Contributed by Jing Zhao.\n",
      "commitDate": "15/02/17 10:44 AM",
      "commitName": "627da6f7178e18aa41996969c408b6f344e297d1",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/02/17 10:08 AM",
      "commitNameOld": "0914fcca312b5e9d20bcf1b6633bc13c9034ba46",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 13.03,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   protected void setupPipelineInternal(DatanodeInfo[] datanodes,\n       StorageType[] nodeStorageTypes) throws IOException {\n     boolean success \u003d false;\n     long newGS \u003d 0L;\n     while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n       if (!handleRestartingDatanode()) {\n         return;\n       }\n \n       final boolean isRecovery \u003d errorState.hasInternalError();\n       if (!handleBadDatanode()) {\n         return;\n       }\n \n       handleDatanodeReplacement();\n \n       // get a new generation stamp and an access token\n       final LocatedBlock lb \u003d updateBlockForPipeline();\n       newGS \u003d lb.getBlock().getGenerationStamp();\n       accessToken \u003d lb.getBlockToken();\n \n       // set up the pipeline again with the remaining nodes\n       success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n \n       failPacket4Testing();\n \n       errorState.checkRestartingNodeDeadline(nodes);\n     } // while\n \n     if (success) {\n-      block \u003d updatePipeline(newGS);\n+      updatePipeline(newGS);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void setupPipelineInternal(DatanodeInfo[] datanodes,\n      StorageType[] nodeStorageTypes) throws IOException {\n    boolean success \u003d false;\n    long newGS \u003d 0L;\n    while (!success \u0026\u0026 !streamerClosed \u0026\u0026 dfsClient.clientRunning) {\n      if (!handleRestartingDatanode()) {\n        return;\n      }\n\n      final boolean isRecovery \u003d errorState.hasInternalError();\n      if (!handleBadDatanode()) {\n        return;\n      }\n\n      handleDatanodeReplacement();\n\n      // get a new generation stamp and an access token\n      final LocatedBlock lb \u003d updateBlockForPipeline();\n      newGS \u003d lb.getBlock().getGenerationStamp();\n      accessToken \u003d lb.getBlockToken();\n\n      // set up the pipeline again with the remaining nodes\n      success \u003d createBlockOutputStream(nodes, storageTypes, newGS, isRecovery);\n\n      failPacket4Testing();\n\n      errorState.checkRestartingNodeDeadline(nodes);\n    } // while\n\n    if (success) {\n      updatePipeline(newGS);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    }
  }
}