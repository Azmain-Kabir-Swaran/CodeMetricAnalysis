{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirConcatOp.java",
  "functionName": "verifySrcFiles",
  "functionId": "verifySrcFiles___fsd-FSDirectory__srcs-String[]__targetIIP-INodesInPath__pc-FSPermissionChecker",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
  "functionStartLine": 119,
  "functionEndLine": 188,
  "numCommitsSeen": 24,
  "timeTaken": 4376,
  "changeHistory": [
    "5e0eda5d5f696aba7fc209874d232baf2a50d547",
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
    "da16c9b3b40f9cbef0ea7d8cffc4c2c77fd1c447",
    "dc7a061668a3f4d86fe1b07a40d46774b5386938",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b",
    "bee5a6a64a1c037308fa4d52249be39c82791590",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0"
  ],
  "changeHistoryShort": {
    "5e0eda5d5f696aba7fc209874d232baf2a50d547": "Ybodychange",
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04": "Ybodychange",
    "da16c9b3b40f9cbef0ea7d8cffc4c2c77fd1c447": "Ybodychange",
    "dc7a061668a3f4d86fe1b07a40d46774b5386938": "Ybodychange",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": "Ybodychange",
    "bee5a6a64a1c037308fa4d52249be39c82791590": "Ybodychange",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": "Yintroduced"
  },
  "changeHistoryDetails": {
    "5e0eda5d5f696aba7fc209874d232baf2a50d547": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15286. Concat on a same file deleting the file. Contributed by hemanthboyina.\n",
      "commitDate": "27/04/20 2:17 PM",
      "commitName": "5e0eda5d5f696aba7fc209874d232baf2a50d547",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "22/11/19 1:00 PM",
      "commitNameOld": "049940e77b055518e1de44b1be60d2ee2e1e9143",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 157.01,
      "commitsBetweenForRepo": 517,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,70 @@\n   private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n       INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n     // to make sure no two files are the same\n     Set\u003cINodeFile\u003e si \u003d new LinkedHashSet\u003c\u003e();\n     final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n     final INodeDirectory targetParent \u003d targetINode.getParent();\n     // now check the srcs\n     for(String src : srcs) {\n       final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE);\n       // permission check for srcs\n       if (pc !\u003d null) {\n         fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n         fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n       }\n       final INode srcINode \u003d iip.getLastINode();\n       final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n       // make sure the src file and the target file are in the same dir\n       if (srcINodeFile.getParent() !\u003d targetParent) {\n         throw new HadoopIllegalArgumentException(\"Source file \" + src\n             + \" is not in the same directory with the target \"\n             + targetIIP.getPath());\n       }\n       // make sure all the source files are not in snapshot\n       if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is in snapshot\");\n       }\n       // check if the file has other references.\n       if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is referred by some other reference in some snapshot.\");\n       }\n       // source file cannot be the same with the target file\n-      if (srcINode \u003d\u003d targetINode) {\n+      if (srcINode.equals(targetINode)) {\n         throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n             + \" is the same with the target file \" + targetIIP.getPath());\n       }\n       // source file cannot be under construction or empty\n       if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // source file\u0027s preferred block size cannot be greater than the target\n       // file\n       if (srcINodeFile.getPreferredBlockSize() \u003e\n           targetINode.getPreferredBlockSize()) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n             + \" which is greater than the target file\u0027s preferred block size \"\n             + targetINode.getPreferredBlockSize());\n       }\n       if(srcINodeFile.getErasureCodingPolicyID() !\u003d\n           targetINode.getErasureCodingPolicyID()) {\n         throw new HadoopIllegalArgumentException(\"Source file \" + src\n             + \" and target file \" + targetIIP.getPath()\n             + \" have different erasure coding policy\");\n       }\n       si.add(srcINodeFile);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length) {\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n     return si.toArray(new INodeFile[si.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n      INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n    // to make sure no two files are the same\n    Set\u003cINodeFile\u003e si \u003d new LinkedHashSet\u003c\u003e();\n    final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n    final INodeDirectory targetParent \u003d targetINode.getParent();\n    // now check the srcs\n    for(String src : srcs) {\n      final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE);\n      // permission check for srcs\n      if (pc !\u003d null) {\n        fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n      }\n      final INode srcINode \u003d iip.getLastINode();\n      final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n      // make sure the src file and the target file are in the same dir\n      if (srcINodeFile.getParent() !\u003d targetParent) {\n        throw new HadoopIllegalArgumentException(\"Source file \" + src\n            + \" is not in the same directory with the target \"\n            + targetIIP.getPath());\n      }\n      // make sure all the source files are not in snapshot\n      if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is in snapshot\");\n      }\n      // check if the file has other references.\n      if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is referred by some other reference in some snapshot.\");\n      }\n      // source file cannot be the same with the target file\n      if (srcINode.equals(targetINode)) {\n        throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n            + \" is the same with the target file \" + targetIIP.getPath());\n      }\n      // source file cannot be under construction or empty\n      if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // source file\u0027s preferred block size cannot be greater than the target\n      // file\n      if (srcINodeFile.getPreferredBlockSize() \u003e\n          targetINode.getPreferredBlockSize()) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n            + \" which is greater than the target file\u0027s preferred block size \"\n            + targetINode.getPreferredBlockSize());\n      }\n      if(srcINodeFile.getErasureCodingPolicyID() !\u003d\n          targetINode.getErasureCodingPolicyID()) {\n        throw new HadoopIllegalArgumentException(\"Source file \" + src\n            + \" and target file \" + targetIIP.getPath()\n            + \" have different erasure coding policy\");\n      }\n      si.add(srcINodeFile);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length) {\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n    return si.toArray(new INodeFile[si.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10997. Reduce number of path resolving methods. Contributed by Daryn Sharp.\n",
      "commitDate": "24/10/16 3:14 PM",
      "commitName": "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "10/12/15 11:55 PM",
      "commitNameOld": "e363417e7b7abdd5d149f303f729ecf3e95ef8f3",
      "commitAuthorOld": "Uma Mahesh",
      "daysBetweenCommits": 318.6,
      "commitsBetweenForRepo": 2181,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,70 @@\n   private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n       INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n     // to make sure no two files are the same\n     Set\u003cINodeFile\u003e si \u003d new LinkedHashSet\u003c\u003e();\n     final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n     final INodeDirectory targetParent \u003d targetINode.getParent();\n     // now check the srcs\n     for(String src : srcs) {\n-      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n+      final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE);\n       // permission check for srcs\n       if (pc !\u003d null) {\n         fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n         fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n       }\n       final INode srcINode \u003d iip.getLastINode();\n       final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n       // make sure the src file and the target file are in the same dir\n       if (srcINodeFile.getParent() !\u003d targetParent) {\n         throw new HadoopIllegalArgumentException(\"Source file \" + src\n             + \" is not in the same directory with the target \"\n             + targetIIP.getPath());\n       }\n       // make sure all the source files are not in snapshot\n       if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is in snapshot\");\n       }\n       // check if the file has other references.\n       if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is referred by some other reference in some snapshot.\");\n       }\n       // source file cannot be the same with the target file\n       if (srcINode \u003d\u003d targetINode) {\n         throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n             + \" is the same with the target file \" + targetIIP.getPath());\n       }\n       // source file cannot be under construction or empty\n       if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // source file\u0027s preferred block size cannot be greater than the target\n       // file\n       if (srcINodeFile.getPreferredBlockSize() \u003e\n           targetINode.getPreferredBlockSize()) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n             + \" which is greater than the target file\u0027s preferred block size \"\n             + targetINode.getPreferredBlockSize());\n       }\n       if(srcINodeFile.getErasureCodingPolicyID() !\u003d\n           targetINode.getErasureCodingPolicyID()) {\n         throw new HadoopIllegalArgumentException(\"Source file \" + src\n             + \" and target file \" + targetIIP.getPath()\n             + \" have different erasure coding policy\");\n       }\n       si.add(srcINodeFile);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length) {\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n     return si.toArray(new INodeFile[si.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n      INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n    // to make sure no two files are the same\n    Set\u003cINodeFile\u003e si \u003d new LinkedHashSet\u003c\u003e();\n    final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n    final INodeDirectory targetParent \u003d targetINode.getParent();\n    // now check the srcs\n    for(String src : srcs) {\n      final INodesInPath iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE);\n      // permission check for srcs\n      if (pc !\u003d null) {\n        fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n      }\n      final INode srcINode \u003d iip.getLastINode();\n      final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n      // make sure the src file and the target file are in the same dir\n      if (srcINodeFile.getParent() !\u003d targetParent) {\n        throw new HadoopIllegalArgumentException(\"Source file \" + src\n            + \" is not in the same directory with the target \"\n            + targetIIP.getPath());\n      }\n      // make sure all the source files are not in snapshot\n      if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is in snapshot\");\n      }\n      // check if the file has other references.\n      if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is referred by some other reference in some snapshot.\");\n      }\n      // source file cannot be the same with the target file\n      if (srcINode \u003d\u003d targetINode) {\n        throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n            + \" is the same with the target file \" + targetIIP.getPath());\n      }\n      // source file cannot be under construction or empty\n      if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // source file\u0027s preferred block size cannot be greater than the target\n      // file\n      if (srcINodeFile.getPreferredBlockSize() \u003e\n          targetINode.getPreferredBlockSize()) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n            + \" which is greater than the target file\u0027s preferred block size \"\n            + targetINode.getPreferredBlockSize());\n      }\n      if(srcINodeFile.getErasureCodingPolicyID() !\u003d\n          targetINode.getErasureCodingPolicyID()) {\n        throw new HadoopIllegalArgumentException(\"Source file \" + src\n            + \" and target file \" + targetIIP.getPath()\n            + \" have different erasure coding policy\");\n      }\n      si.add(srcINodeFile);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length) {\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n    return si.toArray(new INodeFile[si.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "da16c9b3b40f9cbef0ea7d8cffc4c2c77fd1c447": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8438. Erasure Coding: Allow concat striped files if they have the same ErasureCodingPolicy. Contributed by Walter Su.\n",
      "commitDate": "13/10/15 11:03 AM",
      "commitName": "da16c9b3b40f9cbef0ea7d8cffc4c2c77fd1c447",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "29/09/15 1:39 AM",
      "commitNameOld": "8fd55202468b28422b0df888641c9b08906fe4a7",
      "commitAuthorOld": "",
      "daysBetweenCommits": 14.39,
      "commitsBetweenForRepo": 92,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,70 @@\n   private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n       INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n     // to make sure no two files are the same\n     Set\u003cINodeFile\u003e si \u003d new LinkedHashSet\u003c\u003e();\n     final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n     final INodeDirectory targetParent \u003d targetINode.getParent();\n     // now check the srcs\n     for(String src : srcs) {\n       final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n       // permission check for srcs\n       if (pc !\u003d null) {\n         fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n         fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n       }\n       final INode srcINode \u003d iip.getLastINode();\n       final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n       // make sure the src file and the target file are in the same dir\n       if (srcINodeFile.getParent() !\u003d targetParent) {\n         throw new HadoopIllegalArgumentException(\"Source file \" + src\n             + \" is not in the same directory with the target \"\n             + targetIIP.getPath());\n       }\n       // make sure all the source files are not in snapshot\n       if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is in snapshot\");\n       }\n       // check if the file has other references.\n       if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is referred by some other reference in some snapshot.\");\n       }\n       // source file cannot be the same with the target file\n       if (srcINode \u003d\u003d targetINode) {\n         throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n             + \" is the same with the target file \" + targetIIP.getPath());\n       }\n       // source file cannot be under construction or empty\n       if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // source file\u0027s preferred block size cannot be greater than the target\n       // file\n       if (srcINodeFile.getPreferredBlockSize() \u003e\n           targetINode.getPreferredBlockSize()) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n             + \" which is greater than the target file\u0027s preferred block size \"\n             + targetINode.getPreferredBlockSize());\n       }\n-      // TODO currently we do not support concatenating EC files\n-      if (srcINodeFile.isStriped()) {\n-        throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n-            + \" is with striped blocks\");\n+      if(srcINodeFile.getErasureCodingPolicyID() !\u003d\n+          targetINode.getErasureCodingPolicyID()) {\n+        throw new HadoopIllegalArgumentException(\"Source file \" + src\n+            + \" and target file \" + targetIIP.getPath()\n+            + \" have different erasure coding policy\");\n       }\n       si.add(srcINodeFile);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length) {\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n     return si.toArray(new INodeFile[si.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n      INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n    // to make sure no two files are the same\n    Set\u003cINodeFile\u003e si \u003d new LinkedHashSet\u003c\u003e();\n    final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n    final INodeDirectory targetParent \u003d targetINode.getParent();\n    // now check the srcs\n    for(String src : srcs) {\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n      // permission check for srcs\n      if (pc !\u003d null) {\n        fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n      }\n      final INode srcINode \u003d iip.getLastINode();\n      final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n      // make sure the src file and the target file are in the same dir\n      if (srcINodeFile.getParent() !\u003d targetParent) {\n        throw new HadoopIllegalArgumentException(\"Source file \" + src\n            + \" is not in the same directory with the target \"\n            + targetIIP.getPath());\n      }\n      // make sure all the source files are not in snapshot\n      if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is in snapshot\");\n      }\n      // check if the file has other references.\n      if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is referred by some other reference in some snapshot.\");\n      }\n      // source file cannot be the same with the target file\n      if (srcINode \u003d\u003d targetINode) {\n        throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n            + \" is the same with the target file \" + targetIIP.getPath());\n      }\n      // source file cannot be under construction or empty\n      if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // source file\u0027s preferred block size cannot be greater than the target\n      // file\n      if (srcINodeFile.getPreferredBlockSize() \u003e\n          targetINode.getPreferredBlockSize()) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n            + \" which is greater than the target file\u0027s preferred block size \"\n            + targetINode.getPreferredBlockSize());\n      }\n      if(srcINodeFile.getErasureCodingPolicyID() !\u003d\n          targetINode.getErasureCodingPolicyID()) {\n        throw new HadoopIllegalArgumentException(\"Source file \" + src\n            + \" and target file \" + targetIIP.getPath()\n            + \" have different erasure coding policy\");\n      }\n      si.add(srcINodeFile);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length) {\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n    return si.toArray(new INodeFile[si.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "dc7a061668a3f4d86fe1b07a40d46774b5386938": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8891. HDFS concat should keep srcs order. Contributed by Yong Zhang.\n",
      "commitDate": "14/08/15 2:42 PM",
      "commitName": "dc7a061668a3f4d86fe1b07a40d46774b5386938",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "10/07/15 2:15 PM",
      "commitNameOld": "47f4c54106ebb234a7d3dc71320aa584ecba161a",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 35.02,
      "commitsBetweenForRepo": 192,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,63 @@\n   private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n       INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n     // to make sure no two files are the same\n-    Set\u003cINodeFile\u003e si \u003d new HashSet\u003c\u003e();\n+    Set\u003cINodeFile\u003e si \u003d new LinkedHashSet\u003c\u003e();\n     final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n     final INodeDirectory targetParent \u003d targetINode.getParent();\n     // now check the srcs\n     for(String src : srcs) {\n       final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n       // permission check for srcs\n       if (pc !\u003d null) {\n         fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n         fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n       }\n       final INode srcINode \u003d iip.getLastINode();\n       final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n       // make sure the src file and the target file are in the same dir\n       if (srcINodeFile.getParent() !\u003d targetParent) {\n         throw new HadoopIllegalArgumentException(\"Source file \" + src\n             + \" is not in the same directory with the target \"\n             + targetIIP.getPath());\n       }\n       // make sure all the source files are not in snapshot\n       if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is in snapshot\");\n       }\n       // check if the file has other references.\n       if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is referred by some other reference in some snapshot.\");\n       }\n       // source file cannot be the same with the target file\n       if (srcINode \u003d\u003d targetINode) {\n         throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n             + \" is the same with the target file \" + targetIIP.getPath());\n       }\n       // source file cannot be under construction or empty\n       if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n       // source file\u0027s preferred block size cannot be greater than the target\n       // file\n       if (srcINodeFile.getPreferredBlockSize() \u003e\n           targetINode.getPreferredBlockSize()) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n             + \" which is greater than the target file\u0027s preferred block size \"\n             + targetINode.getPreferredBlockSize());\n       }\n       si.add(srcINodeFile);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length) {\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n     return si.toArray(new INodeFile[si.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n      INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n    // to make sure no two files are the same\n    Set\u003cINodeFile\u003e si \u003d new LinkedHashSet\u003c\u003e();\n    final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n    final INodeDirectory targetParent \u003d targetINode.getParent();\n    // now check the srcs\n    for(String src : srcs) {\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n      // permission check for srcs\n      if (pc !\u003d null) {\n        fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n      }\n      final INode srcINode \u003d iip.getLastINode();\n      final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n      // make sure the src file and the target file are in the same dir\n      if (srcINodeFile.getParent() !\u003d targetParent) {\n        throw new HadoopIllegalArgumentException(\"Source file \" + src\n            + \" is not in the same directory with the target \"\n            + targetIIP.getPath());\n      }\n      // make sure all the source files are not in snapshot\n      if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is in snapshot\");\n      }\n      // check if the file has other references.\n      if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is referred by some other reference in some snapshot.\");\n      }\n      // source file cannot be the same with the target file\n      if (srcINode \u003d\u003d targetINode) {\n        throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n            + \" is the same with the target file \" + targetIIP.getPath());\n      }\n      // source file cannot be under construction or empty\n      if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n      // source file\u0027s preferred block size cannot be greater than the target\n      // file\n      if (srcINodeFile.getPreferredBlockSize() \u003e\n          targetINode.getPreferredBlockSize()) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n            + \" which is greater than the target file\u0027s preferred block size \"\n            + targetINode.getPreferredBlockSize());\n      }\n      si.add(srcINodeFile);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length) {\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n    return si.toArray(new INodeFile[si.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7749. Erasure Coding: Add striped block support in INodeFile. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/05/15 6:29 AM",
      "commitNameOld": "6d5da9484185ca9f585195d6da069b9cd5be4044",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 14.19,
      "commitsBetweenForRepo": 118,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,69 @@\n   private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n       INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n     // to make sure no two files are the same\n     Set\u003cINodeFile\u003e si \u003d new HashSet\u003c\u003e();\n     final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n     final INodeDirectory targetParent \u003d targetINode.getParent();\n     // now check the srcs\n     for(String src : srcs) {\n       final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n       // permission check for srcs\n       if (pc !\u003d null) {\n         fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n         fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n       }\n       final INode srcINode \u003d iip.getLastINode();\n       final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n       // make sure the src file and the target file are in the same dir\n       if (srcINodeFile.getParent() !\u003d targetParent) {\n         throw new HadoopIllegalArgumentException(\"Source file \" + src\n             + \" is not in the same directory with the target \"\n             + targetIIP.getPath());\n       }\n       // make sure all the source files are not in snapshot\n       if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is in snapshot\");\n       }\n       // check if the file has other references.\n       if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is referred by some other reference in some snapshot.\");\n       }\n       // source file cannot be the same with the target file\n       if (srcINode \u003d\u003d targetINode) {\n         throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n             + \" is the same with the target file \" + targetIIP.getPath());\n       }\n       // source file cannot be under construction or empty\n       if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n+\n       // source file\u0027s preferred block size cannot be greater than the target\n       // file\n       if (srcINodeFile.getPreferredBlockSize() \u003e\n           targetINode.getPreferredBlockSize()) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n             + \" which is greater than the target file\u0027s preferred block size \"\n             + targetINode.getPreferredBlockSize());\n       }\n+      // TODO currently we do not support concatenating EC files\n+      if (srcINodeFile.isStriped()) {\n+        throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n+            + \" is with striped blocks\");\n+      }\n       si.add(srcINodeFile);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length) {\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n     return si.toArray(new INodeFile[si.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n      INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n    // to make sure no two files are the same\n    Set\u003cINodeFile\u003e si \u003d new HashSet\u003c\u003e();\n    final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n    final INodeDirectory targetParent \u003d targetINode.getParent();\n    // now check the srcs\n    for(String src : srcs) {\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n      // permission check for srcs\n      if (pc !\u003d null) {\n        fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n      }\n      final INode srcINode \u003d iip.getLastINode();\n      final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n      // make sure the src file and the target file are in the same dir\n      if (srcINodeFile.getParent() !\u003d targetParent) {\n        throw new HadoopIllegalArgumentException(\"Source file \" + src\n            + \" is not in the same directory with the target \"\n            + targetIIP.getPath());\n      }\n      // make sure all the source files are not in snapshot\n      if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is in snapshot\");\n      }\n      // check if the file has other references.\n      if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is referred by some other reference in some snapshot.\");\n      }\n      // source file cannot be the same with the target file\n      if (srcINode \u003d\u003d targetINode) {\n        throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n            + \" is the same with the target file \" + targetIIP.getPath());\n      }\n      // source file cannot be under construction or empty\n      if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // source file\u0027s preferred block size cannot be greater than the target\n      // file\n      if (srcINodeFile.getPreferredBlockSize() \u003e\n          targetINode.getPreferredBlockSize()) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n            + \" which is greater than the target file\u0027s preferred block size \"\n            + targetINode.getPreferredBlockSize());\n      }\n      // TODO currently we do not support concatenating EC files\n      if (srcINodeFile.isStriped()) {\n        throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n            + \" is with striped blocks\");\n      }\n      si.add(srcINodeFile);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length) {\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n    return si.toArray(new INodeFile[si.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "bee5a6a64a1c037308fa4d52249be39c82791590": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao.\n",
      "commitDate": "18/03/15 6:40 PM",
      "commitName": "bee5a6a64a1c037308fa4d52249be39c82791590",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "21/02/15 3:38 PM",
      "commitNameOld": "8b465b4b8caed31ca9daeaae108f9a868a30a455",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 25.09,
      "commitsBetweenForRepo": 205,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,63 @@\n   private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n       INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n     // to make sure no two files are the same\n     Set\u003cINodeFile\u003e si \u003d new HashSet\u003c\u003e();\n     final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n     final INodeDirectory targetParent \u003d targetINode.getParent();\n     // now check the srcs\n     for(String src : srcs) {\n       final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n       // permission check for srcs\n       if (pc !\u003d null) {\n         fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n         fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n       }\n       final INode srcINode \u003d iip.getLastINode();\n       final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n       // make sure the src file and the target file are in the same dir\n       if (srcINodeFile.getParent() !\u003d targetParent) {\n         throw new HadoopIllegalArgumentException(\"Source file \" + src\n             + \" is not in the same directory with the target \"\n             + targetIIP.getPath());\n       }\n       // make sure all the source files are not in snapshot\n       if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is in snapshot\");\n       }\n       // check if the file has other references.\n       if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + src\n             + \" is referred by some other reference in some snapshot.\");\n       }\n+      // source file cannot be the same with the target file\n       if (srcINode \u003d\u003d targetINode) {\n         throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n             + \" is the same with the target file \" + targetIIP.getPath());\n       }\n+      // source file cannot be under construction or empty\n       if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n+      // source file\u0027s preferred block size cannot be greater than the target\n+      // file\n+      if (srcINodeFile.getPreferredBlockSize() \u003e\n+          targetINode.getPreferredBlockSize()) {\n+        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n+            + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n+            + \" which is greater than the target file\u0027s preferred block size \"\n+            + targetINode.getPreferredBlockSize());\n+      }\n       si.add(srcINodeFile);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length) {\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n     return si.toArray(new INodeFile[si.size()]);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n      INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n    // to make sure no two files are the same\n    Set\u003cINodeFile\u003e si \u003d new HashSet\u003c\u003e();\n    final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n    final INodeDirectory targetParent \u003d targetINode.getParent();\n    // now check the srcs\n    for(String src : srcs) {\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n      // permission check for srcs\n      if (pc !\u003d null) {\n        fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n      }\n      final INode srcINode \u003d iip.getLastINode();\n      final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n      // make sure the src file and the target file are in the same dir\n      if (srcINodeFile.getParent() !\u003d targetParent) {\n        throw new HadoopIllegalArgumentException(\"Source file \" + src\n            + \" is not in the same directory with the target \"\n            + targetIIP.getPath());\n      }\n      // make sure all the source files are not in snapshot\n      if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is in snapshot\");\n      }\n      // check if the file has other references.\n      if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is referred by some other reference in some snapshot.\");\n      }\n      // source file cannot be the same with the target file\n      if (srcINode \u003d\u003d targetINode) {\n        throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n            + \" is the same with the target file \" + targetIIP.getPath());\n      }\n      // source file cannot be under construction or empty\n      if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n      // source file\u0027s preferred block size cannot be greater than the target\n      // file\n      if (srcINodeFile.getPreferredBlockSize() \u003e\n          targetINode.getPreferredBlockSize()) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" has preferred block size \" + srcINodeFile.getPreferredBlockSize()\n            + \" which is greater than the target file\u0027s preferred block size \"\n            + targetINode.getPreferredBlockSize());\n      }\n      si.add(srcINodeFile);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length) {\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n    return si.toArray(new INodeFile[si.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3689. Add support for variable length block. Contributed by Jing Zhao.\n",
      "commitDate": "27/01/15 12:58 PM",
      "commitName": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,52 @@\n+  private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n+      INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n+    // to make sure no two files are the same\n+    Set\u003cINodeFile\u003e si \u003d new HashSet\u003c\u003e();\n+    final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n+    final INodeDirectory targetParent \u003d targetINode.getParent();\n+    // now check the srcs\n+    for(String src : srcs) {\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n+      // permission check for srcs\n+      if (pc !\u003d null) {\n+        fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n+        fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n+      }\n+      final INode srcINode \u003d iip.getLastINode();\n+      final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n+      // make sure the src file and the target file are in the same dir\n+      if (srcINodeFile.getParent() !\u003d targetParent) {\n+        throw new HadoopIllegalArgumentException(\"Source file \" + src\n+            + \" is not in the same directory with the target \"\n+            + targetIIP.getPath());\n+      }\n+      // make sure all the source files are not in snapshot\n+      if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n+        throw new SnapshotException(\"Concat: the source file \" + src\n+            + \" is in snapshot\");\n+      }\n+      // check if the file has other references.\n+      if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n+          srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n+        throw new SnapshotException(\"Concat: the source file \" + src\n+            + \" is referred by some other reference in some snapshot.\");\n+      }\n+      if (srcINode \u003d\u003d targetINode) {\n+        throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n+            + \" is the same with the target file \" + targetIIP.getPath());\n+      }\n+      if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n+        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n+            + \" is invalid or empty or underConstruction\");\n+      }\n+      si.add(srcINodeFile);\n+    }\n+\n+    // make sure no two files are the same\n+    if(si.size() \u003c srcs.length) {\n+      // it means at least two files are the same\n+      throw new HadoopIllegalArgumentException(\n+          \"concat: at least two of the source files are the same\");\n+    }\n+    return si.toArray(new INodeFile[si.size()]);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n      INodesInPath targetIIP, FSPermissionChecker pc) throws IOException {\n    // to make sure no two files are the same\n    Set\u003cINodeFile\u003e si \u003d new HashSet\u003c\u003e();\n    final INodeFile targetINode \u003d targetIIP.getLastINode().asFile();\n    final INodeDirectory targetParent \u003d targetINode.getParent();\n    // now check the srcs\n    for(String src : srcs) {\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(src);\n      // permission check for srcs\n      if (pc !\u003d null) {\n        fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n      }\n      final INode srcINode \u003d iip.getLastINode();\n      final INodeFile srcINodeFile \u003d INodeFile.valueOf(srcINode, src);\n      // make sure the src file and the target file are in the same dir\n      if (srcINodeFile.getParent() !\u003d targetParent) {\n        throw new HadoopIllegalArgumentException(\"Source file \" + src\n            + \" is not in the same directory with the target \"\n            + targetIIP.getPath());\n      }\n      // make sure all the source files are not in snapshot\n      if (srcINode.isInLatestSnapshot(iip.getLatestSnapshotId())) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is in snapshot\");\n      }\n      // check if the file has other references.\n      if (srcINode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          srcINode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + src\n            + \" is referred by some other reference in some snapshot.\");\n      }\n      if (srcINode \u003d\u003d targetINode) {\n        throw new HadoopIllegalArgumentException(\"concat: the src file \" + src\n            + \" is the same with the target file \" + targetIIP.getPath());\n      }\n      if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n      si.add(srcINodeFile);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length) {\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n    return si.toArray(new INodeFile[si.size()]);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java"
    }
  }
}