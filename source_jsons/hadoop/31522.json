{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "EntityTypeReader.java",
  "functionName": "readEntityTypes",
  "functionId": "readEntityTypes___hbaseConf-Configuration__conn-Connection",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/EntityTypeReader.java",
  "functionStartLine": 70,
  "functionEndLine": 114,
  "numCommitsSeen": 5,
  "timeTaken": 1471,
  "changeHistory": [
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2",
    "dcd0bedcc8113dd5e1d852c242ae9511d1b3d58e",
    "4481561e4a3433197dd8e73f38856eef84f0fd03"
  ],
  "changeHistoryShort": {
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2": "Yfilerename",
    "dcd0bedcc8113dd5e1d852c242ae9511d1b3d58e": "Ybodychange",
    "4481561e4a3433197dd8e73f38856eef84f0fd03": "Yintroduced"
  },
  "changeHistoryDetails": {
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2": {
      "type": "Yfilerename",
      "commitMessage": "YARN-7919. Refactor timelineservice-hbase module into submodules. Contributed by Haibo Chen.\n",
      "commitDate": "17/02/18 7:00 AM",
      "commitName": "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2",
      "commitAuthor": "Rohith Sharma K S",
      "commitDateOld": "17/02/18 3:24 AM",
      "commitNameOld": "a1e56a62863d8d494af309ec5f476c4b7e4d5ef9",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 0.15,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public Set\u003cString\u003e readEntityTypes(Configuration hbaseConf,\n      Connection conn) throws IOException {\n\n    validateParams();\n    augmentParams(hbaseConf, conn);\n\n    Set\u003cString\u003e types \u003d new TreeSet\u003c\u003e();\n    TimelineReaderContext context \u003d getContext();\n    EntityRowKeyPrefix prefix \u003d new EntityRowKeyPrefix(context.getClusterId(),\n        context.getUserId(), context.getFlowName(), context.getFlowRunId(),\n        context.getAppId());\n    byte[] currRowKey \u003d prefix.getRowKeyPrefix();\n    byte[] nextRowKey \u003d prefix.getRowKeyPrefix();\n    nextRowKey[nextRowKey.length - 1]++;\n\n    FilterList typeFilterList \u003d new FilterList();\n    typeFilterList.addFilter(new FirstKeyOnlyFilter());\n    typeFilterList.addFilter(new KeyOnlyFilter());\n    typeFilterList.addFilter(new PageFilter(1));\n    LOG.debug(\"FilterList created for scan is - {}\", typeFilterList);\n\n    int counter \u003d 0;\n    while (true) {\n      try (ResultScanner results \u003d\n          getResult(hbaseConf, conn, typeFilterList, currRowKey, nextRowKey)) {\n        TimelineEntity entity \u003d parseEntityForType(results.next());\n        if (entity \u003d\u003d null) {\n          break;\n        }\n        ++counter;\n        if (!types.add(entity.getType())) {\n          LOG.warn(\"Failed to add type \" + entity.getType()\n              + \" to the result set because there is a duplicated copy. \");\n        }\n        String currType \u003d entity.getType();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Current row key: \" + Arrays.toString(currRowKey));\n          LOG.debug(\"New entity type discovered: \" + currType);\n        }\n        currRowKey \u003d getNextRowKey(prefix.getRowKeyPrefix(), currType);\n      }\n    }\n    LOG.debug(\"Scanned {} records for {} types\", counter, types.size());\n    return types;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/EntityTypeReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/EntityTypeReader.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/EntityTypeReader.java"
      }
    },
    "dcd0bedcc8113dd5e1d852c242ae9511d1b3d58e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-7141. Move logging APIs to slf4j in timelineservice after ATSv2 merge. Contributed by Varun Saxena\n",
      "commitDate": "31/08/17 10:59 PM",
      "commitName": "dcd0bedcc8113dd5e1d852c242ae9511d1b3d58e",
      "commitAuthor": "bibinchundatt",
      "commitDateOld": "29/08/17 10:59 PM",
      "commitNameOld": "3d00c8f3942da931150de79f42cd4913bf751123",
      "commitAuthorOld": "Varun Saxena",
      "daysBetweenCommits": 2.0,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,45 @@\n   public Set\u003cString\u003e readEntityTypes(Configuration hbaseConf,\n       Connection conn) throws IOException {\n \n     validateParams();\n     augmentParams(hbaseConf, conn);\n \n     Set\u003cString\u003e types \u003d new TreeSet\u003c\u003e();\n     TimelineReaderContext context \u003d getContext();\n     EntityRowKeyPrefix prefix \u003d new EntityRowKeyPrefix(context.getClusterId(),\n         context.getUserId(), context.getFlowName(), context.getFlowRunId(),\n         context.getAppId());\n     byte[] currRowKey \u003d prefix.getRowKeyPrefix();\n     byte[] nextRowKey \u003d prefix.getRowKeyPrefix();\n     nextRowKey[nextRowKey.length - 1]++;\n \n     FilterList typeFilterList \u003d new FilterList();\n     typeFilterList.addFilter(new FirstKeyOnlyFilter());\n     typeFilterList.addFilter(new KeyOnlyFilter());\n     typeFilterList.addFilter(new PageFilter(1));\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"FilterList created for scan is - \" + typeFilterList);\n-    }\n+    LOG.debug(\"FilterList created for scan is - {}\", typeFilterList);\n \n     int counter \u003d 0;\n     while (true) {\n       try (ResultScanner results \u003d\n           getResult(hbaseConf, conn, typeFilterList, currRowKey, nextRowKey)) {\n         TimelineEntity entity \u003d parseEntityForType(results.next());\n         if (entity \u003d\u003d null) {\n           break;\n         }\n         ++counter;\n         if (!types.add(entity.getType())) {\n           LOG.warn(\"Failed to add type \" + entity.getType()\n               + \" to the result set because there is a duplicated copy. \");\n         }\n         String currType \u003d entity.getType();\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Current row key: \" + Arrays.toString(currRowKey));\n           LOG.debug(\"New entity type discovered: \" + currType);\n         }\n         currRowKey \u003d getNextRowKey(prefix.getRowKeyPrefix(), currType);\n       }\n     }\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Scanned \" + counter + \"records for \"\n-          + types.size() + \"types\");\n-    }\n+    LOG.debug(\"Scanned {} records for {} types\", counter, types.size());\n     return types;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Set\u003cString\u003e readEntityTypes(Configuration hbaseConf,\n      Connection conn) throws IOException {\n\n    validateParams();\n    augmentParams(hbaseConf, conn);\n\n    Set\u003cString\u003e types \u003d new TreeSet\u003c\u003e();\n    TimelineReaderContext context \u003d getContext();\n    EntityRowKeyPrefix prefix \u003d new EntityRowKeyPrefix(context.getClusterId(),\n        context.getUserId(), context.getFlowName(), context.getFlowRunId(),\n        context.getAppId());\n    byte[] currRowKey \u003d prefix.getRowKeyPrefix();\n    byte[] nextRowKey \u003d prefix.getRowKeyPrefix();\n    nextRowKey[nextRowKey.length - 1]++;\n\n    FilterList typeFilterList \u003d new FilterList();\n    typeFilterList.addFilter(new FirstKeyOnlyFilter());\n    typeFilterList.addFilter(new KeyOnlyFilter());\n    typeFilterList.addFilter(new PageFilter(1));\n    LOG.debug(\"FilterList created for scan is - {}\", typeFilterList);\n\n    int counter \u003d 0;\n    while (true) {\n      try (ResultScanner results \u003d\n          getResult(hbaseConf, conn, typeFilterList, currRowKey, nextRowKey)) {\n        TimelineEntity entity \u003d parseEntityForType(results.next());\n        if (entity \u003d\u003d null) {\n          break;\n        }\n        ++counter;\n        if (!types.add(entity.getType())) {\n          LOG.warn(\"Failed to add type \" + entity.getType()\n              + \" to the result set because there is a duplicated copy. \");\n        }\n        String currType \u003d entity.getType();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Current row key: \" + Arrays.toString(currRowKey));\n          LOG.debug(\"New entity type discovered: \" + currType);\n        }\n        currRowKey \u003d getNextRowKey(prefix.getRowKeyPrefix(), currType);\n      }\n    }\n    LOG.debug(\"Scanned {} records for {} types\", counter, types.size());\n    return types;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/EntityTypeReader.java",
      "extendedDetails": {}
    },
    "4481561e4a3433197dd8e73f38856eef84f0fd03": {
      "type": "Yintroduced",
      "commitMessage": "YARN-5739. Provide timeline reader API to list available timeline entity types for one application. Contributed by Li Lu.\n",
      "commitDate": "29/08/17 10:59 PM",
      "commitName": "4481561e4a3433197dd8e73f38856eef84f0fd03",
      "commitAuthor": "Sangjin Lee",
      "diff": "@@ -0,0 +1,51 @@\n+  public Set\u003cString\u003e readEntityTypes(Configuration hbaseConf,\n+      Connection conn) throws IOException {\n+\n+    validateParams();\n+    augmentParams(hbaseConf, conn);\n+\n+    Set\u003cString\u003e types \u003d new TreeSet\u003c\u003e();\n+    TimelineReaderContext context \u003d getContext();\n+    EntityRowKeyPrefix prefix \u003d new EntityRowKeyPrefix(context.getClusterId(),\n+        context.getUserId(), context.getFlowName(), context.getFlowRunId(),\n+        context.getAppId());\n+    byte[] currRowKey \u003d prefix.getRowKeyPrefix();\n+    byte[] nextRowKey \u003d prefix.getRowKeyPrefix();\n+    nextRowKey[nextRowKey.length - 1]++;\n+\n+    FilterList typeFilterList \u003d new FilterList();\n+    typeFilterList.addFilter(new FirstKeyOnlyFilter());\n+    typeFilterList.addFilter(new KeyOnlyFilter());\n+    typeFilterList.addFilter(new PageFilter(1));\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"FilterList created for scan is - \" + typeFilterList);\n+    }\n+\n+    int counter \u003d 0;\n+    while (true) {\n+      try (ResultScanner results\n+          \u003d getResult(hbaseConf, conn, typeFilterList, currRowKey, nextRowKey))\n+      {\n+        TimelineEntity entity \u003d parseEntityForType(results.next());\n+        if (entity \u003d\u003d null) {\n+          break;\n+        }\n+        ++counter;\n+        if (!types.add(entity.getType())) {\n+          LOG.warn(\"Failed to add type \" + entity.getType()\n+              + \" to the result set because there is a duplicated copy. \");\n+        }\n+        String currType \u003d entity.getType();\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Current row key: \" + Arrays.toString(currRowKey));\n+          LOG.debug(\"New entity type discovered: \" + currType);\n+        }\n+        currRowKey \u003d getNextRowKey(prefix.getRowKeyPrefix(), currType);\n+      }\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Scanned \" + counter + \"records for \"\n+          + types.size() + \"types\");\n+    }\n+    return types;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public Set\u003cString\u003e readEntityTypes(Configuration hbaseConf,\n      Connection conn) throws IOException {\n\n    validateParams();\n    augmentParams(hbaseConf, conn);\n\n    Set\u003cString\u003e types \u003d new TreeSet\u003c\u003e();\n    TimelineReaderContext context \u003d getContext();\n    EntityRowKeyPrefix prefix \u003d new EntityRowKeyPrefix(context.getClusterId(),\n        context.getUserId(), context.getFlowName(), context.getFlowRunId(),\n        context.getAppId());\n    byte[] currRowKey \u003d prefix.getRowKeyPrefix();\n    byte[] nextRowKey \u003d prefix.getRowKeyPrefix();\n    nextRowKey[nextRowKey.length - 1]++;\n\n    FilterList typeFilterList \u003d new FilterList();\n    typeFilterList.addFilter(new FirstKeyOnlyFilter());\n    typeFilterList.addFilter(new KeyOnlyFilter());\n    typeFilterList.addFilter(new PageFilter(1));\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"FilterList created for scan is - \" + typeFilterList);\n    }\n\n    int counter \u003d 0;\n    while (true) {\n      try (ResultScanner results\n          \u003d getResult(hbaseConf, conn, typeFilterList, currRowKey, nextRowKey))\n      {\n        TimelineEntity entity \u003d parseEntityForType(results.next());\n        if (entity \u003d\u003d null) {\n          break;\n        }\n        ++counter;\n        if (!types.add(entity.getType())) {\n          LOG.warn(\"Failed to add type \" + entity.getType()\n              + \" to the result set because there is a duplicated copy. \");\n        }\n        String currType \u003d entity.getType();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Current row key: \" + Arrays.toString(currRowKey));\n          LOG.debug(\"New entity type discovered: \" + currType);\n        }\n        currRowKey \u003d getNextRowKey(prefix.getRowKeyPrefix(), currType);\n      }\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Scanned \" + counter + \"records for \"\n          + types.size() + \"types\");\n    }\n    return types;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/EntityTypeReader.java"
    }
  }
}