{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HHXORErasureEncodingStep.java",
  "functionName": "doEncode",
  "functionId": "doEncode___inputs-ByteBuffer[][]__outputs-ByteBuffer[][]",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java",
  "functionStartLine": 101,
  "functionEndLine": 118,
  "numCommitsSeen": 3,
  "timeTaken": 901,
  "changeHistory": [
    "31ebccc96238136560f4210bdf6766fe18e0650c",
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743"
  ],
  "changeHistoryShort": {
    "31ebccc96238136560f4210bdf6766fe18e0650c": "Yexceptionschange",
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743": "Yintroduced"
  },
  "changeHistoryDetails": {
    "31ebccc96238136560f4210bdf6766fe18e0650c": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-12613. Native EC coder should implement release() as idempotent function. (Lei (Eddy) Xu)\n",
      "commitDate": "16/10/17 7:44 PM",
      "commitName": "31ebccc96238136560f4210bdf6766fe18e0650c",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "17/10/16 11:02 PM",
      "commitNameOld": "c023c748869063fb67d14ea996569c42578d1cea",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 363.86,
      "commitsBetweenForRepo": 2347,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,18 @@\n-  private void doEncode(ByteBuffer[][] inputs, ByteBuffer[][] outputs) {\n+  private void doEncode(ByteBuffer[][] inputs, ByteBuffer[][] outputs)\n+      throws IOException {\n     final int numParityUnits \u003d this.rsRawEncoder.getNumParityUnits();\n \n     // calc piggyBacks using first sub-packet\n     ByteBuffer[] piggyBacks \u003d HHUtil.getPiggyBacksFromInput(inputs[0],\n             piggyBackIndex, numParityUnits, 0, xorRawEncoder);\n \n     // Step1: RS encode each byte-stripe of sub-packets\n     for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n       rsRawEncoder.encode(inputs[i], outputs[i]);\n     }\n \n     // Step2: Adding piggybacks to the parities\n     // Only second sub-packet is added with a piggyback.\n     encodeWithPiggyBacks(piggyBacks, outputs, numParityUnits,\n             inputs[0][0].isDirect());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void doEncode(ByteBuffer[][] inputs, ByteBuffer[][] outputs)\n      throws IOException {\n    final int numParityUnits \u003d this.rsRawEncoder.getNumParityUnits();\n\n    // calc piggyBacks using first sub-packet\n    ByteBuffer[] piggyBacks \u003d HHUtil.getPiggyBacksFromInput(inputs[0],\n            piggyBackIndex, numParityUnits, 0, xorRawEncoder);\n\n    // Step1: RS encode each byte-stripe of sub-packets\n    for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n      rsRawEncoder.encode(inputs[i], outputs[i]);\n    }\n\n    // Step2: Adding piggybacks to the parities\n    // Only second sub-packet is added with a piggyback.\n    encodeWithPiggyBacks(piggyBacks, outputs, numParityUnits,\n            inputs[0][0].isDirect());\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[IOException]"
      }
    },
    "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-11828. Implement the Hitchhiker erasure coding algorithm. Contributed by Jack Liu Quan.\n\nChange-Id: If43475ccc2574df60949c947af562722db076251\n",
      "commitDate": "21/01/16 10:30 AM",
      "commitName": "1bb31fb22e6f8e6df8e9ff4e94adf20308b4c743",
      "commitAuthor": "Zhe Zhang",
      "diff": "@@ -0,0 +1,17 @@\n+  private void doEncode(ByteBuffer[][] inputs, ByteBuffer[][] outputs) {\n+    final int numParityUnits \u003d this.rsRawEncoder.getNumParityUnits();\n+\n+    // calc piggyBacks using first sub-packet\n+    ByteBuffer[] piggyBacks \u003d HHUtil.getPiggyBacksFromInput(inputs[0],\n+            piggyBackIndex, numParityUnits, 0, xorRawEncoder);\n+\n+    // Step1: RS encode each byte-stripe of sub-packets\n+    for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n+      rsRawEncoder.encode(inputs[i], outputs[i]);\n+    }\n+\n+    // Step2: Adding piggybacks to the parities\n+    // Only second sub-packet is added with a piggyback.\n+    encodeWithPiggyBacks(piggyBacks, outputs, numParityUnits,\n+            inputs[0][0].isDirect());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void doEncode(ByteBuffer[][] inputs, ByteBuffer[][] outputs) {\n    final int numParityUnits \u003d this.rsRawEncoder.getNumParityUnits();\n\n    // calc piggyBacks using first sub-packet\n    ByteBuffer[] piggyBacks \u003d HHUtil.getPiggyBacksFromInput(inputs[0],\n            piggyBackIndex, numParityUnits, 0, xorRawEncoder);\n\n    // Step1: RS encode each byte-stripe of sub-packets\n    for (int i \u003d 0; i \u003c getSubPacketSize(); ++i) {\n      rsRawEncoder.encode(inputs[i], outputs[i]);\n    }\n\n    // Step2: Adding piggybacks to the parities\n    // Only second sub-packet is added with a piggyback.\n    encodeWithPiggyBacks(piggyBacks, outputs, numParityUnits,\n            inputs[0][0].isDirect());\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java"
    }
  }
}