{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReaderRemote.java",
  "functionName": "newBlockReader",
  "functionId": "newBlockReader___file-String__block-ExtendedBlock__blockToken-Token__BlockTokenIdentifier____startOffset-long__len-long__verifyChecksum-boolean__clientName-String__peer-Peer__datanodeID-DatanodeID__peerCache-PeerCache__cachingStrategy-CachingStrategy__networkDistance-int__configuration-Configuration",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
  "functionStartLine": 389,
  "functionEndLine": 435,
  "numCommitsSeen": 91,
  "timeTaken": 8948,
  "changeHistory": [
    "3f223bebfa6b382a762edcc518fcbae310ce22e5",
    "5d748bd056a32f2c6922514cd0c5b31d866a9919",
    "8b281bce85474501868d68f8d5590a6086abb7b7",
    "f308561f1d885491b88db73ac63003202056d661",
    "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "826ae1c26d31f87d88efc920b271bec7eec2e17a",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f"
  ],
  "changeHistoryShort": {
    "3f223bebfa6b382a762edcc518fcbae310ce22e5": "Ymultichange(Yparameterchange,Ybodychange)",
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": "Ymultichange(Yparameterchange,Ybodychange)",
    "8b281bce85474501868d68f8d5590a6086abb7b7": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "f308561f1d885491b88db73ac63003202056d661": "Ymultichange(Yfilerename,Yreturntypechange,Ybodychange)",
    "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b": "Ymultichange(Yparameterchange,Ybodychange)",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ymultichange(Yparameterchange,Ybodychange)",
    "826ae1c26d31f87d88efc920b271bec7eec2e17a": "Yfilerename",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": "Ybodychange",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": "Ymultichange(Yparameterchange,Ybodychange)",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "3f223bebfa6b382a762edcc518fcbae310ce22e5": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-14844. Make buffer of BlockReaderRemote#newBlockReader#BufferedOutputStream configurable. Contributed by Lisheng Sun.\n",
      "commitDate": "20/09/19 11:45 AM",
      "commitName": "3f223bebfa6b382a762edcc518fcbae310ce22e5",
      "commitAuthor": "Inigo Goiri",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-14844. Make buffer of BlockReaderRemote#newBlockReader#BufferedOutputStream configurable. Contributed by Lisheng Sun.\n",
          "commitDate": "20/09/19 11:45 AM",
          "commitName": "3f223bebfa6b382a762edcc518fcbae310ce22e5",
          "commitAuthor": "Inigo Goiri",
          "commitDateOld": "03/07/18 2:07 AM",
          "commitNameOld": "344f324710522ffb27852c1a673c4f7d3d6eac4b",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 444.4,
          "commitsBetweenForRepo": 3478,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,47 @@\n   public static BlockReader newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n       boolean verifyChecksum,\n       String clientName,\n       Peer peer, DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n-      int networkDistance) throws IOException {\n+      int networkDistance, Configuration configuration) throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n+    int bufferSize \u003d configuration.getInt(\n+        DFS_CLIENT_BLOCK_READER_REMOTE_BUFFER_SIZE_KEY,\n+        DFS_CLIENT_BLOCK_READER_REMOTE_BUFFER_SIZE_DEFAULT);\n     final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n-        peer.getOutputStream()));\n+        peer.getOutputStream(), bufferSize));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n     // Get bytes in block\n     //\n     DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n     checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n     return new BlockReaderRemote(file, block.getBlockId(), checksum,\n         verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n         peerCache, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockReader newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      boolean verifyChecksum,\n      String clientName,\n      Peer peer, DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      int networkDistance, Configuration configuration) throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    int bufferSize \u003d configuration.getInt(\n        DFS_CLIENT_BLOCK_READER_REMOTE_BUFFER_SIZE_KEY,\n        DFS_CLIENT_BLOCK_READER_REMOTE_BUFFER_SIZE_DEFAULT);\n    final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n        peer.getOutputStream(), bufferSize));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block\n    //\n    DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new BlockReaderRemote(file, block.getBlockId(), checksum,\n        verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n        peerCache, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
          "extendedDetails": {
            "oldValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache, cachingStrategy-CachingStrategy, networkDistance-int]",
            "newValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache, cachingStrategy-CachingStrategy, networkDistance-int, configuration-Configuration]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-14844. Make buffer of BlockReaderRemote#newBlockReader#BufferedOutputStream configurable. Contributed by Lisheng Sun.\n",
          "commitDate": "20/09/19 11:45 AM",
          "commitName": "3f223bebfa6b382a762edcc518fcbae310ce22e5",
          "commitAuthor": "Inigo Goiri",
          "commitDateOld": "03/07/18 2:07 AM",
          "commitNameOld": "344f324710522ffb27852c1a673c4f7d3d6eac4b",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 444.4,
          "commitsBetweenForRepo": 3478,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,47 @@\n   public static BlockReader newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n       boolean verifyChecksum,\n       String clientName,\n       Peer peer, DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n-      int networkDistance) throws IOException {\n+      int networkDistance, Configuration configuration) throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n+    int bufferSize \u003d configuration.getInt(\n+        DFS_CLIENT_BLOCK_READER_REMOTE_BUFFER_SIZE_KEY,\n+        DFS_CLIENT_BLOCK_READER_REMOTE_BUFFER_SIZE_DEFAULT);\n     final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n-        peer.getOutputStream()));\n+        peer.getOutputStream(), bufferSize));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n     // Get bytes in block\n     //\n     DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n     checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n     return new BlockReaderRemote(file, block.getBlockId(), checksum,\n         verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n         peerCache, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockReader newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      boolean verifyChecksum,\n      String clientName,\n      Peer peer, DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      int networkDistance, Configuration configuration) throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    int bufferSize \u003d configuration.getInt(\n        DFS_CLIENT_BLOCK_READER_REMOTE_BUFFER_SIZE_KEY,\n        DFS_CLIENT_BLOCK_READER_REMOTE_BUFFER_SIZE_DEFAULT);\n    final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n        peer.getOutputStream(), bufferSize));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block\n    //\n    DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new BlockReaderRemote(file, block.getBlockId(), checksum,\n        verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n        peerCache, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
          "extendedDetails": {}
        }
      ]
    },
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13702. Remove HTrace hooks from DFSClient to reduce CPU usage. Contributed by Todd Lipcon.\n",
      "commitDate": "02/07/18 3:11 AM",
      "commitName": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13702. Remove HTrace hooks from DFSClient to reduce CPU usage. Contributed by Todd Lipcon.\n",
          "commitDate": "02/07/18 3:11 AM",
          "commitName": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "02/07/16 8:56 PM",
          "commitNameOld": "8b281bce85474501868d68f8d5590a6086abb7b7",
          "commitAuthorOld": "Kai Zheng",
          "daysBetweenCommits": 729.26,
          "commitsBetweenForRepo": 5537,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,44 @@\n   public static BlockReader newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n       boolean verifyChecksum,\n       String clientName,\n       Peer peer, DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n-      Tracer tracer,\n       int networkDistance) throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n         peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n     // Get bytes in block\n     //\n     DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n     checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n     return new BlockReaderRemote(file, block.getBlockId(), checksum,\n         verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n-        peerCache, tracer, networkDistance);\n+        peerCache, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockReader newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      boolean verifyChecksum,\n      String clientName,\n      Peer peer, DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      int networkDistance) throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n        peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block\n    //\n    DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new BlockReaderRemote(file, block.getBlockId(), checksum,\n        verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n        peerCache, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
          "extendedDetails": {
            "oldValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache, cachingStrategy-CachingStrategy, tracer-Tracer, networkDistance-int]",
            "newValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache, cachingStrategy-CachingStrategy, networkDistance-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13702. Remove HTrace hooks from DFSClient to reduce CPU usage. Contributed by Todd Lipcon.\n",
          "commitDate": "02/07/18 3:11 AM",
          "commitName": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "02/07/16 8:56 PM",
          "commitNameOld": "8b281bce85474501868d68f8d5590a6086abb7b7",
          "commitAuthorOld": "Kai Zheng",
          "daysBetweenCommits": 729.26,
          "commitsBetweenForRepo": 5537,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,44 @@\n   public static BlockReader newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n       boolean verifyChecksum,\n       String clientName,\n       Peer peer, DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n-      Tracer tracer,\n       int networkDistance) throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n         peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n     // Get bytes in block\n     //\n     DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n     checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n     return new BlockReaderRemote(file, block.getBlockId(), checksum,\n         verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n-        peerCache, tracer, networkDistance);\n+        peerCache, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockReader newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      boolean verifyChecksum,\n      String clientName,\n      Peer peer, DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      int networkDistance) throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n        peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block\n    //\n    DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new BlockReaderRemote(file, block.getBlockId(), checksum,\n        verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n        peerCache, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
          "extendedDetails": {}
        }
      ]
    },
    "8b281bce85474501868d68f8d5590a6086abb7b7": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-10548. Remove the long deprecated BlockReaderRemote. Contributed by Kai Zheng\n",
      "commitDate": "02/07/16 8:56 PM",
      "commitName": "8b281bce85474501868d68f8d5590a6086abb7b7",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10548. Remove the long deprecated BlockReaderRemote. Contributed by Kai Zheng\n",
          "commitDate": "02/07/16 8:56 PM",
          "commitName": "8b281bce85474501868d68f8d5590a6086abb7b7",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "25/04/16 12:01 PM",
          "commitNameOld": "f308561f1d885491b88db73ac63003202056d661",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 68.37,
          "commitsBetweenForRepo": 478,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,45 @@\n-  public static BlockReaderRemote newBlockReader(String file,\n+  public static BlockReader newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n-      int bufferSize, boolean verifyChecksum,\n-      String clientName, Peer peer,\n-      DatanodeID datanodeID,\n+      boolean verifyChecksum,\n+      String clientName,\n+      Peer peer, DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n-      Tracer tracer, int networkDistance)\n-      throws IOException {\n+      Tracer tracer,\n+      int networkDistance) throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n-    final DataOutputStream out \u003d\n-        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n+    final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n+        peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n-    // Get bytes in block, set streams\n+    // Get bytes in block\n     //\n-\n-    DataInputStream in \u003d new DataInputStream(\n-        new BufferedInputStream(peer.getInputStream(), bufferSize));\n+    DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n-    BlockReaderRemote2.checkSuccess(status, peer, block, file);\n+    checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n-    return new BlockReaderRemote(file, block.getBlockPoolId(), block.getBlockId(),\n-        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n-        peer, datanodeID, peerCache, tracer, networkDistance);\n+    return new BlockReaderRemote(file, block.getBlockId(), checksum,\n+        verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n+        peerCache, tracer, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockReader newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      boolean verifyChecksum,\n      String clientName,\n      Peer peer, DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      Tracer tracer,\n      int networkDistance) throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n        peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block\n    //\n    DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new BlockReaderRemote(file, block.getBlockId(), checksum,\n        verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n        peerCache, tracer, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
          "extendedDetails": {
            "oldValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, bufferSize-int, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache, cachingStrategy-CachingStrategy, tracer-Tracer, networkDistance-int]",
            "newValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache, cachingStrategy-CachingStrategy, tracer-Tracer, networkDistance-int]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-10548. Remove the long deprecated BlockReaderRemote. Contributed by Kai Zheng\n",
          "commitDate": "02/07/16 8:56 PM",
          "commitName": "8b281bce85474501868d68f8d5590a6086abb7b7",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "25/04/16 12:01 PM",
          "commitNameOld": "f308561f1d885491b88db73ac63003202056d661",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 68.37,
          "commitsBetweenForRepo": 478,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,45 @@\n-  public static BlockReaderRemote newBlockReader(String file,\n+  public static BlockReader newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n-      int bufferSize, boolean verifyChecksum,\n-      String clientName, Peer peer,\n-      DatanodeID datanodeID,\n+      boolean verifyChecksum,\n+      String clientName,\n+      Peer peer, DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n-      Tracer tracer, int networkDistance)\n-      throws IOException {\n+      Tracer tracer,\n+      int networkDistance) throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n-    final DataOutputStream out \u003d\n-        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n+    final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n+        peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n-    // Get bytes in block, set streams\n+    // Get bytes in block\n     //\n-\n-    DataInputStream in \u003d new DataInputStream(\n-        new BufferedInputStream(peer.getInputStream(), bufferSize));\n+    DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n-    BlockReaderRemote2.checkSuccess(status, peer, block, file);\n+    checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n-    return new BlockReaderRemote(file, block.getBlockPoolId(), block.getBlockId(),\n-        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n-        peer, datanodeID, peerCache, tracer, networkDistance);\n+    return new BlockReaderRemote(file, block.getBlockId(), checksum,\n+        verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n+        peerCache, tracer, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockReader newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      boolean verifyChecksum,\n      String clientName,\n      Peer peer, DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      Tracer tracer,\n      int networkDistance) throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n        peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block\n    //\n    DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new BlockReaderRemote(file, block.getBlockId(), checksum,\n        verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n        peerCache, tracer, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
          "extendedDetails": {
            "oldValue": "BlockReaderRemote",
            "newValue": "BlockReader"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10548. Remove the long deprecated BlockReaderRemote. Contributed by Kai Zheng\n",
          "commitDate": "02/07/16 8:56 PM",
          "commitName": "8b281bce85474501868d68f8d5590a6086abb7b7",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "25/04/16 12:01 PM",
          "commitNameOld": "f308561f1d885491b88db73ac63003202056d661",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 68.37,
          "commitsBetweenForRepo": 478,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,45 @@\n-  public static BlockReaderRemote newBlockReader(String file,\n+  public static BlockReader newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n-      int bufferSize, boolean verifyChecksum,\n-      String clientName, Peer peer,\n-      DatanodeID datanodeID,\n+      boolean verifyChecksum,\n+      String clientName,\n+      Peer peer, DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n-      Tracer tracer, int networkDistance)\n-      throws IOException {\n+      Tracer tracer,\n+      int networkDistance) throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n-    final DataOutputStream out \u003d\n-        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n+    final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n+        peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n-    // Get bytes in block, set streams\n+    // Get bytes in block\n     //\n-\n-    DataInputStream in \u003d new DataInputStream(\n-        new BufferedInputStream(peer.getInputStream(), bufferSize));\n+    DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n-    BlockReaderRemote2.checkSuccess(status, peer, block, file);\n+    checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n-    return new BlockReaderRemote(file, block.getBlockPoolId(), block.getBlockId(),\n-        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n-        peer, datanodeID, peerCache, tracer, networkDistance);\n+    return new BlockReaderRemote(file, block.getBlockId(), checksum,\n+        verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n+        peerCache, tracer, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockReader newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      boolean verifyChecksum,\n      String clientName,\n      Peer peer, DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      Tracer tracer,\n      int networkDistance) throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n        peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block\n    //\n    DataInputStream in \u003d new DataInputStream(peer.getInputStream());\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new BlockReaderRemote(file, block.getBlockId(), checksum,\n        verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID,\n        peerCache, tracer, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
          "extendedDetails": {}
        }
      ]
    },
    "f308561f1d885491b88db73ac63003202056d661": {
      "type": "Ymultichange(Yfilerename,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
      "commitDate": "25/04/16 12:01 PM",
      "commitName": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
          "commitDate": "25/04/16 12:01 PM",
          "commitName": "f308561f1d885491b88db73ac63003202056d661",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "25/04/16 9:38 AM",
          "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 0.1,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,47 @@\n-  public static RemoteBlockReader newBlockReader(String file,\n+  public static BlockReaderRemote newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n       int bufferSize, boolean verifyChecksum,\n       String clientName, Peer peer,\n       DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n       Tracer tracer, int networkDistance)\n       throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n-    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n+    BlockReaderRemote2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n-    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n+    return new BlockReaderRemote(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n         peer, datanodeID, peerCache, tracer, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockReaderRemote newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      int bufferSize, boolean verifyChecksum,\n      String clientName, Peer peer,\n      DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      Tracer tracer, int networkDistance)\n      throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    BlockReaderRemote2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new BlockReaderRemote(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache, tracer, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
          "commitDate": "25/04/16 12:01 PM",
          "commitName": "f308561f1d885491b88db73ac63003202056d661",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "25/04/16 9:38 AM",
          "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 0.1,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,47 @@\n-  public static RemoteBlockReader newBlockReader(String file,\n+  public static BlockReaderRemote newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n       int bufferSize, boolean verifyChecksum,\n       String clientName, Peer peer,\n       DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n       Tracer tracer, int networkDistance)\n       throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n-    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n+    BlockReaderRemote2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n-    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n+    return new BlockReaderRemote(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n         peer, datanodeID, peerCache, tracer, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockReaderRemote newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      int bufferSize, boolean verifyChecksum,\n      String clientName, Peer peer,\n      DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      Tracer tracer, int networkDistance)\n      throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    BlockReaderRemote2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new BlockReaderRemote(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache, tracer, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
          "extendedDetails": {
            "oldValue": "RemoteBlockReader",
            "newValue": "BlockReaderRemote"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
          "commitDate": "25/04/16 12:01 PM",
          "commitName": "f308561f1d885491b88db73ac63003202056d661",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "25/04/16 9:38 AM",
          "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 0.1,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,47 @@\n-  public static RemoteBlockReader newBlockReader(String file,\n+  public static BlockReaderRemote newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n       int bufferSize, boolean verifyChecksum,\n       String clientName, Peer peer,\n       DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n       Tracer tracer, int networkDistance)\n       throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n-    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n+    BlockReaderRemote2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n-    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n+    return new BlockReaderRemote(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n         peer, datanodeID, peerCache, tracer, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockReaderRemote newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      int bufferSize, boolean verifyChecksum,\n      String clientName, Peer peer,\n      DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      Tracer tracer, int networkDistance)\n      throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    BlockReaderRemote2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new BlockReaderRemote(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache, tracer, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
          "extendedDetails": {}
        }
      ]
    },
    "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9579. Provide bytes-read-by-network-distance metrics at FileSystem.Statistics level (Ming Ma via sjlee)\n",
      "commitDate": "19/03/16 2:02 PM",
      "commitName": "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b",
      "commitAuthor": "Sangjin Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9579. Provide bytes-read-by-network-distance metrics at FileSystem.Statistics level (Ming Ma via sjlee)\n",
          "commitDate": "19/03/16 2:02 PM",
          "commitName": "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b",
          "commitAuthor": "Sangjin Lee",
          "commitDateOld": "03/10/15 11:38 AM",
          "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 168.1,
          "commitsBetweenForRepo": 1133,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,47 @@\n   public static RemoteBlockReader newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n       int bufferSize, boolean verifyChecksum,\n       String clientName, Peer peer,\n       DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n-      Tracer tracer)\n+      Tracer tracer, int networkDistance)\n       throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n     RemoteBlockReader2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n     return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n-        peer, datanodeID, peerCache, tracer);\n+        peer, datanodeID, peerCache, tracer, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static RemoteBlockReader newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      int bufferSize, boolean verifyChecksum,\n      String clientName, Peer peer,\n      DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      Tracer tracer, int networkDistance)\n      throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache, tracer, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
          "extendedDetails": {
            "oldValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, bufferSize-int, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache, cachingStrategy-CachingStrategy, tracer-Tracer]",
            "newValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, bufferSize-int, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache, cachingStrategy-CachingStrategy, tracer-Tracer, networkDistance-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9579. Provide bytes-read-by-network-distance metrics at FileSystem.Statistics level (Ming Ma via sjlee)\n",
          "commitDate": "19/03/16 2:02 PM",
          "commitName": "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b",
          "commitAuthor": "Sangjin Lee",
          "commitDateOld": "03/10/15 11:38 AM",
          "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 168.1,
          "commitsBetweenForRepo": 1133,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,47 @@\n   public static RemoteBlockReader newBlockReader(String file,\n       ExtendedBlock block,\n       Token\u003cBlockTokenIdentifier\u003e blockToken,\n       long startOffset, long len,\n       int bufferSize, boolean verifyChecksum,\n       String clientName, Peer peer,\n       DatanodeID datanodeID,\n       PeerCache peerCache,\n       CachingStrategy cachingStrategy,\n-      Tracer tracer)\n+      Tracer tracer, int networkDistance)\n       throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n     RemoteBlockReader2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n         status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n           firstChunkOffset + \") startOffset is \" +\n           startOffset + \" for file \" + file);\n     }\n \n     return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n-        peer, datanodeID, peerCache, tracer);\n+        peer, datanodeID, peerCache, tracer, networkDistance);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static RemoteBlockReader newBlockReader(String file,\n      ExtendedBlock block,\n      Token\u003cBlockTokenIdentifier\u003e blockToken,\n      long startOffset, long len,\n      int bufferSize, boolean verifyChecksum,\n      String clientName, Peer peer,\n      DatanodeID datanodeID,\n      PeerCache peerCache,\n      CachingStrategy cachingStrategy,\n      Tracer tracer, int networkDistance)\n      throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n\n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n\n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n        status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n\n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n\n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n          firstChunkOffset + \") startOffset is \" +\n          startOffset + \" for file \" + file);\n    }\n\n    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache, tracer, networkDistance);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
          "extendedDetails": {}
        }
      ]
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
          "commitDate": "28/09/15 7:42 AM",
          "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "31/08/15 1:54 PM",
          "commitNameOld": "826ae1c26d31f87d88efc920b271bec7eec2e17a",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 27.74,
          "commitsBetweenForRepo": 189,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,46 +1,47 @@\n   public static RemoteBlockReader newBlockReader(String file,\n                                      ExtendedBlock block, \n                                      Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                      long startOffset, long len,\n                                      int bufferSize, boolean verifyChecksum,\n                                      String clientName, Peer peer,\n                                      DatanodeID datanodeID,\n                                      PeerCache peerCache,\n-                                     CachingStrategy cachingStrategy)\n+                                     CachingStrategy cachingStrategy,\n+                                     Tracer tracer)\n                                        throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n     \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n     \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n     RemoteBlockReader2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n       status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n     \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n     \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                             firstChunkOffset + \") startOffset is \" + \n                             startOffset + \" for file \" + file);\n     }\n \n     return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n-        peer, datanodeID, peerCache);\n+        peer, datanodeID, peerCache, tracer);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static RemoteBlockReader newBlockReader(String file,\n                                     ExtendedBlock block, \n                                     Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                     long startOffset, long len,\n                                     int bufferSize, boolean verifyChecksum,\n                                     String clientName, Peer peer,\n                                     DatanodeID datanodeID,\n                                     PeerCache peerCache,\n                                     CachingStrategy cachingStrategy,\n                                     Tracer tracer)\n                                       throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n    \n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n    \n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n      status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n    \n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n    \n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                            firstChunkOffset + \") startOffset is \" + \n                            startOffset + \" for file \" + file);\n    }\n\n    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache, tracer);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
          "extendedDetails": {
            "oldValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, bufferSize-int, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache, cachingStrategy-CachingStrategy]",
            "newValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, bufferSize-int, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache, cachingStrategy-CachingStrategy, tracer-Tracer]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
          "commitDate": "28/09/15 7:42 AM",
          "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "31/08/15 1:54 PM",
          "commitNameOld": "826ae1c26d31f87d88efc920b271bec7eec2e17a",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 27.74,
          "commitsBetweenForRepo": 189,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,46 +1,47 @@\n   public static RemoteBlockReader newBlockReader(String file,\n                                      ExtendedBlock block, \n                                      Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                      long startOffset, long len,\n                                      int bufferSize, boolean verifyChecksum,\n                                      String clientName, Peer peer,\n                                      DatanodeID datanodeID,\n                                      PeerCache peerCache,\n-                                     CachingStrategy cachingStrategy)\n+                                     CachingStrategy cachingStrategy,\n+                                     Tracer tracer)\n                                        throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n     \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n     \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelperClient.vintPrefixed(in));\n     RemoteBlockReader2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n       status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n     \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n     \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                             firstChunkOffset + \") startOffset is \" + \n                             startOffset + \" for file \" + file);\n     }\n \n     return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n-        peer, datanodeID, peerCache);\n+        peer, datanodeID, peerCache, tracer);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static RemoteBlockReader newBlockReader(String file,\n                                     ExtendedBlock block, \n                                     Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                     long startOffset, long len,\n                                     int bufferSize, boolean verifyChecksum,\n                                     String clientName, Peer peer,\n                                     DatanodeID datanodeID,\n                                     PeerCache peerCache,\n                                     CachingStrategy cachingStrategy,\n                                     Tracer tracer)\n                                       throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n    \n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n    \n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n      status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n    \n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n    \n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                            firstChunkOffset + \") startOffset is \" + \n                            startOffset + \" for file \" + file);\n    }\n\n    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache, tracer);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
          "extendedDetails": {}
        }
      ]
    },
    "826ae1c26d31f87d88efc920b271bec7eec2e17a": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8990. Move RemoteBlockReader to hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "31/08/15 1:54 PM",
      "commitName": "826ae1c26d31f87d88efc920b271bec7eec2e17a",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "31/08/15 11:48 AM",
      "commitNameOld": "caa04de149030691b7bc952b534c6128db217ed2",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.09,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static RemoteBlockReader newBlockReader(String file,\n                                     ExtendedBlock block, \n                                     Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                     long startOffset, long len,\n                                     int bufferSize, boolean verifyChecksum,\n                                     String clientName, Peer peer,\n                                     DatanodeID datanodeID,\n                                     PeerCache peerCache,\n                                     CachingStrategy cachingStrategy)\n                                       throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n    \n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n    \n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n      status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n    \n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n    \n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                            firstChunkOffset + \") startOffset is \" + \n                            startOffset + \" for file \" + file);\n    }\n\n    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java"
      }
    },
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8934. Move ShortCircuitShm to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "22/08/15 1:31 PM",
      "commitName": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/04/15 3:11 AM",
      "commitNameOld": "e89fc53a1d264fde407dd2c36defab5241cd0b52",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 114.43,
      "commitsBetweenForRepo": 845,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n   public static RemoteBlockReader newBlockReader(String file,\n                                      ExtendedBlock block, \n                                      Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                      long startOffset, long len,\n                                      int bufferSize, boolean verifyChecksum,\n                                      String clientName, Peer peer,\n                                      DatanodeID datanodeID,\n                                      PeerCache peerCache,\n                                      CachingStrategy cachingStrategy)\n                                        throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum, cachingStrategy);\n     \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n     \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n-        PBHelper.vintPrefixed(in));\n+        PBHelperClient.vintPrefixed(in));\n     RemoteBlockReader2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n       status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n     \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n     \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                             firstChunkOffset + \") startOffset is \" + \n                             startOffset + \" for file \" + file);\n     }\n \n     return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n         peer, datanodeID, peerCache);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static RemoteBlockReader newBlockReader(String file,\n                                     ExtendedBlock block, \n                                     Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                     long startOffset, long len,\n                                     int bufferSize, boolean verifyChecksum,\n                                     String clientName, Peer peer,\n                                     DatanodeID datanodeID,\n                                     PeerCache peerCache,\n                                     CachingStrategy cachingStrategy)\n                                       throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n    \n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n    \n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelperClient.vintPrefixed(in));\n    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n      status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n    \n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n    \n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                            firstChunkOffset + \") startOffset is \" + \n                            startOffset + \" for file \" + file);\n    }\n\n    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
      "extendedDetails": {}
    },
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4817.  Make HDFS advisory caching configurable on a per-file basis.  (Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505753 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/13 11:15 AM",
      "commitName": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4817.  Make HDFS advisory caching configurable on a per-file basis.  (Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505753 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/13 11:15 AM",
          "commitName": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "19/06/13 9:43 PM",
          "commitNameOld": "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 32.56,
          "commitsBetweenForRepo": 152,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,46 @@\n   public static RemoteBlockReader newBlockReader(String file,\n                                      ExtendedBlock block, \n                                      Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                      long startOffset, long len,\n                                      int bufferSize, boolean verifyChecksum,\n                                      String clientName, Peer peer,\n                                      DatanodeID datanodeID,\n-                                     PeerCache peerCache)\n-                                     throws IOException {\n+                                     PeerCache peerCache,\n+                                     CachingStrategy cachingStrategy)\n+                                       throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n-        verifyChecksum);\n+        verifyChecksum, cachingStrategy);\n     \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n     \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelper.vintPrefixed(in));\n     RemoteBlockReader2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n       status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n     \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n     \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                             firstChunkOffset + \") startOffset is \" + \n                             startOffset + \" for file \" + file);\n     }\n \n     return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n         peer, datanodeID, peerCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static RemoteBlockReader newBlockReader(String file,\n                                     ExtendedBlock block, \n                                     Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                     long startOffset, long len,\n                                     int bufferSize, boolean verifyChecksum,\n                                     String clientName, Peer peer,\n                                     DatanodeID datanodeID,\n                                     PeerCache peerCache,\n                                     CachingStrategy cachingStrategy)\n                                       throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n    \n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n    \n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelper.vintPrefixed(in));\n    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n      status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n    \n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n    \n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                            firstChunkOffset + \") startOffset is \" + \n                            startOffset + \" for file \" + file);\n    }\n\n    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
          "extendedDetails": {
            "oldValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, bufferSize-int, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache]",
            "newValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, bufferSize-int, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache, cachingStrategy-CachingStrategy]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4817.  Make HDFS advisory caching configurable on a per-file basis.  (Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505753 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/13 11:15 AM",
          "commitName": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "19/06/13 9:43 PM",
          "commitNameOld": "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 32.56,
          "commitsBetweenForRepo": 152,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,46 @@\n   public static RemoteBlockReader newBlockReader(String file,\n                                      ExtendedBlock block, \n                                      Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                      long startOffset, long len,\n                                      int bufferSize, boolean verifyChecksum,\n                                      String clientName, Peer peer,\n                                      DatanodeID datanodeID,\n-                                     PeerCache peerCache)\n-                                     throws IOException {\n+                                     PeerCache peerCache,\n+                                     CachingStrategy cachingStrategy)\n+                                       throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n-        verifyChecksum);\n+        verifyChecksum, cachingStrategy);\n     \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n     \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelper.vintPrefixed(in));\n     RemoteBlockReader2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n       status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n     \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n     \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                             firstChunkOffset + \") startOffset is \" + \n                             startOffset + \" for file \" + file);\n     }\n \n     return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n         peer, datanodeID, peerCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static RemoteBlockReader newBlockReader(String file,\n                                     ExtendedBlock block, \n                                     Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                     long startOffset, long len,\n                                     int bufferSize, boolean verifyChecksum,\n                                     String clientName, Peer peer,\n                                     DatanodeID datanodeID,\n                                     PeerCache peerCache,\n                                     CachingStrategy cachingStrategy)\n                                       throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum, cachingStrategy);\n    \n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n    \n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelper.vintPrefixed(in));\n    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n      status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n    \n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n    \n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                            firstChunkOffset + \") startOffset is \" + \n                            startOffset + \" for file \" + file);\n    }\n\n    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
          "extendedDetails": {}
        }
      ]
    },
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4661. A few little code cleanups of some HDFS-347-related code. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1480839 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/13 5:03 PM",
      "commitName": "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
      "commitAuthor": "Aaron Myers",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4661. A few little code cleanups of some HDFS-347-related code. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1480839 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/05/13 5:03 PM",
          "commitName": "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "14/01/13 1:12 PM",
          "commitNameOld": "12bf674e8eeae15ff9ad86dccd91ef644ab52775",
          "commitAuthorOld": "",
          "daysBetweenCommits": 115.12,
          "commitsBetweenForRepo": 680,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,45 @@\n   public static RemoteBlockReader newBlockReader(String file,\n                                      ExtendedBlock block, \n                                      Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                      long startOffset, long len,\n                                      int bufferSize, boolean verifyChecksum,\n                                      String clientName, Peer peer,\n-                                     DatanodeID datanodeID)\n+                                     DatanodeID datanodeID,\n+                                     PeerCache peerCache)\n                                      throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum);\n     \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n     \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelper.vintPrefixed(in));\n     RemoteBlockReader2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n       status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n     \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n     \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                             firstChunkOffset + \") startOffset is \" + \n                             startOffset + \" for file \" + file);\n     }\n \n     return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n-        peer, datanodeID);\n+        peer, datanodeID, peerCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static RemoteBlockReader newBlockReader(String file,\n                                     ExtendedBlock block, \n                                     Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                     long startOffset, long len,\n                                     int bufferSize, boolean verifyChecksum,\n                                     String clientName, Peer peer,\n                                     DatanodeID datanodeID,\n                                     PeerCache peerCache)\n                                     throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum);\n    \n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n    \n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelper.vintPrefixed(in));\n    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n      status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n    \n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n    \n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                            firstChunkOffset + \") startOffset is \" + \n                            startOffset + \" for file \" + file);\n    }\n\n    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
          "extendedDetails": {
            "oldValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, bufferSize-int, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID]",
            "newValue": "[file-String, block-ExtendedBlock, blockToken-Token\u003cBlockTokenIdentifier\u003e, startOffset-long, len-long, bufferSize-int, verifyChecksum-boolean, clientName-String, peer-Peer, datanodeID-DatanodeID, peerCache-PeerCache]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4661. A few little code cleanups of some HDFS-347-related code. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1480839 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/05/13 5:03 PM",
          "commitName": "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "14/01/13 1:12 PM",
          "commitNameOld": "12bf674e8eeae15ff9ad86dccd91ef644ab52775",
          "commitAuthorOld": "",
          "daysBetweenCommits": 115.12,
          "commitsBetweenForRepo": 680,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,45 @@\n   public static RemoteBlockReader newBlockReader(String file,\n                                      ExtendedBlock block, \n                                      Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                      long startOffset, long len,\n                                      int bufferSize, boolean verifyChecksum,\n                                      String clientName, Peer peer,\n-                                     DatanodeID datanodeID)\n+                                     DatanodeID datanodeID,\n+                                     PeerCache peerCache)\n                                      throws IOException {\n     // in and out will be closed when sock is closed (by the caller)\n     final DataOutputStream out \u003d\n         new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n     new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n         verifyChecksum);\n     \n     //\n     // Get bytes in block, set streams\n     //\n \n     DataInputStream in \u003d new DataInputStream(\n         new BufferedInputStream(peer.getInputStream(), bufferSize));\n     \n     BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n         PBHelper.vintPrefixed(in));\n     RemoteBlockReader2.checkSuccess(status, peer, block, file);\n     ReadOpChecksumInfoProto checksumInfo \u003d\n       status.getReadOpChecksumInfo();\n     DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n         checksumInfo.getChecksum());\n     //Warning when we get CHECKSUM_NULL?\n     \n     // Read the first chunk offset.\n     long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n     \n     if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n         firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n       throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                             firstChunkOffset + \") startOffset is \" + \n                             startOffset + \" for file \" + file);\n     }\n \n     return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n         in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n-        peer, datanodeID);\n+        peer, datanodeID, peerCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static RemoteBlockReader newBlockReader(String file,\n                                     ExtendedBlock block, \n                                     Token\u003cBlockTokenIdentifier\u003e blockToken,\n                                     long startOffset, long len,\n                                     int bufferSize, boolean verifyChecksum,\n                                     String clientName, Peer peer,\n                                     DatanodeID datanodeID,\n                                     PeerCache peerCache)\n                                     throws IOException {\n    // in and out will be closed when sock is closed (by the caller)\n    final DataOutputStream out \u003d\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len,\n        verifyChecksum);\n    \n    //\n    // Get bytes in block, set streams\n    //\n\n    DataInputStream in \u003d new DataInputStream(\n        new BufferedInputStream(peer.getInputStream(), bufferSize));\n    \n    BlockOpResponseProto status \u003d BlockOpResponseProto.parseFrom(\n        PBHelper.vintPrefixed(in));\n    RemoteBlockReader2.checkSuccess(status, peer, block, file);\n    ReadOpChecksumInfoProto checksumInfo \u003d\n      status.getReadOpChecksumInfo();\n    DataChecksum checksum \u003d DataTransferProtoUtil.fromProto(\n        checksumInfo.getChecksum());\n    //Warning when we get CHECKSUM_NULL?\n    \n    // Read the first chunk offset.\n    long firstChunkOffset \u003d checksumInfo.getChunkOffset();\n    \n    if ( firstChunkOffset \u003c 0 || firstChunkOffset \u003e startOffset ||\n        firstChunkOffset \u003c\u003d (startOffset - checksum.getBytesPerChecksum())) {\n      throw new IOException(\"BlockReader: error in first chunk offset (\" +\n                            firstChunkOffset + \") startOffset is \" + \n                            startOffset + \" for file \" + file);\n    }\n\n    return new RemoteBlockReader(file, block.getBlockPoolId(), block.getBlockId(),\n        in, checksum, verifyChecksum, startOffset, firstChunkOffset, len,\n        peer, datanodeID, peerCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}