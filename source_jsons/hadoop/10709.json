{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatasetVolumeChecker.java",
  "functionName": "checkAllVolumes",
  "functionId": "checkAllVolumes___dataset-FsDatasetSpi__? extends FsVolumeSpi__(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java",
  "functionStartLine": 191,
  "functionEndLine": 260,
  "numCommitsSeen": 11,
  "timeTaken": 3131,
  "changeHistory": [
    "67020f09502a4f07342dee457e47bb52b03441ae",
    "d7979079ea8c6514858b77a78f0119cffc178086",
    "603f3ef1386048111940b66f3a0750ab84d0588f",
    "f678080dbd25a218e0406463a3c3a1fc03680702",
    "eaaa32950cbae42a74e28e3db3f0cdb1ff158119"
  ],
  "changeHistoryShort": {
    "67020f09502a4f07342dee457e47bb52b03441ae": "Ybodychange",
    "d7979079ea8c6514858b77a78f0119cffc178086": "Ybodychange",
    "603f3ef1386048111940b66f3a0750ab84d0588f": "Ybodychange",
    "f678080dbd25a218e0406463a3c3a1fc03680702": "Ymultichange(Yreturntypechange,Ybodychange)",
    "eaaa32950cbae42a74e28e3db3f0cdb1ff158119": "Yintroduced"
  },
  "changeHistoryDetails": {
    "67020f09502a4f07342dee457e47bb52b03441ae": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14407. Fix misuse of SLF4j logging API in DatasetVolumeChecker#checkAllVolumes. Contributed by Wanqiang Ji.\n",
      "commitDate": "04/04/19 8:29 PM",
      "commitName": "67020f09502a4f07342dee457e47bb52b03441ae",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "03/04/19 11:59 AM",
      "commitNameOld": "d7979079ea8c6514858b77a78f0119cffc178086",
      "commitAuthorOld": "Gabor Bota",
      "daysBetweenCommits": 1.35,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,70 @@\n   public Set\u003cFsVolumeSpi\u003e checkAllVolumes(\n       final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n       throws InterruptedException {\n     final long gap \u003d timer.monotonicNow() - lastAllVolumesCheck;\n     if (gap \u003c minDiskCheckGapMs) {\n       numSkippedChecks.incrementAndGet();\n       LOG.trace(\n           \"Skipped checking all volumes, time since last check {} is less \" +\n           \"than the minimum gap between checks ({} ms).\",\n           gap, minDiskCheckGapMs);\n       return Collections.emptySet();\n     }\n \n     final FsDatasetSpi.FsVolumeReferences references \u003d\n         dataset.getFsVolumeReferences();\n \n     if (references.size() \u003d\u003d 0) {\n       LOG.warn(\"checkAllVolumesAsync - no volumes can be referenced\");\n       return Collections.emptySet();\n     }\n \n     lastAllVolumesCheck \u003d timer.monotonicNow();\n     final Set\u003cFsVolumeSpi\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n     final Set\u003cFsVolumeSpi\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n     final Set\u003cFsVolumeSpi\u003e allVolumes \u003d new HashSet\u003c\u003e();\n \n     final AtomicLong numVolumes \u003d new AtomicLong(references.size());\n     final CountDownLatch latch \u003d new CountDownLatch(1);\n \n     for (int i \u003d 0; i \u003c references.size(); ++i) {\n       final FsVolumeReference reference \u003d references.getReference(i);\n       Optional\u003cListenableFuture\u003cVolumeCheckResult\u003e\u003e olf \u003d\n           delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n       LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n       if (olf.isPresent()) {\n         allVolumes.add(reference.getVolume());\n         Futures.addCallback(olf.get(),\n             new ResultHandler(reference, healthyVolumes, failedVolumes,\n                 numVolumes, new Callback() {\n                   @Override\n                   public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n                                    Set\u003cFsVolumeSpi\u003e ignored2) {\n                     latch.countDown();\n                   }\n                 }), MoreExecutors.directExecutor());\n       } else {\n         IOUtils.cleanup(null, reference);\n         if (numVolumes.decrementAndGet() \u003d\u003d 0) {\n           latch.countDown();\n         }\n       }\n     }\n \n     // Wait until our timeout elapses, after which we give up on\n     // the remaining volumes.\n     if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n-      LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n+      LOG.warn(\"checkAllVolumes timed out after {} ms\",\n           maxAllowedTimeForCheckMs);\n     }\n \n     numSyncDatasetChecks.incrementAndGet();\n     synchronized (this) {\n       // All volumes that have not been detected as healthy should be\n       // considered failed. This is a superset of \u0027failedVolumes\u0027.\n       //\n       // Make a copy under the mutex as Sets.difference() returns a view\n       // of a potentially changing set.\n       return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Set\u003cFsVolumeSpi\u003e checkAllVolumes(\n      final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n      throws InterruptedException {\n    final long gap \u003d timer.monotonicNow() - lastAllVolumesCheck;\n    if (gap \u003c minDiskCheckGapMs) {\n      numSkippedChecks.incrementAndGet();\n      LOG.trace(\n          \"Skipped checking all volumes, time since last check {} is less \" +\n          \"than the minimum gap between checks ({} ms).\",\n          gap, minDiskCheckGapMs);\n      return Collections.emptySet();\n    }\n\n    final FsDatasetSpi.FsVolumeReferences references \u003d\n        dataset.getFsVolumeReferences();\n\n    if (references.size() \u003d\u003d 0) {\n      LOG.warn(\"checkAllVolumesAsync - no volumes can be referenced\");\n      return Collections.emptySet();\n    }\n\n    lastAllVolumesCheck \u003d timer.monotonicNow();\n    final Set\u003cFsVolumeSpi\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cFsVolumeSpi\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cFsVolumeSpi\u003e allVolumes \u003d new HashSet\u003c\u003e();\n\n    final AtomicLong numVolumes \u003d new AtomicLong(references.size());\n    final CountDownLatch latch \u003d new CountDownLatch(1);\n\n    for (int i \u003d 0; i \u003c references.size(); ++i) {\n      final FsVolumeReference reference \u003d references.getReference(i);\n      Optional\u003cListenableFuture\u003cVolumeCheckResult\u003e\u003e olf \u003d\n          delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n      LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n      if (olf.isPresent()) {\n        allVolumes.add(reference.getVolume());\n        Futures.addCallback(olf.get(),\n            new ResultHandler(reference, healthyVolumes, failedVolumes,\n                numVolumes, new Callback() {\n                  @Override\n                  public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n                                   Set\u003cFsVolumeSpi\u003e ignored2) {\n                    latch.countDown();\n                  }\n                }), MoreExecutors.directExecutor());\n      } else {\n        IOUtils.cleanup(null, reference);\n        if (numVolumes.decrementAndGet() \u003d\u003d 0) {\n          latch.countDown();\n        }\n      }\n    }\n\n    // Wait until our timeout elapses, after which we give up on\n    // the remaining volumes.\n    if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n      LOG.warn(\"checkAllVolumes timed out after {} ms\",\n          maxAllowedTimeForCheckMs);\n    }\n\n    numSyncDatasetChecks.incrementAndGet();\n    synchronized (this) {\n      // All volumes that have not been detected as healthy should be\n      // considered failed. This is a superset of \u0027failedVolumes\u0027.\n      //\n      // Make a copy under the mutex as Sets.difference() returns a view\n      // of a potentially changing set.\n      return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java",
      "extendedDetails": {}
    },
    "d7979079ea8c6514858b77a78f0119cffc178086": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16210. Update guava to 27.0-jre in hadoop-project trunk. Contributed by Gabor Bota.\n",
      "commitDate": "03/04/19 11:59 AM",
      "commitName": "d7979079ea8c6514858b77a78f0119cffc178086",
      "commitAuthor": "Gabor Bota",
      "commitDateOld": "21/01/19 8:44 PM",
      "commitNameOld": "1ff658b2ef3fb933897712c728bc628f3f44bded",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 71.59,
      "commitsBetweenForRepo": 566,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,70 @@\n   public Set\u003cFsVolumeSpi\u003e checkAllVolumes(\n       final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n       throws InterruptedException {\n     final long gap \u003d timer.monotonicNow() - lastAllVolumesCheck;\n     if (gap \u003c minDiskCheckGapMs) {\n       numSkippedChecks.incrementAndGet();\n       LOG.trace(\n           \"Skipped checking all volumes, time since last check {} is less \" +\n           \"than the minimum gap between checks ({} ms).\",\n           gap, minDiskCheckGapMs);\n       return Collections.emptySet();\n     }\n \n     final FsDatasetSpi.FsVolumeReferences references \u003d\n         dataset.getFsVolumeReferences();\n \n     if (references.size() \u003d\u003d 0) {\n       LOG.warn(\"checkAllVolumesAsync - no volumes can be referenced\");\n       return Collections.emptySet();\n     }\n \n     lastAllVolumesCheck \u003d timer.monotonicNow();\n     final Set\u003cFsVolumeSpi\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n     final Set\u003cFsVolumeSpi\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n     final Set\u003cFsVolumeSpi\u003e allVolumes \u003d new HashSet\u003c\u003e();\n \n     final AtomicLong numVolumes \u003d new AtomicLong(references.size());\n     final CountDownLatch latch \u003d new CountDownLatch(1);\n \n     for (int i \u003d 0; i \u003c references.size(); ++i) {\n       final FsVolumeReference reference \u003d references.getReference(i);\n       Optional\u003cListenableFuture\u003cVolumeCheckResult\u003e\u003e olf \u003d\n           delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n       LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n       if (olf.isPresent()) {\n         allVolumes.add(reference.getVolume());\n         Futures.addCallback(olf.get(),\n             new ResultHandler(reference, healthyVolumes, failedVolumes,\n                 numVolumes, new Callback() {\n-              @Override\n-              public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n-                               Set\u003cFsVolumeSpi\u003e ignored2) {\n-                latch.countDown();\n-              }\n-            }));\n+                  @Override\n+                  public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n+                                   Set\u003cFsVolumeSpi\u003e ignored2) {\n+                    latch.countDown();\n+                  }\n+                }), MoreExecutors.directExecutor());\n       } else {\n         IOUtils.cleanup(null, reference);\n         if (numVolumes.decrementAndGet() \u003d\u003d 0) {\n           latch.countDown();\n         }\n       }\n     }\n \n     // Wait until our timeout elapses, after which we give up on\n     // the remaining volumes.\n     if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n       LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n           maxAllowedTimeForCheckMs);\n     }\n \n     numSyncDatasetChecks.incrementAndGet();\n     synchronized (this) {\n       // All volumes that have not been detected as healthy should be\n       // considered failed. This is a superset of \u0027failedVolumes\u0027.\n       //\n       // Make a copy under the mutex as Sets.difference() returns a view\n       // of a potentially changing set.\n       return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Set\u003cFsVolumeSpi\u003e checkAllVolumes(\n      final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n      throws InterruptedException {\n    final long gap \u003d timer.monotonicNow() - lastAllVolumesCheck;\n    if (gap \u003c minDiskCheckGapMs) {\n      numSkippedChecks.incrementAndGet();\n      LOG.trace(\n          \"Skipped checking all volumes, time since last check {} is less \" +\n          \"than the minimum gap between checks ({} ms).\",\n          gap, minDiskCheckGapMs);\n      return Collections.emptySet();\n    }\n\n    final FsDatasetSpi.FsVolumeReferences references \u003d\n        dataset.getFsVolumeReferences();\n\n    if (references.size() \u003d\u003d 0) {\n      LOG.warn(\"checkAllVolumesAsync - no volumes can be referenced\");\n      return Collections.emptySet();\n    }\n\n    lastAllVolumesCheck \u003d timer.monotonicNow();\n    final Set\u003cFsVolumeSpi\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cFsVolumeSpi\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cFsVolumeSpi\u003e allVolumes \u003d new HashSet\u003c\u003e();\n\n    final AtomicLong numVolumes \u003d new AtomicLong(references.size());\n    final CountDownLatch latch \u003d new CountDownLatch(1);\n\n    for (int i \u003d 0; i \u003c references.size(); ++i) {\n      final FsVolumeReference reference \u003d references.getReference(i);\n      Optional\u003cListenableFuture\u003cVolumeCheckResult\u003e\u003e olf \u003d\n          delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n      LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n      if (olf.isPresent()) {\n        allVolumes.add(reference.getVolume());\n        Futures.addCallback(olf.get(),\n            new ResultHandler(reference, healthyVolumes, failedVolumes,\n                numVolumes, new Callback() {\n                  @Override\n                  public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n                                   Set\u003cFsVolumeSpi\u003e ignored2) {\n                    latch.countDown();\n                  }\n                }), MoreExecutors.directExecutor());\n      } else {\n        IOUtils.cleanup(null, reference);\n        if (numVolumes.decrementAndGet() \u003d\u003d 0) {\n          latch.countDown();\n        }\n      }\n    }\n\n    // Wait until our timeout elapses, after which we give up on\n    // the remaining volumes.\n    if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n      LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n          maxAllowedTimeForCheckMs);\n    }\n\n    numSyncDatasetChecks.incrementAndGet();\n    synchronized (this) {\n      // All volumes that have not been detected as healthy should be\n      // considered failed. This is a superset of \u0027failedVolumes\u0027.\n      //\n      // Make a copy under the mutex as Sets.difference() returns a view\n      // of a potentially changing set.\n      return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java",
      "extendedDetails": {}
    },
    "603f3ef1386048111940b66f3a0750ab84d0588f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11274. Datanode should only check the failed volume upon IO errors. Contributed by Xiaoyu Yao.\n",
      "commitDate": "28/12/16 10:08 PM",
      "commitName": "603f3ef1386048111940b66f3a0750ab84d0588f",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "20/12/16 1:53 PM",
      "commitNameOld": "f678080dbd25a218e0406463a3c3a1fc03680702",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 8.34,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,70 @@\n   public Set\u003cFsVolumeSpi\u003e checkAllVolumes(\n       final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n       throws InterruptedException {\n     final long gap \u003d timer.monotonicNow() - lastAllVolumesCheck;\n     if (gap \u003c minDiskCheckGapMs) {\n       numSkippedChecks.incrementAndGet();\n       LOG.trace(\n           \"Skipped checking all volumes, time since last check {} is less \" +\n           \"than the minimum gap between checks ({} ms).\",\n           gap, minDiskCheckGapMs);\n       return Collections.emptySet();\n     }\n \n     final FsDatasetSpi.FsVolumeReferences references \u003d\n         dataset.getFsVolumeReferences();\n \n     if (references.size() \u003d\u003d 0) {\n       LOG.warn(\"checkAllVolumesAsync - no volumes can be referenced\");\n       return Collections.emptySet();\n     }\n \n     lastAllVolumesCheck \u003d timer.monotonicNow();\n     final Set\u003cFsVolumeSpi\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n     final Set\u003cFsVolumeSpi\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n     final Set\u003cFsVolumeSpi\u003e allVolumes \u003d new HashSet\u003c\u003e();\n \n     final AtomicLong numVolumes \u003d new AtomicLong(references.size());\n     final CountDownLatch latch \u003d new CountDownLatch(1);\n \n     for (int i \u003d 0; i \u003c references.size(); ++i) {\n       final FsVolumeReference reference \u003d references.getReference(i);\n-      allVolumes.add(reference.getVolume());\n-      ListenableFuture\u003cVolumeCheckResult\u003e future \u003d\n+      Optional\u003cListenableFuture\u003cVolumeCheckResult\u003e\u003e olf \u003d\n           delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n       LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n-      Futures.addCallback(future, new ResultHandler(\n-          reference, healthyVolumes, failedVolumes, numVolumes, new Callback() {\n-        @Override\n-        public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n-                         Set\u003cFsVolumeSpi\u003e ignored2) {\n+      if (olf.isPresent()) {\n+        allVolumes.add(reference.getVolume());\n+        Futures.addCallback(olf.get(),\n+            new ResultHandler(reference, healthyVolumes, failedVolumes,\n+                numVolumes, new Callback() {\n+              @Override\n+              public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n+                               Set\u003cFsVolumeSpi\u003e ignored2) {\n+                latch.countDown();\n+              }\n+            }));\n+      } else {\n+        IOUtils.cleanup(null, reference);\n+        if (numVolumes.decrementAndGet() \u003d\u003d 0) {\n           latch.countDown();\n         }\n-      }));\n+      }\n     }\n \n     // Wait until our timeout elapses, after which we give up on\n     // the remaining volumes.\n     if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n       LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n           maxAllowedTimeForCheckMs);\n     }\n \n     numSyncDatasetChecks.incrementAndGet();\n     synchronized (this) {\n       // All volumes that have not been detected as healthy should be\n       // considered failed. This is a superset of \u0027failedVolumes\u0027.\n       //\n       // Make a copy under the mutex as Sets.difference() returns a view\n       // of a potentially changing set.\n       return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Set\u003cFsVolumeSpi\u003e checkAllVolumes(\n      final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n      throws InterruptedException {\n    final long gap \u003d timer.monotonicNow() - lastAllVolumesCheck;\n    if (gap \u003c minDiskCheckGapMs) {\n      numSkippedChecks.incrementAndGet();\n      LOG.trace(\n          \"Skipped checking all volumes, time since last check {} is less \" +\n          \"than the minimum gap between checks ({} ms).\",\n          gap, minDiskCheckGapMs);\n      return Collections.emptySet();\n    }\n\n    final FsDatasetSpi.FsVolumeReferences references \u003d\n        dataset.getFsVolumeReferences();\n\n    if (references.size() \u003d\u003d 0) {\n      LOG.warn(\"checkAllVolumesAsync - no volumes can be referenced\");\n      return Collections.emptySet();\n    }\n\n    lastAllVolumesCheck \u003d timer.monotonicNow();\n    final Set\u003cFsVolumeSpi\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cFsVolumeSpi\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cFsVolumeSpi\u003e allVolumes \u003d new HashSet\u003c\u003e();\n\n    final AtomicLong numVolumes \u003d new AtomicLong(references.size());\n    final CountDownLatch latch \u003d new CountDownLatch(1);\n\n    for (int i \u003d 0; i \u003c references.size(); ++i) {\n      final FsVolumeReference reference \u003d references.getReference(i);\n      Optional\u003cListenableFuture\u003cVolumeCheckResult\u003e\u003e olf \u003d\n          delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n      LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n      if (olf.isPresent()) {\n        allVolumes.add(reference.getVolume());\n        Futures.addCallback(olf.get(),\n            new ResultHandler(reference, healthyVolumes, failedVolumes,\n                numVolumes, new Callback() {\n              @Override\n              public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n                               Set\u003cFsVolumeSpi\u003e ignored2) {\n                latch.countDown();\n              }\n            }));\n      } else {\n        IOUtils.cleanup(null, reference);\n        if (numVolumes.decrementAndGet() \u003d\u003d 0) {\n          latch.countDown();\n        }\n      }\n    }\n\n    // Wait until our timeout elapses, after which we give up on\n    // the remaining volumes.\n    if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n      LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n          maxAllowedTimeForCheckMs);\n    }\n\n    numSyncDatasetChecks.incrementAndGet();\n    synchronized (this) {\n      // All volumes that have not been detected as healthy should be\n      // considered failed. This is a superset of \u0027failedVolumes\u0027.\n      //\n      // Make a copy under the mutex as Sets.difference() returns a view\n      // of a potentially changing set.\n      return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java",
      "extendedDetails": {}
    },
    "f678080dbd25a218e0406463a3c3a1fc03680702": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-11182. Update DataNode to use DatasetVolumeChecker. Contributed by Arpit Agarwal.\n",
      "commitDate": "20/12/16 1:53 PM",
      "commitName": "f678080dbd25a218e0406463a3c3a1fc03680702",
      "commitAuthor": "Xiaoyu Yao",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-11182. Update DataNode to use DatasetVolumeChecker. Contributed by Arpit Agarwal.\n",
          "commitDate": "20/12/16 1:53 PM",
          "commitName": "f678080dbd25a218e0406463a3c3a1fc03680702",
          "commitAuthor": "Xiaoyu Yao",
          "commitDateOld": "29/11/16 8:31 PM",
          "commitNameOld": "eaaa32950cbae42a74e28e3db3f0cdb1ff158119",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 20.72,
          "commitsBetweenForRepo": 121,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,62 @@\n-  public Set\u003cStorageLocation\u003e checkAllVolumes(\n+  public Set\u003cFsVolumeSpi\u003e checkAllVolumes(\n       final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n       throws InterruptedException {\n-\n-    if (timer.monotonicNow() - lastAllVolumesCheck \u003c minDiskCheckGapMs) {\n+    final long gap \u003d timer.monotonicNow() - lastAllVolumesCheck;\n+    if (gap \u003c minDiskCheckGapMs) {\n       numSkippedChecks.incrementAndGet();\n+      LOG.trace(\n+          \"Skipped checking all volumes, time since last check {} is less \" +\n+          \"than the minimum gap between checks ({} ms).\",\n+          gap, minDiskCheckGapMs);\n+      return Collections.emptySet();\n+    }\n+\n+    final FsDatasetSpi.FsVolumeReferences references \u003d\n+        dataset.getFsVolumeReferences();\n+\n+    if (references.size() \u003d\u003d 0) {\n+      LOG.warn(\"checkAllVolumesAsync - no volumes can be referenced\");\n       return Collections.emptySet();\n     }\n \n     lastAllVolumesCheck \u003d timer.monotonicNow();\n-    final Set\u003cStorageLocation\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n-    final Set\u003cStorageLocation\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n-    final Set\u003cStorageLocation\u003e allVolumes \u003d new HashSet\u003c\u003e();\n+    final Set\u003cFsVolumeSpi\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n+    final Set\u003cFsVolumeSpi\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n+    final Set\u003cFsVolumeSpi\u003e allVolumes \u003d new HashSet\u003c\u003e();\n \n-    final FsDatasetSpi.FsVolumeReferences references \u003d\n-        dataset.getFsVolumeReferences();\n-    final CountDownLatch resultsLatch \u003d new CountDownLatch(references.size());\n+    final AtomicLong numVolumes \u003d new AtomicLong(references.size());\n+    final CountDownLatch latch \u003d new CountDownLatch(1);\n \n     for (int i \u003d 0; i \u003c references.size(); ++i) {\n       final FsVolumeReference reference \u003d references.getReference(i);\n-      allVolumes.add(reference.getVolume().getStorageLocation());\n+      allVolumes.add(reference.getVolume());\n       ListenableFuture\u003cVolumeCheckResult\u003e future \u003d\n           delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n       LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n       Futures.addCallback(future, new ResultHandler(\n-          reference, healthyVolumes, failedVolumes, resultsLatch, null));\n+          reference, healthyVolumes, failedVolumes, numVolumes, new Callback() {\n+        @Override\n+        public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n+                         Set\u003cFsVolumeSpi\u003e ignored2) {\n+          latch.countDown();\n+        }\n+      }));\n     }\n \n     // Wait until our timeout elapses, after which we give up on\n     // the remaining volumes.\n-    if (!resultsLatch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n+    if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n       LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n           maxAllowedTimeForCheckMs);\n     }\n \n     numSyncDatasetChecks.incrementAndGet();\n     synchronized (this) {\n       // All volumes that have not been detected as healthy should be\n       // considered failed. This is a superset of \u0027failedVolumes\u0027.\n       //\n       // Make a copy under the mutex as Sets.difference() returns a view\n       // of a potentially changing set.\n       return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Set\u003cFsVolumeSpi\u003e checkAllVolumes(\n      final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n      throws InterruptedException {\n    final long gap \u003d timer.monotonicNow() - lastAllVolumesCheck;\n    if (gap \u003c minDiskCheckGapMs) {\n      numSkippedChecks.incrementAndGet();\n      LOG.trace(\n          \"Skipped checking all volumes, time since last check {} is less \" +\n          \"than the minimum gap between checks ({} ms).\",\n          gap, minDiskCheckGapMs);\n      return Collections.emptySet();\n    }\n\n    final FsDatasetSpi.FsVolumeReferences references \u003d\n        dataset.getFsVolumeReferences();\n\n    if (references.size() \u003d\u003d 0) {\n      LOG.warn(\"checkAllVolumesAsync - no volumes can be referenced\");\n      return Collections.emptySet();\n    }\n\n    lastAllVolumesCheck \u003d timer.monotonicNow();\n    final Set\u003cFsVolumeSpi\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cFsVolumeSpi\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cFsVolumeSpi\u003e allVolumes \u003d new HashSet\u003c\u003e();\n\n    final AtomicLong numVolumes \u003d new AtomicLong(references.size());\n    final CountDownLatch latch \u003d new CountDownLatch(1);\n\n    for (int i \u003d 0; i \u003c references.size(); ++i) {\n      final FsVolumeReference reference \u003d references.getReference(i);\n      allVolumes.add(reference.getVolume());\n      ListenableFuture\u003cVolumeCheckResult\u003e future \u003d\n          delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n      LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n      Futures.addCallback(future, new ResultHandler(\n          reference, healthyVolumes, failedVolumes, numVolumes, new Callback() {\n        @Override\n        public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n                         Set\u003cFsVolumeSpi\u003e ignored2) {\n          latch.countDown();\n        }\n      }));\n    }\n\n    // Wait until our timeout elapses, after which we give up on\n    // the remaining volumes.\n    if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n      LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n          maxAllowedTimeForCheckMs);\n    }\n\n    numSyncDatasetChecks.incrementAndGet();\n    synchronized (this) {\n      // All volumes that have not been detected as healthy should be\n      // considered failed. This is a superset of \u0027failedVolumes\u0027.\n      //\n      // Make a copy under the mutex as Sets.difference() returns a view\n      // of a potentially changing set.\n      return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java",
          "extendedDetails": {
            "oldValue": "Set\u003cStorageLocation\u003e",
            "newValue": "Set\u003cFsVolumeSpi\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-11182. Update DataNode to use DatasetVolumeChecker. Contributed by Arpit Agarwal.\n",
          "commitDate": "20/12/16 1:53 PM",
          "commitName": "f678080dbd25a218e0406463a3c3a1fc03680702",
          "commitAuthor": "Xiaoyu Yao",
          "commitDateOld": "29/11/16 8:31 PM",
          "commitNameOld": "eaaa32950cbae42a74e28e3db3f0cdb1ff158119",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 20.72,
          "commitsBetweenForRepo": 121,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,62 @@\n-  public Set\u003cStorageLocation\u003e checkAllVolumes(\n+  public Set\u003cFsVolumeSpi\u003e checkAllVolumes(\n       final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n       throws InterruptedException {\n-\n-    if (timer.monotonicNow() - lastAllVolumesCheck \u003c minDiskCheckGapMs) {\n+    final long gap \u003d timer.monotonicNow() - lastAllVolumesCheck;\n+    if (gap \u003c minDiskCheckGapMs) {\n       numSkippedChecks.incrementAndGet();\n+      LOG.trace(\n+          \"Skipped checking all volumes, time since last check {} is less \" +\n+          \"than the minimum gap between checks ({} ms).\",\n+          gap, minDiskCheckGapMs);\n+      return Collections.emptySet();\n+    }\n+\n+    final FsDatasetSpi.FsVolumeReferences references \u003d\n+        dataset.getFsVolumeReferences();\n+\n+    if (references.size() \u003d\u003d 0) {\n+      LOG.warn(\"checkAllVolumesAsync - no volumes can be referenced\");\n       return Collections.emptySet();\n     }\n \n     lastAllVolumesCheck \u003d timer.monotonicNow();\n-    final Set\u003cStorageLocation\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n-    final Set\u003cStorageLocation\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n-    final Set\u003cStorageLocation\u003e allVolumes \u003d new HashSet\u003c\u003e();\n+    final Set\u003cFsVolumeSpi\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n+    final Set\u003cFsVolumeSpi\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n+    final Set\u003cFsVolumeSpi\u003e allVolumes \u003d new HashSet\u003c\u003e();\n \n-    final FsDatasetSpi.FsVolumeReferences references \u003d\n-        dataset.getFsVolumeReferences();\n-    final CountDownLatch resultsLatch \u003d new CountDownLatch(references.size());\n+    final AtomicLong numVolumes \u003d new AtomicLong(references.size());\n+    final CountDownLatch latch \u003d new CountDownLatch(1);\n \n     for (int i \u003d 0; i \u003c references.size(); ++i) {\n       final FsVolumeReference reference \u003d references.getReference(i);\n-      allVolumes.add(reference.getVolume().getStorageLocation());\n+      allVolumes.add(reference.getVolume());\n       ListenableFuture\u003cVolumeCheckResult\u003e future \u003d\n           delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n       LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n       Futures.addCallback(future, new ResultHandler(\n-          reference, healthyVolumes, failedVolumes, resultsLatch, null));\n+          reference, healthyVolumes, failedVolumes, numVolumes, new Callback() {\n+        @Override\n+        public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n+                         Set\u003cFsVolumeSpi\u003e ignored2) {\n+          latch.countDown();\n+        }\n+      }));\n     }\n \n     // Wait until our timeout elapses, after which we give up on\n     // the remaining volumes.\n-    if (!resultsLatch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n+    if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n       LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n           maxAllowedTimeForCheckMs);\n     }\n \n     numSyncDatasetChecks.incrementAndGet();\n     synchronized (this) {\n       // All volumes that have not been detected as healthy should be\n       // considered failed. This is a superset of \u0027failedVolumes\u0027.\n       //\n       // Make a copy under the mutex as Sets.difference() returns a view\n       // of a potentially changing set.\n       return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Set\u003cFsVolumeSpi\u003e checkAllVolumes(\n      final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n      throws InterruptedException {\n    final long gap \u003d timer.monotonicNow() - lastAllVolumesCheck;\n    if (gap \u003c minDiskCheckGapMs) {\n      numSkippedChecks.incrementAndGet();\n      LOG.trace(\n          \"Skipped checking all volumes, time since last check {} is less \" +\n          \"than the minimum gap between checks ({} ms).\",\n          gap, minDiskCheckGapMs);\n      return Collections.emptySet();\n    }\n\n    final FsDatasetSpi.FsVolumeReferences references \u003d\n        dataset.getFsVolumeReferences();\n\n    if (references.size() \u003d\u003d 0) {\n      LOG.warn(\"checkAllVolumesAsync - no volumes can be referenced\");\n      return Collections.emptySet();\n    }\n\n    lastAllVolumesCheck \u003d timer.monotonicNow();\n    final Set\u003cFsVolumeSpi\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cFsVolumeSpi\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cFsVolumeSpi\u003e allVolumes \u003d new HashSet\u003c\u003e();\n\n    final AtomicLong numVolumes \u003d new AtomicLong(references.size());\n    final CountDownLatch latch \u003d new CountDownLatch(1);\n\n    for (int i \u003d 0; i \u003c references.size(); ++i) {\n      final FsVolumeReference reference \u003d references.getReference(i);\n      allVolumes.add(reference.getVolume());\n      ListenableFuture\u003cVolumeCheckResult\u003e future \u003d\n          delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n      LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n      Futures.addCallback(future, new ResultHandler(\n          reference, healthyVolumes, failedVolumes, numVolumes, new Callback() {\n        @Override\n        public void call(Set\u003cFsVolumeSpi\u003e ignored1,\n                         Set\u003cFsVolumeSpi\u003e ignored2) {\n          latch.countDown();\n        }\n      }));\n    }\n\n    // Wait until our timeout elapses, after which we give up on\n    // the remaining volumes.\n    if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n      LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n          maxAllowedTimeForCheckMs);\n    }\n\n    numSyncDatasetChecks.incrementAndGet();\n    synchronized (this) {\n      // All volumes that have not been detected as healthy should be\n      // considered failed. This is a superset of \u0027failedVolumes\u0027.\n      //\n      // Make a copy under the mutex as Sets.difference() returns a view\n      // of a potentially changing set.\n      return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java",
          "extendedDetails": {}
        }
      ]
    },
    "eaaa32950cbae42a74e28e3db3f0cdb1ff158119": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11149. Support for parallel checking of FsVolumes.\n",
      "commitDate": "29/11/16 8:31 PM",
      "commitName": "eaaa32950cbae42a74e28e3db3f0cdb1ff158119",
      "commitAuthor": "Arpit Agarwal",
      "diff": "@@ -0,0 +1,45 @@\n+  public Set\u003cStorageLocation\u003e checkAllVolumes(\n+      final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n+      throws InterruptedException {\n+\n+    if (timer.monotonicNow() - lastAllVolumesCheck \u003c minDiskCheckGapMs) {\n+      numSkippedChecks.incrementAndGet();\n+      return Collections.emptySet();\n+    }\n+\n+    lastAllVolumesCheck \u003d timer.monotonicNow();\n+    final Set\u003cStorageLocation\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n+    final Set\u003cStorageLocation\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n+    final Set\u003cStorageLocation\u003e allVolumes \u003d new HashSet\u003c\u003e();\n+\n+    final FsDatasetSpi.FsVolumeReferences references \u003d\n+        dataset.getFsVolumeReferences();\n+    final CountDownLatch resultsLatch \u003d new CountDownLatch(references.size());\n+\n+    for (int i \u003d 0; i \u003c references.size(); ++i) {\n+      final FsVolumeReference reference \u003d references.getReference(i);\n+      allVolumes.add(reference.getVolume().getStorageLocation());\n+      ListenableFuture\u003cVolumeCheckResult\u003e future \u003d\n+          delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n+      LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n+      Futures.addCallback(future, new ResultHandler(\n+          reference, healthyVolumes, failedVolumes, resultsLatch, null));\n+    }\n+\n+    // Wait until our timeout elapses, after which we give up on\n+    // the remaining volumes.\n+    if (!resultsLatch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n+      LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n+          maxAllowedTimeForCheckMs);\n+    }\n+\n+    numSyncDatasetChecks.incrementAndGet();\n+    synchronized (this) {\n+      // All volumes that have not been detected as healthy should be\n+      // considered failed. This is a superset of \u0027failedVolumes\u0027.\n+      //\n+      // Make a copy under the mutex as Sets.difference() returns a view\n+      // of a potentially changing set.\n+      return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public Set\u003cStorageLocation\u003e checkAllVolumes(\n      final FsDatasetSpi\u003c? extends FsVolumeSpi\u003e dataset)\n      throws InterruptedException {\n\n    if (timer.monotonicNow() - lastAllVolumesCheck \u003c minDiskCheckGapMs) {\n      numSkippedChecks.incrementAndGet();\n      return Collections.emptySet();\n    }\n\n    lastAllVolumesCheck \u003d timer.monotonicNow();\n    final Set\u003cStorageLocation\u003e healthyVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cStorageLocation\u003e failedVolumes \u003d new HashSet\u003c\u003e();\n    final Set\u003cStorageLocation\u003e allVolumes \u003d new HashSet\u003c\u003e();\n\n    final FsDatasetSpi.FsVolumeReferences references \u003d\n        dataset.getFsVolumeReferences();\n    final CountDownLatch resultsLatch \u003d new CountDownLatch(references.size());\n\n    for (int i \u003d 0; i \u003c references.size(); ++i) {\n      final FsVolumeReference reference \u003d references.getReference(i);\n      allVolumes.add(reference.getVolume().getStorageLocation());\n      ListenableFuture\u003cVolumeCheckResult\u003e future \u003d\n          delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);\n      LOG.info(\"Scheduled health check for volume {}\", reference.getVolume());\n      Futures.addCallback(future, new ResultHandler(\n          reference, healthyVolumes, failedVolumes, resultsLatch, null));\n    }\n\n    // Wait until our timeout elapses, after which we give up on\n    // the remaining volumes.\n    if (!resultsLatch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n      LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n          maxAllowedTimeForCheckMs);\n    }\n\n    numSyncDatasetChecks.incrementAndGet();\n    synchronized (this) {\n      // All volumes that have not been detected as healthy should be\n      // considered failed. This is a superset of \u0027failedVolumes\u0027.\n      //\n      // Make a copy under the mutex as Sets.difference() returns a view\n      // of a potentially changing set.\n      return new HashSet\u003c\u003e(Sets.difference(allVolumes, healthyVolumes));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java"
    }
  }
}