{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Command.java",
  "functionName": "getNodeList",
  "functionId": "getNodeList___listArg-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java",
  "functionStartLine": 263,
  "functionEndLine": 299,
  "numCommitsSeen": 24,
  "timeTaken": 1323,
  "changeHistory": [
    "7a3188d054481b9bd563e337901e93476303ce7f",
    "3b908f71c5825a8fd6ded2a6108eb4c6c4a5b9c4",
    "20ae1fa259b36a7bc11b0f8de1ebf753c858f93c",
    "75882ec0b096da862b8b373b70a091c19f281b2a"
  ],
  "changeHistoryShort": {
    "7a3188d054481b9bd563e337901e93476303ce7f": "Ybodychange",
    "3b908f71c5825a8fd6ded2a6108eb4c6c4a5b9c4": "Ybodychange",
    "20ae1fa259b36a7bc11b0f8de1ebf753c858f93c": "Ymodifierchange",
    "75882ec0b096da862b8b373b70a091c19f281b2a": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7a3188d054481b9bd563e337901e93476303ce7f": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16282. Avoid FileStream to improve performance. Contributed by Ayush Saxena.\n",
      "commitDate": "02/05/19 12:58 PM",
      "commitName": "7a3188d054481b9bd563e337901e93476303ce7f",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "13/07/18 8:42 AM",
      "commitNameOld": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 293.18,
      "commitsBetweenForRepo": 2268,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n   protected Set\u003cString\u003e getNodeList(String listArg) throws IOException {\n     URL listURL;\n     String nodeData;\n     Set\u003cString\u003e resultSet \u003d new TreeSet\u003c\u003e();\n \n     if ((listArg \u003d\u003d null) || listArg.isEmpty()) {\n       return resultSet;\n     }\n \n     if (listArg.startsWith(\"file://\")) {\n       listURL \u003d new URL(listArg);\n       try {\n         HostsFileReader.readFileToSet(\"include\",\n             Paths.get(listURL.getPath()).toString(), resultSet);\n-      } catch (FileNotFoundException e) {\n+      } catch (NoSuchFileException e) {\n         String warnMsg \u003d String\n             .format(\"The input host file path \u0027%s\u0027 is not a valid path. \"\n                 + \"Please make sure the host file exists.\", listArg);\n         throw new DiskBalancerException(warnMsg,\n             DiskBalancerException.Result.INVALID_HOST_FILE_PATH);\n       }\n     } else {\n       nodeData \u003d listArg;\n       String[] nodes \u003d nodeData.split(\",\");\n \n       if (nodes.length \u003d\u003d 0) {\n         String warnMsg \u003d \"The number of input nodes is 0. \"\n             + \"Please input the valid nodes.\";\n         throw new DiskBalancerException(warnMsg,\n             DiskBalancerException.Result.INVALID_NODE);\n       }\n \n       Collections.addAll(resultSet, nodes);\n     }\n \n     return resultSet;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected Set\u003cString\u003e getNodeList(String listArg) throws IOException {\n    URL listURL;\n    String nodeData;\n    Set\u003cString\u003e resultSet \u003d new TreeSet\u003c\u003e();\n\n    if ((listArg \u003d\u003d null) || listArg.isEmpty()) {\n      return resultSet;\n    }\n\n    if (listArg.startsWith(\"file://\")) {\n      listURL \u003d new URL(listArg);\n      try {\n        HostsFileReader.readFileToSet(\"include\",\n            Paths.get(listURL.getPath()).toString(), resultSet);\n      } catch (NoSuchFileException e) {\n        String warnMsg \u003d String\n            .format(\"The input host file path \u0027%s\u0027 is not a valid path. \"\n                + \"Please make sure the host file exists.\", listArg);\n        throw new DiskBalancerException(warnMsg,\n            DiskBalancerException.Result.INVALID_HOST_FILE_PATH);\n      }\n    } else {\n      nodeData \u003d listArg;\n      String[] nodes \u003d nodeData.split(\",\");\n\n      if (nodes.length \u003d\u003d 0) {\n        String warnMsg \u003d \"The number of input nodes is 0. \"\n            + \"Please input the valid nodes.\";\n        throw new DiskBalancerException(warnMsg,\n            DiskBalancerException.Result.INVALID_NODE);\n      }\n\n      Collections.addAll(resultSet, nodes);\n    }\n\n    return resultSet;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java",
      "extendedDetails": {}
    },
    "3b908f71c5825a8fd6ded2a6108eb4c6c4a5b9c4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11358. DiskBalancer: Report command supports reading nodes from host file. Contributed by Yiqun Lin.\n",
      "commitDate": "21/03/17 2:44 AM",
      "commitName": "3b908f71c5825a8fd6ded2a6108eb4c6c4a5b9c4",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "12/12/16 6:11 PM",
      "commitNameOld": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 98.31,
      "commitsBetweenForRepo": 496,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,37 @@\n   protected Set\u003cString\u003e getNodeList(String listArg) throws IOException {\n     URL listURL;\n     String nodeData;\n     Set\u003cString\u003e resultSet \u003d new TreeSet\u003c\u003e();\n \n     if ((listArg \u003d\u003d null) || listArg.isEmpty()) {\n       return resultSet;\n     }\n+\n     if (listArg.startsWith(\"file://\")) {\n       listURL \u003d new URL(listArg);\n-      byte[] data \u003d Files.readAllBytes(Paths.get(listURL.getPath()));\n-      nodeData \u003d new String(data, Charset.forName(\"UTF-8\"));\n+      try {\n+        HostsFileReader.readFileToSet(\"include\",\n+            Paths.get(listURL.getPath()).toString(), resultSet);\n+      } catch (FileNotFoundException e) {\n+        String warnMsg \u003d String\n+            .format(\"The input host file path \u0027%s\u0027 is not a valid path. \"\n+                + \"Please make sure the host file exists.\", listArg);\n+        throw new DiskBalancerException(warnMsg,\n+            DiskBalancerException.Result.INVALID_HOST_FILE_PATH);\n+      }\n     } else {\n       nodeData \u003d listArg;\n+      String[] nodes \u003d nodeData.split(\",\");\n+\n+      if (nodes.length \u003d\u003d 0) {\n+        String warnMsg \u003d \"The number of input nodes is 0. \"\n+            + \"Please input the valid nodes.\";\n+        throw new DiskBalancerException(warnMsg,\n+            DiskBalancerException.Result.INVALID_NODE);\n+      }\n+\n+      Collections.addAll(resultSet, nodes);\n     }\n \n-    String[] nodes \u003d nodeData.split(\",\");\n-    Collections.addAll(resultSet, nodes);\n     return resultSet;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected Set\u003cString\u003e getNodeList(String listArg) throws IOException {\n    URL listURL;\n    String nodeData;\n    Set\u003cString\u003e resultSet \u003d new TreeSet\u003c\u003e();\n\n    if ((listArg \u003d\u003d null) || listArg.isEmpty()) {\n      return resultSet;\n    }\n\n    if (listArg.startsWith(\"file://\")) {\n      listURL \u003d new URL(listArg);\n      try {\n        HostsFileReader.readFileToSet(\"include\",\n            Paths.get(listURL.getPath()).toString(), resultSet);\n      } catch (FileNotFoundException e) {\n        String warnMsg \u003d String\n            .format(\"The input host file path \u0027%s\u0027 is not a valid path. \"\n                + \"Please make sure the host file exists.\", listArg);\n        throw new DiskBalancerException(warnMsg,\n            DiskBalancerException.Result.INVALID_HOST_FILE_PATH);\n      }\n    } else {\n      nodeData \u003d listArg;\n      String[] nodes \u003d nodeData.split(\",\");\n\n      if (nodes.length \u003d\u003d 0) {\n        String warnMsg \u003d \"The number of input nodes is 0. \"\n            + \"Please input the valid nodes.\";\n        throw new DiskBalancerException(warnMsg,\n            DiskBalancerException.Result.INVALID_NODE);\n      }\n\n      Collections.addAll(resultSet, nodes);\n    }\n\n    return resultSet;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java",
      "extendedDetails": {}
    },
    "20ae1fa259b36a7bc11b0f8de1ebf753c858f93c": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-10813. DiskBalancer: Add the getNodeList method in Command. Contributed by Yiqun Lin.\n",
      "commitDate": "30/08/16 6:42 PM",
      "commitName": "20ae1fa259b36a7bc11b0f8de1ebf753c858f93c",
      "commitAuthor": "Anu Engineer",
      "commitDateOld": "16/08/16 10:20 AM",
      "commitNameOld": "b047bc7270f3461156e4d08423c728ee9c67dba5",
      "commitAuthorOld": "Anu Engineer",
      "daysBetweenCommits": 14.35,
      "commitsBetweenForRepo": 101,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n-  private Set\u003cString\u003e getNodeList(String listArg) throws IOException {\n+  protected Set\u003cString\u003e getNodeList(String listArg) throws IOException {\n     URL listURL;\n     String nodeData;\n     Set\u003cString\u003e resultSet \u003d new TreeSet\u003c\u003e();\n \n     if ((listArg \u003d\u003d null) || listArg.isEmpty()) {\n       return resultSet;\n     }\n     if (listArg.startsWith(\"file://\")) {\n       listURL \u003d new URL(listArg);\n       byte[] data \u003d Files.readAllBytes(Paths.get(listURL.getPath()));\n       nodeData \u003d new String(data, Charset.forName(\"UTF-8\"));\n     } else {\n       nodeData \u003d listArg;\n     }\n \n     String[] nodes \u003d nodeData.split(\",\");\n     Collections.addAll(resultSet, nodes);\n     return resultSet;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected Set\u003cString\u003e getNodeList(String listArg) throws IOException {\n    URL listURL;\n    String nodeData;\n    Set\u003cString\u003e resultSet \u003d new TreeSet\u003c\u003e();\n\n    if ((listArg \u003d\u003d null) || listArg.isEmpty()) {\n      return resultSet;\n    }\n    if (listArg.startsWith(\"file://\")) {\n      listURL \u003d new URL(listArg);\n      byte[] data \u003d Files.readAllBytes(Paths.get(listURL.getPath()));\n      nodeData \u003d new String(data, Charset.forName(\"UTF-8\"));\n    } else {\n      nodeData \u003d listArg;\n    }\n\n    String[] nodes \u003d nodeData.split(\",\");\n    Collections.addAll(resultSet, nodes);\n    return resultSet;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[protected]"
      }
    },
    "75882ec0b096da862b8b373b70a091c19f281b2a": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9545: DiskBalancer: Add Plan Command. Contributed by Anu Engineer.\n",
      "commitDate": "23/06/16 6:21 PM",
      "commitName": "75882ec0b096da862b8b373b70a091c19f281b2a",
      "commitAuthor": "Anu Engineer",
      "diff": "@@ -0,0 +1,20 @@\n+  private Set\u003cString\u003e getNodeList(String listArg) throws IOException {\n+    URL listURL;\n+    String nodeData;\n+    Set\u003cString\u003e resultSet \u003d new TreeSet\u003c\u003e();\n+\n+    if ((listArg \u003d\u003d null) || listArg.isEmpty()) {\n+      return resultSet;\n+    }\n+    if (listArg.startsWith(\"file://\")) {\n+      listURL \u003d new URL(listArg);\n+      byte[] data \u003d Files.readAllBytes(Paths.get(listURL.getPath()));\n+      nodeData \u003d new String(data, Charset.forName(\"UTF-8\"));\n+    } else {\n+      nodeData \u003d listArg;\n+    }\n+\n+    String[] nodes \u003d nodeData.split(\",\");\n+    Collections.addAll(resultSet, nodes);\n+    return resultSet;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private Set\u003cString\u003e getNodeList(String listArg) throws IOException {\n    URL listURL;\n    String nodeData;\n    Set\u003cString\u003e resultSet \u003d new TreeSet\u003c\u003e();\n\n    if ((listArg \u003d\u003d null) || listArg.isEmpty()) {\n      return resultSet;\n    }\n    if (listArg.startsWith(\"file://\")) {\n      listURL \u003d new URL(listArg);\n      byte[] data \u003d Files.readAllBytes(Paths.get(listURL.getPath()));\n      nodeData \u003d new String(data, Charset.forName(\"UTF-8\"));\n    } else {\n      nodeData \u003d listArg;\n    }\n\n    String[] nodes \u003d nodeData.split(\",\");\n    Collections.addAll(resultSet, nodes);\n    return resultSet;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java"
    }
  }
}