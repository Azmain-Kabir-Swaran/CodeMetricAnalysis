{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirStatAndListingOp.java",
  "functionName": "getFileInfo",
  "functionId": "getFileInfo___fsd-FSDirectory__iip-INodesInPath__includeStoragePolicy-boolean__needLocation-boolean__needBlockToken-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
  "functionStartLine": 334,
  "functionEndLine": 351,
  "numCommitsSeen": 464,
  "timeTaken": 11831,
  "changeHistory": [
    "693169ef34f856a27dc09d90a45fb4ec5b66ed2c",
    "a0730aa5ced7666a8c92f9fb830b615f5f9f477a",
    "ec252ce0fc0998ce13f31af3440c08a236328e5a",
    "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
    "22fc46d7659972ff016ccf1c6f781f0c160be26f",
    "9f4bf3bdf9e74800643477cfb18361e01cf6859c",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "c55d609053fe24b3a50fbe17dc1b47717b453ed6",
    "5c97db07fb306842f49d73a67a90cecec19a7833",
    "53a28afe293e5bf185c8d4f2c7aea212e66015c2",
    "65f2a4ee600dfffa5203450261da3c1989de25a9",
    "5776a41da08af653206bb94d7c76c9c4dcce059a",
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4",
    "1737950d0fc83c68f386881b843c41b0b1e342de"
  ],
  "changeHistoryShort": {
    "693169ef34f856a27dc09d90a45fb4ec5b66ed2c": "Ymultichange(Yparameterchange,Ybodychange)",
    "a0730aa5ced7666a8c92f9fb830b615f5f9f477a": "Ybodychange",
    "ec252ce0fc0998ce13f31af3440c08a236328e5a": "Ymultichange(Yparameterchange,Ybodychange)",
    "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88": "Ymultichange(Yparameterchange,Ybodychange)",
    "22fc46d7659972ff016ccf1c6f781f0c160be26f": "Ymultichange(Yparameterchange,Ybodychange)",
    "9f4bf3bdf9e74800643477cfb18361e01cf6859c": "Ymultichange(Yparameterchange,Ybodychange)",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "c55d609053fe24b3a50fbe17dc1b47717b453ed6": "Ybodychange",
    "5c97db07fb306842f49d73a67a90cecec19a7833": "Ybodychange",
    "53a28afe293e5bf185c8d4f2c7aea212e66015c2": "Ymultichange(Yparameterchange,Ybodychange)",
    "65f2a4ee600dfffa5203450261da3c1989de25a9": "Ymultichange(Yparameterchange,Ybodychange)",
    "5776a41da08af653206bb94d7c76c9c4dcce059a": "Ybodychange",
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4": "Ybodychange",
    "1737950d0fc83c68f386881b843c41b0b1e342de": "Ybodychange"
  },
  "changeHistoryDetails": {
    "693169ef34f856a27dc09d90a45fb4ec5b66ed2c": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-12882. Support full open(PathHandle) contract in HDFS\n",
      "commitDate": "11/12/17 8:14 PM",
      "commitName": "693169ef34f856a27dc09d90a45fb4ec5b66ed2c",
      "commitAuthor": "Chris Douglas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-12882. Support full open(PathHandle) contract in HDFS\n",
          "commitDate": "11/12/17 8:14 PM",
          "commitName": "693169ef34f856a27dc09d90a45fb4ec5b66ed2c",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "29/11/17 8:28 PM",
          "commitNameOld": "0e560f3b8d194c10dce06443979df4074e14b0db",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 11.99,
          "commitsBetweenForRepo": 78,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,18 @@\n-  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n-      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n+  static HdfsFileStatus getFileInfo(FSDirectory fsd, INodesInPath iip,\n+      boolean includeStoragePolicy, boolean needLocation,\n+      boolean needBlockToken) throws IOException {\n     fsd.readLock();\n     try {\n       final INode node \u003d iip.getLastINode();\n       if (node \u003d\u003d null) {\n         return null;\n       }\n       byte policy \u003d (includeStoragePolicy \u0026\u0026 !node.isSymlink())\n           ? node.getStoragePolicyID()\n           : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-      return createFileStatus(fsd, iip, null, policy, false);\n+      return createFileStatus(fsd, iip, null, policy, needLocation,\n+          needBlockToken);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(FSDirectory fsd, INodesInPath iip,\n      boolean includeStoragePolicy, boolean needLocation,\n      boolean needBlockToken) throws IOException {\n    fsd.readLock();\n    try {\n      final INode node \u003d iip.getLastINode();\n      if (node \u003d\u003d null) {\n        return null;\n      }\n      byte policy \u003d (includeStoragePolicy \u0026\u0026 !node.isSymlink())\n          ? node.getStoragePolicyID()\n          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      return createFileStatus(fsd, iip, null, policy, needLocation,\n          needBlockToken);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, iip-INodesInPath, includeStoragePolicy-boolean]",
            "newValue": "[fsd-FSDirectory, iip-INodesInPath, includeStoragePolicy-boolean, needLocation-boolean, needBlockToken-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-12882. Support full open(PathHandle) contract in HDFS\n",
          "commitDate": "11/12/17 8:14 PM",
          "commitName": "693169ef34f856a27dc09d90a45fb4ec5b66ed2c",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "29/11/17 8:28 PM",
          "commitNameOld": "0e560f3b8d194c10dce06443979df4074e14b0db",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 11.99,
          "commitsBetweenForRepo": 78,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,18 @@\n-  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n-      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n+  static HdfsFileStatus getFileInfo(FSDirectory fsd, INodesInPath iip,\n+      boolean includeStoragePolicy, boolean needLocation,\n+      boolean needBlockToken) throws IOException {\n     fsd.readLock();\n     try {\n       final INode node \u003d iip.getLastINode();\n       if (node \u003d\u003d null) {\n         return null;\n       }\n       byte policy \u003d (includeStoragePolicy \u0026\u0026 !node.isSymlink())\n           ? node.getStoragePolicyID()\n           : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-      return createFileStatus(fsd, iip, null, policy, false);\n+      return createFileStatus(fsd, iip, null, policy, needLocation,\n+          needBlockToken);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(FSDirectory fsd, INodesInPath iip,\n      boolean includeStoragePolicy, boolean needLocation,\n      boolean needBlockToken) throws IOException {\n    fsd.readLock();\n    try {\n      final INode node \u003d iip.getLastINode();\n      if (node \u003d\u003d null) {\n        return null;\n      }\n      byte policy \u003d (includeStoragePolicy \u0026\u0026 !node.isSymlink())\n          ? node.getStoragePolicyID()\n          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      return createFileStatus(fsd, iip, null, policy, needLocation,\n          needBlockToken);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "a0730aa5ced7666a8c92f9fb830b615f5f9f477a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10851. FSDirStatAndListingOp: stop passing path as string. Contributed by Daryn Sharp.\n",
      "commitDate": "30/09/16 11:03 AM",
      "commitName": "a0730aa5ced7666a8c92f9fb830b615f5f9f477a",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "24/08/16 1:21 PM",
      "commitNameOld": "a1f3293762dddb0ca953d1145f5b53d9086b25b8",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 36.9,
      "commitsBetweenForRepo": 208,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,16 @@\n   static HdfsFileStatus getFileInfo(FSDirectory fsd,\n       INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n     fsd.readLock();\n     try {\n       final INode node \u003d iip.getLastINode();\n       if (node \u003d\u003d null) {\n         return null;\n       }\n-\n-      byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n-          node.getStoragePolicyID() :\n-          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, iip.getPath(),\n-                                                     HdfsFileStatus.EMPTY_NAME,\n-                                                     node, iip.getPathSnapshotId());\n-      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                              policyId, iip);\n+      byte policy \u003d (includeStoragePolicy \u0026\u0026 !node.isSymlink())\n+          ? node.getStoragePolicyID()\n+          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n+      return createFileStatus(fsd, iip, null, policy, false);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n    fsd.readLock();\n    try {\n      final INode node \u003d iip.getLastINode();\n      if (node \u003d\u003d null) {\n        return null;\n      }\n      byte policy \u003d (includeStoragePolicy \u0026\u0026 !node.isSymlink())\n          ? node.getStoragePolicyID()\n          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      return createFileStatus(fsd, iip, null, policy, false);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "ec252ce0fc0998ce13f31af3440c08a236328e5a": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10762. Pass IIP for file status related methods\n",
      "commitDate": "24/08/16 6:46 AM",
      "commitName": "ec252ce0fc0998ce13f31af3440c08a236328e5a",
      "commitAuthor": "Daryn Sharp",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10762. Pass IIP for file status related methods\n",
          "commitDate": "24/08/16 6:46 AM",
          "commitName": "ec252ce0fc0998ce13f31af3440c08a236328e5a",
          "commitAuthor": "Daryn Sharp",
          "commitDateOld": "22/08/16 2:57 PM",
          "commitNameOld": "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 1.66,
          "commitsBetweenForRepo": 14,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,21 @@\n-  static HdfsFileStatus getFileInfo(\n-      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n-      boolean includeStoragePolicy)\n-      throws IOException {\n+  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n+      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n     fsd.readLock();\n     try {\n       final INode node \u003d iip.getLastINode();\n       if (node \u003d\u003d null) {\n         return null;\n       }\n \n       byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n           node.getStoragePolicyID() :\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n+      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, iip.getPath(),\n                                                      HdfsFileStatus.EMPTY_NAME,\n                                                      node, iip.getPathSnapshotId());\n       return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n+                              policyId, iip);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n    fsd.readLock();\n    try {\n      final INode node \u003d iip.getLastINode();\n      if (node \u003d\u003d null) {\n        return null;\n      }\n\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n          node.getStoragePolicyID() :\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, iip.getPath(),\n                                                     HdfsFileStatus.EMPTY_NAME,\n                                                     node, iip.getPathSnapshotId());\n      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                              policyId, iip);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, path-String, iip-INodesInPath, isRawPath-boolean, includeStoragePolicy-boolean]",
            "newValue": "[fsd-FSDirectory, iip-INodesInPath, includeStoragePolicy-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10762. Pass IIP for file status related methods\n",
          "commitDate": "24/08/16 6:46 AM",
          "commitName": "ec252ce0fc0998ce13f31af3440c08a236328e5a",
          "commitAuthor": "Daryn Sharp",
          "commitDateOld": "22/08/16 2:57 PM",
          "commitNameOld": "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 1.66,
          "commitsBetweenForRepo": 14,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,21 @@\n-  static HdfsFileStatus getFileInfo(\n-      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n-      boolean includeStoragePolicy)\n-      throws IOException {\n+  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n+      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n     fsd.readLock();\n     try {\n       final INode node \u003d iip.getLastINode();\n       if (node \u003d\u003d null) {\n         return null;\n       }\n \n       byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n           node.getStoragePolicyID() :\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n+      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, iip.getPath(),\n                                                      HdfsFileStatus.EMPTY_NAME,\n                                                      node, iip.getPathSnapshotId());\n       return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n+                              policyId, iip);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n    fsd.readLock();\n    try {\n      final INode node \u003d iip.getLastINode();\n      if (node \u003d\u003d null) {\n        return null;\n      }\n\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n          node.getStoragePolicyID() :\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, iip.getPath(),\n                                                     HdfsFileStatus.EMPTY_NAME,\n                                                     node, iip.getPathSnapshotId());\n      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                              policyId, iip);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "Revert \"HDFS-10762. Pass IIP for file status related methods. Contributed by Daryn Sharp.\"\n\nThis reverts commit 22fc46d7659972ff016ccf1c6f781f0c160be26f.\n",
      "commitDate": "22/08/16 2:57 PM",
      "commitName": "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "Revert \"HDFS-10762. Pass IIP for file status related methods. Contributed by Daryn Sharp.\"\n\nThis reverts commit 22fc46d7659972ff016ccf1c6f781f0c160be26f.\n",
          "commitDate": "22/08/16 2:57 PM",
          "commitName": "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "22/08/16 1:37 PM",
          "commitNameOld": "22fc46d7659972ff016ccf1c6f781f0c160be26f",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 0.06,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,23 @@\n-  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n-      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n+  static HdfsFileStatus getFileInfo(\n+      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n+      boolean includeStoragePolicy)\n+      throws IOException {\n     fsd.readLock();\n     try {\n       final INode node \u003d iip.getLastINode();\n       if (node \u003d\u003d null) {\n         return null;\n       }\n \n       byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n           node.getStoragePolicyID() :\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, iip.getPath(),\n+      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n                                                      HdfsFileStatus.EMPTY_NAME,\n                                                      node, iip.getPathSnapshotId());\n       return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                              policyId, iip);\n+                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n      boolean includeStoragePolicy)\n      throws IOException {\n    fsd.readLock();\n    try {\n      final INode node \u003d iip.getLastINode();\n      if (node \u003d\u003d null) {\n        return null;\n      }\n\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n          node.getStoragePolicyID() :\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n                                                     HdfsFileStatus.EMPTY_NAME,\n                                                     node, iip.getPathSnapshotId());\n      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, iip-INodesInPath, includeStoragePolicy-boolean]",
            "newValue": "[fsd-FSDirectory, path-String, iip-INodesInPath, isRawPath-boolean, includeStoragePolicy-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "Revert \"HDFS-10762. Pass IIP for file status related methods. Contributed by Daryn Sharp.\"\n\nThis reverts commit 22fc46d7659972ff016ccf1c6f781f0c160be26f.\n",
          "commitDate": "22/08/16 2:57 PM",
          "commitName": "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "22/08/16 1:37 PM",
          "commitNameOld": "22fc46d7659972ff016ccf1c6f781f0c160be26f",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 0.06,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,23 @@\n-  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n-      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n+  static HdfsFileStatus getFileInfo(\n+      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n+      boolean includeStoragePolicy)\n+      throws IOException {\n     fsd.readLock();\n     try {\n       final INode node \u003d iip.getLastINode();\n       if (node \u003d\u003d null) {\n         return null;\n       }\n \n       byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n           node.getStoragePolicyID() :\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, iip.getPath(),\n+      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n                                                      HdfsFileStatus.EMPTY_NAME,\n                                                      node, iip.getPathSnapshotId());\n       return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                              policyId, iip);\n+                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n      boolean includeStoragePolicy)\n      throws IOException {\n    fsd.readLock();\n    try {\n      final INode node \u003d iip.getLastINode();\n      if (node \u003d\u003d null) {\n        return null;\n      }\n\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n          node.getStoragePolicyID() :\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n                                                     HdfsFileStatus.EMPTY_NAME,\n                                                     node, iip.getPathSnapshotId());\n      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "22fc46d7659972ff016ccf1c6f781f0c160be26f": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10762. Pass IIP for file status related methods. Contributed by Daryn Sharp.\n",
      "commitDate": "22/08/16 1:37 PM",
      "commitName": "22fc46d7659972ff016ccf1c6f781f0c160be26f",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10762. Pass IIP for file status related methods. Contributed by Daryn Sharp.\n",
          "commitDate": "22/08/16 1:37 PM",
          "commitName": "22fc46d7659972ff016ccf1c6f781f0c160be26f",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "17/08/16 1:53 PM",
          "commitNameOld": "869393643de23dcb010cc33091c8eb398de0fd6c",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 4.99,
          "commitsBetweenForRepo": 29,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,21 @@\n-  static HdfsFileStatus getFileInfo(\n-      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n-      boolean includeStoragePolicy)\n-      throws IOException {\n+  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n+      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n     fsd.readLock();\n     try {\n       final INode node \u003d iip.getLastINode();\n       if (node \u003d\u003d null) {\n         return null;\n       }\n \n       byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n           node.getStoragePolicyID() :\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n+      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, iip.getPath(),\n                                                      HdfsFileStatus.EMPTY_NAME,\n                                                      node, iip.getPathSnapshotId());\n       return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n+                              policyId, iip);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n    fsd.readLock();\n    try {\n      final INode node \u003d iip.getLastINode();\n      if (node \u003d\u003d null) {\n        return null;\n      }\n\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n          node.getStoragePolicyID() :\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, iip.getPath(),\n                                                     HdfsFileStatus.EMPTY_NAME,\n                                                     node, iip.getPathSnapshotId());\n      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                              policyId, iip);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, path-String, iip-INodesInPath, isRawPath-boolean, includeStoragePolicy-boolean]",
            "newValue": "[fsd-FSDirectory, iip-INodesInPath, includeStoragePolicy-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10762. Pass IIP for file status related methods. Contributed by Daryn Sharp.\n",
          "commitDate": "22/08/16 1:37 PM",
          "commitName": "22fc46d7659972ff016ccf1c6f781f0c160be26f",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "17/08/16 1:53 PM",
          "commitNameOld": "869393643de23dcb010cc33091c8eb398de0fd6c",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 4.99,
          "commitsBetweenForRepo": 29,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,21 @@\n-  static HdfsFileStatus getFileInfo(\n-      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n-      boolean includeStoragePolicy)\n-      throws IOException {\n+  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n+      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n     fsd.readLock();\n     try {\n       final INode node \u003d iip.getLastINode();\n       if (node \u003d\u003d null) {\n         return null;\n       }\n \n       byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n           node.getStoragePolicyID() :\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n+      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, iip.getPath(),\n                                                      HdfsFileStatus.EMPTY_NAME,\n                                                      node, iip.getPathSnapshotId());\n       return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n+                              policyId, iip);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(FSDirectory fsd,\n      INodesInPath iip, boolean includeStoragePolicy) throws IOException {\n    fsd.readLock();\n    try {\n      final INode node \u003d iip.getLastINode();\n      if (node \u003d\u003d null) {\n        return null;\n      }\n\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n          node.getStoragePolicyID() :\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, iip.getPath(),\n                                                     HdfsFileStatus.EMPTY_NAME,\n                                                     node, iip.getPathSnapshotId());\n      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                              policyId, iip);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "9f4bf3bdf9e74800643477cfb18361e01cf6859c": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9621. getListing wrongly associates Erasure Coding policy to pre-existing replicated files under an EC directory. Contributed by Jing Zhao.\n",
      "commitDate": "11/01/16 11:31 AM",
      "commitName": "9f4bf3bdf9e74800643477cfb18361e01cf6859c",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9621. getListing wrongly associates Erasure Coding policy to pre-existing replicated files under an EC directory. Contributed by Jing Zhao.\n",
          "commitDate": "11/01/16 11:31 AM",
          "commitName": "9f4bf3bdf9e74800643477cfb18361e01cf6859c",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/11/15 1:14 PM",
          "commitNameOld": "977e0b3c4ce76746a3d8590d2d790fdc96c86ca5",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 47.93,
          "commitsBetweenForRepo": 246,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n   static HdfsFileStatus getFileInfo(\n-      FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n+      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n       boolean includeStoragePolicy)\n       throws IOException {\n     fsd.readLock();\n     try {\n-      final INode i \u003d src.getLastINode();\n-      if (i \u003d\u003d null) {\n+      final INode node \u003d iip.getLastINode();\n+      if (node \u003d\u003d null) {\n         return null;\n       }\n \n-      byte policyId \u003d includeStoragePolicy \u0026\u0026 !i.isSymlink() ?\n-          i.getStoragePolicyID() :\n+      byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n+          node.getStoragePolicyID() :\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n       INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n                                                      HdfsFileStatus.EMPTY_NAME,\n-                                                     i, src.getPathSnapshotId());\n-      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, i, nodeAttrs,\n-                              policyId, src.getPathSnapshotId(), isRawPath, src);\n+                                                     node, iip.getPathSnapshotId());\n+      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n+                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n      boolean includeStoragePolicy)\n      throws IOException {\n    fsd.readLock();\n    try {\n      final INode node \u003d iip.getLastINode();\n      if (node \u003d\u003d null) {\n        return null;\n      }\n\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n          node.getStoragePolicyID() :\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n                                                     HdfsFileStatus.EMPTY_NAME,\n                                                     node, iip.getPathSnapshotId());\n      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, path-String, src-INodesInPath, isRawPath-boolean, includeStoragePolicy-boolean]",
            "newValue": "[fsd-FSDirectory, path-String, iip-INodesInPath, isRawPath-boolean, includeStoragePolicy-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9621. getListing wrongly associates Erasure Coding policy to pre-existing replicated files under an EC directory. Contributed by Jing Zhao.\n",
          "commitDate": "11/01/16 11:31 AM",
          "commitName": "9f4bf3bdf9e74800643477cfb18361e01cf6859c",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/11/15 1:14 PM",
          "commitNameOld": "977e0b3c4ce76746a3d8590d2d790fdc96c86ca5",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 47.93,
          "commitsBetweenForRepo": 246,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n   static HdfsFileStatus getFileInfo(\n-      FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n+      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n       boolean includeStoragePolicy)\n       throws IOException {\n     fsd.readLock();\n     try {\n-      final INode i \u003d src.getLastINode();\n-      if (i \u003d\u003d null) {\n+      final INode node \u003d iip.getLastINode();\n+      if (node \u003d\u003d null) {\n         return null;\n       }\n \n-      byte policyId \u003d includeStoragePolicy \u0026\u0026 !i.isSymlink() ?\n-          i.getStoragePolicyID() :\n+      byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n+          node.getStoragePolicyID() :\n           HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n       INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n                                                      HdfsFileStatus.EMPTY_NAME,\n-                                                     i, src.getPathSnapshotId());\n-      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, i, nodeAttrs,\n-                              policyId, src.getPathSnapshotId(), isRawPath, src);\n+                                                     node, iip.getPathSnapshotId());\n+      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n+                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String path, INodesInPath iip, boolean isRawPath,\n      boolean includeStoragePolicy)\n      throws IOException {\n    fsd.readLock();\n    try {\n      final INode node \u003d iip.getLastINode();\n      if (node \u003d\u003d null) {\n        return null;\n      }\n\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 !node.isSymlink() ?\n          node.getStoragePolicyID() :\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(fsd, path,\n                                                     HdfsFileStatus.EMPTY_NAME,\n                                                     node, iip.getPathSnapshotId());\n      return createFileStatus(fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                              policyId, iip.getPathSnapshotId(), isRawPath, iip);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/04/15 1:41 PM",
      "commitNameOld": "c55d609053fe24b3a50fbe17dc1b47717b453ed6",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.85,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   static HdfsFileStatus getFileInfo(\n       FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n       boolean includeStoragePolicy)\n       throws IOException {\n     fsd.readLock();\n     try {\n       final INode i \u003d src.getLastINode();\n       if (i \u003d\u003d null) {\n         return null;\n       }\n \n       byte policyId \u003d includeStoragePolicy \u0026\u0026 !i.isSymlink() ?\n-          i.getStoragePolicyID() : HdfsConstantsClient\n-          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n+          i.getStoragePolicyID() :\n+          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n       INodeAttributes nodeAttrs \u003d getINodeAttributes(\n           fsd, path, HdfsFileStatus.EMPTY_NAME, i, src.getPathSnapshotId());\n       return createFileStatus(\n           fsd, HdfsFileStatus.EMPTY_NAME,\n           i, nodeAttrs, policyId,\n           src.getPathSnapshotId(),\n           isRawPath, src);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n      boolean includeStoragePolicy)\n      throws IOException {\n    fsd.readLock();\n    try {\n      final INode i \u003d src.getLastINode();\n      if (i \u003d\u003d null) {\n        return null;\n      }\n\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() :\n          HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n          fsd, path, HdfsFileStatus.EMPTY_NAME, i, src.getPathSnapshotId());\n      return createFileStatus(\n          fsd, HdfsFileStatus.EMPTY_NAME,\n          i, nodeAttrs, policyId,\n          src.getPathSnapshotId(),\n          isRawPath, src);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "c55d609053fe24b3a50fbe17dc1b47717b453ed6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8200. Refactor FSDirStatAndListingOp. Contributed by Haohui Mai.\n",
      "commitDate": "30/04/15 1:41 PM",
      "commitName": "c55d609053fe24b3a50fbe17dc1b47717b453ed6",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "20/04/15 12:36 AM",
      "commitNameOld": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 10.55,
      "commitsBetweenForRepo": 106,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,25 @@\n   static HdfsFileStatus getFileInfo(\n       FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n       boolean includeStoragePolicy)\n       throws IOException {\n     fsd.readLock();\n     try {\n       final INode i \u003d src.getLastINode();\n-      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n-          i.getStoragePolicyID() : HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-      return i \u003d\u003d null ? null : createFileStatus(\n-          fsd, path, HdfsFileStatus.EMPTY_NAME, i, policyId,\n-          src.getPathSnapshotId(), isRawPath, src);\n+      if (i \u003d\u003d null) {\n+        return null;\n+      }\n+\n+      byte policyId \u003d includeStoragePolicy \u0026\u0026 !i.isSymlink() ?\n+          i.getStoragePolicyID() : HdfsConstantsClient\n+          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n+      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n+          fsd, path, HdfsFileStatus.EMPTY_NAME, i, src.getPathSnapshotId());\n+      return createFileStatus(\n+          fsd, HdfsFileStatus.EMPTY_NAME,\n+          i, nodeAttrs, policyId,\n+          src.getPathSnapshotId(),\n+          isRawPath, src);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n      boolean includeStoragePolicy)\n      throws IOException {\n    fsd.readLock();\n    try {\n      final INode i \u003d src.getLastINode();\n      if (i \u003d\u003d null) {\n        return null;\n      }\n\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : HdfsConstantsClient\n          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      INodeAttributes nodeAttrs \u003d getINodeAttributes(\n          fsd, path, HdfsFileStatus.EMPTY_NAME, i, src.getPathSnapshotId());\n      return createFileStatus(\n          fsd, HdfsFileStatus.EMPTY_NAME,\n          i, nodeAttrs, policyId,\n          src.getPathSnapshotId(),\n          isRawPath, src);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "5c97db07fb306842f49d73a67a90cecec19a7833": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8169. Move LocatedBlocks and related classes to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "20/04/15 12:36 AM",
      "commitName": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "08/04/15 1:38 PM",
      "commitNameOld": "285b31e75e51ec8e3a796c2cb0208739368ca9b8",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 11.46,
      "commitsBetweenForRepo": 85,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   static HdfsFileStatus getFileInfo(\n       FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n       boolean includeStoragePolicy)\n       throws IOException {\n     fsd.readLock();\n     try {\n       final INode i \u003d src.getLastINode();\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n-          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n+          i.getStoragePolicyID() : HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n       return i \u003d\u003d null ? null : createFileStatus(\n           fsd, path, HdfsFileStatus.EMPTY_NAME, i, policyId,\n           src.getPathSnapshotId(), isRawPath, src);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n      boolean includeStoragePolicy)\n      throws IOException {\n    fsd.readLock();\n    try {\n      final INode i \u003d src.getLastINode();\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(\n          fsd, path, HdfsFileStatus.EMPTY_NAME, i, policyId,\n          src.getPathSnapshotId(), isRawPath, src);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "53a28afe293e5bf185c8d4f2c7aea212e66015c2": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6826. Plugin interface to enable delegation of HDFS authorization assertions. Contributed by Arun Suresh.\n",
      "commitDate": "24/03/15 4:02 PM",
      "commitName": "53a28afe293e5bf185c8d4f2c7aea212e66015c2",
      "commitAuthor": "Jitendra Pandey",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6826. Plugin interface to enable delegation of HDFS authorization assertions. Contributed by Arun Suresh.\n",
          "commitDate": "24/03/15 4:02 PM",
          "commitName": "53a28afe293e5bf185c8d4f2c7aea212e66015c2",
          "commitAuthor": "Jitendra Pandey",
          "commitDateOld": "13/01/15 12:24 AM",
          "commitNameOld": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
          "commitAuthorOld": "Konstantin V Shvachko",
          "daysBetweenCommits": 70.61,
          "commitsBetweenForRepo": 649,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,16 @@\n   static HdfsFileStatus getFileInfo(\n-      FSDirectory fsd, INodesInPath src, boolean isRawPath,\n+      FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n       boolean includeStoragePolicy)\n       throws IOException {\n     fsd.readLock();\n     try {\n       final INode i \u003d src.getLastINode();\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n           i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n       return i \u003d\u003d null ? null : createFileStatus(\n-          fsd, HdfsFileStatus.EMPTY_NAME, i, policyId,\n+          fsd, path, HdfsFileStatus.EMPTY_NAME, i, policyId,\n           src.getPathSnapshotId(), isRawPath, src);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n      boolean includeStoragePolicy)\n      throws IOException {\n    fsd.readLock();\n    try {\n      final INode i \u003d src.getLastINode();\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(\n          fsd, path, HdfsFileStatus.EMPTY_NAME, i, policyId,\n          src.getPathSnapshotId(), isRawPath, src);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, src-INodesInPath, isRawPath-boolean, includeStoragePolicy-boolean]",
            "newValue": "[fsd-FSDirectory, path-String, src-INodesInPath, isRawPath-boolean, includeStoragePolicy-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6826. Plugin interface to enable delegation of HDFS authorization assertions. Contributed by Arun Suresh.\n",
          "commitDate": "24/03/15 4:02 PM",
          "commitName": "53a28afe293e5bf185c8d4f2c7aea212e66015c2",
          "commitAuthor": "Jitendra Pandey",
          "commitDateOld": "13/01/15 12:24 AM",
          "commitNameOld": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
          "commitAuthorOld": "Konstantin V Shvachko",
          "daysBetweenCommits": 70.61,
          "commitsBetweenForRepo": 649,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,16 @@\n   static HdfsFileStatus getFileInfo(\n-      FSDirectory fsd, INodesInPath src, boolean isRawPath,\n+      FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n       boolean includeStoragePolicy)\n       throws IOException {\n     fsd.readLock();\n     try {\n       final INode i \u003d src.getLastINode();\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n           i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n       return i \u003d\u003d null ? null : createFileStatus(\n-          fsd, HdfsFileStatus.EMPTY_NAME, i, policyId,\n+          fsd, path, HdfsFileStatus.EMPTY_NAME, i, policyId,\n           src.getPathSnapshotId(), isRawPath, src);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String path, INodesInPath src, boolean isRawPath,\n      boolean includeStoragePolicy)\n      throws IOException {\n    fsd.readLock();\n    try {\n      final INode i \u003d src.getLastINode();\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(\n          fsd, path, HdfsFileStatus.EMPTY_NAME, i, policyId,\n          src.getPathSnapshotId(), isRawPath, src);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "65f2a4ee600dfffa5203450261da3c1989de25a9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7543. Avoid path resolution when getting FileStatus for audit logs. Contributed by Haohui Mai.\n",
      "commitDate": "18/12/14 11:25 AM",
      "commitName": "65f2a4ee600dfffa5203450261da3c1989de25a9",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7543. Avoid path resolution when getting FileStatus for audit logs. Contributed by Haohui Mai.\n",
          "commitDate": "18/12/14 11:25 AM",
          "commitName": "65f2a4ee600dfffa5203450261da3c1989de25a9",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/12/14 10:40 AM",
          "commitNameOld": "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 3.03,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,16 @@\n   static HdfsFileStatus getFileInfo(\n-      FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n+      FSDirectory fsd, INodesInPath src, boolean isRawPath,\n       boolean includeStoragePolicy)\n-    throws IOException {\n-    String srcs \u003d FSDirectory.normalizePath(src);\n+      throws IOException {\n     fsd.readLock();\n     try {\n-      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getFileInfo4DotSnapshot(fsd, srcs);\n-      }\n-      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n-      final INode i \u003d inodesInPath.getLastINode();\n+      final INode i \u003d src.getLastINode();\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n           i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n-      return i \u003d\u003d null ? null : createFileStatus(fsd,\n-          HdfsFileStatus.EMPTY_NAME, i, policyId,\n-          inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n+      return i \u003d\u003d null ? null : createFileStatus(\n+          fsd, HdfsFileStatus.EMPTY_NAME, i, policyId,\n+          src.getPathSnapshotId(), isRawPath, src);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, INodesInPath src, boolean isRawPath,\n      boolean includeStoragePolicy)\n      throws IOException {\n    fsd.readLock();\n    try {\n      final INode i \u003d src.getLastINode();\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(\n          fsd, HdfsFileStatus.EMPTY_NAME, i, policyId,\n          src.getPathSnapshotId(), isRawPath, src);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, src-String, resolveLink-boolean, isRawPath-boolean, includeStoragePolicy-boolean]",
            "newValue": "[fsd-FSDirectory, src-INodesInPath, isRawPath-boolean, includeStoragePolicy-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7543. Avoid path resolution when getting FileStatus for audit logs. Contributed by Haohui Mai.\n",
          "commitDate": "18/12/14 11:25 AM",
          "commitName": "65f2a4ee600dfffa5203450261da3c1989de25a9",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/12/14 10:40 AM",
          "commitNameOld": "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 3.03,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,16 @@\n   static HdfsFileStatus getFileInfo(\n-      FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n+      FSDirectory fsd, INodesInPath src, boolean isRawPath,\n       boolean includeStoragePolicy)\n-    throws IOException {\n-    String srcs \u003d FSDirectory.normalizePath(src);\n+      throws IOException {\n     fsd.readLock();\n     try {\n-      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getFileInfo4DotSnapshot(fsd, srcs);\n-      }\n-      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n-      final INode i \u003d inodesInPath.getLastINode();\n+      final INode i \u003d src.getLastINode();\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n           i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n-      return i \u003d\u003d null ? null : createFileStatus(fsd,\n-          HdfsFileStatus.EMPTY_NAME, i, policyId,\n-          inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n+      return i \u003d\u003d null ? null : createFileStatus(\n+          fsd, HdfsFileStatus.EMPTY_NAME, i, policyId,\n+          src.getPathSnapshotId(), isRawPath, src);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, INodesInPath src, boolean isRawPath,\n      boolean includeStoragePolicy)\n      throws IOException {\n    fsd.readLock();\n    try {\n      final INode i \u003d src.getLastINode();\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(\n          fsd, HdfsFileStatus.EMPTY_NAME, i, policyId,\n          src.getPathSnapshotId(), isRawPath, src);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "5776a41da08af653206bb94d7c76c9c4dcce059a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7498. Simplify the logic in INodesInPath. Contributed by Jing Zhao.\n",
      "commitDate": "09/12/14 11:37 AM",
      "commitName": "5776a41da08af653206bb94d7c76c9c4dcce059a",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "05/12/14 2:17 PM",
      "commitNameOld": "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 3.89,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,21 @@\n   static HdfsFileStatus getFileInfo(\n       FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n       boolean includeStoragePolicy)\n     throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getFileInfo4DotSnapshot(fsd, srcs);\n       }\n       final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n-      final INode[] inodes \u003d inodesInPath.getINodes();\n-      final INode i \u003d inodes[inodes.length - 1];\n+      final INode i \u003d inodesInPath.getLastINode();\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n           i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n       return i \u003d\u003d null ? null : createFileStatus(fsd,\n           HdfsFileStatus.EMPTY_NAME, i, policyId,\n           inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n      boolean includeStoragePolicy)\n    throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getFileInfo4DotSnapshot(fsd, srcs);\n      }\n      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n      final INode i \u003d inodesInPath.getLastINode();\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(fsd,\n          HdfsFileStatus.EMPTY_NAME, i, policyId,\n          inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "01/12/14 9:36 PM",
      "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,22 @@\n-  HdfsFileStatus getFileInfo(String src, boolean resolveLink,\n-      boolean isRawPath, boolean includeStoragePolicy)\n+  static HdfsFileStatus getFileInfo(\n+      FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n+      boolean includeStoragePolicy)\n     throws IOException {\n-    String srcs \u003d normalizePath(src);\n-    readLock();\n+    String srcs \u003d FSDirectory.normalizePath(src);\n+    fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getFileInfo4DotSnapshot(srcs);\n+        return getFileInfo4DotSnapshot(fsd, srcs);\n       }\n-      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, resolveLink);\n+      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n       final INode[] inodes \u003d inodesInPath.getINodes();\n       final INode i \u003d inodes[inodes.length - 1];\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n           i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n-      return i \u003d\u003d null ? null : createFileStatus(HdfsFileStatus.EMPTY_NAME, i,\n-          policyId, inodesInPath.getPathSnapshotId(), isRawPath,\n-          inodesInPath);\n+      return i \u003d\u003d null ? null : createFileStatus(fsd,\n+          HdfsFileStatus.EMPTY_NAME, i, policyId,\n+          inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n     } finally {\n-      readUnlock();\n+      fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n      boolean includeStoragePolicy)\n    throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getFileInfo4DotSnapshot(fsd, srcs);\n      }\n      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final INode i \u003d inodes[inodes.length - 1];\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(fsd,\n          HdfsFileStatus.EMPTY_NAME, i, policyId,\n          inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
            "oldMethodName": "getFileInfo",
            "newMethodName": "getFileInfo"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,22 @@\n-  HdfsFileStatus getFileInfo(String src, boolean resolveLink,\n-      boolean isRawPath, boolean includeStoragePolicy)\n+  static HdfsFileStatus getFileInfo(\n+      FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n+      boolean includeStoragePolicy)\n     throws IOException {\n-    String srcs \u003d normalizePath(src);\n-    readLock();\n+    String srcs \u003d FSDirectory.normalizePath(src);\n+    fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getFileInfo4DotSnapshot(srcs);\n+        return getFileInfo4DotSnapshot(fsd, srcs);\n       }\n-      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, resolveLink);\n+      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n       final INode[] inodes \u003d inodesInPath.getINodes();\n       final INode i \u003d inodes[inodes.length - 1];\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n           i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n-      return i \u003d\u003d null ? null : createFileStatus(HdfsFileStatus.EMPTY_NAME, i,\n-          policyId, inodesInPath.getPathSnapshotId(), isRawPath,\n-          inodesInPath);\n+      return i \u003d\u003d null ? null : createFileStatus(fsd,\n+          HdfsFileStatus.EMPTY_NAME, i, policyId,\n+          inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n     } finally {\n-      readUnlock();\n+      fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n      boolean includeStoragePolicy)\n    throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getFileInfo4DotSnapshot(fsd, srcs);\n      }\n      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final INode i \u003d inodes[inodes.length - 1];\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(fsd,\n          HdfsFileStatus.EMPTY_NAME, i, policyId,\n          inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,22 @@\n-  HdfsFileStatus getFileInfo(String src, boolean resolveLink,\n-      boolean isRawPath, boolean includeStoragePolicy)\n+  static HdfsFileStatus getFileInfo(\n+      FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n+      boolean includeStoragePolicy)\n     throws IOException {\n-    String srcs \u003d normalizePath(src);\n-    readLock();\n+    String srcs \u003d FSDirectory.normalizePath(src);\n+    fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getFileInfo4DotSnapshot(srcs);\n+        return getFileInfo4DotSnapshot(fsd, srcs);\n       }\n-      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, resolveLink);\n+      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n       final INode[] inodes \u003d inodesInPath.getINodes();\n       final INode i \u003d inodes[inodes.length - 1];\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n           i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n-      return i \u003d\u003d null ? null : createFileStatus(HdfsFileStatus.EMPTY_NAME, i,\n-          policyId, inodesInPath.getPathSnapshotId(), isRawPath,\n-          inodesInPath);\n+      return i \u003d\u003d null ? null : createFileStatus(fsd,\n+          HdfsFileStatus.EMPTY_NAME, i, policyId,\n+          inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n     } finally {\n-      readUnlock();\n+      fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n      boolean includeStoragePolicy)\n    throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getFileInfo4DotSnapshot(fsd, srcs);\n      }\n      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final INode i \u003d inodes[inodes.length - 1];\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(fsd,\n          HdfsFileStatus.EMPTY_NAME, i, policyId,\n          inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,22 @@\n-  HdfsFileStatus getFileInfo(String src, boolean resolveLink,\n-      boolean isRawPath, boolean includeStoragePolicy)\n+  static HdfsFileStatus getFileInfo(\n+      FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n+      boolean includeStoragePolicy)\n     throws IOException {\n-    String srcs \u003d normalizePath(src);\n-    readLock();\n+    String srcs \u003d FSDirectory.normalizePath(src);\n+    fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getFileInfo4DotSnapshot(srcs);\n+        return getFileInfo4DotSnapshot(fsd, srcs);\n       }\n-      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, resolveLink);\n+      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n       final INode[] inodes \u003d inodesInPath.getINodes();\n       final INode i \u003d inodes[inodes.length - 1];\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n           i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n-      return i \u003d\u003d null ? null : createFileStatus(HdfsFileStatus.EMPTY_NAME, i,\n-          policyId, inodesInPath.getPathSnapshotId(), isRawPath,\n-          inodesInPath);\n+      return i \u003d\u003d null ? null : createFileStatus(fsd,\n+          HdfsFileStatus.EMPTY_NAME, i, policyId,\n+          inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n     } finally {\n-      readUnlock();\n+      fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus getFileInfo(\n      FSDirectory fsd, String src, boolean resolveLink, boolean isRawPath,\n      boolean includeStoragePolicy)\n    throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getFileInfo4DotSnapshot(fsd, srcs);\n      }\n      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, resolveLink);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final INode i \u003d inodes[inodes.length - 1];\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(fsd,\n          HdfsFileStatus.EMPTY_NAME, i, policyId,\n          inodesInPath.getPathSnapshotId(), isRawPath, inodesInPath);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[src-String, resolveLink-boolean, isRawPath-boolean, includeStoragePolicy-boolean]",
            "newValue": "[fsd-FSDirectory, src-String, resolveLink-boolean, isRawPath-boolean, includeStoragePolicy-boolean]"
          }
        }
      ]
    },
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7081. Add new DistributedFileSystem API for getting all the existing storage policies. Contributed by Jing Zhao.\n",
      "commitDate": "24/09/14 10:05 AM",
      "commitName": "073bbd805c6680f47bbfcc6e8efd708ad729bca4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "21/09/14 9:29 PM",
      "commitNameOld": "1737950d0fc83c68f386881b843c41b0b1e342de",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.52,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   HdfsFileStatus getFileInfo(String src, boolean resolveLink,\n       boolean isRawPath, boolean includeStoragePolicy)\n     throws IOException {\n     String srcs \u003d normalizePath(src);\n     readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getFileInfo4DotSnapshot(srcs);\n       }\n       final INodesInPath inodesInPath \u003d getINodesInPath(srcs, resolveLink);\n       final INode[] inodes \u003d inodesInPath.getINodes();\n       final INode i \u003d inodes[inodes.length - 1];\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n-          i.getStoragePolicyID() : BlockStoragePolicy.ID_UNSPECIFIED;\n+          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n       return i \u003d\u003d null ? null : createFileStatus(HdfsFileStatus.EMPTY_NAME, i,\n           policyId, inodesInPath.getPathSnapshotId(), isRawPath,\n           inodesInPath);\n     } finally {\n       readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HdfsFileStatus getFileInfo(String src, boolean resolveLink,\n      boolean isRawPath, boolean includeStoragePolicy)\n    throws IOException {\n    String srcs \u003d normalizePath(src);\n    readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getFileInfo4DotSnapshot(srcs);\n      }\n      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, resolveLink);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final INode i \u003d inodes[inodes.length - 1];\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(HdfsFileStatus.EMPTY_NAME, i,\n          policyId, inodesInPath.getPathSnapshotId(), isRawPath,\n          inodesInPath);\n    } finally {\n      readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "1737950d0fc83c68f386881b843c41b0b1e342de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6987. Move CipherSuite xattr information up to the encryption zone root. Contributed by Zhe Zhang.\n",
      "commitDate": "21/09/14 9:29 PM",
      "commitName": "1737950d0fc83c68f386881b843c41b0b1e342de",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "17/09/14 10:00 PM",
      "commitNameOld": "2d2b0009e662db75cf22e2ce8d618ed0a8e61c2f",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.98,
      "commitsBetweenForRepo": 42,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,21 @@\n   HdfsFileStatus getFileInfo(String src, boolean resolveLink,\n       boolean isRawPath, boolean includeStoragePolicy)\n     throws IOException {\n     String srcs \u003d normalizePath(src);\n     readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getFileInfo4DotSnapshot(srcs);\n       }\n-      final INodesInPath inodesInPath \u003d getLastINodeInPath(srcs, resolveLink);\n-      final INode i \u003d inodesInPath.getINode(0);\n+      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, resolveLink);\n+      final INode[] inodes \u003d inodesInPath.getINodes();\n+      final INode i \u003d inodes[inodes.length - 1];\n       byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n           i.getStoragePolicyID() : BlockStoragePolicy.ID_UNSPECIFIED;\n       return i \u003d\u003d null ? null : createFileStatus(HdfsFileStatus.EMPTY_NAME, i,\n-          policyId, inodesInPath.getPathSnapshotId(), isRawPath);\n+          policyId, inodesInPath.getPathSnapshotId(), isRawPath,\n+          inodesInPath);\n     } finally {\n       readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  HdfsFileStatus getFileInfo(String src, boolean resolveLink,\n      boolean isRawPath, boolean includeStoragePolicy)\n    throws IOException {\n    String srcs \u003d normalizePath(src);\n    readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getFileInfo4DotSnapshot(srcs);\n      }\n      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, resolveLink);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final INode i \u003d inodes[inodes.length - 1];\n      byte policyId \u003d includeStoragePolicy \u0026\u0026 i !\u003d null \u0026\u0026 !i.isSymlink() ?\n          i.getStoragePolicyID() : BlockStoragePolicy.ID_UNSPECIFIED;\n      return i \u003d\u003d null ? null : createFileStatus(HdfsFileStatus.EMPTY_NAME, i,\n          policyId, inodesInPath.getPathSnapshotId(), isRawPath,\n          inodesInPath);\n    } finally {\n      readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    }
  }
}