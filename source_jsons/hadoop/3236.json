{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WebHdfsFileSystem.java",
  "functionName": "listStatus",
  "functionId": "listStatus___f-Path(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
  "functionStartLine": 1689,
  "functionEndLine": 1708,
  "numCommitsSeen": 179,
  "timeTaken": 5630,
  "changeHistory": [
    "12e44e7bdaf53d3720a89d32f0cc2717241bd6b2",
    "db6d243cf89d25fefbffd4c8721e14d9246b5a16",
    "687233f20d24c29041929dd0a99d963cec54b6df",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
    "ab04ff9efe632b4eca6faca7407ac35e00e6a379",
    "e2262d3d18c6d5c2aa20f96920104dc07271b869",
    "e4ee1d111be15ae6cca2f79be7ca73c204288d2b",
    "94c631af1fc49f5ae5881fcd5f0e80b17308d15d",
    "8cb0d4b380e0fd4437310c1dd6ef8b8995cc383d",
    "c46dbedaf94cb72e35e9e63d7f99e382ae9f0974",
    "676f488efffd50eb47e75cd750f9bc948b9e12fb",
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
    "6c3b59505b863f03629da52a1e9b886fe9b496d0"
  ],
  "changeHistoryShort": {
    "12e44e7bdaf53d3720a89d32f0cc2717241bd6b2": "Ybodychange",
    "db6d243cf89d25fefbffd4c8721e14d9246b5a16": "Ybodychange",
    "687233f20d24c29041929dd0a99d963cec54b6df": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35": "Yfilerename",
    "ab04ff9efe632b4eca6faca7407ac35e00e6a379": "Ybodychange",
    "e2262d3d18c6d5c2aa20f96920104dc07271b869": "Ybodychange",
    "e4ee1d111be15ae6cca2f79be7ca73c204288d2b": "Ybodychange",
    "94c631af1fc49f5ae5881fcd5f0e80b17308d15d": "Ybodychange",
    "8cb0d4b380e0fd4437310c1dd6ef8b8995cc383d": "Ybodychange",
    "c46dbedaf94cb72e35e9e63d7f99e382ae9f0974": "Ybodychange",
    "676f488efffd50eb47e75cd750f9bc948b9e12fb": "Ybodychange",
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57": "Ybodychange",
    "6c3b59505b863f03629da52a1e9b886fe9b496d0": "Yintroduced"
  },
  "changeHistoryDetails": {
    "12e44e7bdaf53d3720a89d32f0cc2717241bd6b2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6984. Serialize FileStatus via protobuf.\n",
      "commitDate": "02/08/17 12:12 PM",
      "commitName": "12e44e7bdaf53d3720a89d32f0cc2717241bd6b2",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "05/07/17 11:10 AM",
      "commitNameOld": "6436768baf1b2ac05f6786edcd76fd3a66c03eaa",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 28.04,
      "commitsBetweenForRepo": 199,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,20 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n     storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\n \n+    final URI fsUri \u003d getUri();\n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n     return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n       @Override\n       FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n         HdfsFileStatus[] hdfsStatuses \u003d\n             JsonUtilClient.toHdfsFileStatusArray(json);\n         final FileStatus[] statuses \u003d new FileStatus[hdfsStatuses.length];\n         for (int i \u003d 0; i \u003c hdfsStatuses.length; i++) {\n-          statuses[i] \u003d makeQualified(hdfsStatuses[i], f);\n+          statuses[i] \u003d hdfsStatuses[i].makeQualified(fsUri, f);\n         }\n \n         return statuses;\n       }\n     }.run();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\n\n    final URI fsUri \u003d getUri();\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n      @Override\n      FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n        HdfsFileStatus[] hdfsStatuses \u003d\n            JsonUtilClient.toHdfsFileStatusArray(json);\n        final FileStatus[] statuses \u003d new FileStatus[hdfsStatuses.length];\n        for (int i \u003d 0; i \u003c hdfsStatuses.length; i++) {\n          statuses[i] \u003d hdfsStatuses[i].makeQualified(fsUri, f);\n        }\n\n        return statuses;\n      }\n    }.run();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "db6d243cf89d25fefbffd4c8721e14d9246b5a16": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10837. Standardize serializiation of WebHDFS DirectoryListing.\n",
      "commitDate": "13/09/16 11:02 AM",
      "commitName": "db6d243cf89d25fefbffd4c8721e14d9246b5a16",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "06/09/16 11:02 AM",
      "commitNameOld": "f0d5382ff3e31a47d13e4cb6c3a244cca82b17ce",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 7.0,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,19 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n     storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\n \n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n     return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n       @Override\n       FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n-        final Map\u003c?, ?\u003e rootmap \u003d\n-            (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n-        final List\u003c?\u003e array \u003d JsonUtilClient.getList(rootmap,\n-            FileStatus.class.getSimpleName());\n-\n-        //convert FileStatus\n-        assert array !\u003d null;\n-        final FileStatus[] statuses \u003d new FileStatus[array.size()];\n-        int i \u003d 0;\n-        for (Object object : array) {\n-          final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e) object;\n-          statuses[i++] \u003d makeQualified(JsonUtilClient.toFileStatus(m, false),\n-              f);\n+        HdfsFileStatus[] hdfsStatuses \u003d\n+            JsonUtilClient.toHdfsFileStatusArray(json);\n+        final FileStatus[] statuses \u003d new FileStatus[hdfsStatuses.length];\n+        for (int i \u003d 0; i \u003c hdfsStatuses.length; i++) {\n+          statuses[i] \u003d makeQualified(hdfsStatuses[i], f);\n         }\n+\n         return statuses;\n       }\n     }.run();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n      @Override\n      FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n        HdfsFileStatus[] hdfsStatuses \u003d\n            JsonUtilClient.toHdfsFileStatusArray(json);\n        final FileStatus[] statuses \u003d new FileStatus[hdfsStatuses.length];\n        for (int i \u003d 0; i \u003c hdfsStatuses.length; i++) {\n          statuses[i] \u003d makeQualified(hdfsStatuses[i], f);\n        }\n\n        return statuses;\n      }\n    }.run();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "687233f20d24c29041929dd0a99d963cec54b6df": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13065. Add a new interface for retrieving FS and FC Statistics (Mingliang Liu via cmccabe)\n",
      "commitDate": "11/05/16 1:45 PM",
      "commitName": "687233f20d24c29041929dd0a99d963cec54b6df",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "23/04/16 7:37 AM",
      "commitNameOld": "6fcde2e38da04cae3aad6b13cf442af211f71506",
      "commitAuthorOld": "Masatake Iwasaki",
      "daysBetweenCommits": 18.26,
      "commitsBetweenForRepo": 102,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,26 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n+    storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\n \n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n     return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n       @Override\n       FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n         final Map\u003c?, ?\u003e rootmap \u003d\n             (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n         final List\u003c?\u003e array \u003d JsonUtilClient.getList(rootmap,\n             FileStatus.class.getSimpleName());\n \n         //convert FileStatus\n         assert array !\u003d null;\n         final FileStatus[] statuses \u003d new FileStatus[array.size()];\n         int i \u003d 0;\n         for (Object object : array) {\n           final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e) object;\n           statuses[i++] \u003d makeQualified(JsonUtilClient.toFileStatus(m, false),\n               f);\n         }\n         return statuses;\n       }\n     }.run();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n      @Override\n      FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n        final Map\u003c?, ?\u003e rootmap \u003d\n            (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n        final List\u003c?\u003e array \u003d JsonUtilClient.getList(rootmap,\n            FileStatus.class.getSimpleName());\n\n        //convert FileStatus\n        assert array !\u003d null;\n        final FileStatus[] statuses \u003d new FileStatus[array.size()];\n        int i \u003d 0;\n        for (Object object : array) {\n          final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e) object;\n          statuses[i++] \u003d makeQualified(JsonUtilClient.toFileStatus(m, false),\n              f);\n        }\n        return statuses;\n      }\n    }.run();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:52 PM",
      "commitNameOld": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.74,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,25 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n \n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n     return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n       @Override\n       FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n-        final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n+        final Map\u003c?, ?\u003e rootmap \u003d\n+            (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n         final List\u003c?\u003e array \u003d JsonUtilClient.getList(rootmap,\n-                                                     FileStatus.class.getSimpleName());\n+            FileStatus.class.getSimpleName());\n \n         //convert FileStatus\n+        assert array !\u003d null;\n         final FileStatus[] statuses \u003d new FileStatus[array.size()];\n         int i \u003d 0;\n         for (Object object : array) {\n           final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e) object;\n-          statuses[i++] \u003d makeQualified(JsonUtilClient.toFileStatus(m, false), f);\n+          statuses[i++] \u003d makeQualified(JsonUtilClient.toFileStatus(m, false),\n+              f);\n         }\n         return statuses;\n       }\n     }.run();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n      @Override\n      FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n        final Map\u003c?, ?\u003e rootmap \u003d\n            (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n        final List\u003c?\u003e array \u003d JsonUtilClient.getList(rootmap,\n            FileStatus.class.getSimpleName());\n\n        //convert FileStatus\n        assert array !\u003d null;\n        final FileStatus[] statuses \u003d new FileStatus[array.size()];\n        int i \u003d 0;\n        for (Object object : array) {\n          final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e) object;\n          statuses[i++] \u003d makeQualified(JsonUtilClient.toFileStatus(m, false),\n              f);\n        }\n        return statuses;\n      }\n    }.run();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8052. Move WebHdfsFileSystem into hadoop-hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "23/04/15 5:33 PM",
      "commitName": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "23/04/15 4:40 PM",
      "commitNameOld": "0b3f8957a87ada1a275c9904b211fdbdcefafb02",
      "commitAuthorOld": "Xuan",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n      @Override\n      FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n        final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n        final List\u003c?\u003e array \u003d JsonUtilClient.getList(rootmap,\n                                                     FileStatus.class.getSimpleName());\n\n        //convert FileStatus\n        final FileStatus[] statuses \u003d new FileStatus[array.size()];\n        int i \u003d 0;\n        for (Object object : array) {\n          final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e) object;\n          statuses[i++] \u003d makeQualified(JsonUtilClient.toFileStatus(m, false), f);\n        }\n        return statuses;\n      }\n    }.run();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java"
      }
    },
    "ab04ff9efe632b4eca6faca7407ac35e00e6a379": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8080. Separate JSON related routines used by WebHdfsFileSystem to a package local class. Contributed by Haohui Mai.\n",
      "commitDate": "07/04/15 9:30 PM",
      "commitName": "ab04ff9efe632b4eca6faca7407ac35e00e6a379",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "01/04/15 12:54 PM",
      "commitNameOld": "ed72daa5df97669906234e8ac9a406d78136b206",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 6.36,
      "commitsBetweenForRepo": 48,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n \n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n     return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n       @Override\n       FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n         final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n-        final List\u003c?\u003e array \u003d JsonUtil.getList(\n-            rootmap, FileStatus.class.getSimpleName());\n+        final List\u003c?\u003e array \u003d JsonUtilClient.getList(rootmap,\n+                                                     FileStatus.class.getSimpleName());\n \n         //convert FileStatus\n         final FileStatus[] statuses \u003d new FileStatus[array.size()];\n         int i \u003d 0;\n         for (Object object : array) {\n           final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e) object;\n-          statuses[i++] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n+          statuses[i++] \u003d makeQualified(JsonUtilClient.toFileStatus(m, false), f);\n         }\n         return statuses;\n       }\n     }.run();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n      @Override\n      FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n        final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n        final List\u003c?\u003e array \u003d JsonUtilClient.getList(rootmap,\n                                                     FileStatus.class.getSimpleName());\n\n        //convert FileStatus\n        final FileStatus[] statuses \u003d new FileStatus[array.size()];\n        int i \u003d 0;\n        for (Object object : array) {\n          final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e) object;\n          statuses[i++] \u003d makeQualified(JsonUtilClient.toFileStatus(m, false), f);\n        }\n        return statuses;\n      }\n    }.run();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "e2262d3d18c6d5c2aa20f96920104dc07271b869": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6565. Use jackson instead jetty json in hdfs-client. Contributed by Akira AJISAKA.\n",
      "commitDate": "03/03/15 5:54 PM",
      "commitName": "e2262d3d18c6d5c2aa20f96920104dc07271b869",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/03/15 9:17 PM",
      "commitNameOld": "d1c6accb6f87b08975175580e15f1ff1fe29ab04",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 0.86,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,22 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n \n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n     return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n       @Override\n       FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n         final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n-        final Object[] array \u003d (Object[])rootmap.get(FileStatus.class.getSimpleName());\n+        final List\u003c?\u003e array \u003d JsonUtil.getList(\n+            rootmap, FileStatus.class.getSimpleName());\n \n         //convert FileStatus\n-        final FileStatus[] statuses \u003d new FileStatus[array.length];\n-        for (int i \u003d 0; i \u003c array.length; i++) {\n-          final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e)array[i];\n-          statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n+        final FileStatus[] statuses \u003d new FileStatus[array.size()];\n+        int i \u003d 0;\n+        for (Object object : array) {\n+          final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e) object;\n+          statuses[i++] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n         }\n         return statuses;\n       }\n     }.run();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n      @Override\n      FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n        final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n        final List\u003c?\u003e array \u003d JsonUtil.getList(\n            rootmap, FileStatus.class.getSimpleName());\n\n        //convert FileStatus\n        final FileStatus[] statuses \u003d new FileStatus[array.size()];\n        int i \u003d 0;\n        for (Object object : array) {\n          final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e) object;\n          statuses[i++] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n        }\n        return statuses;\n      }\n    }.run();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "e4ee1d111be15ae6cca2f79be7ca73c204288d2b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6305. WebHdfs response decoding may throw RuntimeExceptions (Daryn Sharp via jeagles)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594273 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/05/14 9:40 AM",
      "commitName": "e4ee1d111be15ae6cca2f79be7ca73c204288d2b",
      "commitAuthor": "Jonathan Turner Eagles",
      "commitDateOld": "13/05/14 9:19 AM",
      "commitNameOld": "33ade356b35223654a077103ed7fbed89f3f2321",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,20 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n \n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n-    final Map\u003c?, ?\u003e json  \u003d run(op, f);\n-    final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n-    final Object[] array \u003d (Object[])rootmap.get(FileStatus.class.getSimpleName());\n+    return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n+      @Override\n+      FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n+        final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n+        final Object[] array \u003d (Object[])rootmap.get(FileStatus.class.getSimpleName());\n \n-    //convert FileStatus\n-    final FileStatus[] statuses \u003d new FileStatus[array.length];\n-    for(int i \u003d 0; i \u003c array.length; i++) {\n-      final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e)array[i];\n-      statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n-    }\n-    return statuses;\n+        //convert FileStatus\n+        final FileStatus[] statuses \u003d new FileStatus[array.length];\n+        for (int i \u003d 0; i \u003c array.length; i++) {\n+          final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e)array[i];\n+          statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n+        }\n+        return statuses;\n+      }\n+    }.run();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    return new FsPathResponseRunner\u003cFileStatus[]\u003e(op, f) {\n      @Override\n      FileStatus[] decodeResponse(Map\u003c?,?\u003e json) {\n        final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n        final Object[] array \u003d (Object[])rootmap.get(FileStatus.class.getSimpleName());\n\n        //convert FileStatus\n        final FileStatus[] statuses \u003d new FileStatus[array.length];\n        for (int i \u003d 0; i \u003c array.length; i++) {\n          final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e)array[i];\n          statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n        }\n        return statuses;\n      }\n    }.run();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "94c631af1fc49f5ae5881fcd5f0e80b17308d15d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2540. Webhdfs: change \"Expect: 100-continue\" to two-step write; change \"HdfsFileStatus\" and \"localName\" respectively to \"FileStatus\" and \"pathSuffix\" in JSON response.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1199396 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/11/11 11:25 AM",
      "commitName": "94c631af1fc49f5ae5881fcd5f0e80b17308d15d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "07/11/11 12:05 PM",
      "commitNameOld": "a590b498acf1a424ffbb3a9d8849c0abb409366d",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.97,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n \n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n     final Map\u003c?, ?\u003e json  \u003d run(op, f);\n-    final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(HdfsFileStatus.class.getSimpleName() + \"es\");\n-    final Object[] array \u003d (Object[])rootmap.get(HdfsFileStatus.class.getSimpleName());\n+    final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n+    final Object[] array \u003d (Object[])rootmap.get(FileStatus.class.getSimpleName());\n \n     //convert FileStatus\n     final FileStatus[] statuses \u003d new FileStatus[array.length];\n     for(int i \u003d 0; i \u003c array.length; i++) {\n       final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e)array[i];\n       statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n     }\n     return statuses;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    final Map\u003c?, ?\u003e json  \u003d run(op, f);\n    final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(FileStatus.class.getSimpleName() + \"es\");\n    final Object[] array \u003d (Object[])rootmap.get(FileStatus.class.getSimpleName());\n\n    //convert FileStatus\n    final FileStatus[] statuses \u003d new FileStatus[array.length];\n    for(int i \u003d 0; i \u003c array.length; i++) {\n      final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e)array[i];\n      statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n    }\n    return statuses;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "8cb0d4b380e0fd4437310c1dd6ef8b8995cc383d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2432. Webhdfs: response FORBIDDEN when setReplication on non-files; clear umask before creating a flie; throw IllegalArgumentException if setOwner with both owner and group empty; throw FileNotFoundException if getFileStatus on non-existing files; fix bugs in getBlockLocations; and changed getFileChecksum json response root to \"FileChecksum\".\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190077 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/10/11 4:13 PM",
      "commitName": "8cb0d4b380e0fd4437310c1dd6ef8b8995cc383d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "25/10/11 10:16 PM",
      "commitNameOld": "8335995630e2c4288795fa0dfa9b670090a6790b",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.75,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,16 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n \n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n     final Map\u003c?, ?\u003e json  \u003d run(op, f);\n     final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(HdfsFileStatus.class.getSimpleName() + \"es\");\n     final Object[] array \u003d (Object[])rootmap.get(HdfsFileStatus.class.getSimpleName());\n \n     //convert FileStatus\n     final FileStatus[] statuses \u003d new FileStatus[array.length];\n     for(int i \u003d 0; i \u003c array.length; i++) {\n-      @SuppressWarnings(\"unchecked\")\n-      final Map\u003cString, Object\u003e m \u003d (Map\u003cString, Object\u003e)array[i];\n+      final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e)array[i];\n       statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n     }\n     return statuses;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    final Map\u003c?, ?\u003e json  \u003d run(op, f);\n    final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(HdfsFileStatus.class.getSimpleName() + \"es\");\n    final Object[] array \u003d (Object[])rootmap.get(HdfsFileStatus.class.getSimpleName());\n\n    //convert FileStatus\n    final FileStatus[] statuses \u003d new FileStatus[array.length];\n    for(int i \u003d 0; i \u003c array.length; i++) {\n      final Map\u003c?, ?\u003e m \u003d (Map\u003c?, ?\u003e)array[i];\n      statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n    }\n    return statuses;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "c46dbedaf94cb72e35e9e63d7f99e382ae9f0974": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2424. Added a root element \"HdfsFileStatuses\" for the response of webhdfs listStatus.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1183175 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/10/11 5:30 PM",
      "commitName": "c46dbedaf94cb72e35e9e63d7f99e382ae9f0974",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "09/10/11 6:49 PM",
      "commitNameOld": "676f488efffd50eb47e75cd750f9bc948b9e12fb",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 3.95,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,17 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n \n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n     final Map\u003c?, ?\u003e json  \u003d run(op, f);\n-    final Object[] array \u003d (Object[])json.get(\n-        HdfsFileStatus.class.getSimpleName());\n+    final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(HdfsFileStatus.class.getSimpleName() + \"es\");\n+    final Object[] array \u003d (Object[])rootmap.get(HdfsFileStatus.class.getSimpleName());\n \n     //convert FileStatus\n     final FileStatus[] statuses \u003d new FileStatus[array.length];\n     for(int i \u003d 0; i \u003c array.length; i++) {\n       @SuppressWarnings(\"unchecked\")\n       final Map\u003cString, Object\u003e m \u003d (Map\u003cString, Object\u003e)array[i];\n       statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n     }\n     return statuses;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    final Map\u003c?, ?\u003e json  \u003d run(op, f);\n    final Map\u003c?, ?\u003e rootmap \u003d (Map\u003c?, ?\u003e)json.get(HdfsFileStatus.class.getSimpleName() + \"es\");\n    final Object[] array \u003d (Object[])rootmap.get(HdfsFileStatus.class.getSimpleName());\n\n    //convert FileStatus\n    final FileStatus[] statuses \u003d new FileStatus[array.length];\n    for(int i \u003d 0; i \u003c array.length; i++) {\n      @SuppressWarnings(\"unchecked\")\n      final Map\u003cString, Object\u003e m \u003d (Map\u003cString, Object\u003e)array[i];\n      statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n    }\n    return statuses;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "676f488efffd50eb47e75cd750f9bc948b9e12fb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2404. webhdfs liststatus json response is not correct. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1180757 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/10/11 6:49 PM",
      "commitName": "676f488efffd50eb47e75cd750f9bc948b9e12fb",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "05/10/11 4:29 AM",
      "commitNameOld": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.6,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,17 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n \n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n     final Map\u003c?, ?\u003e json  \u003d run(op, f);\n     final Object[] array \u003d (Object[])json.get(\n-        HdfsFileStatus[].class.getSimpleName());\n+        HdfsFileStatus.class.getSimpleName());\n \n     //convert FileStatus\n     final FileStatus[] statuses \u003d new FileStatus[array.length];\n     for(int i \u003d 0; i \u003c array.length; i++) {\n       @SuppressWarnings(\"unchecked\")\n       final Map\u003cString, Object\u003e m \u003d (Map\u003cString, Object\u003e)array[i];\n-      statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m), f);\n+      statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n     }\n     return statuses;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    final Map\u003c?, ?\u003e json  \u003d run(op, f);\n    final Object[] array \u003d (Object[])json.get(\n        HdfsFileStatus.class.getSimpleName());\n\n    //convert FileStatus\n    final FileStatus[] statuses \u003d new FileStatus[array.length];\n    for(int i \u003d 0; i \u003c array.length; i++) {\n      @SuppressWarnings(\"unchecked\")\n      final Map\u003cString, Object\u003e m \u003d (Map\u003cString, Object\u003e)array[i];\n      statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m, false), f);\n    }\n    return statuses;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2395. Add a root element in the JSON responses of webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/11 4:29 AM",
      "commitName": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/09/11 9:49 PM",
      "commitNameOld": "dc8464f943b61b795df0cc8baec171bf07355763",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.28,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,17 @@\n   public FileStatus[] listStatus(final Path f) throws IOException {\n     statistics.incrementReadOps(1);\n \n     final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n-    final Object[] array \u003d run(op, f);\n+    final Map\u003c?, ?\u003e json  \u003d run(op, f);\n+    final Object[] array \u003d (Object[])json.get(\n+        HdfsFileStatus[].class.getSimpleName());\n \n     //convert FileStatus\n     final FileStatus[] statuses \u003d new FileStatus[array.length];\n     for(int i \u003d 0; i \u003c array.length; i++) {\n       @SuppressWarnings(\"unchecked\")\n       final Map\u003cString, Object\u003e m \u003d (Map\u003cString, Object\u003e)array[i];\n       statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m), f);\n     }\n     return statuses;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    final Map\u003c?, ?\u003e json  \u003d run(op, f);\n    final Object[] array \u003d (Object[])json.get(\n        HdfsFileStatus[].class.getSimpleName());\n\n    //convert FileStatus\n    final FileStatus[] statuses \u003d new FileStatus[array.length];\n    for(int i \u003d 0; i \u003c array.length; i++) {\n      @SuppressWarnings(\"unchecked\")\n      final Map\u003cString, Object\u003e m \u003d (Map\u003cString, Object\u003e)array[i];\n      statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m), f);\n    }\n    return statuses;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "6c3b59505b863f03629da52a1e9b886fe9b496d0": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2317. Support read access to HDFS in webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1170085 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/09/11 1:34 AM",
      "commitName": "6c3b59505b863f03629da52a1e9b886fe9b496d0",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,15 @@\n+  public FileStatus[] listStatus(final Path f) throws IOException {\n+    statistics.incrementReadOps(1);\n+\n+    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n+    final Object[] array \u003d run(op, f);\n+\n+    //convert FileStatus\n+    final FileStatus[] statuses \u003d new FileStatus[array.length];\n+    for(int i \u003d 0; i \u003c array.length; i++) {\n+      @SuppressWarnings(\"unchecked\")\n+      final Map\u003cString, Object\u003e m \u003d (Map\u003cString, Object\u003e)array[i];\n+      statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m), f);\n+    }\n+    return statuses;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public FileStatus[] listStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n\n    final HttpOpParam.Op op \u003d GetOpParam.Op.LISTSTATUS;\n    final Object[] array \u003d run(op, f);\n\n    //convert FileStatus\n    final FileStatus[] statuses \u003d new FileStatus[array.length];\n    for(int i \u003d 0; i \u003c array.length; i++) {\n      @SuppressWarnings(\"unchecked\")\n      final Map\u003cString, Object\u003e m \u003d (Map\u003cString, Object\u003e)array[i];\n      statuses[i] \u003d makeQualified(JsonUtil.toFileStatus(m), f);\n    }\n    return statuses;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java"
    }
  }
}