{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReaderLocal.java",
  "functionName": "skip",
  "functionId": "skip___n-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java",
  "functionStartLine": 609,
  "functionEndLine": 622,
  "numCommitsSeen": 47,
  "timeTaken": 3336,
  "changeHistory": [
    "f308561f1d885491b88db73ac63003202056d661",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "e2c9b288b223b9fd82dc12018936e13128413492",
    "124e507674c0d396f8494585e64226957199097b",
    "06d635cd2cc0254a107c8aa1f0076f194e7649d7",
    "05a73a3a1ea064b0d819d1851bce820e7d1f3f65",
    "bef65d0601a9334cdcfa97796755eb71350d50e3",
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
    "b8448dea82c72ff6c1558b9ebf3f24cd1c6e728b",
    "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04",
    "2ab10e29d9cca5018064be46a40e3c74423615a8"
  ],
  "changeHistoryShort": {
    "f308561f1d885491b88db73ac63003202056d661": "Yfilerename",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "e2c9b288b223b9fd82dc12018936e13128413492": "Yfilerename",
    "124e507674c0d396f8494585e64226957199097b": "Ybodychange",
    "06d635cd2cc0254a107c8aa1f0076f194e7649d7": "Ybodychange",
    "05a73a3a1ea064b0d819d1851bce820e7d1f3f65": "Ybodychange",
    "bef65d0601a9334cdcfa97796755eb71350d50e3": "Ybodychange",
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a": "Ybodychange",
    "b8448dea82c72ff6c1558b9ebf3f24cd1c6e728b": "Ybodychange",
    "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04": "Ybodychange",
    "2ab10e29d9cca5018064be46a40e3c74423615a8": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f308561f1d885491b88db73ac63003202056d661": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
      "commitDate": "25/04/16 12:01 PM",
      "commitName": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "25/04/16 9:38 AM",
      "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    int discardedFromBuf \u003d 0;\n    long remaining \u003d n;\n    if ((dataBuf !\u003d null) \u0026\u0026 dataBuf.hasRemaining()) {\n      discardedFromBuf \u003d (int)Math.min(dataBuf.remaining(), n);\n      dataBuf.position(dataBuf.position() + discardedFromBuf);\n      remaining -\u003d discardedFromBuf;\n    }\n    LOG.trace(\"skip(n\u003d{}, block\u003d{}, filename\u003d{}): discarded {} bytes from \"\n            + \"dataBuf and advanced dataPos by {}\",\n        n, block, filename, discardedFromBuf, remaining);\n    dataPos +\u003d remaining;\n    return n;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java"
      }
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,14 @@\n   public synchronized long skip(long n) throws IOException {\n     int discardedFromBuf \u003d 0;\n     long remaining \u003d n;\n     if ((dataBuf !\u003d null) \u0026\u0026 dataBuf.hasRemaining()) {\n       discardedFromBuf \u003d (int)Math.min(dataBuf.remaining(), n);\n       dataBuf.position(dataBuf.position() + discardedFromBuf);\n       remaining -\u003d discardedFromBuf;\n     }\n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"skip(n\u003d\" + n + \", block\u003d\" + block + \", filename\u003d\" + \n-        filename + \"): discarded \" + discardedFromBuf + \" bytes from \" +\n-        \"dataBuf and advanced dataPos by \" + remaining);\n-    }\n+    LOG.trace(\"skip(n\u003d{}, block\u003d{}, filename\u003d{}): discarded {} bytes from \"\n+            + \"dataBuf and advanced dataPos by {}\",\n+        n, block, filename, discardedFromBuf, remaining);\n     dataPos +\u003d remaining;\n     return n;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    int discardedFromBuf \u003d 0;\n    long remaining \u003d n;\n    if ((dataBuf !\u003d null) \u0026\u0026 dataBuf.hasRemaining()) {\n      discardedFromBuf \u003d (int)Math.min(dataBuf.remaining(), n);\n      dataBuf.position(dataBuf.position() + discardedFromBuf);\n      remaining -\u003d discardedFromBuf;\n    }\n    LOG.trace(\"skip(n\u003d{}, block\u003d{}, filename\u003d{}): discarded {} bytes from \"\n            + \"dataBuf and advanced dataPos by {}\",\n        n, block, filename, discardedFromBuf, remaining);\n    dataPos +\u003d remaining;\n    return n;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,16 @@\n   public synchronized long skip(long n) throws IOException {\n     int discardedFromBuf \u003d 0;\n     long remaining \u003d n;\n     if ((dataBuf !\u003d null) \u0026\u0026 dataBuf.hasRemaining()) {\n       discardedFromBuf \u003d (int)Math.min(dataBuf.remaining(), n);\n       dataBuf.position(dataBuf.position() + discardedFromBuf);\n       remaining -\u003d discardedFromBuf;\n     }\n-    LOG.trace(\"skip(n\u003d{}, block\u003d{}, filename\u003d{}): discarded {} bytes from \"\n-            + \"dataBuf and advanced dataPos by {}\",\n-        n, block, filename, discardedFromBuf, remaining);\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"skip(n\u003d\" + n + \", block\u003d\" + block + \", filename\u003d\" + \n+        filename + \"): discarded \" + discardedFromBuf + \" bytes from \" +\n+        \"dataBuf and advanced dataPos by \" + remaining);\n+    }\n     dataPos +\u003d remaining;\n     return n;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    int discardedFromBuf \u003d 0;\n    long remaining \u003d n;\n    if ((dataBuf !\u003d null) \u0026\u0026 dataBuf.hasRemaining()) {\n      discardedFromBuf \u003d (int)Math.min(dataBuf.remaining(), n);\n      dataBuf.position(dataBuf.position() + discardedFromBuf);\n      remaining -\u003d discardedFromBuf;\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"skip(n\u003d\" + n + \", block\u003d\" + block + \", filename\u003d\" + \n        filename + \"): discarded \" + discardedFromBuf + \" bytes from \" +\n        \"dataBuf and advanced dataPos by \" + remaining);\n    }\n    dataPos +\u003d remaining;\n    return n;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,14 @@\n   public synchronized long skip(long n) throws IOException {\n     int discardedFromBuf \u003d 0;\n     long remaining \u003d n;\n     if ((dataBuf !\u003d null) \u0026\u0026 dataBuf.hasRemaining()) {\n       discardedFromBuf \u003d (int)Math.min(dataBuf.remaining(), n);\n       dataBuf.position(dataBuf.position() + discardedFromBuf);\n       remaining -\u003d discardedFromBuf;\n     }\n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"skip(n\u003d\" + n + \", block\u003d\" + block + \", filename\u003d\" + \n-        filename + \"): discarded \" + discardedFromBuf + \" bytes from \" +\n-        \"dataBuf and advanced dataPos by \" + remaining);\n-    }\n+    LOG.trace(\"skip(n\u003d{}, block\u003d{}, filename\u003d{}): discarded {} bytes from \"\n+            + \"dataBuf and advanced dataPos by {}\",\n+        n, block, filename, discardedFromBuf, remaining);\n     dataPos +\u003d remaining;\n     return n;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    int discardedFromBuf \u003d 0;\n    long remaining \u003d n;\n    if ((dataBuf !\u003d null) \u0026\u0026 dataBuf.hasRemaining()) {\n      discardedFromBuf \u003d (int)Math.min(dataBuf.remaining(), n);\n      dataBuf.position(dataBuf.position() + discardedFromBuf);\n      remaining -\u003d discardedFromBuf;\n    }\n    LOG.trace(\"skip(n\u003d{}, block\u003d{}, filename\u003d{}): discarded {} bytes from \"\n            + \"dataBuf and advanced dataPos by {}\",\n        n, block, filename, discardedFromBuf, remaining);\n    dataPos +\u003d remaining;\n    return n;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "e2c9b288b223b9fd82dc12018936e13128413492": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "28/08/15 2:38 PM",
      "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/08/15 2:21 PM",
      "commitNameOld": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    int discardedFromBuf \u003d 0;\n    long remaining \u003d n;\n    if ((dataBuf !\u003d null) \u0026\u0026 dataBuf.hasRemaining()) {\n      discardedFromBuf \u003d (int)Math.min(dataBuf.remaining(), n);\n      dataBuf.position(dataBuf.position() + discardedFromBuf);\n      remaining -\u003d discardedFromBuf;\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"skip(n\u003d\" + n + \", block\u003d\" + block + \", filename\u003d\" + \n        filename + \"): discarded \" + discardedFromBuf + \" bytes from \" +\n        \"dataBuf and advanced dataPos by \" + remaining);\n    }\n    dataPos +\u003d remaining;\n    return n;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java"
      }
    },
    "124e507674c0d396f8494585e64226957199097b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5634. Allow BlockReaderLocal to switch between checksumming and not (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551701 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 12:57 PM",
      "commitName": "124e507674c0d396f8494585e64226957199097b",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "27/09/13 3:51 PM",
      "commitNameOld": "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 80.92,
      "commitsBetweenForRepo": 532,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,16 @@\n   public synchronized long skip(long n) throws IOException {\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"skip \" + n);\n+    int discardedFromBuf \u003d 0;\n+    long remaining \u003d n;\n+    if ((dataBuf !\u003d null) \u0026\u0026 dataBuf.hasRemaining()) {\n+      discardedFromBuf \u003d (int)Math.min(dataBuf.remaining(), n);\n+      dataBuf.position(dataBuf.position() + discardedFromBuf);\n+      remaining -\u003d discardedFromBuf;\n     }\n-    if (n \u003c\u003d 0) {\n-      return 0;\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"skip(n\u003d\" + n + \", block\u003d\" + block + \", filename\u003d\" + \n+        filename + \"): discarded \" + discardedFromBuf + \" bytes from \" +\n+        \"dataBuf and advanced dataPos by \" + remaining);\n     }\n-    if (!verifyChecksum) {\n-      return dataIn.skip(n);\n-    }\n-  \n-    // caller made sure newPosition is not beyond EOF.\n-    int remaining \u003d slowReadBuff.remaining();\n-    int position \u003d slowReadBuff.position();\n-    int newPosition \u003d position + (int)n;\n-  \n-    // if the new offset is already read into dataBuff, just reposition\n-    if (n \u003c\u003d remaining) {\n-      assert offsetFromChunkBoundary \u003d\u003d 0;\n-      slowReadBuff.position(newPosition);\n-      return n;\n-    }\n-  \n-    // for small gap, read through to keep the data/checksum in sync\n-    if (n - remaining \u003c\u003d bytesPerChecksum) {\n-      slowReadBuff.position(position + remaining);\n-      if (skipBuf \u003d\u003d null) {\n-        skipBuf \u003d new byte[bytesPerChecksum];\n-      }\n-      int ret \u003d read(skipBuf, 0, (int)(n - remaining));\n-      return ret;\n-    }\n-  \n-    // optimize for big gap: discard the current buffer, skip to\n-    // the beginning of the appropriate checksum chunk and then\n-    // read to the middle of that chunk to be in sync with checksums.\n-  \n-    // We can\u0027t use this.offsetFromChunkBoundary because we need to know how\n-    // many bytes of the offset were really read. Calling read(..) with a\n-    // positive this.offsetFromChunkBoundary causes that many bytes to get\n-    // silently skipped.\n-    int myOffsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n-    long toskip \u003d n - remaining - myOffsetFromChunkBoundary;\n-\n-    slowReadBuff.position(slowReadBuff.limit());\n-    checksumBuff.position(checksumBuff.limit());\n-  \n-    IOUtils.skipFully(dataIn, toskip);\n-    long checkSumOffset \u003d (toskip / bytesPerChecksum) * checksumSize;\n-    IOUtils.skipFully(checksumIn, checkSumOffset);\n-\n-    // read into the middle of the chunk\n-    if (skipBuf \u003d\u003d null) {\n-      skipBuf \u003d new byte[bytesPerChecksum];\n-    }\n-    assert skipBuf.length \u003d\u003d bytesPerChecksum;\n-    assert myOffsetFromChunkBoundary \u003c bytesPerChecksum;\n-\n-    int ret \u003d read(skipBuf, 0, myOffsetFromChunkBoundary);\n-\n-    if (ret \u003d\u003d -1) {  // EOS\n-      return toskip;\n-    } else {\n-      return (toskip + ret);\n-    }\n+    dataPos +\u003d remaining;\n+    return n;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    int discardedFromBuf \u003d 0;\n    long remaining \u003d n;\n    if ((dataBuf !\u003d null) \u0026\u0026 dataBuf.hasRemaining()) {\n      discardedFromBuf \u003d (int)Math.min(dataBuf.remaining(), n);\n      dataBuf.position(dataBuf.position() + discardedFromBuf);\n      remaining -\u003d discardedFromBuf;\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"skip(n\u003d\" + n + \", block\u003d\" + block + \", filename\u003d\" + \n        filename + \"): discarded \" + discardedFromBuf + \" bytes from \" +\n        \"dataBuf and advanced dataPos by \" + remaining);\n    }\n    dataPos +\u003d remaining;\n    return n;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "06d635cd2cc0254a107c8aa1f0076f194e7649d7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3492. fix some misuses of InputStream#skip. Contributed by Colin Patrick McCabe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1361449 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/07/12 5:18 PM",
      "commitName": "06d635cd2cc0254a107c8aa1f0076f194e7649d7",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "07/06/12 2:06 PM",
      "commitNameOld": "05a73a3a1ea064b0d819d1851bce820e7d1f3f65",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 36.13,
      "commitsBetweenForRepo": 190,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,66 @@\n   public synchronized long skip(long n) throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"skip \" + n);\n     }\n     if (n \u003c\u003d 0) {\n       return 0;\n     }\n     if (!verifyChecksum) {\n       return dataIn.skip(n);\n     }\n   \n     // caller made sure newPosition is not beyond EOF.\n     int remaining \u003d slowReadBuff.remaining();\n     int position \u003d slowReadBuff.position();\n     int newPosition \u003d position + (int)n;\n   \n     // if the new offset is already read into dataBuff, just reposition\n     if (n \u003c\u003d remaining) {\n       assert offsetFromChunkBoundary \u003d\u003d 0;\n       slowReadBuff.position(newPosition);\n       return n;\n     }\n   \n     // for small gap, read through to keep the data/checksum in sync\n     if (n - remaining \u003c\u003d bytesPerChecksum) {\n       slowReadBuff.position(position + remaining);\n       if (skipBuf \u003d\u003d null) {\n         skipBuf \u003d new byte[bytesPerChecksum];\n       }\n       int ret \u003d read(skipBuf, 0, (int)(n - remaining));\n       return ret;\n     }\n   \n     // optimize for big gap: discard the current buffer, skip to\n     // the beginning of the appropriate checksum chunk and then\n     // read to the middle of that chunk to be in sync with checksums.\n   \n     // We can\u0027t use this.offsetFromChunkBoundary because we need to know how\n     // many bytes of the offset were really read. Calling read(..) with a\n     // positive this.offsetFromChunkBoundary causes that many bytes to get\n     // silently skipped.\n     int myOffsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n     long toskip \u003d n - remaining - myOffsetFromChunkBoundary;\n \n     slowReadBuff.position(slowReadBuff.limit());\n     checksumBuff.position(checksumBuff.limit());\n   \n-    long dataSkipped \u003d dataIn.skip(toskip);\n-    if (dataSkipped !\u003d toskip) {\n-      throw new IOException(\"skip error in data input stream\");\n-    }\n-    long checkSumOffset \u003d (dataSkipped / bytesPerChecksum) * checksumSize;\n-    if (checkSumOffset \u003e 0) {\n-      long skipped \u003d checksumIn.skip(checkSumOffset);\n-      if (skipped !\u003d checkSumOffset) {\n-        throw new IOException(\"skip error in checksum input stream\");\n-      }\n-    }\n+    IOUtils.skipFully(dataIn, toskip);\n+    long checkSumOffset \u003d (toskip / bytesPerChecksum) * checksumSize;\n+    IOUtils.skipFully(checksumIn, checkSumOffset);\n \n     // read into the middle of the chunk\n     if (skipBuf \u003d\u003d null) {\n       skipBuf \u003d new byte[bytesPerChecksum];\n     }\n     assert skipBuf.length \u003d\u003d bytesPerChecksum;\n     assert myOffsetFromChunkBoundary \u003c bytesPerChecksum;\n \n     int ret \u003d read(skipBuf, 0, myOffsetFromChunkBoundary);\n \n     if (ret \u003d\u003d -1) {  // EOS\n       return toskip;\n     } else {\n       return (toskip + ret);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"skip \" + n);\n    }\n    if (n \u003c\u003d 0) {\n      return 0;\n    }\n    if (!verifyChecksum) {\n      return dataIn.skip(n);\n    }\n  \n    // caller made sure newPosition is not beyond EOF.\n    int remaining \u003d slowReadBuff.remaining();\n    int position \u003d slowReadBuff.position();\n    int newPosition \u003d position + (int)n;\n  \n    // if the new offset is already read into dataBuff, just reposition\n    if (n \u003c\u003d remaining) {\n      assert offsetFromChunkBoundary \u003d\u003d 0;\n      slowReadBuff.position(newPosition);\n      return n;\n    }\n  \n    // for small gap, read through to keep the data/checksum in sync\n    if (n - remaining \u003c\u003d bytesPerChecksum) {\n      slowReadBuff.position(position + remaining);\n      if (skipBuf \u003d\u003d null) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      int ret \u003d read(skipBuf, 0, (int)(n - remaining));\n      return ret;\n    }\n  \n    // optimize for big gap: discard the current buffer, skip to\n    // the beginning of the appropriate checksum chunk and then\n    // read to the middle of that chunk to be in sync with checksums.\n  \n    // We can\u0027t use this.offsetFromChunkBoundary because we need to know how\n    // many bytes of the offset were really read. Calling read(..) with a\n    // positive this.offsetFromChunkBoundary causes that many bytes to get\n    // silently skipped.\n    int myOffsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n    long toskip \u003d n - remaining - myOffsetFromChunkBoundary;\n\n    slowReadBuff.position(slowReadBuff.limit());\n    checksumBuff.position(checksumBuff.limit());\n  \n    IOUtils.skipFully(dataIn, toskip);\n    long checkSumOffset \u003d (toskip / bytesPerChecksum) * checksumSize;\n    IOUtils.skipFully(checksumIn, checkSumOffset);\n\n    // read into the middle of the chunk\n    if (skipBuf \u003d\u003d null) {\n      skipBuf \u003d new byte[bytesPerChecksum];\n    }\n    assert skipBuf.length \u003d\u003d bytesPerChecksum;\n    assert myOffsetFromChunkBoundary \u003c bytesPerChecksum;\n\n    int ret \u003d read(skipBuf, 0, myOffsetFromChunkBoundary);\n\n    if (ret \u003d\u003d -1) {  // EOS\n      return toskip;\n    } else {\n      return (toskip + ret);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "05a73a3a1ea064b0d819d1851bce820e7d1f3f65": {
      "type": "Ybodychange",
      "commitMessage": "Revert HDFS-3492 from r1347192: patch broke TestShortCircuitLocalRead\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1347796 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/06/12 2:06 PM",
      "commitName": "05a73a3a1ea064b0d819d1851bce820e7d1f3f65",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "06/06/12 3:48 PM",
      "commitNameOld": "bef65d0601a9334cdcfa97796755eb71350d50e3",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.93,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,74 @@\n   public synchronized long skip(long n) throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"skip \" + n);\n     }\n     if (n \u003c\u003d 0) {\n       return 0;\n     }\n     if (!verifyChecksum) {\n       return dataIn.skip(n);\n     }\n   \n     // caller made sure newPosition is not beyond EOF.\n     int remaining \u003d slowReadBuff.remaining();\n     int position \u003d slowReadBuff.position();\n     int newPosition \u003d position + (int)n;\n   \n     // if the new offset is already read into dataBuff, just reposition\n     if (n \u003c\u003d remaining) {\n       assert offsetFromChunkBoundary \u003d\u003d 0;\n       slowReadBuff.position(newPosition);\n       return n;\n     }\n   \n     // for small gap, read through to keep the data/checksum in sync\n     if (n - remaining \u003c\u003d bytesPerChecksum) {\n       slowReadBuff.position(position + remaining);\n       if (skipBuf \u003d\u003d null) {\n         skipBuf \u003d new byte[bytesPerChecksum];\n       }\n       int ret \u003d read(skipBuf, 0, (int)(n - remaining));\n       return ret;\n     }\n   \n     // optimize for big gap: discard the current buffer, skip to\n     // the beginning of the appropriate checksum chunk and then\n     // read to the middle of that chunk to be in sync with checksums.\n   \n     // We can\u0027t use this.offsetFromChunkBoundary because we need to know how\n     // many bytes of the offset were really read. Calling read(..) with a\n     // positive this.offsetFromChunkBoundary causes that many bytes to get\n     // silently skipped.\n     int myOffsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n     long toskip \u003d n - remaining - myOffsetFromChunkBoundary;\n \n     slowReadBuff.position(slowReadBuff.limit());\n     checksumBuff.position(checksumBuff.limit());\n   \n-    IOUtils.skipFully(dataIn, toskip);\n-    long checkSumOffset \u003d (toskip / bytesPerChecksum) * checksumSize;\n-    IOUtils.skipFully(checksumIn, checkSumOffset);\n+    long dataSkipped \u003d dataIn.skip(toskip);\n+    if (dataSkipped !\u003d toskip) {\n+      throw new IOException(\"skip error in data input stream\");\n+    }\n+    long checkSumOffset \u003d (dataSkipped / bytesPerChecksum) * checksumSize;\n+    if (checkSumOffset \u003e 0) {\n+      long skipped \u003d checksumIn.skip(checkSumOffset);\n+      if (skipped !\u003d checkSumOffset) {\n+        throw new IOException(\"skip error in checksum input stream\");\n+      }\n+    }\n \n     // read into the middle of the chunk\n     if (skipBuf \u003d\u003d null) {\n       skipBuf \u003d new byte[bytesPerChecksum];\n     }\n     assert skipBuf.length \u003d\u003d bytesPerChecksum;\n     assert myOffsetFromChunkBoundary \u003c bytesPerChecksum;\n \n     int ret \u003d read(skipBuf, 0, myOffsetFromChunkBoundary);\n \n     if (ret \u003d\u003d -1) {  // EOS\n       return toskip;\n     } else {\n       return (toskip + ret);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"skip \" + n);\n    }\n    if (n \u003c\u003d 0) {\n      return 0;\n    }\n    if (!verifyChecksum) {\n      return dataIn.skip(n);\n    }\n  \n    // caller made sure newPosition is not beyond EOF.\n    int remaining \u003d slowReadBuff.remaining();\n    int position \u003d slowReadBuff.position();\n    int newPosition \u003d position + (int)n;\n  \n    // if the new offset is already read into dataBuff, just reposition\n    if (n \u003c\u003d remaining) {\n      assert offsetFromChunkBoundary \u003d\u003d 0;\n      slowReadBuff.position(newPosition);\n      return n;\n    }\n  \n    // for small gap, read through to keep the data/checksum in sync\n    if (n - remaining \u003c\u003d bytesPerChecksum) {\n      slowReadBuff.position(position + remaining);\n      if (skipBuf \u003d\u003d null) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      int ret \u003d read(skipBuf, 0, (int)(n - remaining));\n      return ret;\n    }\n  \n    // optimize for big gap: discard the current buffer, skip to\n    // the beginning of the appropriate checksum chunk and then\n    // read to the middle of that chunk to be in sync with checksums.\n  \n    // We can\u0027t use this.offsetFromChunkBoundary because we need to know how\n    // many bytes of the offset were really read. Calling read(..) with a\n    // positive this.offsetFromChunkBoundary causes that many bytes to get\n    // silently skipped.\n    int myOffsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n    long toskip \u003d n - remaining - myOffsetFromChunkBoundary;\n\n    slowReadBuff.position(slowReadBuff.limit());\n    checksumBuff.position(checksumBuff.limit());\n  \n    long dataSkipped \u003d dataIn.skip(toskip);\n    if (dataSkipped !\u003d toskip) {\n      throw new IOException(\"skip error in data input stream\");\n    }\n    long checkSumOffset \u003d (dataSkipped / bytesPerChecksum) * checksumSize;\n    if (checkSumOffset \u003e 0) {\n      long skipped \u003d checksumIn.skip(checkSumOffset);\n      if (skipped !\u003d checkSumOffset) {\n        throw new IOException(\"skip error in checksum input stream\");\n      }\n    }\n\n    // read into the middle of the chunk\n    if (skipBuf \u003d\u003d null) {\n      skipBuf \u003d new byte[bytesPerChecksum];\n    }\n    assert skipBuf.length \u003d\u003d bytesPerChecksum;\n    assert myOffsetFromChunkBoundary \u003c bytesPerChecksum;\n\n    int ret \u003d read(skipBuf, 0, myOffsetFromChunkBoundary);\n\n    if (ret \u003d\u003d -1) {  // EOS\n      return toskip;\n    } else {\n      return (toskip + ret);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "bef65d0601a9334cdcfa97796755eb71350d50e3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3492. Fix some misuses of InputStream#skip. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1347192 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/06/12 3:48 PM",
      "commitName": "bef65d0601a9334cdcfa97796755eb71350d50e3",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "31/03/12 12:58 PM",
      "commitNameOld": "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 67.12,
      "commitsBetweenForRepo": 459,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,66 @@\n   public synchronized long skip(long n) throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"skip \" + n);\n     }\n     if (n \u003c\u003d 0) {\n       return 0;\n     }\n     if (!verifyChecksum) {\n       return dataIn.skip(n);\n     }\n   \n     // caller made sure newPosition is not beyond EOF.\n     int remaining \u003d slowReadBuff.remaining();\n     int position \u003d slowReadBuff.position();\n     int newPosition \u003d position + (int)n;\n   \n     // if the new offset is already read into dataBuff, just reposition\n     if (n \u003c\u003d remaining) {\n       assert offsetFromChunkBoundary \u003d\u003d 0;\n       slowReadBuff.position(newPosition);\n       return n;\n     }\n   \n     // for small gap, read through to keep the data/checksum in sync\n     if (n - remaining \u003c\u003d bytesPerChecksum) {\n       slowReadBuff.position(position + remaining);\n       if (skipBuf \u003d\u003d null) {\n         skipBuf \u003d new byte[bytesPerChecksum];\n       }\n       int ret \u003d read(skipBuf, 0, (int)(n - remaining));\n       return ret;\n     }\n   \n     // optimize for big gap: discard the current buffer, skip to\n     // the beginning of the appropriate checksum chunk and then\n     // read to the middle of that chunk to be in sync with checksums.\n   \n     // We can\u0027t use this.offsetFromChunkBoundary because we need to know how\n     // many bytes of the offset were really read. Calling read(..) with a\n     // positive this.offsetFromChunkBoundary causes that many bytes to get\n     // silently skipped.\n     int myOffsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n     long toskip \u003d n - remaining - myOffsetFromChunkBoundary;\n \n     slowReadBuff.position(slowReadBuff.limit());\n     checksumBuff.position(checksumBuff.limit());\n   \n-    long dataSkipped \u003d dataIn.skip(toskip);\n-    if (dataSkipped !\u003d toskip) {\n-      throw new IOException(\"skip error in data input stream\");\n-    }\n-    long checkSumOffset \u003d (dataSkipped / bytesPerChecksum) * checksumSize;\n-    if (checkSumOffset \u003e 0) {\n-      long skipped \u003d checksumIn.skip(checkSumOffset);\n-      if (skipped !\u003d checkSumOffset) {\n-        throw new IOException(\"skip error in checksum input stream\");\n-      }\n-    }\n+    IOUtils.skipFully(dataIn, toskip);\n+    long checkSumOffset \u003d (toskip / bytesPerChecksum) * checksumSize;\n+    IOUtils.skipFully(checksumIn, checkSumOffset);\n \n     // read into the middle of the chunk\n     if (skipBuf \u003d\u003d null) {\n       skipBuf \u003d new byte[bytesPerChecksum];\n     }\n     assert skipBuf.length \u003d\u003d bytesPerChecksum;\n     assert myOffsetFromChunkBoundary \u003c bytesPerChecksum;\n \n     int ret \u003d read(skipBuf, 0, myOffsetFromChunkBoundary);\n \n     if (ret \u003d\u003d -1) {  // EOS\n       return toskip;\n     } else {\n       return (toskip + ret);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"skip \" + n);\n    }\n    if (n \u003c\u003d 0) {\n      return 0;\n    }\n    if (!verifyChecksum) {\n      return dataIn.skip(n);\n    }\n  \n    // caller made sure newPosition is not beyond EOF.\n    int remaining \u003d slowReadBuff.remaining();\n    int position \u003d slowReadBuff.position();\n    int newPosition \u003d position + (int)n;\n  \n    // if the new offset is already read into dataBuff, just reposition\n    if (n \u003c\u003d remaining) {\n      assert offsetFromChunkBoundary \u003d\u003d 0;\n      slowReadBuff.position(newPosition);\n      return n;\n    }\n  \n    // for small gap, read through to keep the data/checksum in sync\n    if (n - remaining \u003c\u003d bytesPerChecksum) {\n      slowReadBuff.position(position + remaining);\n      if (skipBuf \u003d\u003d null) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      int ret \u003d read(skipBuf, 0, (int)(n - remaining));\n      return ret;\n    }\n  \n    // optimize for big gap: discard the current buffer, skip to\n    // the beginning of the appropriate checksum chunk and then\n    // read to the middle of that chunk to be in sync with checksums.\n  \n    // We can\u0027t use this.offsetFromChunkBoundary because we need to know how\n    // many bytes of the offset were really read. Calling read(..) with a\n    // positive this.offsetFromChunkBoundary causes that many bytes to get\n    // silently skipped.\n    int myOffsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n    long toskip \u003d n - remaining - myOffsetFromChunkBoundary;\n\n    slowReadBuff.position(slowReadBuff.limit());\n    checksumBuff.position(checksumBuff.limit());\n  \n    IOUtils.skipFully(dataIn, toskip);\n    long checkSumOffset \u003d (toskip / bytesPerChecksum) * checksumSize;\n    IOUtils.skipFully(checksumIn, checkSumOffset);\n\n    // read into the middle of the chunk\n    if (skipBuf \u003d\u003d null) {\n      skipBuf \u003d new byte[bytesPerChecksum];\n    }\n    assert skipBuf.length \u003d\u003d bytesPerChecksum;\n    assert myOffsetFromChunkBoundary \u003c bytesPerChecksum;\n\n    int ret \u003d read(skipBuf, 0, myOffsetFromChunkBoundary);\n\n    if (ret \u003d\u003d -1) {  // EOS\n      return toskip;\n    } else {\n      return (toskip + ret);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2834. Add a ByteBuffer-based read API to DFSInputStream. Contributed by Henry Robinson.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/03/12 10:30 AM",
      "commitName": "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "16/02/12 10:58 AM",
      "commitNameOld": "b8448dea82c72ff6c1558b9ebf3f24cd1c6e728b",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 33.94,
      "commitsBetweenForRepo": 229,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,74 @@\n   public synchronized long skip(long n) throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"skip \" + n);\n     }\n     if (n \u003c\u003d 0) {\n       return 0;\n     }\n     if (!verifyChecksum) {\n       return dataIn.skip(n);\n     }\n   \n     // caller made sure newPosition is not beyond EOF.\n-    int remaining \u003d dataBuff.remaining();\n-    int position \u003d dataBuff.position();\n+    int remaining \u003d slowReadBuff.remaining();\n+    int position \u003d slowReadBuff.position();\n     int newPosition \u003d position + (int)n;\n   \n     // if the new offset is already read into dataBuff, just reposition\n     if (n \u003c\u003d remaining) {\n       assert offsetFromChunkBoundary \u003d\u003d 0;\n-      dataBuff.position(newPosition);\n+      slowReadBuff.position(newPosition);\n       return n;\n     }\n   \n     // for small gap, read through to keep the data/checksum in sync\n     if (n - remaining \u003c\u003d bytesPerChecksum) {\n-      dataBuff.position(position + remaining);\n+      slowReadBuff.position(position + remaining);\n       if (skipBuf \u003d\u003d null) {\n         skipBuf \u003d new byte[bytesPerChecksum];\n       }\n       int ret \u003d read(skipBuf, 0, (int)(n - remaining));\n       return ret;\n     }\n   \n     // optimize for big gap: discard the current buffer, skip to\n     // the beginning of the appropriate checksum chunk and then\n     // read to the middle of that chunk to be in sync with checksums.\n-    this.offsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n-    long toskip \u003d n - remaining - this.offsetFromChunkBoundary;\n   \n-    dataBuff.clear();\n-    checksumBuff.clear();\n+    // We can\u0027t use this.offsetFromChunkBoundary because we need to know how\n+    // many bytes of the offset were really read. Calling read(..) with a\n+    // positive this.offsetFromChunkBoundary causes that many bytes to get\n+    // silently skipped.\n+    int myOffsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n+    long toskip \u003d n - remaining - myOffsetFromChunkBoundary;\n+\n+    slowReadBuff.position(slowReadBuff.limit());\n+    checksumBuff.position(checksumBuff.limit());\n   \n     long dataSkipped \u003d dataIn.skip(toskip);\n     if (dataSkipped !\u003d toskip) {\n       throw new IOException(\"skip error in data input stream\");\n     }\n     long checkSumOffset \u003d (dataSkipped / bytesPerChecksum) * checksumSize;\n     if (checkSumOffset \u003e 0) {\n       long skipped \u003d checksumIn.skip(checkSumOffset);\n       if (skipped !\u003d checkSumOffset) {\n         throw new IOException(\"skip error in checksum input stream\");\n       }\n     }\n \n     // read into the middle of the chunk\n     if (skipBuf \u003d\u003d null) {\n       skipBuf \u003d new byte[bytesPerChecksum];\n     }\n     assert skipBuf.length \u003d\u003d bytesPerChecksum;\n-    assert this.offsetFromChunkBoundary \u003c bytesPerChecksum;\n-    int ret \u003d read(skipBuf, 0, this.offsetFromChunkBoundary);\n+    assert myOffsetFromChunkBoundary \u003c bytesPerChecksum;\n+\n+    int ret \u003d read(skipBuf, 0, myOffsetFromChunkBoundary);\n+\n     if (ret \u003d\u003d -1) {  // EOS\n       return toskip;\n     } else {\n       return (toskip + ret);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"skip \" + n);\n    }\n    if (n \u003c\u003d 0) {\n      return 0;\n    }\n    if (!verifyChecksum) {\n      return dataIn.skip(n);\n    }\n  \n    // caller made sure newPosition is not beyond EOF.\n    int remaining \u003d slowReadBuff.remaining();\n    int position \u003d slowReadBuff.position();\n    int newPosition \u003d position + (int)n;\n  \n    // if the new offset is already read into dataBuff, just reposition\n    if (n \u003c\u003d remaining) {\n      assert offsetFromChunkBoundary \u003d\u003d 0;\n      slowReadBuff.position(newPosition);\n      return n;\n    }\n  \n    // for small gap, read through to keep the data/checksum in sync\n    if (n - remaining \u003c\u003d bytesPerChecksum) {\n      slowReadBuff.position(position + remaining);\n      if (skipBuf \u003d\u003d null) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      int ret \u003d read(skipBuf, 0, (int)(n - remaining));\n      return ret;\n    }\n  \n    // optimize for big gap: discard the current buffer, skip to\n    // the beginning of the appropriate checksum chunk and then\n    // read to the middle of that chunk to be in sync with checksums.\n  \n    // We can\u0027t use this.offsetFromChunkBoundary because we need to know how\n    // many bytes of the offset were really read. Calling read(..) with a\n    // positive this.offsetFromChunkBoundary causes that many bytes to get\n    // silently skipped.\n    int myOffsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n    long toskip \u003d n - remaining - myOffsetFromChunkBoundary;\n\n    slowReadBuff.position(slowReadBuff.limit());\n    checksumBuff.position(checksumBuff.limit());\n  \n    long dataSkipped \u003d dataIn.skip(toskip);\n    if (dataSkipped !\u003d toskip) {\n      throw new IOException(\"skip error in data input stream\");\n    }\n    long checkSumOffset \u003d (dataSkipped / bytesPerChecksum) * checksumSize;\n    if (checkSumOffset \u003e 0) {\n      long skipped \u003d checksumIn.skip(checkSumOffset);\n      if (skipped !\u003d checkSumOffset) {\n        throw new IOException(\"skip error in checksum input stream\");\n      }\n    }\n\n    // read into the middle of the chunk\n    if (skipBuf \u003d\u003d null) {\n      skipBuf \u003d new byte[bytesPerChecksum];\n    }\n    assert skipBuf.length \u003d\u003d bytesPerChecksum;\n    assert myOffsetFromChunkBoundary \u003c bytesPerChecksum;\n\n    int ret \u003d read(skipBuf, 0, myOffsetFromChunkBoundary);\n\n    if (ret \u003d\u003d -1) {  // EOS\n      return toskip;\n    } else {\n      return (toskip + ret);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "b8448dea82c72ff6c1558b9ebf3f24cd1c6e728b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2655. BlockReaderLocal#skip performs unnecessary IO. Contributed by Brandon Li.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1245118 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/02/12 10:58 AM",
      "commitName": "b8448dea82c72ff6c1558b9ebf3f24cd1c6e728b",
      "commitAuthor": "Jitendra Nath Pandey",
      "commitDateOld": "31/01/12 11:46 PM",
      "commitNameOld": "dbbfaebb71eb9d69d67fd5becd2e357397d0f68b",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 15.47,
      "commitsBetweenForRepo": 126,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,67 @@\n   public synchronized long skip(long n) throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"skip \" + n);\n     }\n+    if (n \u003c\u003d 0) {\n+      return 0;\n+    }\n     if (!verifyChecksum) {\n       return dataIn.skip(n);\n     }\n-    // Skip by reading the data so we stay in sync with checksums.\n-    // This could be implemented more efficiently in the future to\n-    // skip to the beginning of the appropriate checksum chunk\n-    // and then only read to the middle of that chunk.\n+  \n+    // caller made sure newPosition is not beyond EOF.\n+    int remaining \u003d dataBuff.remaining();\n+    int position \u003d dataBuff.position();\n+    int newPosition \u003d position + (int)n;\n+  \n+    // if the new offset is already read into dataBuff, just reposition\n+    if (n \u003c\u003d remaining) {\n+      assert offsetFromChunkBoundary \u003d\u003d 0;\n+      dataBuff.position(newPosition);\n+      return n;\n+    }\n+  \n+    // for small gap, read through to keep the data/checksum in sync\n+    if (n - remaining \u003c\u003d bytesPerChecksum) {\n+      dataBuff.position(position + remaining);\n+      if (skipBuf \u003d\u003d null) {\n+        skipBuf \u003d new byte[bytesPerChecksum];\n+      }\n+      int ret \u003d read(skipBuf, 0, (int)(n - remaining));\n+      return ret;\n+    }\n+  \n+    // optimize for big gap: discard the current buffer, skip to\n+    // the beginning of the appropriate checksum chunk and then\n+    // read to the middle of that chunk to be in sync with checksums.\n+    this.offsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n+    long toskip \u003d n - remaining - this.offsetFromChunkBoundary;\n+  \n+    dataBuff.clear();\n+    checksumBuff.clear();\n+  \n+    long dataSkipped \u003d dataIn.skip(toskip);\n+    if (dataSkipped !\u003d toskip) {\n+      throw new IOException(\"skip error in data input stream\");\n+    }\n+    long checkSumOffset \u003d (dataSkipped / bytesPerChecksum) * checksumSize;\n+    if (checkSumOffset \u003e 0) {\n+      long skipped \u003d checksumIn.skip(checkSumOffset);\n+      if (skipped !\u003d checkSumOffset) {\n+        throw new IOException(\"skip error in checksum input stream\");\n+      }\n+    }\n+\n+    // read into the middle of the chunk\n     if (skipBuf \u003d\u003d null) {\n       skipBuf \u003d new byte[bytesPerChecksum];\n     }\n-    long nSkipped \u003d 0;\n-    while ( nSkipped \u003c n ) {\n-      int toSkip \u003d (int)Math.min(n-nSkipped, skipBuf.length);\n-      int ret \u003d read(skipBuf, 0, toSkip);\n-      if ( ret \u003c\u003d 0 ) {\n-        return nSkipped;\n-      }\n-      nSkipped +\u003d ret;\n+    assert skipBuf.length \u003d\u003d bytesPerChecksum;\n+    assert this.offsetFromChunkBoundary \u003c bytesPerChecksum;\n+    int ret \u003d read(skipBuf, 0, this.offsetFromChunkBoundary);\n+    if (ret \u003d\u003d -1) {  // EOS\n+      return toskip;\n+    } else {\n+      return (toskip + ret);\n     }\n-    return nSkipped;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"skip \" + n);\n    }\n    if (n \u003c\u003d 0) {\n      return 0;\n    }\n    if (!verifyChecksum) {\n      return dataIn.skip(n);\n    }\n  \n    // caller made sure newPosition is not beyond EOF.\n    int remaining \u003d dataBuff.remaining();\n    int position \u003d dataBuff.position();\n    int newPosition \u003d position + (int)n;\n  \n    // if the new offset is already read into dataBuff, just reposition\n    if (n \u003c\u003d remaining) {\n      assert offsetFromChunkBoundary \u003d\u003d 0;\n      dataBuff.position(newPosition);\n      return n;\n    }\n  \n    // for small gap, read through to keep the data/checksum in sync\n    if (n - remaining \u003c\u003d bytesPerChecksum) {\n      dataBuff.position(position + remaining);\n      if (skipBuf \u003d\u003d null) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      int ret \u003d read(skipBuf, 0, (int)(n - remaining));\n      return ret;\n    }\n  \n    // optimize for big gap: discard the current buffer, skip to\n    // the beginning of the appropriate checksum chunk and then\n    // read to the middle of that chunk to be in sync with checksums.\n    this.offsetFromChunkBoundary \u003d newPosition % bytesPerChecksum;\n    long toskip \u003d n - remaining - this.offsetFromChunkBoundary;\n  \n    dataBuff.clear();\n    checksumBuff.clear();\n  \n    long dataSkipped \u003d dataIn.skip(toskip);\n    if (dataSkipped !\u003d toskip) {\n      throw new IOException(\"skip error in data input stream\");\n    }\n    long checkSumOffset \u003d (dataSkipped / bytesPerChecksum) * checksumSize;\n    if (checkSumOffset \u003e 0) {\n      long skipped \u003d checksumIn.skip(checkSumOffset);\n      if (skipped !\u003d checkSumOffset) {\n        throw new IOException(\"skip error in checksum input stream\");\n      }\n    }\n\n    // read into the middle of the chunk\n    if (skipBuf \u003d\u003d null) {\n      skipBuf \u003d new byte[bytesPerChecksum];\n    }\n    assert skipBuf.length \u003d\u003d bytesPerChecksum;\n    assert this.offsetFromChunkBoundary \u003c bytesPerChecksum;\n    int ret \u003d read(skipBuf, 0, this.offsetFromChunkBoundary);\n    if (ret \u003d\u003d -1) {  // EOS\n      return toskip;\n    } else {\n      return (toskip + ret);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2654. Make BlockReaderLocal not extend RemoteBlockReader2. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213592 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 12:09 AM",
      "commitName": "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "21/11/11 6:57 PM",
      "commitNameOld": "2ab10e29d9cca5018064be46a40e3c74423615a8",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 21.22,
      "commitsBetweenForRepo": 111,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,25 @@\n   public synchronized long skip(long n) throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"skip \" + n);\n     }\n     if (!verifyChecksum) {\n       return dataIn.skip(n);\n-    } else {\n-     return super.skip(n);\n     }\n+    // Skip by reading the data so we stay in sync with checksums.\n+    // This could be implemented more efficiently in the future to\n+    // skip to the beginning of the appropriate checksum chunk\n+    // and then only read to the middle of that chunk.\n+    if (skipBuf \u003d\u003d null) {\n+      skipBuf \u003d new byte[bytesPerChecksum];\n+    }\n+    long nSkipped \u003d 0;\n+    while ( nSkipped \u003c n ) {\n+      int toSkip \u003d (int)Math.min(n-nSkipped, skipBuf.length);\n+      int ret \u003d read(skipBuf, 0, toSkip);\n+      if ( ret \u003c\u003d 0 ) {\n+        return nSkipped;\n+      }\n+      nSkipped +\u003d ret;\n+    }\n+    return nSkipped;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"skip \" + n);\n    }\n    if (!verifyChecksum) {\n      return dataIn.skip(n);\n    }\n    // Skip by reading the data so we stay in sync with checksums.\n    // This could be implemented more efficiently in the future to\n    // skip to the beginning of the appropriate checksum chunk\n    // and then only read to the middle of that chunk.\n    if (skipBuf \u003d\u003d null) {\n      skipBuf \u003d new byte[bytesPerChecksum];\n    }\n    long nSkipped \u003d 0;\n    while ( nSkipped \u003c n ) {\n      int toSkip \u003d (int)Math.min(n-nSkipped, skipBuf.length);\n      int ret \u003d read(skipBuf, 0, toSkip);\n      if ( ret \u003c\u003d 0 ) {\n        return nSkipped;\n      }\n      nSkipped +\u003d ret;\n    }\n    return nSkipped;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "2ab10e29d9cca5018064be46a40e3c74423615a8": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2246. Enable reading a block directly from local file system for a client on the same node as the block file.  Contributed by Andrew Purtell, Suresh Srinivas and Jitendra Nath Pandey\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1204792 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/11 6:57 PM",
      "commitName": "2ab10e29d9cca5018064be46a40e3c74423615a8",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,10 @@\n+  public synchronized long skip(long n) throws IOException {\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"skip \" + n);\n+    }\n+    if (!verifyChecksum) {\n+      return dataIn.skip(n);\n+    } else {\n+     return super.skip(n);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized long skip(long n) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"skip \" + n);\n    }\n    if (!verifyChecksum) {\n      return dataIn.skip(n);\n    } else {\n     return super.skip(n);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java"
    }
  }
}