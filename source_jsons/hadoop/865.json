{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSOutputStream.java",
  "functionName": "writeChunk",
  "functionId": "writeChunk___b-byte[]__offset-int__len-int__checksum-byte[]__ckoff-int__cklen-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
  "functionStartLine": 428,
  "functionEndLine": 442,
  "numCommitsSeen": 212,
  "timeTaken": 10469,
  "changeHistory": [
    "b5af9be72c72734d668f817c99d889031922a951",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "1c13519e1e7588c3e2974138d37bf3449ca8b3df",
    "efc510a570cf880e7df1b69932aa41932658ee51",
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a",
    "c94d594a57806dec515e2a2053a1221f8ce48cc4",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
    "952640fa4cbdc23fe8781e5627c2e8eab565c535",
    "36ccf097a95eae0761de7b657752e4808a86c094",
    "463aec11718e47d4aabb86a7a539cb973460aae6",
    "ab638e77b811d9592470f7d342cd11a66efbbf0d",
    "1c309f763be3dd2e3d7d1616d2c960ff80cf9b03",
    "735046ebecd9e803398be56fbf79dbde5226b4c1",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
    "83cf475050dba27e72b4e399491638c670621175",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "b5af9be72c72734d668f817c99d889031922a951": "Ybodychange",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "1c13519e1e7588c3e2974138d37bf3449ca8b3df": "Ybodychange",
    "efc510a570cf880e7df1b69932aa41932658ee51": "Ybodychange",
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a": "Ybodychange",
    "c94d594a57806dec515e2a2053a1221f8ce48cc4": "Ybodychange",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": "Ybodychange",
    "952640fa4cbdc23fe8781e5627c2e8eab565c535": "Ybodychange",
    "36ccf097a95eae0761de7b657752e4808a86c094": "Ybodychange",
    "463aec11718e47d4aabb86a7a539cb973460aae6": "Ybodychange",
    "ab638e77b811d9592470f7d342cd11a66efbbf0d": "Ymultichange(Yparameterchange,Ybodychange)",
    "1c309f763be3dd2e3d7d1616d2c960ff80cf9b03": "Ybodychange",
    "735046ebecd9e803398be56fbf79dbde5226b4c1": "Ybodychange",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": "Ybodychange",
    "83cf475050dba27e72b4e399491638c670621175": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b5af9be72c72734d668f817c99d889031922a951": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8668. Erasure Coding: revisit buffer used for encoding and decoding. Contributed by Sammi Chen\n",
      "commitDate": "12/08/16 10:52 PM",
      "commitName": "b5af9be72c72734d668f817c99d889031922a951",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "27/04/16 2:22 PM",
      "commitNameOld": "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 107.35,
      "commitsBetweenForRepo": 844,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,15 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n-    dfsClient.checkOpen();\n-    checkClosed();\n-\n-    if (len \u003e bytesPerChecksum) {\n-      throw new IOException(\"writeChunk() buffer size is \" + len +\n-                            \" is larger than supported  bytesPerChecksum \" +\n-                            bytesPerChecksum);\n-    }\n-    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n-      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n-                            getChecksumSize() + \" but found to be \" + cklen);\n-    }\n-\n-    if (currentPacket \u003d\u003d null) {\n-      currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n-          .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n-      DFSClient.LOG.debug(\"WriteChunk allocating new packet seqno\u003d{},\"\n-              + \" src\u003d{}, packetSize\u003d{}, chunksPerPacket\u003d{}, bytesCurBlock\u003d{}\",\n-          currentPacket.getSeqno(), src, packetSize, chunksPerPacket,\n-          getStreamer().getBytesCurBlock() + \", \" + this);\n-    }\n+    writeChunkPrepare(len, ckoff, cklen);\n \n     currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.incNumChunks();\n     getStreamer().incBytesCurBlock(len);\n \n     // If packet is full, enqueue it for transmission\n     if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n         getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n       enqueueCurrentPacketFull();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    writeChunkPrepare(len, ckoff, cklen);\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.incNumChunks();\n    getStreamer().incBytesCurBlock(len);\n\n    // If packet is full, enqueue it for transmission\n    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n        getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n      enqueueCurrentPacketFull();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,36 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n     dfsClient.checkOpen();\n     checkClosed();\n \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             getChecksumSize() + \" but found to be \" + cklen);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n           .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n-            currentPacket.getSeqno() +\n-            \", src\u003d\" + src +\n-            \", packetSize\u003d\" + packetSize +\n-            \", chunksPerPacket\u003d\" + chunksPerPacket +\n-            \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock());\n-      }\n+      DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d{},\"\n+              + \" src\u003d{}, packetSize\u003d{}, chunksPerPacket\u003d{}, bytesCurBlock\u003d{}\",\n+          currentPacket.getSeqno(), src, packetSize, chunksPerPacket,\n+          getStreamer().getBytesCurBlock());\n     }\n \n     currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.incNumChunks();\n     getStreamer().incBytesCurBlock(len);\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n         getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n       enqueueCurrentPacketFull();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n          .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n      DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d{},\"\n              + \" src\u003d{}, packetSize\u003d{}, chunksPerPacket\u003d{}, bytesCurBlock\u003d{}\",\n          currentPacket.getSeqno(), src, packetSize, chunksPerPacket,\n          getStreamer().getBytesCurBlock());\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.incNumChunks();\n    getStreamer().incBytesCurBlock(len);\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n        getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n      enqueueCurrentPacketFull();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,40 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n     dfsClient.checkOpen();\n     checkClosed();\n \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             getChecksumSize() + \" but found to be \" + cklen);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n           .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n-      DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d{},\"\n-              + \" src\u003d{}, packetSize\u003d{}, chunksPerPacket\u003d{}, bytesCurBlock\u003d{}\",\n-          currentPacket.getSeqno(), src, packetSize, chunksPerPacket,\n-          getStreamer().getBytesCurBlock());\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n+            currentPacket.getSeqno() +\n+            \", src\u003d\" + src +\n+            \", packetSize\u003d\" + packetSize +\n+            \", chunksPerPacket\u003d\" + chunksPerPacket +\n+            \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock());\n+      }\n     }\n \n     currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.incNumChunks();\n     getStreamer().incBytesCurBlock(len);\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n         getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n       enqueueCurrentPacketFull();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n          .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.getSeqno() +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock());\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.incNumChunks();\n    getStreamer().incBytesCurBlock(len);\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n        getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n      enqueueCurrentPacketFull();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,36 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n     dfsClient.checkOpen();\n     checkClosed();\n \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             getChecksumSize() + \" but found to be \" + cklen);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n           .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n-            currentPacket.getSeqno() +\n-            \", src\u003d\" + src +\n-            \", packetSize\u003d\" + packetSize +\n-            \", chunksPerPacket\u003d\" + chunksPerPacket +\n-            \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock());\n-      }\n+      DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d{},\"\n+              + \" src\u003d{}, packetSize\u003d{}, chunksPerPacket\u003d{}, bytesCurBlock\u003d{}\",\n+          currentPacket.getSeqno(), src, packetSize, chunksPerPacket,\n+          getStreamer().getBytesCurBlock());\n     }\n \n     currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.incNumChunks();\n     getStreamer().incBytesCurBlock(len);\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n         getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n       enqueueCurrentPacketFull();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n          .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n      DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d{},\"\n              + \" src\u003d{}, packetSize\u003d{}, chunksPerPacket\u003d{}, bytesCurBlock\u003d{}\",\n          currentPacket.getSeqno(), src, packetSize, chunksPerPacket,\n          getStreamer().getBytesCurBlock());\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.incNumChunks();\n    getStreamer().incBytesCurBlock(len);\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n        getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n      enqueueCurrentPacketFull();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n          .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.getSeqno() +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock());\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.incNumChunks();\n    getStreamer().incBytesCurBlock(len);\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n        getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n      enqueueCurrentPacketFull();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "1c13519e1e7588c3e2974138d37bf3449ca8b3df": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8605. Merge Refactor of DFSOutputStream from HDFS-7285 branch. (vinayakumarb)\n",
      "commitDate": "18/06/15 8:48 AM",
      "commitName": "1c13519e1e7588c3e2974138d37bf3449ca8b3df",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "16/06/15 6:08 PM",
      "commitNameOld": "d4929f448f95815af99100780a08b172e0262c17",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 1.61,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,40 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n     dfsClient.checkOpen();\n     checkClosed();\n \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             getChecksumSize() + \" but found to be \" + cklen);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n           .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.getSeqno() +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock());\n       }\n     }\n \n     currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.incNumChunks();\n     getStreamer().incBytesCurBlock(len);\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n         getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n-            currentPacket.getSeqno() +\n-            \", src\u003d\" + src +\n-            \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock() +\n-            \", blockSize\u003d\" + blockSize +\n-            \", appendChunk\u003d\" + getStreamer().getAppendChunk());\n-      }\n-      getStreamer().waitAndQueuePacket(currentPacket);\n-      currentPacket \u003d null;\n-\n-      adjustChunkBoundary();\n-\n-      endBlock();\n+      enqueueCurrentPacketFull();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n          .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.getSeqno() +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock());\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.incNumChunks();\n    getStreamer().incBytesCurBlock(len);\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n        getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n      enqueueCurrentPacketFull();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "efc510a570cf880e7df1b69932aa41932658ee51": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8386. Improve synchronization of \u0027streamer\u0027 reference in DFSOutputStream. Contributed by Rakesh R.\n",
      "commitDate": "02/06/15 3:39 PM",
      "commitName": "efc510a570cf880e7df1b69932aa41932658ee51",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/04/15 7:27 PM",
      "commitNameOld": "98a61766286321468bf801a9f17a843d7eae8d9e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 32.84,
      "commitsBetweenForRepo": 337,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,53 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n     dfsClient.checkOpen();\n     checkClosed();\n \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             getChecksumSize() + \" but found to be \" + cklen);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n-      currentPacket \u003d createPacket(packetSize, chunksPerPacket, \n-          streamer.getBytesCurBlock(), streamer.getAndIncCurrentSeqno(), false);\n+      currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n+          .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.getSeqno() +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n-            \", bytesCurBlock\u003d\" + streamer.getBytesCurBlock());\n+            \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock());\n       }\n     }\n \n     currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.incNumChunks();\n-    streamer.incBytesCurBlock(len);\n+    getStreamer().incBytesCurBlock(len);\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n-        streamer.getBytesCurBlock() \u003d\u003d blockSize) {\n+        getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n             currentPacket.getSeqno() +\n             \", src\u003d\" + src +\n-            \", bytesCurBlock\u003d\" + streamer.getBytesCurBlock() +\n+            \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock() +\n             \", blockSize\u003d\" + blockSize +\n-            \", appendChunk\u003d\" + streamer.getAppendChunk());\n+            \", appendChunk\u003d\" + getStreamer().getAppendChunk());\n       }\n-      streamer.waitAndQueuePacket(currentPacket);\n+      getStreamer().waitAndQueuePacket(currentPacket);\n       currentPacket \u003d null;\n \n       adjustChunkBoundary();\n \n       endBlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d createPacket(packetSize, chunksPerPacket, getStreamer()\n          .getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.getSeqno() +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock());\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.incNumChunks();\n    getStreamer().incBytesCurBlock(len);\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n        getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.getSeqno() +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + getStreamer().getBytesCurBlock() +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + getStreamer().getAppendChunk());\n      }\n      getStreamer().waitAndQueuePacket(currentPacket);\n      currentPacket \u003d null;\n\n      adjustChunkBoundary();\n\n      endBlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7888. Change DFSOutputStream and DataStreamer for convenience of subclassing. Contributed by Li Bo\n",
      "commitDate": "02/04/15 10:59 AM",
      "commitName": "9ed43f2189fb4674b7379e8e995d53d4970d5c3a",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "01/04/15 2:10 PM",
      "commitNameOld": "c94d594a57806dec515e2a2053a1221f8ce48cc4",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.87,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,53 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n     dfsClient.checkOpen();\n     checkClosed();\n \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             getChecksumSize() + \" but found to be \" + cklen);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d createPacket(packetSize, chunksPerPacket, \n           streamer.getBytesCurBlock(), streamer.getAndIncCurrentSeqno(), false);\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.getSeqno() +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + streamer.getBytesCurBlock());\n       }\n     }\n \n     currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.incNumChunks();\n     streamer.incBytesCurBlock(len);\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n         streamer.getBytesCurBlock() \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n             currentPacket.getSeqno() +\n             \", src\u003d\" + src +\n             \", bytesCurBlock\u003d\" + streamer.getBytesCurBlock() +\n             \", blockSize\u003d\" + blockSize +\n             \", appendChunk\u003d\" + streamer.getAppendChunk());\n       }\n       streamer.waitAndQueuePacket(currentPacket);\n       currentPacket \u003d null;\n \n-      // If the reopened file did not end at chunk boundary and the above\n-      // write filled up its partial chunk. Tell the summer to generate full \n-      // crc chunks from now on.\n-      if (streamer.getAppendChunk() \u0026\u0026\n-          streamer.getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n-        streamer.setAppendChunk(false);\n-        resetChecksumBufSize();\n-      }\n+      adjustChunkBoundary();\n \n-      if (!streamer.getAppendChunk()) {\n-        int psize \u003d Math.min((int)(blockSize-streamer.getBytesCurBlock()),\n-            dfsClient.getConf().writePacketSize);\n-        computePacketChunkSize(psize, bytesPerChecksum);\n-      }\n-      //\n-      // if encountering a block boundary, send an empty packet to \n-      // indicate the end of block and reset bytesCurBlock.\n-      //\n-      if (streamer.getBytesCurBlock() \u003d\u003d blockSize) {\n-        currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n-            streamer.getAndIncCurrentSeqno(), true);\n-        currentPacket.setSyncBlock(shouldSyncBlock);\n-        streamer.waitAndQueuePacket(currentPacket);\n-        currentPacket \u003d null;\n-        streamer.setBytesCurBlock(0);\n-        lastFlushOffset \u003d 0;\n-      }\n+      endBlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d createPacket(packetSize, chunksPerPacket, \n          streamer.getBytesCurBlock(), streamer.getAndIncCurrentSeqno(), false);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.getSeqno() +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + streamer.getBytesCurBlock());\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.incNumChunks();\n    streamer.incBytesCurBlock(len);\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n        streamer.getBytesCurBlock() \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.getSeqno() +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + streamer.getBytesCurBlock() +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + streamer.getAppendChunk());\n      }\n      streamer.waitAndQueuePacket(currentPacket);\n      currentPacket \u003d null;\n\n      adjustChunkBoundary();\n\n      endBlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "c94d594a57806dec515e2a2053a1221f8ce48cc4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8026. Trace FSOutputSummer#writeChecksumChunks rather than DFSOutputStream#writeChunk (cmccabe)\n",
      "commitDate": "01/04/15 2:10 PM",
      "commitName": "c94d594a57806dec515e2a2053a1221f8ce48cc4",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/03/15 1:21 PM",
      "commitNameOld": "61df1b27a797efd094328c7d9141b9e157e01bf4",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 6.03,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,77 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n-    TraceScope scope \u003d\n-        dfsClient.getPathTraceScope(\"DFSOutputStream#writeChunk\", src);\n-    try {\n-      writeChunkImpl(b, offset, len, checksum, ckoff, cklen);\n-    } finally {\n-      scope.close();\n+    dfsClient.checkOpen();\n+    checkClosed();\n+\n+    if (len \u003e bytesPerChecksum) {\n+      throw new IOException(\"writeChunk() buffer size is \" + len +\n+                            \" is larger than supported  bytesPerChecksum \" +\n+                            bytesPerChecksum);\n+    }\n+    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n+      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n+                            getChecksumSize() + \" but found to be \" + cklen);\n+    }\n+\n+    if (currentPacket \u003d\u003d null) {\n+      currentPacket \u003d createPacket(packetSize, chunksPerPacket, \n+          streamer.getBytesCurBlock(), streamer.getAndIncCurrentSeqno(), false);\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n+            currentPacket.getSeqno() +\n+            \", src\u003d\" + src +\n+            \", packetSize\u003d\" + packetSize +\n+            \", chunksPerPacket\u003d\" + chunksPerPacket +\n+            \", bytesCurBlock\u003d\" + streamer.getBytesCurBlock());\n+      }\n+    }\n+\n+    currentPacket.writeChecksum(checksum, ckoff, cklen);\n+    currentPacket.writeData(b, offset, len);\n+    currentPacket.incNumChunks();\n+    streamer.incBytesCurBlock(len);\n+\n+    // If packet is full, enqueue it for transmission\n+    //\n+    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n+        streamer.getBytesCurBlock() \u003d\u003d blockSize) {\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n+            currentPacket.getSeqno() +\n+            \", src\u003d\" + src +\n+            \", bytesCurBlock\u003d\" + streamer.getBytesCurBlock() +\n+            \", blockSize\u003d\" + blockSize +\n+            \", appendChunk\u003d\" + streamer.getAppendChunk());\n+      }\n+      streamer.waitAndQueuePacket(currentPacket);\n+      currentPacket \u003d null;\n+\n+      // If the reopened file did not end at chunk boundary and the above\n+      // write filled up its partial chunk. Tell the summer to generate full \n+      // crc chunks from now on.\n+      if (streamer.getAppendChunk() \u0026\u0026\n+          streamer.getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n+        streamer.setAppendChunk(false);\n+        resetChecksumBufSize();\n+      }\n+\n+      if (!streamer.getAppendChunk()) {\n+        int psize \u003d Math.min((int)(blockSize-streamer.getBytesCurBlock()),\n+            dfsClient.getConf().writePacketSize);\n+        computePacketChunkSize(psize, bytesPerChecksum);\n+      }\n+      //\n+      // if encountering a block boundary, send an empty packet to \n+      // indicate the end of block and reset bytesCurBlock.\n+      //\n+      if (streamer.getBytesCurBlock() \u003d\u003d blockSize) {\n+        currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n+            streamer.getAndIncCurrentSeqno(), true);\n+        currentPacket.setSyncBlock(shouldSyncBlock);\n+        streamer.waitAndQueuePacket(currentPacket);\n+        currentPacket \u003d null;\n+        streamer.setBytesCurBlock(0);\n+        lastFlushOffset \u003d 0;\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d createPacket(packetSize, chunksPerPacket, \n          streamer.getBytesCurBlock(), streamer.getAndIncCurrentSeqno(), false);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.getSeqno() +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + streamer.getBytesCurBlock());\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.incNumChunks();\n    streamer.incBytesCurBlock(len);\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n        streamer.getBytesCurBlock() \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.getSeqno() +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + streamer.getBytesCurBlock() +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + streamer.getAppendChunk());\n      }\n      streamer.waitAndQueuePacket(currentPacket);\n      currentPacket \u003d null;\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (streamer.getAppendChunk() \u0026\u0026\n          streamer.getBytesCurBlock() % bytesPerChecksum \u003d\u003d 0) {\n        streamer.setAppendChunk(false);\n        resetChecksumBufSize();\n      }\n\n      if (!streamer.getAppendChunk()) {\n        int psize \u003d Math.min((int)(blockSize-streamer.getBytesCurBlock()),\n            dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (streamer.getBytesCurBlock() \u003d\u003d blockSize) {\n        currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n            streamer.getAndIncCurrentSeqno(), true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n        streamer.waitAndQueuePacket(currentPacket);\n        currentPacket \u003d null;\n        streamer.setBytesCurBlock(0);\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7054. Make DFSOutputStream tracing more fine-grained (cmccabe)\n",
      "commitDate": "18/03/15 6:14 PM",
      "commitName": "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "16/03/15 9:58 PM",
      "commitNameOld": "046521cd6511b7fc6d9478cb2bed90d8e75fca20",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,72 +1,10 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n-    dfsClient.checkOpen();\n-    checkClosed();\n-\n-    if (len \u003e bytesPerChecksum) {\n-      throw new IOException(\"writeChunk() buffer size is \" + len +\n-                            \" is larger than supported  bytesPerChecksum \" +\n-                            bytesPerChecksum);\n-    }\n-    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n-      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n-                            getChecksumSize() + \" but found to be \" + cklen);\n-    }\n-\n-    if (currentPacket \u003d\u003d null) {\n-      currentPacket \u003d createPacket(packetSize, chunksPerPacket, \n-          bytesCurBlock, currentSeqno++, false);\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n-            currentPacket.getSeqno() +\n-            \", src\u003d\" + src +\n-            \", packetSize\u003d\" + packetSize +\n-            \", chunksPerPacket\u003d\" + chunksPerPacket +\n-            \", bytesCurBlock\u003d\" + bytesCurBlock);\n-      }\n-    }\n-\n-    currentPacket.writeChecksum(checksum, ckoff, cklen);\n-    currentPacket.writeData(b, offset, len);\n-    currentPacket.incNumChunks();\n-    bytesCurBlock +\u003d len;\n-\n-    // If packet is full, enqueue it for transmission\n-    //\n-    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n-        bytesCurBlock \u003d\u003d blockSize) {\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n-            currentPacket.getSeqno() +\n-            \", src\u003d\" + src +\n-            \", bytesCurBlock\u003d\" + bytesCurBlock +\n-            \", blockSize\u003d\" + blockSize +\n-            \", appendChunk\u003d\" + appendChunk);\n-      }\n-      waitAndQueueCurrentPacket();\n-\n-      // If the reopened file did not end at chunk boundary and the above\n-      // write filled up its partial chunk. Tell the summer to generate full \n-      // crc chunks from now on.\n-      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n-        appendChunk \u003d false;\n-        resetChecksumBufSize();\n-      }\n-\n-      if (!appendChunk) {\n-        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n-        computePacketChunkSize(psize, bytesPerChecksum);\n-      }\n-      //\n-      // if encountering a block boundary, send an empty packet to \n-      // indicate the end of block and reset bytesCurBlock.\n-      //\n-      if (bytesCurBlock \u003d\u003d blockSize) {\n-        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n-        currentPacket.setSyncBlock(shouldSyncBlock);\n-        waitAndQueueCurrentPacket();\n-        bytesCurBlock \u003d 0;\n-        lastFlushOffset \u003d 0;\n-      }\n+    TraceScope scope \u003d\n+        dfsClient.getPathTraceScope(\"DFSOutputStream#writeChunk\", src);\n+    try {\n+      writeChunkImpl(b, offset, len, checksum, ckoff, cklen);\n+    } finally {\n+      scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    TraceScope scope \u003d\n        dfsClient.getPathTraceScope(\"DFSOutputStream#writeChunk\", src);\n    try {\n      writeChunkImpl(b, offset, len, checksum, ckoff, cklen);\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "952640fa4cbdc23fe8781e5627c2e8eab565c535": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7855. Separate class Packet from DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "05/03/15 10:58 AM",
      "commitName": "952640fa4cbdc23fe8781e5627c2e8eab565c535",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "01/03/15 11:03 PM",
      "commitNameOld": "67ed59348d638d56e6752ba2c71fdcd69567546d",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 3.5,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,72 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n     dfsClient.checkOpen();\n     checkClosed();\n \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             getChecksumSize() + \" but found to be \" + cklen);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d createPacket(packetSize, chunksPerPacket, \n-          bytesCurBlock, currentSeqno++);\n+          bytesCurBlock, currentSeqno++, false);\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n-            currentPacket.seqno +\n+            currentPacket.getSeqno() +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + bytesCurBlock);\n       }\n     }\n \n     currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n-    currentPacket.numChunks++;\n+    currentPacket.incNumChunks();\n     bytesCurBlock +\u003d len;\n \n     // If packet is full, enqueue it for transmission\n     //\n-    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n+    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n         bytesCurBlock \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n-            currentPacket.seqno +\n+            currentPacket.getSeqno() +\n             \", src\u003d\" + src +\n             \", bytesCurBlock\u003d\" + bytesCurBlock +\n             \", blockSize\u003d\" + blockSize +\n             \", appendChunk\u003d\" + appendChunk);\n       }\n       waitAndQueueCurrentPacket();\n \n       // If the reopened file did not end at chunk boundary and the above\n       // write filled up its partial chunk. Tell the summer to generate full \n       // crc chunks from now on.\n       if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n         appendChunk \u003d false;\n         resetChecksumBufSize();\n       }\n \n       if (!appendChunk) {\n         int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n         computePacketChunkSize(psize, bytesPerChecksum);\n       }\n       //\n       // if encountering a block boundary, send an empty packet to \n       // indicate the end of block and reset bytesCurBlock.\n       //\n       if (bytesCurBlock \u003d\u003d blockSize) {\n-        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n-        currentPacket.lastPacketInBlock \u003d true;\n-        currentPacket.syncBlock \u003d shouldSyncBlock;\n+        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n+        currentPacket.setSyncBlock(shouldSyncBlock);\n         waitAndQueueCurrentPacket();\n         bytesCurBlock \u003d 0;\n         lastFlushOffset \u003d 0;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d createPacket(packetSize, chunksPerPacket, \n          bytesCurBlock, currentSeqno++, false);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.getSeqno() +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.incNumChunks();\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.getSeqno() +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumBufSize();\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "36ccf097a95eae0761de7b657752e4808a86c094": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7276. Limit the number of byte arrays used by DFSOutputStream and provide a mechanism for recycling arrays.\n",
      "commitDate": "01/11/14 11:22 AM",
      "commitName": "36ccf097a95eae0761de7b657752e4808a86c094",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "27/10/14 9:38 AM",
      "commitNameOld": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 5.07,
      "commitsBetweenForRepo": 75,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,73 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n     dfsClient.checkOpen();\n     checkClosed();\n \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             getChecksumSize() + \" but found to be \" + cklen);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n-      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n-          bytesCurBlock, currentSeqno++, getChecksumSize());\n+      currentPacket \u003d createPacket(packetSize, chunksPerPacket, \n+          bytesCurBlock, currentSeqno++);\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + bytesCurBlock);\n       }\n     }\n \n     currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.numChunks++;\n     bytesCurBlock +\u003d len;\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n         bytesCurBlock \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", bytesCurBlock\u003d\" + bytesCurBlock +\n             \", blockSize\u003d\" + blockSize +\n             \", appendChunk\u003d\" + appendChunk);\n       }\n       waitAndQueueCurrentPacket();\n \n       // If the reopened file did not end at chunk boundary and the above\n       // write filled up its partial chunk. Tell the summer to generate full \n       // crc chunks from now on.\n       if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n         appendChunk \u003d false;\n         resetChecksumBufSize();\n       }\n \n       if (!appendChunk) {\n         int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n         computePacketChunkSize(psize, bytesPerChecksum);\n       }\n       //\n       // if encountering a block boundary, send an empty packet to \n       // indicate the end of block and reset bytesCurBlock.\n       //\n       if (bytesCurBlock \u003d\u003d blockSize) {\n-        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n-            currentSeqno++, getChecksumSize());\n+        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n         waitAndQueueCurrentPacket();\n         bytesCurBlock \u003d 0;\n         lastFlushOffset \u003d 0;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d createPacket(packetSize, chunksPerPacket, \n          bytesCurBlock, currentSeqno++);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumBufSize();\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "463aec11718e47d4aabb86a7a539cb973460aae6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
      "commitDate": "27/10/14 9:38 AM",
      "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthor": "cnauroth",
      "commitDateOld": "17/10/14 6:30 PM",
      "commitNameOld": "2e140523d3ccb27809cde4a55e95f7e0006c028f",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 9.63,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,74 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len,\n       byte[] checksum, int ckoff, int cklen) throws IOException {\n     dfsClient.checkOpen();\n     checkClosed();\n \n-    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n-    if (cklen !\u003d this.checksum.getChecksumSize()) {\n+    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n-                            this.checksum.getChecksumSize() + \n-                            \" but found to be \" + cklen);\n+                            getChecksumSize() + \" but found to be \" + cklen);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n-          bytesCurBlock, currentSeqno++, this.checksum.getChecksumSize());\n+          bytesCurBlock, currentSeqno++, getChecksumSize());\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + bytesCurBlock);\n       }\n     }\n \n     currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.numChunks++;\n     bytesCurBlock +\u003d len;\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n         bytesCurBlock \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", bytesCurBlock\u003d\" + bytesCurBlock +\n             \", blockSize\u003d\" + blockSize +\n             \", appendChunk\u003d\" + appendChunk);\n       }\n       waitAndQueueCurrentPacket();\n \n       // If the reopened file did not end at chunk boundary and the above\n       // write filled up its partial chunk. Tell the summer to generate full \n       // crc chunks from now on.\n       if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n         appendChunk \u003d false;\n         resetChecksumBufSize();\n       }\n \n       if (!appendChunk) {\n         int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n         computePacketChunkSize(psize, bytesPerChecksum);\n       }\n       //\n       // if encountering a block boundary, send an empty packet to \n       // indicate the end of block and reset bytesCurBlock.\n       //\n       if (bytesCurBlock \u003d\u003d blockSize) {\n         currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n-            currentSeqno++, this.checksum.getChecksumSize());\n+            currentSeqno++, getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n         waitAndQueueCurrentPacket();\n         bytesCurBlock \u003d 0;\n         lastFlushOffset \u003d 0;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d 0 \u0026\u0026 cklen !\u003d getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            getChecksumSize() + \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n          bytesCurBlock, currentSeqno++, getChecksumSize());\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumBufSize();\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "ab638e77b811d9592470f7d342cd11a66efbbf0d": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6865. Byte array native checksumming on client side. Contributed by James Thomas.\n",
      "commitDate": "28/08/14 4:44 PM",
      "commitName": "ab638e77b811d9592470f7d342cd11a66efbbf0d",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6865. Byte array native checksumming on client side. Contributed by James Thomas.\n",
          "commitDate": "28/08/14 4:44 PM",
          "commitName": "ab638e77b811d9592470f7d342cd11a66efbbf0d",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "20/08/14 6:13 PM",
          "commitNameOld": "6824abc19e12ed142d9f32b8706ef73d97edd1cc",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 7.94,
          "commitsBetweenForRepo": 43,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,76 @@\n-  protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n-                                                        throws IOException {\n+  protected synchronized void writeChunk(byte[] b, int offset, int len,\n+      byte[] checksum, int ckoff, int cklen) throws IOException {\n     dfsClient.checkOpen();\n     checkClosed();\n \n-    int cklen \u003d checksum.length;\n     int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n-    if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n+    if (cklen !\u003d this.checksum.getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             this.checksum.getChecksumSize() + \n-                            \" but found to be \" + checksum.length);\n+                            \" but found to be \" + cklen);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n           bytesCurBlock, currentSeqno++, this.checksum.getChecksumSize());\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + bytesCurBlock);\n       }\n     }\n \n-    currentPacket.writeChecksum(checksum, 0, cklen);\n+    currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.numChunks++;\n     bytesCurBlock +\u003d len;\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n         bytesCurBlock \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", bytesCurBlock\u003d\" + bytesCurBlock +\n             \", blockSize\u003d\" + blockSize +\n             \", appendChunk\u003d\" + appendChunk);\n       }\n       waitAndQueueCurrentPacket();\n \n       // If the reopened file did not end at chunk boundary and the above\n       // write filled up its partial chunk. Tell the summer to generate full \n       // crc chunks from now on.\n       if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n         appendChunk \u003d false;\n-        resetChecksumChunk(bytesPerChecksum);\n+        resetChecksumBufSize();\n       }\n \n       if (!appendChunk) {\n         int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n         computePacketChunkSize(psize, bytesPerChecksum);\n       }\n       //\n       // if encountering a block boundary, send an empty packet to \n       // indicate the end of block and reset bytesCurBlock.\n       //\n       if (bytesCurBlock \u003d\u003d blockSize) {\n         currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n             currentSeqno++, this.checksum.getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n         waitAndQueueCurrentPacket();\n         bytesCurBlock \u003d 0;\n         lastFlushOffset \u003d 0;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d this.checksum.getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            this.checksum.getChecksumSize() + \n                            \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n          bytesCurBlock, currentSeqno++, this.checksum.getChecksumSize());\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumBufSize();\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, this.checksum.getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[b-byte[], offset-int, len-int, checksum-byte[]]",
            "newValue": "[b-byte[], offset-int, len-int, checksum-byte[], ckoff-int, cklen-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6865. Byte array native checksumming on client side. Contributed by James Thomas.\n",
          "commitDate": "28/08/14 4:44 PM",
          "commitName": "ab638e77b811d9592470f7d342cd11a66efbbf0d",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "20/08/14 6:13 PM",
          "commitNameOld": "6824abc19e12ed142d9f32b8706ef73d97edd1cc",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 7.94,
          "commitsBetweenForRepo": 43,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,76 @@\n-  protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n-                                                        throws IOException {\n+  protected synchronized void writeChunk(byte[] b, int offset, int len,\n+      byte[] checksum, int ckoff, int cklen) throws IOException {\n     dfsClient.checkOpen();\n     checkClosed();\n \n-    int cklen \u003d checksum.length;\n     int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n-    if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n+    if (cklen !\u003d this.checksum.getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             this.checksum.getChecksumSize() + \n-                            \" but found to be \" + checksum.length);\n+                            \" but found to be \" + cklen);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n           bytesCurBlock, currentSeqno++, this.checksum.getChecksumSize());\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + bytesCurBlock);\n       }\n     }\n \n-    currentPacket.writeChecksum(checksum, 0, cklen);\n+    currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.numChunks++;\n     bytesCurBlock +\u003d len;\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n         bytesCurBlock \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", bytesCurBlock\u003d\" + bytesCurBlock +\n             \", blockSize\u003d\" + blockSize +\n             \", appendChunk\u003d\" + appendChunk);\n       }\n       waitAndQueueCurrentPacket();\n \n       // If the reopened file did not end at chunk boundary and the above\n       // write filled up its partial chunk. Tell the summer to generate full \n       // crc chunks from now on.\n       if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n         appendChunk \u003d false;\n-        resetChecksumChunk(bytesPerChecksum);\n+        resetChecksumBufSize();\n       }\n \n       if (!appendChunk) {\n         int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n         computePacketChunkSize(psize, bytesPerChecksum);\n       }\n       //\n       // if encountering a block boundary, send an empty packet to \n       // indicate the end of block and reset bytesCurBlock.\n       //\n       if (bytesCurBlock \u003d\u003d blockSize) {\n         currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n             currentSeqno++, this.checksum.getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n         waitAndQueueCurrentPacket();\n         bytesCurBlock \u003d 0;\n         lastFlushOffset \u003d 0;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (cklen !\u003d this.checksum.getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            this.checksum.getChecksumSize() + \n                            \" but found to be \" + cklen);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n          bytesCurBlock, currentSeqno++, this.checksum.getChecksumSize());\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumBufSize();\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, this.checksum.getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "1c309f763be3dd2e3d7d1616d2c960ff80cf9b03": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4906. HDFS Output streams should not accept writes after being closed. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1494303 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/06/13 2:05 PM",
      "commitName": "1c309f763be3dd2e3d7d1616d2c960ff80cf9b03",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "23/05/13 2:35 PM",
      "commitNameOld": "2ad7397c49844b5c12e122779c8760f51fa3a998",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 25.98,
      "commitsBetweenForRepo": 200,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,77 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                         throws IOException {\n     dfsClient.checkOpen();\n-    isClosed();\n+    checkClosed();\n \n     int cklen \u003d checksum.length;\n     int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             this.checksum.getChecksumSize() + \n                             \" but found to be \" + checksum.length);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n           bytesCurBlock, currentSeqno++, this.checksum.getChecksumSize());\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + bytesCurBlock);\n       }\n     }\n \n     currentPacket.writeChecksum(checksum, 0, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.numChunks++;\n     bytesCurBlock +\u003d len;\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n         bytesCurBlock \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", bytesCurBlock\u003d\" + bytesCurBlock +\n             \", blockSize\u003d\" + blockSize +\n             \", appendChunk\u003d\" + appendChunk);\n       }\n       waitAndQueueCurrentPacket();\n \n       // If the reopened file did not end at chunk boundary and the above\n       // write filled up its partial chunk. Tell the summer to generate full \n       // crc chunks from now on.\n       if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n         appendChunk \u003d false;\n         resetChecksumChunk(bytesPerChecksum);\n       }\n \n       if (!appendChunk) {\n         int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n         computePacketChunkSize(psize, bytesPerChecksum);\n       }\n       //\n       // if encountering a block boundary, send an empty packet to \n       // indicate the end of block and reset bytesCurBlock.\n       //\n       if (bytesCurBlock \u003d\u003d blockSize) {\n         currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n             currentSeqno++, this.checksum.getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n         waitAndQueueCurrentPacket();\n         bytesCurBlock \u003d 0;\n         lastFlushOffset \u003d 0;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                        throws IOException {\n    dfsClient.checkOpen();\n    checkClosed();\n\n    int cklen \u003d checksum.length;\n    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            this.checksum.getChecksumSize() + \n                            \" but found to be \" + checksum.length);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n          bytesCurBlock, currentSeqno++, this.checksum.getChecksumSize());\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, 0, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumChunk(bytesPerChecksum);\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, this.checksum.getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "735046ebecd9e803398be56fbf79dbde5226b4c1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3851. DFSOutputStream class code cleanup. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1377372 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/08/12 9:00 PM",
      "commitName": "735046ebecd9e803398be56fbf79dbde5226b4c1",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/08/12 1:59 PM",
      "commitNameOld": "f98d8eb291be364102b5c3011ce72e8f43eab389",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 11.29,
      "commitsBetweenForRepo": 87,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,77 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                         throws IOException {\n     dfsClient.checkOpen();\n     isClosed();\n \n     int cklen \u003d checksum.length;\n     int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             this.checksum.getChecksumSize() + \n                             \" but found to be \" + checksum.length);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n-          bytesCurBlock);\n+          bytesCurBlock, currentSeqno++, this.checksum.getChecksumSize());\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + bytesCurBlock);\n       }\n     }\n \n     currentPacket.writeChecksum(checksum, 0, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.numChunks++;\n     bytesCurBlock +\u003d len;\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n         bytesCurBlock \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", bytesCurBlock\u003d\" + bytesCurBlock +\n             \", blockSize\u003d\" + blockSize +\n             \", appendChunk\u003d\" + appendChunk);\n       }\n       waitAndQueueCurrentPacket();\n \n       // If the reopened file did not end at chunk boundary and the above\n       // write filled up its partial chunk. Tell the summer to generate full \n       // crc chunks from now on.\n       if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n         appendChunk \u003d false;\n         resetChecksumChunk(bytesPerChecksum);\n       }\n \n       if (!appendChunk) {\n         int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n         computePacketChunkSize(psize, bytesPerChecksum);\n       }\n       //\n       // if encountering a block boundary, send an empty packet to \n       // indicate the end of block and reset bytesCurBlock.\n       //\n       if (bytesCurBlock \u003d\u003d blockSize) {\n-        currentPacket \u003d new Packet(0, 0, bytesCurBlock);\n+        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n+            currentSeqno++, this.checksum.getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n         waitAndQueueCurrentPacket();\n         bytesCurBlock \u003d 0;\n         lastFlushOffset \u003d 0;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                        throws IOException {\n    dfsClient.checkOpen();\n    isClosed();\n\n    int cklen \u003d checksum.length;\n    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            this.checksum.getChecksumSize() + \n                            \" but found to be \" + checksum.length);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n          bytesCurBlock, currentSeqno++, this.checksum.getChecksumSize());\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, 0, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumChunk(bytesPerChecksum);\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, this.checksum.getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3721. hsync support broke wire compatibility. Contributed by Todd Lipcon and Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1371495 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/12 2:31 PM",
      "commitName": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "07/08/12 9:40 AM",
      "commitNameOld": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 2.2,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,76 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                         throws IOException {\n     dfsClient.checkOpen();\n     isClosed();\n \n     int cklen \u003d checksum.length;\n     int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             this.checksum.getChecksumSize() + \n                             \" but found to be \" + checksum.length);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n           bytesCurBlock);\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + bytesCurBlock);\n       }\n     }\n \n     currentPacket.writeChecksum(checksum, 0, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.numChunks++;\n     bytesCurBlock +\u003d len;\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n         bytesCurBlock \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", bytesCurBlock\u003d\" + bytesCurBlock +\n             \", blockSize\u003d\" + blockSize +\n             \", appendChunk\u003d\" + appendChunk);\n       }\n       waitAndQueueCurrentPacket();\n \n       // If the reopened file did not end at chunk boundary and the above\n       // write filled up its partial chunk. Tell the summer to generate full \n       // crc chunks from now on.\n       if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n         appendChunk \u003d false;\n         resetChecksumChunk(bytesPerChecksum);\n       }\n \n       if (!appendChunk) {\n         int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n         computePacketChunkSize(psize, bytesPerChecksum);\n       }\n       //\n       // if encountering a block boundary, send an empty packet to \n       // indicate the end of block and reset bytesCurBlock.\n       //\n       if (bytesCurBlock \u003d\u003d blockSize) {\n-        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n-            bytesCurBlock);\n+        currentPacket \u003d new Packet(0, 0, bytesCurBlock);\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n         waitAndQueueCurrentPacket();\n         bytesCurBlock \u003d 0;\n         lastFlushOffset \u003d 0;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                        throws IOException {\n    dfsClient.checkOpen();\n    isClosed();\n\n    int cklen \u003d checksum.length;\n    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            this.checksum.getChecksumSize() + \n                            \" but found to be \" + checksum.length);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n          bytesCurBlock);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, 0, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumChunk(bytesPerChecksum);\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "83cf475050dba27e72b4e399491638c670621175": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-744. Support hsync in HDFS. Contributed by Lars Hofhans\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1344419 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/05/12 12:10 PM",
      "commitName": "83cf475050dba27e72b4e399491638c670621175",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "29/05/12 12:37 PM",
      "commitNameOld": "47a29c63291f1f9f09b89ce6f3305c0a2ef27b3f",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.98,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,77 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                         throws IOException {\n     dfsClient.checkOpen();\n     isClosed();\n \n     int cklen \u003d checksum.length;\n     int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             this.checksum.getChecksumSize() + \n                             \" but found to be \" + checksum.length);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n           bytesCurBlock);\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + bytesCurBlock);\n       }\n     }\n \n     currentPacket.writeChecksum(checksum, 0, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.numChunks++;\n     bytesCurBlock +\u003d len;\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n         bytesCurBlock \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", bytesCurBlock\u003d\" + bytesCurBlock +\n             \", blockSize\u003d\" + blockSize +\n             \", appendChunk\u003d\" + appendChunk);\n       }\n       waitAndQueueCurrentPacket();\n \n       // If the reopened file did not end at chunk boundary and the above\n       // write filled up its partial chunk. Tell the summer to generate full \n       // crc chunks from now on.\n       if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n         appendChunk \u003d false;\n         resetChecksumChunk(bytesPerChecksum);\n       }\n \n       if (!appendChunk) {\n         int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n         computePacketChunkSize(psize, bytesPerChecksum);\n       }\n       //\n       // if encountering a block boundary, send an empty packet to \n       // indicate the end of block and reset bytesCurBlock.\n       //\n       if (bytesCurBlock \u003d\u003d blockSize) {\n         currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n             bytesCurBlock);\n         currentPacket.lastPacketInBlock \u003d true;\n+        currentPacket.syncBlock \u003d shouldSyncBlock;\n         waitAndQueueCurrentPacket();\n         bytesCurBlock \u003d 0;\n         lastFlushOffset \u003d 0;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                        throws IOException {\n    dfsClient.checkOpen();\n    isClosed();\n\n    int cklen \u003d checksum.length;\n    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            this.checksum.getChecksumSize() + \n                            \" but found to be \" + checksum.length);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n          bytesCurBlock);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, 0, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumChunk(bytesPerChecksum);\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                        throws IOException {\n    dfsClient.checkOpen();\n    isClosed();\n\n    int cklen \u003d checksum.length;\n    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            this.checksum.getChecksumSize() + \n                            \" but found to be \" + checksum.length);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n          bytesCurBlock);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, 0, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumChunk(bytesPerChecksum);\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                        throws IOException {\n    dfsClient.checkOpen();\n    isClosed();\n\n    int cklen \u003d checksum.length;\n    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            this.checksum.getChecksumSize() + \n                            \" but found to be \" + checksum.length);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n          bytesCurBlock);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, 0, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumChunk(bytesPerChecksum);\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2092. Remove some object references to Configuration in DFSClient.  Contributed by Bharath Mundlapudi\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/06/11 3:24 PM",
      "commitName": "fd9997989c1f1c6f806c57a806e7225ca599fc0c",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 11.02,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,76 @@\n   protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                         throws IOException {\n     dfsClient.checkOpen();\n     isClosed();\n \n     int cklen \u003d checksum.length;\n     int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n     if (len \u003e bytesPerChecksum) {\n       throw new IOException(\"writeChunk() buffer size is \" + len +\n                             \" is larger than supported  bytesPerChecksum \" +\n                             bytesPerChecksum);\n     }\n     if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n       throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                             this.checksum.getChecksumSize() + \n                             \" but found to be \" + checksum.length);\n     }\n \n     if (currentPacket \u003d\u003d null) {\n       currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n           bytesCurBlock);\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", packetSize\u003d\" + packetSize +\n             \", chunksPerPacket\u003d\" + chunksPerPacket +\n             \", bytesCurBlock\u003d\" + bytesCurBlock);\n       }\n     }\n \n     currentPacket.writeChecksum(checksum, 0, cklen);\n     currentPacket.writeData(b, offset, len);\n     currentPacket.numChunks++;\n     bytesCurBlock +\u003d len;\n \n     // If packet is full, enqueue it for transmission\n     //\n     if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n         bytesCurBlock \u003d\u003d blockSize) {\n       if (DFSClient.LOG.isDebugEnabled()) {\n         DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n             currentPacket.seqno +\n             \", src\u003d\" + src +\n             \", bytesCurBlock\u003d\" + bytesCurBlock +\n             \", blockSize\u003d\" + blockSize +\n             \", appendChunk\u003d\" + appendChunk);\n       }\n       waitAndQueueCurrentPacket();\n \n       // If the reopened file did not end at chunk boundary and the above\n       // write filled up its partial chunk. Tell the summer to generate full \n       // crc chunks from now on.\n       if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n         appendChunk \u003d false;\n         resetChecksumChunk(bytesPerChecksum);\n       }\n \n       if (!appendChunk) {\n-        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.writePacketSize);\n+        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n         computePacketChunkSize(psize, bytesPerChecksum);\n       }\n       //\n       // if encountering a block boundary, send an empty packet to \n       // indicate the end of block and reset bytesCurBlock.\n       //\n       if (bytesCurBlock \u003d\u003d blockSize) {\n         currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n             bytesCurBlock);\n         currentPacket.lastPacketInBlock \u003d true;\n         waitAndQueueCurrentPacket();\n         bytesCurBlock \u003d 0;\n         lastFlushOffset \u003d 0;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                        throws IOException {\n    dfsClient.checkOpen();\n    isClosed();\n\n    int cklen \u003d checksum.length;\n    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            this.checksum.getChecksumSize() + \n                            \" but found to be \" + checksum.length);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n          bytesCurBlock);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, 0, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumChunk(bytesPerChecksum);\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.getConf().writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,76 @@\n+  protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n+                                                        throws IOException {\n+    dfsClient.checkOpen();\n+    isClosed();\n+\n+    int cklen \u003d checksum.length;\n+    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n+    if (len \u003e bytesPerChecksum) {\n+      throw new IOException(\"writeChunk() buffer size is \" + len +\n+                            \" is larger than supported  bytesPerChecksum \" +\n+                            bytesPerChecksum);\n+    }\n+    if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n+      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n+                            this.checksum.getChecksumSize() + \n+                            \" but found to be \" + checksum.length);\n+    }\n+\n+    if (currentPacket \u003d\u003d null) {\n+      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n+          bytesCurBlock);\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n+            currentPacket.seqno +\n+            \", src\u003d\" + src +\n+            \", packetSize\u003d\" + packetSize +\n+            \", chunksPerPacket\u003d\" + chunksPerPacket +\n+            \", bytesCurBlock\u003d\" + bytesCurBlock);\n+      }\n+    }\n+\n+    currentPacket.writeChecksum(checksum, 0, cklen);\n+    currentPacket.writeData(b, offset, len);\n+    currentPacket.numChunks++;\n+    bytesCurBlock +\u003d len;\n+\n+    // If packet is full, enqueue it for transmission\n+    //\n+    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n+        bytesCurBlock \u003d\u003d blockSize) {\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n+            currentPacket.seqno +\n+            \", src\u003d\" + src +\n+            \", bytesCurBlock\u003d\" + bytesCurBlock +\n+            \", blockSize\u003d\" + blockSize +\n+            \", appendChunk\u003d\" + appendChunk);\n+      }\n+      waitAndQueueCurrentPacket();\n+\n+      // If the reopened file did not end at chunk boundary and the above\n+      // write filled up its partial chunk. Tell the summer to generate full \n+      // crc chunks from now on.\n+      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n+        appendChunk \u003d false;\n+        resetChecksumChunk(bytesPerChecksum);\n+      }\n+\n+      if (!appendChunk) {\n+        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.writePacketSize);\n+        computePacketChunkSize(psize, bytesPerChecksum);\n+      }\n+      //\n+      // if encountering a block boundary, send an empty packet to \n+      // indicate the end of block and reset bytesCurBlock.\n+      //\n+      if (bytesCurBlock \u003d\u003d blockSize) {\n+        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n+            bytesCurBlock);\n+        currentPacket.lastPacketInBlock \u003d true;\n+        waitAndQueueCurrentPacket();\n+        bytesCurBlock \u003d 0;\n+        lastFlushOffset \u003d 0;\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) \n                                                        throws IOException {\n    dfsClient.checkOpen();\n    isClosed();\n\n    int cklen \u003d checksum.length;\n    int bytesPerChecksum \u003d this.checksum.getBytesPerChecksum(); \n    if (len \u003e bytesPerChecksum) {\n      throw new IOException(\"writeChunk() buffer size is \" + len +\n                            \" is larger than supported  bytesPerChecksum \" +\n                            bytesPerChecksum);\n    }\n    if (checksum.length !\u003d this.checksum.getChecksumSize()) {\n      throw new IOException(\"writeChunk() checksum size is supposed to be \" +\n                            this.checksum.getChecksumSize() + \n                            \" but found to be \" + checksum.length);\n    }\n\n    if (currentPacket \u003d\u003d null) {\n      currentPacket \u003d new Packet(packetSize, chunksPerPacket, \n          bytesCurBlock);\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk allocating new packet seqno\u003d\" + \n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", packetSize\u003d\" + packetSize +\n            \", chunksPerPacket\u003d\" + chunksPerPacket +\n            \", bytesCurBlock\u003d\" + bytesCurBlock);\n      }\n    }\n\n    currentPacket.writeChecksum(checksum, 0, cklen);\n    currentPacket.writeData(b, offset, len);\n    currentPacket.numChunks++;\n    bytesCurBlock +\u003d len;\n\n    // If packet is full, enqueue it for transmission\n    //\n    if (currentPacket.numChunks \u003d\u003d currentPacket.maxChunks ||\n        bytesCurBlock \u003d\u003d blockSize) {\n      if (DFSClient.LOG.isDebugEnabled()) {\n        DFSClient.LOG.debug(\"DFSClient writeChunk packet full seqno\u003d\" +\n            currentPacket.seqno +\n            \", src\u003d\" + src +\n            \", bytesCurBlock\u003d\" + bytesCurBlock +\n            \", blockSize\u003d\" + blockSize +\n            \", appendChunk\u003d\" + appendChunk);\n      }\n      waitAndQueueCurrentPacket();\n\n      // If the reopened file did not end at chunk boundary and the above\n      // write filled up its partial chunk. Tell the summer to generate full \n      // crc chunks from now on.\n      if (appendChunk \u0026\u0026 bytesCurBlock%bytesPerChecksum \u003d\u003d 0) {\n        appendChunk \u003d false;\n        resetChecksumChunk(bytesPerChecksum);\n      }\n\n      if (!appendChunk) {\n        int psize \u003d Math.min((int)(blockSize-bytesCurBlock), dfsClient.writePacketSize);\n        computePacketChunkSize(psize, bytesPerChecksum);\n      }\n      //\n      // if encountering a block boundary, send an empty packet to \n      // indicate the end of block and reset bytesCurBlock.\n      //\n      if (bytesCurBlock \u003d\u003d blockSize) {\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        waitAndQueueCurrentPacket();\n        bytesCurBlock \u003d 0;\n        lastFlushOffset \u003d 0;\n      }\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}