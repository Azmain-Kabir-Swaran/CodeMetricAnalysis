{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BackupImage.java",
  "functionName": "applyEdits",
  "functionId": "applyEdits___firstTxId-long__numTxns-int__data-byte[]",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
  "functionStartLine": 195,
  "functionEndLine": 230,
  "numCommitsSeen": 30,
  "timeTaken": 4532,
  "changeHistory": [
    "ecbcb058b8bc0fbc3903acb56814c6d9608bc396",
    "dfd807afab0fae3839c9cc5d552aa0304444f956",
    "a40342b0dab1f9137ae4b3679a5aca7f2a57d23d",
    "b6ceee9bf42eec15891f60a014bbfa47e03f563c",
    "5dae97a584d30cef3e34141edfaca49c4ec57913",
    "a4ceea60f57a32d531549e492aa5894dd34e0d0f",
    "d8bc523754181b4c1321bcfab886ebf228d9c98f",
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
    "74dfa8f1f22d58df64a78c660af111e17ab7053e",
    "706394d03992b394e9f907aff2155df493e4ea4e",
    "9a07ba8945407cd8f63169faf9e0faa4311d38c7",
    "06e84a1bca19bd01568a3095e33944d4d6387fd3",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "53190cfa1d43762e463bcb957929097742db08ba",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63"
  ],
  "changeHistoryShort": {
    "ecbcb058b8bc0fbc3903acb56814c6d9608bc396": "Ybodychange",
    "dfd807afab0fae3839c9cc5d552aa0304444f956": "Ybodychange",
    "a40342b0dab1f9137ae4b3679a5aca7f2a57d23d": "Ybodychange",
    "b6ceee9bf42eec15891f60a014bbfa47e03f563c": "Ybodychange",
    "5dae97a584d30cef3e34141edfaca49c4ec57913": "Ybodychange",
    "a4ceea60f57a32d531549e492aa5894dd34e0d0f": "Ybodychange",
    "d8bc523754181b4c1321bcfab886ebf228d9c98f": "Ybodychange",
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd": "Ybodychange",
    "74dfa8f1f22d58df64a78c660af111e17ab7053e": "Ybodychange",
    "706394d03992b394e9f907aff2155df493e4ea4e": "Ybodychange",
    "9a07ba8945407cd8f63169faf9e0faa4311d38c7": "Ybodychange",
    "06e84a1bca19bd01568a3095e33944d4d6387fd3": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "53190cfa1d43762e463bcb957929097742db08ba": "Ybodychange",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": "Yintroduced"
  },
  "changeHistoryDetails": {
    "ecbcb058b8bc0fbc3903acb56814c6d9608bc396": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14731. [FGL] Remove redundant locking on NameNode. Contributed by Konstantin V Shvachko.",
      "commitDate": "21/02/20 5:53 PM",
      "commitName": "ecbcb058b8bc0fbc3903acb56814c6d9608bc396",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "31/05/17 8:09 AM",
      "commitNameOld": "13de636b4079b077890ad10389ff350dcf8086a2",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 996.45,
      "commitsBetweenForRepo": 7522,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,36 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader \u003d\n           new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n       long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n           backupInputStream, true, lastAppliedTxId + 1, null, null);\n       if (numTxnsAdvanced !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions, but we were only able to advance by \" +\n             numTxnsAdvanced);\n       }\n       lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n \n-      getNamesystem().dir.updateCountForQuota();\n+      getNamesystem().writeLock();\n+      try {\n+        getNamesystem().dir.updateCountForQuota();\n+      } finally {\n+        getNamesystem().writeUnlock();\n+      }\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d\n          new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n          backupInputStream, true, lastAppliedTxId + 1, null, null);\n      if (numTxnsAdvanced !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions, but we were only able to advance by \" +\n            numTxnsAdvanced);\n      }\n      lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n\n      getNamesystem().writeLock();\n      try {\n        getNamesystem().dir.updateCountForQuota();\n      } finally {\n        getNamesystem().writeUnlock();\n      }\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "dfd807afab0fae3839c9cc5d552aa0304444f956": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12428. Fix inconsistency between log-level guards and statements. Contributed by Jagadesh Kiran N and Jackie Chang.\n",
      "commitDate": "21/09/15 8:54 PM",
      "commitName": "dfd807afab0fae3839c9cc5d552aa0304444f956",
      "commitAuthor": "Tsuyoshi Ozawa",
      "commitDateOld": "10/09/15 7:16 AM",
      "commitNameOld": "a40342b0dab1f9137ae4b3679a5aca7f2a57d23d",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 11.57,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n-        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n+        LOG.trace(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader \u003d\n           new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n       long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n           backupInputStream, true, lastAppliedTxId + 1, null, null);\n       if (numTxnsAdvanced !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions, but we were only able to advance by \" +\n             numTxnsAdvanced);\n       }\n       lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n \n       getNamesystem().dir.updateCountForQuota();\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d\n          new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n          backupInputStream, true, lastAppliedTxId + 1, null, null);\n      if (numTxnsAdvanced !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions, but we were only able to advance by \" +\n            numTxnsAdvanced);\n      }\n      lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n\n      getNamesystem().dir.updateCountForQuota();\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "a40342b0dab1f9137ae4b3679a5aca7f2a57d23d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6763. Initialize file system-wide quota once on transitioning to active. Contributed by Kihwal Lee\n",
      "commitDate": "10/09/15 7:16 AM",
      "commitName": "a40342b0dab1f9137ae4b3679a5aca7f2a57d23d",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "28/08/15 11:14 AM",
      "commitNameOld": "b6ceee9bf42eec15891f60a014bbfa47e03f563c",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 12.83,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,31 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader \u003d\n           new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n       long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n           backupInputStream, true, lastAppliedTxId + 1, null, null);\n       if (numTxnsAdvanced !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions, but we were only able to advance by \" +\n             numTxnsAdvanced);\n       }\n       lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n \n-      FSImage.updateCountForQuota(\n-          getNamesystem().dir.getBlockStoragePolicySuite(),\n-          getNamesystem().dir.rootDir, quotaInitThreads);\n+      getNamesystem().dir.updateCountForQuota();\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d\n          new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n          backupInputStream, true, lastAppliedTxId + 1, null, null);\n      if (numTxnsAdvanced !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions, but we were only able to advance by \" +\n            numTxnsAdvanced);\n      }\n      lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n\n      getNamesystem().dir.updateCountForQuota();\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "b6ceee9bf42eec15891f60a014bbfa47e03f563c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8865. Improve quota initialization performance. Contributed by Kihwal Lee.\n",
      "commitDate": "28/08/15 11:14 AM",
      "commitName": "b6ceee9bf42eec15891f60a014bbfa47e03f563c",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "06/06/15 9:43 AM",
      "commitNameOld": "71de367c5e80ea76d1e8d21f0216cd6b879dcee5",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 83.06,
      "commitsBetweenForRepo": 486,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader \u003d\n           new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n       long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n           backupInputStream, true, lastAppliedTxId + 1, null, null);\n       if (numTxnsAdvanced !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions, but we were only able to advance by \" +\n             numTxnsAdvanced);\n       }\n       lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n \n       FSImage.updateCountForQuota(\n           getNamesystem().dir.getBlockStoragePolicySuite(),\n-          getNamesystem().dir.rootDir); // inefficient!\n+          getNamesystem().dir.rootDir, quotaInitThreads);\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d\n          new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n          backupInputStream, true, lastAppliedTxId + 1, null, null);\n      if (numTxnsAdvanced !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions, but we were only able to advance by \" +\n            numTxnsAdvanced);\n      }\n      lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n\n      FSImage.updateCountForQuota(\n          getNamesystem().dir.getBlockStoragePolicySuite(),\n          getNamesystem().dir.rootDir, quotaInitThreads);\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "5dae97a584d30cef3e34141edfaca49c4ec57913": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
      "commitDate": "11/02/15 10:41 AM",
      "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "11/02/15 12:45 AM",
      "commitNameOld": "a4ceea60f57a32d531549e492aa5894dd34e0d0f",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 0.41,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,33 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader \u003d\n           new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n       long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n           backupInputStream, true, lastAppliedTxId + 1, null, null);\n       if (numTxnsAdvanced !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions, but we were only able to advance by \" +\n             numTxnsAdvanced);\n       }\n       lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n \n-      FSImage.updateCountForQuota(getNamesystem().dir.getRoot()); // inefficient!\n+      FSImage.updateCountForQuota(\n+          getNamesystem().dir.getBlockStoragePolicySuite(),\n+          getNamesystem().dir.rootDir); // inefficient!\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d\n          new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n          backupInputStream, true, lastAppliedTxId + 1, null, null);\n      if (numTxnsAdvanced !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions, but we were only able to advance by \" +\n            numTxnsAdvanced);\n      }\n      lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n\n      FSImage.updateCountForQuota(\n          getNamesystem().dir.getBlockStoragePolicySuite(),\n          getNamesystem().dir.rootDir); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "a4ceea60f57a32d531549e492aa5894dd34e0d0f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7753. Fix Multithreaded correctness Warnings in BackupImage. Contributed by Rakesh R and Konstantin Shvachko.",
      "commitDate": "11/02/15 12:45 AM",
      "commitName": "a4ceea60f57a32d531549e492aa5894dd34e0d0f",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "06/02/15 6:59 PM",
      "commitNameOld": "cfb829ecd5b91fc9adcf5406e788184cfd75300f",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 4.24,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader \u003d\n-          new FSEditLogLoader(namesystem, lastAppliedTxId);\n+          new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n       long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n           backupInputStream, true, lastAppliedTxId + 1, null, null);\n       if (numTxnsAdvanced !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions, but we were only able to advance by \" +\n             numTxnsAdvanced);\n       }\n       lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n \n-      FSImage.updateCountForQuota(namesystem.dir.rootDir); // inefficient!\n+      FSImage.updateCountForQuota(getNamesystem().dir.getRoot()); // inefficient!\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d\n          new FSEditLogLoader(getNamesystem(), lastAppliedTxId);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n          backupInputStream, true, lastAppliedTxId + 1, null, null);\n      if (numTxnsAdvanced !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions, but we were only able to advance by \" +\n            numTxnsAdvanced);\n      }\n      lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n\n      FSImage.updateCountForQuota(getNamesystem().dir.getRoot()); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "d8bc523754181b4c1321bcfab886ebf228d9c98f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5753. Add new NN startup options for downgrade and rollback using upgrade marker.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1559907 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/01/14 6:38 PM",
      "commitName": "d8bc523754181b4c1321bcfab886ebf228d9c98f",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "20/02/13 12:02 PM",
      "commitNameOld": "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 334.28,
      "commitsBetweenForRepo": 2039,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader \u003d\n           new FSEditLogLoader(namesystem, lastAppliedTxId);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n       long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n-          backupInputStream, true, lastAppliedTxId + 1, null);\n+          backupInputStream, true, lastAppliedTxId + 1, null, null);\n       if (numTxnsAdvanced !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions, but we were only able to advance by \" +\n             numTxnsAdvanced);\n       }\n       lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n \n       FSImage.updateCountForQuota(namesystem.dir.rootDir); // inefficient!\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d\n          new FSEditLogLoader(namesystem, lastAppliedTxId);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n          backupInputStream, true, lastAppliedTxId + 1, null, null);\n      if (numTxnsAdvanced !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions, but we were only able to advance by \" +\n            numTxnsAdvanced);\n      }\n      lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n\n      FSImage.updateCountForQuota(namesystem.dir.rootDir); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "fac3883188d9c4f1fe188d98f88cb3c83b243bbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4503. Update computeContentSummary(..), spaceConsumedInTree(..) and diskspaceConsumed(..) in INode for snapshot.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1448373 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/02/13 12:02 PM",
      "commitName": "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "23/05/12 1:42 PM",
      "commitNameOld": "74dfa8f1f22d58df64a78c660af111e17ab7053e",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 272.97,
      "commitsBetweenForRepo": 1456,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader \u003d\n           new FSEditLogLoader(namesystem, lastAppliedTxId);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n       long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n           backupInputStream, true, lastAppliedTxId + 1, null);\n       if (numTxnsAdvanced !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions, but we were only able to advance by \" +\n             numTxnsAdvanced);\n       }\n       lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n \n-      namesystem.dir.updateCountForINodeWithQuota(); // inefficient!\n+      FSImage.updateCountForQuota(namesystem.dir.rootDir); // inefficient!\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d\n          new FSEditLogLoader(namesystem, lastAppliedTxId);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n          backupInputStream, true, lastAppliedTxId + 1, null);\n      if (numTxnsAdvanced !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions, but we were only able to advance by \" +\n            numTxnsAdvanced);\n      }\n      lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n\n      FSImage.updateCountForQuota(namesystem.dir.rootDir); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "74dfa8f1f22d58df64a78c660af111e17ab7053e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2982. Startup performance suffers when there are many edit log segments. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1342042 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/05/12 1:42 PM",
      "commitName": "74dfa8f1f22d58df64a78c660af111e17ab7053e",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "13/04/12 4:41 PM",
      "commitNameOld": "841fdc5628fbba341efe0bfc6763fe12e7fca7f4",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 39.88,
      "commitsBetweenForRepo": 241,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader \u003d\n           new FSEditLogLoader(namesystem, lastAppliedTxId);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n-      long numTxnsAdvanced \u003d logLoader.loadEditRecords(logVersion, \n+      long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n           backupInputStream, true, lastAppliedTxId + 1, null);\n       if (numTxnsAdvanced !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions, but we were only able to advance by \" +\n             numTxnsAdvanced);\n       }\n       lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n \n       namesystem.dir.updateCountForINodeWithQuota(); // inefficient!\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d\n          new FSEditLogLoader(namesystem, lastAppliedTxId);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      long numTxnsAdvanced \u003d logLoader.loadEditRecords(\n          backupInputStream, true, lastAppliedTxId + 1, null);\n      if (numTxnsAdvanced !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions, but we were only able to advance by \" +\n            numTxnsAdvanced);\n      }\n      lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n\n      namesystem.dir.updateCountForINodeWithQuota(); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "706394d03992b394e9f907aff2155df493e4ea4e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3004. Implement Recovery Mode. Contributed by Colin Patrick McCabe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1311394 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/04/12 12:39 PM",
      "commitName": "706394d03992b394e9f907aff2155df493e4ea4e",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "05/04/12 5:08 PM",
      "commitNameOld": "861c872541b614971c73a3ae46fa3846d729dbee",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 3.81,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,31 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n-      FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n+      FSEditLogLoader logLoader \u003d\n+          new FSEditLogLoader(namesystem, lastAppliedTxId);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n-      long numLoaded \u003d logLoader.loadEditRecords(logVersion, backupInputStream, \n-                                                true, lastAppliedTxId + 1);\n-      if (numLoaded !\u003d numTxns) {\n+      long numTxnsAdvanced \u003d logLoader.loadEditRecords(logVersion, \n+          backupInputStream, true, lastAppliedTxId + 1, null);\n+      if (numTxnsAdvanced !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n-            \" transactions but only was able to apply \" + numLoaded);\n+            \" transactions, but we were only able to advance by \" +\n+            numTxnsAdvanced);\n       }\n-      lastAppliedTxId +\u003d numTxns;\n-      \n+      lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n+\n       namesystem.dir.updateCountForINodeWithQuota(); // inefficient!\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d\n          new FSEditLogLoader(namesystem, lastAppliedTxId);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      long numTxnsAdvanced \u003d logLoader.loadEditRecords(logVersion, \n          backupInputStream, true, lastAppliedTxId + 1, null);\n      if (numTxnsAdvanced !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions, but we were only able to advance by \" +\n            numTxnsAdvanced);\n      }\n      lastAppliedTxId \u003d logLoader.getLastAppliedTxId();\n\n      namesystem.dir.updateCountForINodeWithQuota(); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "9a07ba8945407cd8f63169faf9e0faa4311d38c7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2709. Appropriately handle error conditions in EditLogTailer. Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1228390 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/01/12 12:44 PM",
      "commitName": "9a07ba8945407cd8f63169faf9e0faa4311d38c7",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "08/12/11 3:55 PM",
      "commitNameOld": "2481474bd9c50a23e4fd2eea67ac2dea11ca1f58",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 28.87,
      "commitsBetweenForRepo": 175,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n-      int numLoaded \u003d logLoader.loadEditRecords(logVersion, backupInputStream, \n+      long numLoaded \u003d logLoader.loadEditRecords(logVersion, backupInputStream, \n                                                 true, lastAppliedTxId + 1);\n       if (numLoaded !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions but only was able to apply \" + numLoaded);\n       }\n       lastAppliedTxId +\u003d numTxns;\n       \n       namesystem.dir.updateCountForINodeWithQuota(); // inefficient!\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      long numLoaded \u003d logLoader.loadEditRecords(logVersion, backupInputStream, \n                                                true, lastAppliedTxId + 1);\n      if (numLoaded !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions but only was able to apply \" + numLoaded);\n      }\n      lastAppliedTxId +\u003d numTxns;\n      \n      namesystem.dir.updateCountForINodeWithQuota(); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "06e84a1bca19bd01568a3095e33944d4d6387fd3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2223. Untangle depencencies between NN components. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1166466 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/09/11 4:23 PM",
      "commitName": "06e84a1bca19bd01568a3095e33944d4d6387fd3",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "06/09/11 1:27 PM",
      "commitNameOld": "bdc3720d5b67a1c8fc2dfb29be16e4155c0e7f15",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 1.12,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n       int logVersion \u003d storage.getLayoutVersion();\n       backupInputStream.setBytes(data, logVersion);\n \n       int numLoaded \u003d logLoader.loadEditRecords(logVersion, backupInputStream, \n                                                 true, lastAppliedTxId + 1);\n       if (numLoaded !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions but only was able to apply \" + numLoaded);\n       }\n       lastAppliedTxId +\u003d numTxns;\n       \n-      getFSNamesystem().dir.updateCountForINodeWithQuota(); // inefficient!\n+      namesystem.dir.updateCountForINodeWithQuota(); // inefficient!\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      int numLoaded \u003d logLoader.loadEditRecords(logVersion, backupInputStream, \n                                                true, lastAppliedTxId + 1);\n      if (numLoaded !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions but only was able to apply \" + numLoaded);\n      }\n      lastAppliedTxId +\u003d numTxns;\n      \n      namesystem.dir.updateCountForINodeWithQuota(); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      int numLoaded \u003d logLoader.loadEditRecords(logVersion, backupInputStream, \n                                                true, lastAppliedTxId + 1);\n      if (numLoaded !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions but only was able to apply \" + numLoaded);\n      }\n      lastAppliedTxId +\u003d numTxns;\n      \n      getFSNamesystem().dir.updateCountForINodeWithQuota(); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      int numLoaded \u003d logLoader.loadEditRecords(logVersion, backupInputStream, \n                                                true, lastAppliedTxId + 1);\n      if (numLoaded !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions but only was able to apply \" + numLoaded);\n      }\n      lastAppliedTxId +\u003d numTxns;\n      \n      getFSNamesystem().dir.updateCountForINodeWithQuota(); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java"
      }
    },
    "53190cfa1d43762e463bcb957929097742db08ba": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2187. Make EditLogInputStream act like an iterator over FSEditLogOps. Contributed by Ivan Kelly and Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1153996 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/08/11 1:22 PM",
      "commitName": "53190cfa1d43762e463bcb957929097742db08ba",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "29/07/11 9:28 AM",
      "commitNameOld": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 6.16,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,29 @@\n   private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n       throws IOException {\n     Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n         \"Received txn batch starting at %s but expected %s\",\n         firstTxId, lastAppliedTxId + 1);\n     assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n         LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n       }\n-      backupInputStream.setBytes(data);\n+\n       FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n       int logVersion \u003d storage.getLayoutVersion();\n-      BufferedInputStream bin \u003d new BufferedInputStream(backupInputStream);\n-      DataInputStream in \u003d new DataInputStream(bin);\n-      Checksum checksum \u003d FSEditLog.getChecksum();\n-      int numLoaded \u003d logLoader.loadEditRecords(logVersion, in, checksum, true,\n-                                lastAppliedTxId + 1);\n+      backupInputStream.setBytes(data, logVersion);\n+\n+      int numLoaded \u003d logLoader.loadEditRecords(logVersion, backupInputStream, \n+                                                true, lastAppliedTxId + 1);\n       if (numLoaded !\u003d numTxns) {\n         throw new IOException(\"Batch of txns starting at txnid \" +\n             firstTxId + \" was supposed to contain \" + numTxns +\n             \" transactions but only was able to apply \" + numLoaded);\n       }\n       lastAppliedTxId +\u003d numTxns;\n       \n       getFSNamesystem().dir.updateCountForINodeWithQuota(); // inefficient!\n     } finally {\n       backupInputStream.clear();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n\n      FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n      int logVersion \u003d storage.getLayoutVersion();\n      backupInputStream.setBytes(data, logVersion);\n\n      int numLoaded \u003d logLoader.loadEditRecords(logVersion, backupInputStream, \n                                                true, lastAppliedTxId + 1);\n      if (numLoaded !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions but only was able to apply \" + numLoaded);\n      }\n      lastAppliedTxId +\u003d numTxns;\n      \n      getFSNamesystem().dir.updateCountForINodeWithQuota(); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java",
      "extendedDetails": {}
    },
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/07/11 9:28 AM",
      "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,30 @@\n+  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n+      throws IOException {\n+    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n+        \"Received txn batch starting at %s but expected %s\",\n+        firstTxId, lastAppliedTxId + 1);\n+    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n+    try {\n+      if (LOG.isTraceEnabled()) {\n+        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n+      }\n+      backupInputStream.setBytes(data);\n+      FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n+      int logVersion \u003d storage.getLayoutVersion();\n+      BufferedInputStream bin \u003d new BufferedInputStream(backupInputStream);\n+      DataInputStream in \u003d new DataInputStream(bin);\n+      Checksum checksum \u003d FSEditLog.getChecksum();\n+      int numLoaded \u003d logLoader.loadEditRecords(logVersion, in, checksum, true,\n+                                lastAppliedTxId + 1);\n+      if (numLoaded !\u003d numTxns) {\n+        throw new IOException(\"Batch of txns starting at txnid \" +\n+            firstTxId + \" was supposed to contain \" + numTxns +\n+            \" transactions but only was able to apply \" + numLoaded);\n+      }\n+      lastAppliedTxId +\u003d numTxns;\n+      \n+      getFSNamesystem().dir.updateCountForINodeWithQuota(); // inefficient!\n+    } finally {\n+      backupInputStream.clear();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n      throws IOException {\n    Preconditions.checkArgument(firstTxId \u003d\u003d lastAppliedTxId + 1,\n        \"Received txn batch starting at %s but expected %s\",\n        firstTxId, lastAppliedTxId + 1);\n    assert backupInputStream.length() \u003d\u003d 0 : \"backup input stream is not empty\";\n    try {\n      if (LOG.isTraceEnabled()) {\n        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n      }\n      backupInputStream.setBytes(data);\n      FSEditLogLoader logLoader \u003d new FSEditLogLoader(namesystem);\n      int logVersion \u003d storage.getLayoutVersion();\n      BufferedInputStream bin \u003d new BufferedInputStream(backupInputStream);\n      DataInputStream in \u003d new DataInputStream(bin);\n      Checksum checksum \u003d FSEditLog.getChecksum();\n      int numLoaded \u003d logLoader.loadEditRecords(logVersion, in, checksum, true,\n                                lastAppliedTxId + 1);\n      if (numLoaded !\u003d numTxns) {\n        throw new IOException(\"Batch of txns starting at txnid \" +\n            firstTxId + \" was supposed to contain \" + numTxns +\n            \" transactions but only was able to apply \" + numLoaded);\n      }\n      lastAppliedTxId +\u003d numTxns;\n      \n      getFSNamesystem().dir.updateCountForINodeWithQuota(); // inefficient!\n    } finally {\n      backupInputStream.clear();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java"
    }
  }
}