{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReaderLocal.java",
  "functionName": "close",
  "functionId": "close",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java",
  "functionStartLine": 631,
  "functionEndLine": 641,
  "numCommitsSeen": 119,
  "timeTaken": 5004,
  "changeHistory": [
    "6d116ffad23b470f8e9ca131d8e89cbbbb4378d7",
    "f308561f1d885491b88db73ac63003202056d661",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "e2c9b288b223b9fd82dc12018936e13128413492",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
    "124e507674c0d396f8494585e64226957199097b",
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
    "694a6721316aea14c1244447974231abc8dff0cb",
    "9a4030e0e84a688c12daa21fe9a165808c3eca70",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
    "837e17b2eac1471d93e2eff395272063b265fee7",
    "239b2742d0e80d13c970fd062af4930e672fe903",
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
    "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04",
    "2ab10e29d9cca5018064be46a40e3c74423615a8"
  ],
  "changeHistoryShort": {
    "6d116ffad23b470f8e9ca131d8e89cbbbb4378d7": "Ybodychange",
    "f308561f1d885491b88db73ac63003202056d661": "Yfilerename",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "e2c9b288b223b9fd82dc12018936e13128413492": "Yfilerename",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": "Ybodychange",
    "124e507674c0d396f8494585e64226957199097b": "Ybodychange",
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": "Ybodychange",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": "Yparameterchange",
    "694a6721316aea14c1244447974231abc8dff0cb": "Ybodychange",
    "9a4030e0e84a688c12daa21fe9a165808c3eca70": "Ymultichange(Yparameterchange,Ybodychange)",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": "Yparameterchange",
    "837e17b2eac1471d93e2eff395272063b265fee7": "Yparameterchange",
    "239b2742d0e80d13c970fd062af4930e672fe903": "Yparameterchange",
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a": "Ybodychange",
    "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04": "Ybodychange",
    "2ab10e29d9cca5018064be46a40e3c74423615a8": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6d116ffad23b470f8e9ca131d8e89cbbbb4378d7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11789. Maintain Short-Circuit Read Statistics. Contributed by Hanisha Koneru.\n",
      "commitDate": "22/06/17 1:35 PM",
      "commitName": "6d116ffad23b470f8e9ca131d8e89cbbbb4378d7",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "25/04/16 12:01 PM",
      "commitNameOld": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 423.07,
      "commitsBetweenForRepo": 2737,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,11 @@\n   public synchronized void close() throws IOException {\n     if (closed) return;\n     closed \u003d true;\n     LOG.trace(\"close(filename\u003d{}, block\u003d{})\", filename, block);\n     replica.unref();\n     freeDataBufIfExists();\n     freeChecksumBufIfExists();\n+    if (metrics !\u003d null) {\n+      metrics.collectThreadLocalStates();\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) return;\n    closed \u003d true;\n    LOG.trace(\"close(filename\u003d{}, block\u003d{})\", filename, block);\n    replica.unref();\n    freeDataBufIfExists();\n    freeChecksumBufIfExists();\n    if (metrics !\u003d null) {\n      metrics.collectThreadLocalStates();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "f308561f1d885491b88db73ac63003202056d661": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
      "commitDate": "25/04/16 12:01 PM",
      "commitName": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "25/04/16 9:38 AM",
      "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) return;\n    closed \u003d true;\n    LOG.trace(\"close(filename\u003d{}, block\u003d{})\", filename, block);\n    replica.unref();\n    freeDataBufIfExists();\n    freeChecksumBufIfExists();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java"
      }
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,8 @@\n   public synchronized void close() throws IOException {\n     if (closed) return;\n     closed \u003d true;\n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"close(filename\u003d\" + filename + \", block\u003d\" + block + \")\");\n-    }\n+    LOG.trace(\"close(filename\u003d{}, block\u003d{})\", filename, block);\n     replica.unref();\n     freeDataBufIfExists();\n     freeChecksumBufIfExists();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) return;\n    closed \u003d true;\n    LOG.trace(\"close(filename\u003d{}, block\u003d{})\", filename, block);\n    replica.unref();\n    freeDataBufIfExists();\n    freeChecksumBufIfExists();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,10 @@\n   public synchronized void close() throws IOException {\n     if (closed) return;\n     closed \u003d true;\n-    LOG.trace(\"close(filename\u003d{}, block\u003d{})\", filename, block);\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"close(filename\u003d\" + filename + \", block\u003d\" + block + \")\");\n+    }\n     replica.unref();\n     freeDataBufIfExists();\n     freeChecksumBufIfExists();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) return;\n    closed \u003d true;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"close(filename\u003d\" + filename + \", block\u003d\" + block + \")\");\n    }\n    replica.unref();\n    freeDataBufIfExists();\n    freeChecksumBufIfExists();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,8 @@\n   public synchronized void close() throws IOException {\n     if (closed) return;\n     closed \u003d true;\n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"close(filename\u003d\" + filename + \", block\u003d\" + block + \")\");\n-    }\n+    LOG.trace(\"close(filename\u003d{}, block\u003d{})\", filename, block);\n     replica.unref();\n     freeDataBufIfExists();\n     freeChecksumBufIfExists();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) return;\n    closed \u003d true;\n    LOG.trace(\"close(filename\u003d{}, block\u003d{})\", filename, block);\n    replica.unref();\n    freeDataBufIfExists();\n    freeChecksumBufIfExists();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "e2c9b288b223b9fd82dc12018936e13128413492": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "28/08/15 2:38 PM",
      "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/08/15 2:21 PM",
      "commitNameOld": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) return;\n    closed \u003d true;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"close(filename\u003d\" + filename + \", block\u003d\" + block + \")\");\n    }\n    replica.unref();\n    freeDataBufIfExists();\n    freeChecksumBufIfExists();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java"
      }
    },
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 11:08 AM",
      "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "15/01/14 11:17 AM",
      "commitNameOld": "037a89abc5cc5ea6b983b21c568a50bc729aa194",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 27.99,
      "commitsBetweenForRepo": 191,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,10 @@\n   public synchronized void close() throws IOException {\n     if (closed) return;\n     closed \u003d true;\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"close(filename\u003d\" + filename + \", block\u003d\" + block + \")\");\n     }\n-    if (clientMmap !\u003d null) {\n-      clientMmap.unref();\n-      clientMmap \u003d null;\n-    }\n-    if (fisCache !\u003d null) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"putting FileInputStream for \" + filename +\n-            \" back into FileInputStreamCache\");\n-      }\n-      fisCache.put(datanodeID, block, streams);\n-    } else {\n-      LOG.debug(\"closing FileInputStream for \" + filename);\n-      IOUtils.cleanup(LOG, dataIn, checksumIn);\n-    }\n+    replica.unref();\n     freeDataBufIfExists();\n     freeChecksumBufIfExists();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) return;\n    closed \u003d true;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"close(filename\u003d\" + filename + \", block\u003d\" + block + \")\");\n    }\n    replica.unref();\n    freeDataBufIfExists();\n    freeChecksumBufIfExists();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "124e507674c0d396f8494585e64226957199097b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5634. Allow BlockReaderLocal to switch between checksumming and not (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551701 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 12:57 PM",
      "commitName": "124e507674c0d396f8494585e64226957199097b",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "27/09/13 3:51 PM",
      "commitNameOld": "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 80.92,
      "commitsBetweenForRepo": 532,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,23 @@\n   public synchronized void close() throws IOException {\n+    if (closed) return;\n+    closed \u003d true;\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"close(filename\u003d\" + filename + \", block\u003d\" + block + \")\");\n+    }\n     if (clientMmap !\u003d null) {\n       clientMmap.unref();\n       clientMmap \u003d null;\n     }\n     if (fisCache !\u003d null) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"putting FileInputStream for \" + filename +\n             \" back into FileInputStreamCache\");\n       }\n-      fisCache.put(datanodeID, block, new FileInputStream[] {dataIn, checksumIn});\n+      fisCache.put(datanodeID, block, streams);\n     } else {\n       LOG.debug(\"closing FileInputStream for \" + filename);\n       IOUtils.cleanup(LOG, dataIn, checksumIn);\n     }\n-    if (slowReadBuff !\u003d null) {\n-      bufferPool.returnBuffer(slowReadBuff);\n-      slowReadBuff \u003d null;\n-    }\n-    if (checksumBuff !\u003d null) {\n-      bufferPool.returnBuffer(checksumBuff);\n-      checksumBuff \u003d null;\n-    }\n-    startOffset \u003d -1;\n-    checksum \u003d null;\n+    freeDataBufIfExists();\n+    freeChecksumBufIfExists();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) return;\n    closed \u003d true;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"close(filename\u003d\" + filename + \", block\u003d\" + block + \")\");\n    }\n    if (clientMmap !\u003d null) {\n      clientMmap.unref();\n      clientMmap \u003d null;\n    }\n    if (fisCache !\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"putting FileInputStream for \" + filename +\n            \" back into FileInputStreamCache\");\n      }\n      fisCache.put(datanodeID, block, streams);\n    } else {\n      LOG.debug(\"closing FileInputStream for \" + filename);\n      IOUtils.cleanup(LOG, dataIn, checksumIn);\n    }\n    freeDataBufIfExists();\n    freeChecksumBufIfExists();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5260. Merge zero-copy memory-mapped HDFS client reads to trunk and branch-2. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1527113 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 3:51 PM",
      "commitName": "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "19/06/13 9:43 PM",
      "commitNameOld": "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 99.76,
      "commitsBetweenForRepo": 545,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,26 @@\n   public synchronized void close() throws IOException {\n+    if (clientMmap !\u003d null) {\n+      clientMmap.unref();\n+      clientMmap \u003d null;\n+    }\n     if (fisCache !\u003d null) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"putting FileInputStream for \" + filename +\n             \" back into FileInputStreamCache\");\n       }\n       fisCache.put(datanodeID, block, new FileInputStream[] {dataIn, checksumIn});\n     } else {\n       LOG.debug(\"closing FileInputStream for \" + filename);\n       IOUtils.cleanup(LOG, dataIn, checksumIn);\n     }\n     if (slowReadBuff !\u003d null) {\n       bufferPool.returnBuffer(slowReadBuff);\n       slowReadBuff \u003d null;\n     }\n     if (checksumBuff !\u003d null) {\n       bufferPool.returnBuffer(checksumBuff);\n       checksumBuff \u003d null;\n     }\n     startOffset \u003d -1;\n     checksum \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (clientMmap !\u003d null) {\n      clientMmap.unref();\n      clientMmap \u003d null;\n    }\n    if (fisCache !\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"putting FileInputStream for \" + filename +\n            \" back into FileInputStreamCache\");\n      }\n      fisCache.put(datanodeID, block, new FileInputStream[] {dataIn, checksumIn});\n    } else {\n      LOG.debug(\"closing FileInputStream for \" + filename);\n      IOUtils.cleanup(LOG, dataIn, checksumIn);\n    }\n    if (slowReadBuff !\u003d null) {\n      bufferPool.returnBuffer(slowReadBuff);\n      slowReadBuff \u003d null;\n    }\n    if (checksumBuff !\u003d null) {\n      bufferPool.returnBuffer(checksumBuff);\n      checksumBuff \u003d null;\n    }\n    startOffset \u003d -1;\n    checksum \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-4661. A few little code cleanups of some HDFS-347-related code. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1480839 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/13 5:03 PM",
      "commitName": "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "27/03/13 12:28 PM",
      "commitNameOld": "694a6721316aea14c1244447974231abc8dff0cb",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 43.19,
      "commitsBetweenForRepo": 277,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,22 @@\n-  public synchronized void close(PeerCache peerCache,\n-      FileInputStreamCache fisCache) throws IOException {\n+  public synchronized void close() throws IOException {\n     if (fisCache !\u003d null) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"putting FileInputStream for \" + filename +\n             \" back into FileInputStreamCache\");\n       }\n       fisCache.put(datanodeID, block, new FileInputStream[] {dataIn, checksumIn});\n     } else {\n       LOG.debug(\"closing FileInputStream for \" + filename);\n       IOUtils.cleanup(LOG, dataIn, checksumIn);\n     }\n     if (slowReadBuff !\u003d null) {\n       bufferPool.returnBuffer(slowReadBuff);\n       slowReadBuff \u003d null;\n     }\n     if (checksumBuff !\u003d null) {\n       bufferPool.returnBuffer(checksumBuff);\n       checksumBuff \u003d null;\n     }\n     startOffset \u003d -1;\n     checksum \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (fisCache !\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"putting FileInputStream for \" + filename +\n            \" back into FileInputStreamCache\");\n      }\n      fisCache.put(datanodeID, block, new FileInputStream[] {dataIn, checksumIn});\n    } else {\n      LOG.debug(\"closing FileInputStream for \" + filename);\n      IOUtils.cleanup(LOG, dataIn, checksumIn);\n    }\n    if (slowReadBuff !\u003d null) {\n      bufferPool.returnBuffer(slowReadBuff);\n      slowReadBuff \u003d null;\n    }\n    if (checksumBuff !\u003d null) {\n      bufferPool.returnBuffer(checksumBuff);\n      checksumBuff \u003d null;\n    }\n    startOffset \u003d -1;\n    checksum \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {
        "oldValue": "[peerCache-PeerCache, fisCache-FileInputStreamCache]",
        "newValue": "[]"
      }
    },
    "694a6721316aea14c1244447974231abc8dff0cb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4538. Allow use of legacy blockreader. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1461818 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/03/13 12:28 PM",
      "commitName": "694a6721316aea14c1244447974231abc8dff0cb",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "14/01/13 4:31 PM",
      "commitNameOld": "2fd41b3b429775a7151e84d971f593d99ef8de14",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 71.79,
      "commitsBetweenForRepo": 158,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,23 @@\n   public synchronized void close(PeerCache peerCache,\n       FileInputStreamCache fisCache) throws IOException {\n     if (fisCache !\u003d null) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"putting FileInputStream for \" + filename +\n             \" back into FileInputStreamCache\");\n       }\n       fisCache.put(datanodeID, block, new FileInputStream[] {dataIn, checksumIn});\n     } else {\n       LOG.debug(\"closing FileInputStream for \" + filename);\n-      dataIn.close();\n-      checksumIn.close();\n+      IOUtils.cleanup(LOG, dataIn, checksumIn);\n     }\n     if (slowReadBuff !\u003d null) {\n       bufferPool.returnBuffer(slowReadBuff);\n       slowReadBuff \u003d null;\n     }\n     if (checksumBuff !\u003d null) {\n       bufferPool.returnBuffer(checksumBuff);\n       checksumBuff \u003d null;\n     }\n     startOffset \u003d -1;\n     checksum \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close(PeerCache peerCache,\n      FileInputStreamCache fisCache) throws IOException {\n    if (fisCache !\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"putting FileInputStream for \" + filename +\n            \" back into FileInputStreamCache\");\n      }\n      fisCache.put(datanodeID, block, new FileInputStream[] {dataIn, checksumIn});\n    } else {\n      LOG.debug(\"closing FileInputStream for \" + filename);\n      IOUtils.cleanup(LOG, dataIn, checksumIn);\n    }\n    if (slowReadBuff !\u003d null) {\n      bufferPool.returnBuffer(slowReadBuff);\n      slowReadBuff \u003d null;\n    }\n    if (checksumBuff !\u003d null) {\n      bufferPool.returnBuffer(checksumBuff);\n      checksumBuff \u003d null;\n    }\n    startOffset \u003d -1;\n    checksum \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "9a4030e0e84a688c12daa21fe9a165808c3eca70": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4356. BlockReaderLocal should use passed file descriptors rather than paths. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1432335 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/01/13 3:52 PM",
      "commitName": "9a4030e0e84a688c12daa21fe9a165808c3eca70",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4356. BlockReaderLocal should use passed file descriptors rather than paths. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1432335 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/01/13 3:52 PM",
          "commitName": "9a4030e0e84a688c12daa21fe9a165808c3eca70",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "09/01/13 1:34 PM",
          "commitNameOld": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 2.1,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,24 @@\n-  public synchronized void close(PeerCache peerCache) throws IOException {\n-    dataIn.close();\n-    if (checksumIn !\u003d null) {\n+  public synchronized void close(PeerCache peerCache,\n+      FileInputStreamCache fisCache) throws IOException {\n+    if (fisCache !\u003d null) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"putting FileInputStream for \" + filename +\n+            \" back into FileInputStreamCache\");\n+      }\n+      fisCache.put(datanodeID, block, new FileInputStream[] {dataIn, checksumIn});\n+    } else {\n+      LOG.debug(\"closing FileInputStream for \" + filename);\n+      dataIn.close();\n       checksumIn.close();\n     }\n     if (slowReadBuff !\u003d null) {\n       bufferPool.returnBuffer(slowReadBuff);\n       slowReadBuff \u003d null;\n     }\n     if (checksumBuff !\u003d null) {\n       bufferPool.returnBuffer(checksumBuff);\n       checksumBuff \u003d null;\n     }\n     startOffset \u003d -1;\n     checksum \u003d null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void close(PeerCache peerCache,\n      FileInputStreamCache fisCache) throws IOException {\n    if (fisCache !\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"putting FileInputStream for \" + filename +\n            \" back into FileInputStreamCache\");\n      }\n      fisCache.put(datanodeID, block, new FileInputStream[] {dataIn, checksumIn});\n    } else {\n      LOG.debug(\"closing FileInputStream for \" + filename);\n      dataIn.close();\n      checksumIn.close();\n    }\n    if (slowReadBuff !\u003d null) {\n      bufferPool.returnBuffer(slowReadBuff);\n      slowReadBuff \u003d null;\n    }\n    if (checksumBuff !\u003d null) {\n      bufferPool.returnBuffer(checksumBuff);\n      checksumBuff \u003d null;\n    }\n    startOffset \u003d -1;\n    checksum \u003d null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
          "extendedDetails": {
            "oldValue": "[peerCache-PeerCache]",
            "newValue": "[peerCache-PeerCache, fisCache-FileInputStreamCache]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4356. BlockReaderLocal should use passed file descriptors rather than paths. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1432335 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/01/13 3:52 PM",
          "commitName": "9a4030e0e84a688c12daa21fe9a165808c3eca70",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "09/01/13 1:34 PM",
          "commitNameOld": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 2.1,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,24 @@\n-  public synchronized void close(PeerCache peerCache) throws IOException {\n-    dataIn.close();\n-    if (checksumIn !\u003d null) {\n+  public synchronized void close(PeerCache peerCache,\n+      FileInputStreamCache fisCache) throws IOException {\n+    if (fisCache !\u003d null) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"putting FileInputStream for \" + filename +\n+            \" back into FileInputStreamCache\");\n+      }\n+      fisCache.put(datanodeID, block, new FileInputStream[] {dataIn, checksumIn});\n+    } else {\n+      LOG.debug(\"closing FileInputStream for \" + filename);\n+      dataIn.close();\n       checksumIn.close();\n     }\n     if (slowReadBuff !\u003d null) {\n       bufferPool.returnBuffer(slowReadBuff);\n       slowReadBuff \u003d null;\n     }\n     if (checksumBuff !\u003d null) {\n       bufferPool.returnBuffer(checksumBuff);\n       checksumBuff \u003d null;\n     }\n     startOffset \u003d -1;\n     checksum \u003d null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void close(PeerCache peerCache,\n      FileInputStreamCache fisCache) throws IOException {\n    if (fisCache !\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"putting FileInputStream for \" + filename +\n            \" back into FileInputStreamCache\");\n      }\n      fisCache.put(datanodeID, block, new FileInputStream[] {dataIn, checksumIn});\n    } else {\n      LOG.debug(\"closing FileInputStream for \" + filename);\n      dataIn.close();\n      checksumIn.close();\n    }\n    if (slowReadBuff !\u003d null) {\n      bufferPool.returnBuffer(slowReadBuff);\n      slowReadBuff \u003d null;\n    }\n    if (checksumBuff !\u003d null) {\n      bufferPool.returnBuffer(checksumBuff);\n      checksumBuff \u003d null;\n    }\n    startOffset \u003d -1;\n    checksum \u003d null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
          "extendedDetails": {}
        }
      ]
    },
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:34 PM",
      "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "08/01/13 6:39 PM",
      "commitNameOld": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n-  public synchronized void close() throws IOException {\n+  public synchronized void close(PeerCache peerCache) throws IOException {\n     dataIn.close();\n     if (checksumIn !\u003d null) {\n       checksumIn.close();\n     }\n     if (slowReadBuff !\u003d null) {\n       bufferPool.returnBuffer(slowReadBuff);\n       slowReadBuff \u003d null;\n     }\n     if (checksumBuff !\u003d null) {\n       bufferPool.returnBuffer(checksumBuff);\n       checksumBuff \u003d null;\n     }\n     startOffset \u003d -1;\n     checksum \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close(PeerCache peerCache) throws IOException {\n    dataIn.close();\n    if (checksumIn !\u003d null) {\n      checksumIn.close();\n    }\n    if (slowReadBuff !\u003d null) {\n      bufferPool.returnBuffer(slowReadBuff);\n      slowReadBuff \u003d null;\n    }\n    if (checksumBuff !\u003d null) {\n      bufferPool.returnBuffer(checksumBuff);\n      checksumBuff \u003d null;\n    }\n    startOffset \u003d -1;\n    checksum \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[peerCache-PeerCache]"
      }
    },
    "837e17b2eac1471d93e2eff395272063b265fee7": {
      "type": "Yparameterchange",
      "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 6:39 PM",
      "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/01/13 12:44 PM",
      "commitNameOld": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.25,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n-  public synchronized void close(PeerCache peerCache) throws IOException {\n+  public synchronized void close() throws IOException {\n     dataIn.close();\n     if (checksumIn !\u003d null) {\n       checksumIn.close();\n     }\n     if (slowReadBuff !\u003d null) {\n       bufferPool.returnBuffer(slowReadBuff);\n       slowReadBuff \u003d null;\n     }\n     if (checksumBuff !\u003d null) {\n       bufferPool.returnBuffer(checksumBuff);\n       checksumBuff \u003d null;\n     }\n     startOffset \u003d -1;\n     checksum \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    dataIn.close();\n    if (checksumIn !\u003d null) {\n      checksumIn.close();\n    }\n    if (slowReadBuff !\u003d null) {\n      bufferPool.returnBuffer(slowReadBuff);\n      slowReadBuff \u003d null;\n    }\n    if (checksumBuff !\u003d null) {\n      bufferPool.returnBuffer(checksumBuff);\n      checksumBuff \u003d null;\n    }\n    startOffset \u003d -1;\n    checksum \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {
        "oldValue": "[peerCache-PeerCache]",
        "newValue": "[]"
      }
    },
    "239b2742d0e80d13c970fd062af4930e672fe903": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 12:44 PM",
      "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "15/08/12 6:32 PM",
      "commitNameOld": "3cab01ba6e0349126a23063e135cd5c814a4ae18",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 145.8,
      "commitsBetweenForRepo": 754,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n-  public synchronized void close() throws IOException {\n+  public synchronized void close(PeerCache peerCache) throws IOException {\n     dataIn.close();\n     if (checksumIn !\u003d null) {\n       checksumIn.close();\n     }\n     if (slowReadBuff !\u003d null) {\n       bufferPool.returnBuffer(slowReadBuff);\n       slowReadBuff \u003d null;\n     }\n     if (checksumBuff !\u003d null) {\n       bufferPool.returnBuffer(checksumBuff);\n       checksumBuff \u003d null;\n     }\n     startOffset \u003d -1;\n     checksum \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close(PeerCache peerCache) throws IOException {\n    dataIn.close();\n    if (checksumIn !\u003d null) {\n      checksumIn.close();\n    }\n    if (slowReadBuff !\u003d null) {\n      bufferPool.returnBuffer(slowReadBuff);\n      slowReadBuff \u003d null;\n    }\n    if (checksumBuff !\u003d null) {\n      bufferPool.returnBuffer(checksumBuff);\n      checksumBuff \u003d null;\n    }\n    startOffset \u003d -1;\n    checksum \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[peerCache-PeerCache]"
      }
    },
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2834. Add a ByteBuffer-based read API to DFSInputStream. Contributed by Henry Robinson.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/03/12 10:30 AM",
      "commitName": "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "16/02/12 10:58 AM",
      "commitNameOld": "b8448dea82c72ff6c1558b9ebf3f24cd1c6e728b",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 33.94,
      "commitsBetweenForRepo": 229,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   public synchronized void close() throws IOException {\n     dataIn.close();\n     if (checksumIn !\u003d null) {\n       checksumIn.close();\n     }\n-    if (dataBuff !\u003d null) {\n-      bufferPool.returnBuffer(dataBuff);\n-      dataBuff \u003d null;\n+    if (slowReadBuff !\u003d null) {\n+      bufferPool.returnBuffer(slowReadBuff);\n+      slowReadBuff \u003d null;\n     }\n     if (checksumBuff !\u003d null) {\n       bufferPool.returnBuffer(checksumBuff);\n       checksumBuff \u003d null;\n     }\n     startOffset \u003d -1;\n     checksum \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    dataIn.close();\n    if (checksumIn !\u003d null) {\n      checksumIn.close();\n    }\n    if (slowReadBuff !\u003d null) {\n      bufferPool.returnBuffer(slowReadBuff);\n      slowReadBuff \u003d null;\n    }\n    if (checksumBuff !\u003d null) {\n      bufferPool.returnBuffer(checksumBuff);\n      checksumBuff \u003d null;\n    }\n    startOffset \u003d -1;\n    checksum \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2654. Make BlockReaderLocal not extend RemoteBlockReader2. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213592 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 12:09 AM",
      "commitName": "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "21/11/11 6:57 PM",
      "commitNameOld": "2ab10e29d9cca5018064be46a40e3c74423615a8",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 21.22,
      "commitsBetweenForRepo": 111,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,16 @@\n   public synchronized void close() throws IOException {\n     dataIn.close();\n     if (checksumIn !\u003d null) {\n       checksumIn.close();\n     }\n     if (dataBuff !\u003d null) {\n       bufferPool.returnBuffer(dataBuff);\n       dataBuff \u003d null;\n     }\n     if (checksumBuff !\u003d null) {\n       bufferPool.returnBuffer(checksumBuff);\n       checksumBuff \u003d null;\n     }\n-    super.close();\n+    startOffset \u003d -1;\n+    checksum \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    dataIn.close();\n    if (checksumIn !\u003d null) {\n      checksumIn.close();\n    }\n    if (dataBuff !\u003d null) {\n      bufferPool.returnBuffer(dataBuff);\n      dataBuff \u003d null;\n    }\n    if (checksumBuff !\u003d null) {\n      bufferPool.returnBuffer(checksumBuff);\n      checksumBuff \u003d null;\n    }\n    startOffset \u003d -1;\n    checksum \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "2ab10e29d9cca5018064be46a40e3c74423615a8": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2246. Enable reading a block directly from local file system for a client on the same node as the block file.  Contributed by Andrew Purtell, Suresh Srinivas and Jitendra Nath Pandey\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1204792 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/11 6:57 PM",
      "commitName": "2ab10e29d9cca5018064be46a40e3c74423615a8",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,15 @@\n+  public synchronized void close() throws IOException {\n+    dataIn.close();\n+    if (checksumIn !\u003d null) {\n+      checksumIn.close();\n+    }\n+    if (dataBuff !\u003d null) {\n+      bufferPool.returnBuffer(dataBuff);\n+      dataBuff \u003d null;\n+    }\n+    if (checksumBuff !\u003d null) {\n+      bufferPool.returnBuffer(checksumBuff);\n+      checksumBuff \u003d null;\n+    }\n+    super.close();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    dataIn.close();\n    if (checksumIn !\u003d null) {\n      checksumIn.close();\n    }\n    if (dataBuff !\u003d null) {\n      bufferPool.returnBuffer(dataBuff);\n      dataBuff \u003d null;\n    }\n    if (checksumBuff !\u003d null) {\n      bufferPool.returnBuffer(checksumBuff);\n      checksumBuff \u003d null;\n    }\n    super.close();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java"
    }
  }
}