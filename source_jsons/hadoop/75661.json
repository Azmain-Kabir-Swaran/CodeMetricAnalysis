{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "SequenceFile.java",
  "functionName": "merge",
  "functionId": "merge",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java",
  "functionStartLine": 3478,
  "functionEndLine": 3578,
  "numCommitsSeen": 46,
  "timeTaken": 3030,
  "changeHistory": [
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "0f6dfeeacbab65a31a33927a4eb84871d371fe52",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
    "7efb9640be26aabe3878310e82248a1b6b767a9a",
    "5128a9a453d64bfe1ed978cf9ffed27985eeef36"
  ],
  "changeHistoryShort": {
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "0f6dfeeacbab65a31a33927a4eb84871d371fe52": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yfilerename",
    "7efb9640be26aabe3878310e82248a1b6b767a9a": "Ybodychange",
    "5128a9a453d64bfe1ed978cf9ffed27985eeef36": "Yintroduced"
  },
  "changeHistoryDetails": {
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "      public RawKeyValueIterator merge() throws IOException {\n        //create the MergeStreams from the sorted map created in the constructor\n        //and dump the final output to a file\n        int numSegments \u003d sortedSegmentSizes.size();\n        int origFactor \u003d factor;\n        int passNo \u003d 1;\n        LocalDirAllocator lDirAlloc \u003d new LocalDirAllocator(\"io.seqfile.local.dir\");\n        do {\n          //get the factor for this pass of merge\n          factor \u003d getPassFactor(passNo, numSegments);\n          List\u003cSegmentDescriptor\u003e segmentsToMerge \u003d\n            new ArrayList\u003cSegmentDescriptor\u003e();\n          int segmentsConsidered \u003d 0;\n          int numSegmentsToConsider \u003d factor;\n          while (true) {\n            //extract the smallest \u0027factor\u0027 number of segment pointers from the \n            //TreeMap. Call cleanup on the empty segments (no key/value data)\n            SegmentDescriptor[] mStream \u003d \n              getSegmentDescriptors(numSegmentsToConsider);\n            for (int i \u003d 0; i \u003c mStream.length; i++) {\n              if (mStream[i].nextRawKey()) {\n                segmentsToMerge.add(mStream[i]);\n                segmentsConsidered++;\n                // Count the fact that we read some bytes in calling nextRawKey()\n                updateProgress(mStream[i].in.getPosition());\n              }\n              else {\n                mStream[i].cleanup();\n                numSegments--; //we ignore this segment for the merge\n              }\n            }\n            //if we have the desired number of segments\n            //or looked at all available segments, we break\n            if (segmentsConsidered \u003d\u003d factor || \n                sortedSegmentSizes.size() \u003d\u003d 0) {\n              break;\n            }\n              \n            numSegmentsToConsider \u003d factor - segmentsConsidered;\n          }\n          //feed the streams to the priority queue\n          initialize(segmentsToMerge.size()); clear();\n          for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n            put(segmentsToMerge.get(i));\n          }\n          //if we have lesser number of segments remaining, then just return the\n          //iterator, else do another single level merge\n          if (numSegments \u003c\u003d factor) {\n            //calculate the length of the remaining segments. Required for \n            //calculating the merge progress\n            long totalBytes \u003d 0;\n            for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n              totalBytes +\u003d segmentsToMerge.get(i).segmentLength;\n            }\n            if (totalBytes !\u003d 0) //being paranoid\n              progPerByte \u003d 1.0f / (float)totalBytes;\n            //reset factor to what it originally was\n            factor \u003d origFactor;\n            return this;\n          } else {\n            //we want to spread the creation of temp files on multiple disks if \n            //available under the space constraints\n            long approxOutputSize \u003d 0; \n            for (SegmentDescriptor s : segmentsToMerge) {\n              approxOutputSize +\u003d s.segmentLength + \n                                  ChecksumFileSystem.getApproxChkSumLength(\n                                  s.segmentLength);\n            }\n            Path tmpFilename \u003d \n              new Path(tmpDir, \"intermediate\").suffix(\".\" + passNo);\n\n            Path outputFile \u003d  lDirAlloc.getLocalPathForWrite(\n                                                tmpFilename.toString(),\n                                                approxOutputSize, conf);\n            if(LOG.isDebugEnabled()) { \n              LOG.debug(\"writing intermediate results to \" + outputFile);\n            }\n            Writer writer \u003d cloneFileAttributes(\n                                                fs.makeQualified(segmentsToMerge.get(0).segmentPathName), \n                                                fs.makeQualified(outputFile), null);\n            writer.sync \u003d null; //disable sync for temp files\n            writeFile(this, writer);\n            writer.close();\n            \n            //we finished one single level merge; now clean up the priority \n            //queue\n            this.close();\n            \n            SegmentDescriptor tempSegment \u003d \n              new SegmentDescriptor(0,\n                  fs.getFileStatus(outputFile).getLen(), outputFile);\n            //put the segment back in the TreeMap\n            sortedSegmentSizes.put(tempSegment, null);\n            numSegments \u003d sortedSegmentSizes.size();\n            passNo++;\n          }\n          //we are worried about only the first pass merge factor. So reset the \n          //factor to what it originally was\n          factor \u003d origFactor;\n        } while(true);\n      }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java",
      "extendedDetails": {
        "oldPath": "hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java",
        "newPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java"
      }
    },
    "0f6dfeeacbab65a31a33927a4eb84871d371fe52": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-6671. Use maven for hadoop common builds. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1153184 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/08/11 9:37 AM",
      "commitName": "0f6dfeeacbab65a31a33927a4eb84871d371fe52",
      "commitAuthor": "Thomas White",
      "commitDateOld": "01/08/11 3:53 PM",
      "commitNameOld": "9bac807cedbcff34e1a144fb475eff267e5ed86d",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.74,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "      public RawKeyValueIterator merge() throws IOException {\n        //create the MergeStreams from the sorted map created in the constructor\n        //and dump the final output to a file\n        int numSegments \u003d sortedSegmentSizes.size();\n        int origFactor \u003d factor;\n        int passNo \u003d 1;\n        LocalDirAllocator lDirAlloc \u003d new LocalDirAllocator(\"io.seqfile.local.dir\");\n        do {\n          //get the factor for this pass of merge\n          factor \u003d getPassFactor(passNo, numSegments);\n          List\u003cSegmentDescriptor\u003e segmentsToMerge \u003d\n            new ArrayList\u003cSegmentDescriptor\u003e();\n          int segmentsConsidered \u003d 0;\n          int numSegmentsToConsider \u003d factor;\n          while (true) {\n            //extract the smallest \u0027factor\u0027 number of segment pointers from the \n            //TreeMap. Call cleanup on the empty segments (no key/value data)\n            SegmentDescriptor[] mStream \u003d \n              getSegmentDescriptors(numSegmentsToConsider);\n            for (int i \u003d 0; i \u003c mStream.length; i++) {\n              if (mStream[i].nextRawKey()) {\n                segmentsToMerge.add(mStream[i]);\n                segmentsConsidered++;\n                // Count the fact that we read some bytes in calling nextRawKey()\n                updateProgress(mStream[i].in.getPosition());\n              }\n              else {\n                mStream[i].cleanup();\n                numSegments--; //we ignore this segment for the merge\n              }\n            }\n            //if we have the desired number of segments\n            //or looked at all available segments, we break\n            if (segmentsConsidered \u003d\u003d factor || \n                sortedSegmentSizes.size() \u003d\u003d 0) {\n              break;\n            }\n              \n            numSegmentsToConsider \u003d factor - segmentsConsidered;\n          }\n          //feed the streams to the priority queue\n          initialize(segmentsToMerge.size()); clear();\n          for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n            put(segmentsToMerge.get(i));\n          }\n          //if we have lesser number of segments remaining, then just return the\n          //iterator, else do another single level merge\n          if (numSegments \u003c\u003d factor) {\n            //calculate the length of the remaining segments. Required for \n            //calculating the merge progress\n            long totalBytes \u003d 0;\n            for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n              totalBytes +\u003d segmentsToMerge.get(i).segmentLength;\n            }\n            if (totalBytes !\u003d 0) //being paranoid\n              progPerByte \u003d 1.0f / (float)totalBytes;\n            //reset factor to what it originally was\n            factor \u003d origFactor;\n            return this;\n          } else {\n            //we want to spread the creation of temp files on multiple disks if \n            //available under the space constraints\n            long approxOutputSize \u003d 0; \n            for (SegmentDescriptor s : segmentsToMerge) {\n              approxOutputSize +\u003d s.segmentLength + \n                                  ChecksumFileSystem.getApproxChkSumLength(\n                                  s.segmentLength);\n            }\n            Path tmpFilename \u003d \n              new Path(tmpDir, \"intermediate\").suffix(\".\" + passNo);\n\n            Path outputFile \u003d  lDirAlloc.getLocalPathForWrite(\n                                                tmpFilename.toString(),\n                                                approxOutputSize, conf);\n            if(LOG.isDebugEnabled()) { \n              LOG.debug(\"writing intermediate results to \" + outputFile);\n            }\n            Writer writer \u003d cloneFileAttributes(\n                                                fs.makeQualified(segmentsToMerge.get(0).segmentPathName), \n                                                fs.makeQualified(outputFile), null);\n            writer.sync \u003d null; //disable sync for temp files\n            writeFile(this, writer);\n            writer.close();\n            \n            //we finished one single level merge; now clean up the priority \n            //queue\n            this.close();\n            \n            SegmentDescriptor tempSegment \u003d \n              new SegmentDescriptor(0,\n                  fs.getFileStatus(outputFile).getLen(), outputFile);\n            //put the segment back in the TreeMap\n            sortedSegmentSizes.put(tempSegment, null);\n            numSegments \u003d sortedSegmentSizes.size();\n            passNo++;\n          }\n          //we are worried about only the first pass merge factor. So reset the \n          //factor to what it originally was\n          factor \u003d origFactor;\n        } while(true);\n      }",
      "path": "hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java",
      "extendedDetails": {
        "oldPath": "common/src/java/org/apache/hadoop/io/SequenceFile.java",
        "newPath": "hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "11/06/11 9:13 PM",
      "commitNameOld": "a285fb5effe9ba3be4ec5f942afaf5ddd1186151",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.74,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "      public RawKeyValueIterator merge() throws IOException {\n        //create the MergeStreams from the sorted map created in the constructor\n        //and dump the final output to a file\n        int numSegments \u003d sortedSegmentSizes.size();\n        int origFactor \u003d factor;\n        int passNo \u003d 1;\n        LocalDirAllocator lDirAlloc \u003d new LocalDirAllocator(\"io.seqfile.local.dir\");\n        do {\n          //get the factor for this pass of merge\n          factor \u003d getPassFactor(passNo, numSegments);\n          List\u003cSegmentDescriptor\u003e segmentsToMerge \u003d\n            new ArrayList\u003cSegmentDescriptor\u003e();\n          int segmentsConsidered \u003d 0;\n          int numSegmentsToConsider \u003d factor;\n          while (true) {\n            //extract the smallest \u0027factor\u0027 number of segment pointers from the \n            //TreeMap. Call cleanup on the empty segments (no key/value data)\n            SegmentDescriptor[] mStream \u003d \n              getSegmentDescriptors(numSegmentsToConsider);\n            for (int i \u003d 0; i \u003c mStream.length; i++) {\n              if (mStream[i].nextRawKey()) {\n                segmentsToMerge.add(mStream[i]);\n                segmentsConsidered++;\n                // Count the fact that we read some bytes in calling nextRawKey()\n                updateProgress(mStream[i].in.getPosition());\n              }\n              else {\n                mStream[i].cleanup();\n                numSegments--; //we ignore this segment for the merge\n              }\n            }\n            //if we have the desired number of segments\n            //or looked at all available segments, we break\n            if (segmentsConsidered \u003d\u003d factor || \n                sortedSegmentSizes.size() \u003d\u003d 0) {\n              break;\n            }\n              \n            numSegmentsToConsider \u003d factor - segmentsConsidered;\n          }\n          //feed the streams to the priority queue\n          initialize(segmentsToMerge.size()); clear();\n          for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n            put(segmentsToMerge.get(i));\n          }\n          //if we have lesser number of segments remaining, then just return the\n          //iterator, else do another single level merge\n          if (numSegments \u003c\u003d factor) {\n            //calculate the length of the remaining segments. Required for \n            //calculating the merge progress\n            long totalBytes \u003d 0;\n            for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n              totalBytes +\u003d segmentsToMerge.get(i).segmentLength;\n            }\n            if (totalBytes !\u003d 0) //being paranoid\n              progPerByte \u003d 1.0f / (float)totalBytes;\n            //reset factor to what it originally was\n            factor \u003d origFactor;\n            return this;\n          } else {\n            //we want to spread the creation of temp files on multiple disks if \n            //available under the space constraints\n            long approxOutputSize \u003d 0; \n            for (SegmentDescriptor s : segmentsToMerge) {\n              approxOutputSize +\u003d s.segmentLength + \n                                  ChecksumFileSystem.getApproxChkSumLength(\n                                  s.segmentLength);\n            }\n            Path tmpFilename \u003d \n              new Path(tmpDir, \"intermediate\").suffix(\".\" + passNo);\n\n            Path outputFile \u003d  lDirAlloc.getLocalPathForWrite(\n                                                tmpFilename.toString(),\n                                                approxOutputSize, conf);\n            if(LOG.isDebugEnabled()) { \n              LOG.debug(\"writing intermediate results to \" + outputFile);\n            }\n            Writer writer \u003d cloneFileAttributes(\n                                                fs.makeQualified(segmentsToMerge.get(0).segmentPathName), \n                                                fs.makeQualified(outputFile), null);\n            writer.sync \u003d null; //disable sync for temp files\n            writeFile(this, writer);\n            writer.close();\n            \n            //we finished one single level merge; now clean up the priority \n            //queue\n            this.close();\n            \n            SegmentDescriptor tempSegment \u003d \n              new SegmentDescriptor(0,\n                  fs.getFileStatus(outputFile).getLen(), outputFile);\n            //put the segment back in the TreeMap\n            sortedSegmentSizes.put(tempSegment, null);\n            numSegments \u003d sortedSegmentSizes.size();\n            passNo++;\n          }\n          //we are worried about only the first pass merge factor. So reset the \n          //factor to what it originally was\n          factor \u003d origFactor;\n        } while(true);\n      }",
      "path": "common/src/java/org/apache/hadoop/io/SequenceFile.java",
      "extendedDetails": {
        "oldPath": "src/java/org/apache/hadoop/io/SequenceFile.java",
        "newPath": "common/src/java/org/apache/hadoop/io/SequenceFile.java"
      }
    },
    "7efb9640be26aabe3878310e82248a1b6b767a9a": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-6884. Add LOG.isDebugEnabled() guard for each LOG.debug(..).  Contributed by Erik Steffl\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@990460 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/08/10 3:44 PM",
      "commitName": "7efb9640be26aabe3878310e82248a1b6b767a9a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "11/06/10 2:34 PM",
      "commitNameOld": "6378822a67c0baa502d22201f5c2b478cbe1261c",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 78.05,
      "commitsBetweenForRepo": 66,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,99 +1,101 @@\n       public RawKeyValueIterator merge() throws IOException {\n         //create the MergeStreams from the sorted map created in the constructor\n         //and dump the final output to a file\n         int numSegments \u003d sortedSegmentSizes.size();\n         int origFactor \u003d factor;\n         int passNo \u003d 1;\n         LocalDirAllocator lDirAlloc \u003d new LocalDirAllocator(\"io.seqfile.local.dir\");\n         do {\n           //get the factor for this pass of merge\n           factor \u003d getPassFactor(passNo, numSegments);\n           List\u003cSegmentDescriptor\u003e segmentsToMerge \u003d\n             new ArrayList\u003cSegmentDescriptor\u003e();\n           int segmentsConsidered \u003d 0;\n           int numSegmentsToConsider \u003d factor;\n           while (true) {\n             //extract the smallest \u0027factor\u0027 number of segment pointers from the \n             //TreeMap. Call cleanup on the empty segments (no key/value data)\n             SegmentDescriptor[] mStream \u003d \n               getSegmentDescriptors(numSegmentsToConsider);\n             for (int i \u003d 0; i \u003c mStream.length; i++) {\n               if (mStream[i].nextRawKey()) {\n                 segmentsToMerge.add(mStream[i]);\n                 segmentsConsidered++;\n                 // Count the fact that we read some bytes in calling nextRawKey()\n                 updateProgress(mStream[i].in.getPosition());\n               }\n               else {\n                 mStream[i].cleanup();\n                 numSegments--; //we ignore this segment for the merge\n               }\n             }\n             //if we have the desired number of segments\n             //or looked at all available segments, we break\n             if (segmentsConsidered \u003d\u003d factor || \n                 sortedSegmentSizes.size() \u003d\u003d 0) {\n               break;\n             }\n               \n             numSegmentsToConsider \u003d factor - segmentsConsidered;\n           }\n           //feed the streams to the priority queue\n           initialize(segmentsToMerge.size()); clear();\n           for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n             put(segmentsToMerge.get(i));\n           }\n           //if we have lesser number of segments remaining, then just return the\n           //iterator, else do another single level merge\n           if (numSegments \u003c\u003d factor) {\n             //calculate the length of the remaining segments. Required for \n             //calculating the merge progress\n             long totalBytes \u003d 0;\n             for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n               totalBytes +\u003d segmentsToMerge.get(i).segmentLength;\n             }\n             if (totalBytes !\u003d 0) //being paranoid\n               progPerByte \u003d 1.0f / (float)totalBytes;\n             //reset factor to what it originally was\n             factor \u003d origFactor;\n             return this;\n           } else {\n             //we want to spread the creation of temp files on multiple disks if \n             //available under the space constraints\n             long approxOutputSize \u003d 0; \n             for (SegmentDescriptor s : segmentsToMerge) {\n               approxOutputSize +\u003d s.segmentLength + \n                                   ChecksumFileSystem.getApproxChkSumLength(\n                                   s.segmentLength);\n             }\n             Path tmpFilename \u003d \n               new Path(tmpDir, \"intermediate\").suffix(\".\" + passNo);\n \n             Path outputFile \u003d  lDirAlloc.getLocalPathForWrite(\n                                                 tmpFilename.toString(),\n                                                 approxOutputSize, conf);\n-            LOG.debug(\"writing intermediate results to \" + outputFile);\n+            if(LOG.isDebugEnabled()) { \n+              LOG.debug(\"writing intermediate results to \" + outputFile);\n+            }\n             Writer writer \u003d cloneFileAttributes(\n                                                 fs.makeQualified(segmentsToMerge.get(0).segmentPathName), \n                                                 fs.makeQualified(outputFile), null);\n             writer.sync \u003d null; //disable sync for temp files\n             writeFile(this, writer);\n             writer.close();\n             \n             //we finished one single level merge; now clean up the priority \n             //queue\n             this.close();\n             \n             SegmentDescriptor tempSegment \u003d \n               new SegmentDescriptor(0,\n                   fs.getFileStatus(outputFile).getLen(), outputFile);\n             //put the segment back in the TreeMap\n             sortedSegmentSizes.put(tempSegment, null);\n             numSegments \u003d sortedSegmentSizes.size();\n             passNo++;\n           }\n           //we are worried about only the first pass merge factor. So reset the \n           //factor to what it originally was\n           factor \u003d origFactor;\n         } while(true);\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public RawKeyValueIterator merge() throws IOException {\n        //create the MergeStreams from the sorted map created in the constructor\n        //and dump the final output to a file\n        int numSegments \u003d sortedSegmentSizes.size();\n        int origFactor \u003d factor;\n        int passNo \u003d 1;\n        LocalDirAllocator lDirAlloc \u003d new LocalDirAllocator(\"io.seqfile.local.dir\");\n        do {\n          //get the factor for this pass of merge\n          factor \u003d getPassFactor(passNo, numSegments);\n          List\u003cSegmentDescriptor\u003e segmentsToMerge \u003d\n            new ArrayList\u003cSegmentDescriptor\u003e();\n          int segmentsConsidered \u003d 0;\n          int numSegmentsToConsider \u003d factor;\n          while (true) {\n            //extract the smallest \u0027factor\u0027 number of segment pointers from the \n            //TreeMap. Call cleanup on the empty segments (no key/value data)\n            SegmentDescriptor[] mStream \u003d \n              getSegmentDescriptors(numSegmentsToConsider);\n            for (int i \u003d 0; i \u003c mStream.length; i++) {\n              if (mStream[i].nextRawKey()) {\n                segmentsToMerge.add(mStream[i]);\n                segmentsConsidered++;\n                // Count the fact that we read some bytes in calling nextRawKey()\n                updateProgress(mStream[i].in.getPosition());\n              }\n              else {\n                mStream[i].cleanup();\n                numSegments--; //we ignore this segment for the merge\n              }\n            }\n            //if we have the desired number of segments\n            //or looked at all available segments, we break\n            if (segmentsConsidered \u003d\u003d factor || \n                sortedSegmentSizes.size() \u003d\u003d 0) {\n              break;\n            }\n              \n            numSegmentsToConsider \u003d factor - segmentsConsidered;\n          }\n          //feed the streams to the priority queue\n          initialize(segmentsToMerge.size()); clear();\n          for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n            put(segmentsToMerge.get(i));\n          }\n          //if we have lesser number of segments remaining, then just return the\n          //iterator, else do another single level merge\n          if (numSegments \u003c\u003d factor) {\n            //calculate the length of the remaining segments. Required for \n            //calculating the merge progress\n            long totalBytes \u003d 0;\n            for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n              totalBytes +\u003d segmentsToMerge.get(i).segmentLength;\n            }\n            if (totalBytes !\u003d 0) //being paranoid\n              progPerByte \u003d 1.0f / (float)totalBytes;\n            //reset factor to what it originally was\n            factor \u003d origFactor;\n            return this;\n          } else {\n            //we want to spread the creation of temp files on multiple disks if \n            //available under the space constraints\n            long approxOutputSize \u003d 0; \n            for (SegmentDescriptor s : segmentsToMerge) {\n              approxOutputSize +\u003d s.segmentLength + \n                                  ChecksumFileSystem.getApproxChkSumLength(\n                                  s.segmentLength);\n            }\n            Path tmpFilename \u003d \n              new Path(tmpDir, \"intermediate\").suffix(\".\" + passNo);\n\n            Path outputFile \u003d  lDirAlloc.getLocalPathForWrite(\n                                                tmpFilename.toString(),\n                                                approxOutputSize, conf);\n            if(LOG.isDebugEnabled()) { \n              LOG.debug(\"writing intermediate results to \" + outputFile);\n            }\n            Writer writer \u003d cloneFileAttributes(\n                                                fs.makeQualified(segmentsToMerge.get(0).segmentPathName), \n                                                fs.makeQualified(outputFile), null);\n            writer.sync \u003d null; //disable sync for temp files\n            writeFile(this, writer);\n            writer.close();\n            \n            //we finished one single level merge; now clean up the priority \n            //queue\n            this.close();\n            \n            SegmentDescriptor tempSegment \u003d \n              new SegmentDescriptor(0,\n                  fs.getFileStatus(outputFile).getLen(), outputFile);\n            //put the segment back in the TreeMap\n            sortedSegmentSizes.put(tempSegment, null);\n            numSegments \u003d sortedSegmentSizes.size();\n            passNo++;\n          }\n          //we are worried about only the first pass merge factor. So reset the \n          //factor to what it originally was\n          factor \u003d origFactor;\n        } while(true);\n      }",
      "path": "src/java/org/apache/hadoop/io/SequenceFile.java",
      "extendedDetails": {}
    },
    "5128a9a453d64bfe1ed978cf9ffed27985eeef36": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-4687 Moving src directories on branch\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/core/branches/HADOOP-4687/core@776174 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/05/09 9:20 PM",
      "commitName": "5128a9a453d64bfe1ed978cf9ffed27985eeef36",
      "commitAuthor": "Owen O\u0027Malley",
      "diff": "@@ -0,0 +1,99 @@\n+      public RawKeyValueIterator merge() throws IOException {\n+        //create the MergeStreams from the sorted map created in the constructor\n+        //and dump the final output to a file\n+        int numSegments \u003d sortedSegmentSizes.size();\n+        int origFactor \u003d factor;\n+        int passNo \u003d 1;\n+        LocalDirAllocator lDirAlloc \u003d new LocalDirAllocator(\"io.seqfile.local.dir\");\n+        do {\n+          //get the factor for this pass of merge\n+          factor \u003d getPassFactor(passNo, numSegments);\n+          List\u003cSegmentDescriptor\u003e segmentsToMerge \u003d\n+            new ArrayList\u003cSegmentDescriptor\u003e();\n+          int segmentsConsidered \u003d 0;\n+          int numSegmentsToConsider \u003d factor;\n+          while (true) {\n+            //extract the smallest \u0027factor\u0027 number of segment pointers from the \n+            //TreeMap. Call cleanup on the empty segments (no key/value data)\n+            SegmentDescriptor[] mStream \u003d \n+              getSegmentDescriptors(numSegmentsToConsider);\n+            for (int i \u003d 0; i \u003c mStream.length; i++) {\n+              if (mStream[i].nextRawKey()) {\n+                segmentsToMerge.add(mStream[i]);\n+                segmentsConsidered++;\n+                // Count the fact that we read some bytes in calling nextRawKey()\n+                updateProgress(mStream[i].in.getPosition());\n+              }\n+              else {\n+                mStream[i].cleanup();\n+                numSegments--; //we ignore this segment for the merge\n+              }\n+            }\n+            //if we have the desired number of segments\n+            //or looked at all available segments, we break\n+            if (segmentsConsidered \u003d\u003d factor || \n+                sortedSegmentSizes.size() \u003d\u003d 0) {\n+              break;\n+            }\n+              \n+            numSegmentsToConsider \u003d factor - segmentsConsidered;\n+          }\n+          //feed the streams to the priority queue\n+          initialize(segmentsToMerge.size()); clear();\n+          for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n+            put(segmentsToMerge.get(i));\n+          }\n+          //if we have lesser number of segments remaining, then just return the\n+          //iterator, else do another single level merge\n+          if (numSegments \u003c\u003d factor) {\n+            //calculate the length of the remaining segments. Required for \n+            //calculating the merge progress\n+            long totalBytes \u003d 0;\n+            for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n+              totalBytes +\u003d segmentsToMerge.get(i).segmentLength;\n+            }\n+            if (totalBytes !\u003d 0) //being paranoid\n+              progPerByte \u003d 1.0f / (float)totalBytes;\n+            //reset factor to what it originally was\n+            factor \u003d origFactor;\n+            return this;\n+          } else {\n+            //we want to spread the creation of temp files on multiple disks if \n+            //available under the space constraints\n+            long approxOutputSize \u003d 0; \n+            for (SegmentDescriptor s : segmentsToMerge) {\n+              approxOutputSize +\u003d s.segmentLength + \n+                                  ChecksumFileSystem.getApproxChkSumLength(\n+                                  s.segmentLength);\n+            }\n+            Path tmpFilename \u003d \n+              new Path(tmpDir, \"intermediate\").suffix(\".\" + passNo);\n+\n+            Path outputFile \u003d  lDirAlloc.getLocalPathForWrite(\n+                                                tmpFilename.toString(),\n+                                                approxOutputSize, conf);\n+            LOG.debug(\"writing intermediate results to \" + outputFile);\n+            Writer writer \u003d cloneFileAttributes(\n+                                                fs.makeQualified(segmentsToMerge.get(0).segmentPathName), \n+                                                fs.makeQualified(outputFile), null);\n+            writer.sync \u003d null; //disable sync for temp files\n+            writeFile(this, writer);\n+            writer.close();\n+            \n+            //we finished one single level merge; now clean up the priority \n+            //queue\n+            this.close();\n+            \n+            SegmentDescriptor tempSegment \u003d \n+              new SegmentDescriptor(0,\n+                  fs.getFileStatus(outputFile).getLen(), outputFile);\n+            //put the segment back in the TreeMap\n+            sortedSegmentSizes.put(tempSegment, null);\n+            numSegments \u003d sortedSegmentSizes.size();\n+            passNo++;\n+          }\n+          //we are worried about only the first pass merge factor. So reset the \n+          //factor to what it originally was\n+          factor \u003d origFactor;\n+        } while(true);\n+      }\n\\ No newline at end of file\n",
      "actualSource": "      public RawKeyValueIterator merge() throws IOException {\n        //create the MergeStreams from the sorted map created in the constructor\n        //and dump the final output to a file\n        int numSegments \u003d sortedSegmentSizes.size();\n        int origFactor \u003d factor;\n        int passNo \u003d 1;\n        LocalDirAllocator lDirAlloc \u003d new LocalDirAllocator(\"io.seqfile.local.dir\");\n        do {\n          //get the factor for this pass of merge\n          factor \u003d getPassFactor(passNo, numSegments);\n          List\u003cSegmentDescriptor\u003e segmentsToMerge \u003d\n            new ArrayList\u003cSegmentDescriptor\u003e();\n          int segmentsConsidered \u003d 0;\n          int numSegmentsToConsider \u003d factor;\n          while (true) {\n            //extract the smallest \u0027factor\u0027 number of segment pointers from the \n            //TreeMap. Call cleanup on the empty segments (no key/value data)\n            SegmentDescriptor[] mStream \u003d \n              getSegmentDescriptors(numSegmentsToConsider);\n            for (int i \u003d 0; i \u003c mStream.length; i++) {\n              if (mStream[i].nextRawKey()) {\n                segmentsToMerge.add(mStream[i]);\n                segmentsConsidered++;\n                // Count the fact that we read some bytes in calling nextRawKey()\n                updateProgress(mStream[i].in.getPosition());\n              }\n              else {\n                mStream[i].cleanup();\n                numSegments--; //we ignore this segment for the merge\n              }\n            }\n            //if we have the desired number of segments\n            //or looked at all available segments, we break\n            if (segmentsConsidered \u003d\u003d factor || \n                sortedSegmentSizes.size() \u003d\u003d 0) {\n              break;\n            }\n              \n            numSegmentsToConsider \u003d factor - segmentsConsidered;\n          }\n          //feed the streams to the priority queue\n          initialize(segmentsToMerge.size()); clear();\n          for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n            put(segmentsToMerge.get(i));\n          }\n          //if we have lesser number of segments remaining, then just return the\n          //iterator, else do another single level merge\n          if (numSegments \u003c\u003d factor) {\n            //calculate the length of the remaining segments. Required for \n            //calculating the merge progress\n            long totalBytes \u003d 0;\n            for (int i \u003d 0; i \u003c segmentsToMerge.size(); i++) {\n              totalBytes +\u003d segmentsToMerge.get(i).segmentLength;\n            }\n            if (totalBytes !\u003d 0) //being paranoid\n              progPerByte \u003d 1.0f / (float)totalBytes;\n            //reset factor to what it originally was\n            factor \u003d origFactor;\n            return this;\n          } else {\n            //we want to spread the creation of temp files on multiple disks if \n            //available under the space constraints\n            long approxOutputSize \u003d 0; \n            for (SegmentDescriptor s : segmentsToMerge) {\n              approxOutputSize +\u003d s.segmentLength + \n                                  ChecksumFileSystem.getApproxChkSumLength(\n                                  s.segmentLength);\n            }\n            Path tmpFilename \u003d \n              new Path(tmpDir, \"intermediate\").suffix(\".\" + passNo);\n\n            Path outputFile \u003d  lDirAlloc.getLocalPathForWrite(\n                                                tmpFilename.toString(),\n                                                approxOutputSize, conf);\n            LOG.debug(\"writing intermediate results to \" + outputFile);\n            Writer writer \u003d cloneFileAttributes(\n                                                fs.makeQualified(segmentsToMerge.get(0).segmentPathName), \n                                                fs.makeQualified(outputFile), null);\n            writer.sync \u003d null; //disable sync for temp files\n            writeFile(this, writer);\n            writer.close();\n            \n            //we finished one single level merge; now clean up the priority \n            //queue\n            this.close();\n            \n            SegmentDescriptor tempSegment \u003d \n              new SegmentDescriptor(0,\n                  fs.getFileStatus(outputFile).getLen(), outputFile);\n            //put the segment back in the TreeMap\n            sortedSegmentSizes.put(tempSegment, null);\n            numSegments \u003d sortedSegmentSizes.size();\n            passNo++;\n          }\n          //we are worried about only the first pass merge factor. So reset the \n          //factor to what it originally was\n          factor \u003d origFactor;\n        } while(true);\n      }",
      "path": "src/java/org/apache/hadoop/io/SequenceFile.java"
    }
  }
}