{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HadoopArchives.java",
  "functionName": "reduce",
  "functionId": "reduce___key-IntWritable__values-Iterator__Text____out-OutputCollector__Text,Text____reporter-Reporter",
  "sourceFilePath": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
  "functionStartLine": 762,
  "functionEndLine": 784,
  "numCommitsSeen": 18,
  "timeTaken": 5152,
  "changeHistory": [
    "c89977f89cb4520164c1747fe1abbaad215c42a0",
    "0201be46c298e94176ec6297e9d9cdba3afc2bbd",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "c89977f89cb4520164c1747fe1abbaad215c42a0": "Ybodychange",
    "0201be46c298e94176ec6297e9d9cdba3afc2bbd": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c89977f89cb4520164c1747fe1abbaad215c42a0": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11529. Fix findbugs warnings in hadoop-archives. Contributed by Masatake Iwasaki.\n",
      "commitDate": "03/02/15 10:53 AM",
      "commitName": "c89977f89cb4520164c1747fe1abbaad215c42a0",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "18/11/14 5:05 PM",
      "commitNameOld": "79301e80d7510f055c01a06970bb409607a4197c",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 76.74,
      "commitsBetweenForRepo": 464,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n     public void reduce(IntWritable key, Iterator\u003cText\u003e values,\n         OutputCollector\u003cText, Text\u003e out,\n         Reporter reporter) throws IOException {\n       keyVal \u003d key.get();\n       while(values.hasNext()) {\n         Text value \u003d values.next();\n         String towrite \u003d value.toString() + \"\\n\";\n-        indexStream.write(towrite.getBytes());\n+        indexStream.write(towrite.getBytes(Charsets.UTF_8));\n         written++;\n         if (written \u003e numIndexes -1) {\n           // every 1000 indexes we report status\n           reporter.setStatus(\"Creating index for archives\");\n           reporter.progress();\n           endIndex \u003d keyVal;\n           String masterWrite \u003d startIndex + \" \" + endIndex + \" \" + startPos \n                               +  \" \" + indexStream.getPos() + \" \\n\" ;\n-          outStream.write(masterWrite.getBytes());\n+          outStream.write(masterWrite.getBytes(Charsets.UTF_8));\n           startPos \u003d indexStream.getPos();\n           startIndex \u003d endIndex;\n           written \u003d 0;\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void reduce(IntWritable key, Iterator\u003cText\u003e values,\n        OutputCollector\u003cText, Text\u003e out,\n        Reporter reporter) throws IOException {\n      keyVal \u003d key.get();\n      while(values.hasNext()) {\n        Text value \u003d values.next();\n        String towrite \u003d value.toString() + \"\\n\";\n        indexStream.write(towrite.getBytes(Charsets.UTF_8));\n        written++;\n        if (written \u003e numIndexes -1) {\n          // every 1000 indexes we report status\n          reporter.setStatus(\"Creating index for archives\");\n          reporter.progress();\n          endIndex \u003d keyVal;\n          String masterWrite \u003d startIndex + \" \" + endIndex + \" \" + startPos \n                              +  \" \" + indexStream.getPos() + \" \\n\" ;\n          outStream.write(masterWrite.getBytes(Charsets.UTF_8));\n          startPos \u003d indexStream.getPos();\n          startIndex \u003d endIndex;\n          written \u003d 0;\n        }\n      }\n    }",
      "path": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {}
    },
    "0201be46c298e94176ec6297e9d9cdba3afc2bbd": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7810. move hadoop archive to core from tools. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213907 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 12:17 PM",
      "commitName": "0201be46c298e94176ec6297e9d9cdba3afc2bbd",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "13/12/11 10:07 AM",
      "commitNameOld": "f2f4e9341387199e04679ebc8de5e05c0fdbd437",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.09,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void reduce(IntWritable key, Iterator\u003cText\u003e values,\n        OutputCollector\u003cText, Text\u003e out,\n        Reporter reporter) throws IOException {\n      keyVal \u003d key.get();\n      while(values.hasNext()) {\n        Text value \u003d values.next();\n        String towrite \u003d value.toString() + \"\\n\";\n        indexStream.write(towrite.getBytes());\n        written++;\n        if (written \u003e numIndexes -1) {\n          // every 1000 indexes we report status\n          reporter.setStatus(\"Creating index for archives\");\n          reporter.progress();\n          endIndex \u003d keyVal;\n          String masterWrite \u003d startIndex + \" \" + endIndex + \" \" + startPos \n                              +  \" \" + indexStream.getPos() + \" \\n\" ;\n          outStream.write(masterWrite.getBytes());\n          startPos \u003d indexStream.getPos();\n          startIndex \u003d endIndex;\n          written \u003d 0;\n        }\n      }\n    }",
      "path": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
        "newPath": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void reduce(IntWritable key, Iterator\u003cText\u003e values,\n        OutputCollector\u003cText, Text\u003e out,\n        Reporter reporter) throws IOException {\n      keyVal \u003d key.get();\n      while(values.hasNext()) {\n        Text value \u003d values.next();\n        String towrite \u003d value.toString() + \"\\n\";\n        indexStream.write(towrite.getBytes());\n        written++;\n        if (written \u003e numIndexes -1) {\n          // every 1000 indexes we report status\n          reporter.setStatus(\"Creating index for archives\");\n          reporter.progress();\n          endIndex \u003d keyVal;\n          String masterWrite \u003d startIndex + \" \" + endIndex + \" \" + startPos \n                              +  \" \" + indexStream.getPos() + \" \\n\" ;\n          outStream.write(masterWrite.getBytes());\n          startPos \u003d indexStream.getPos();\n          startIndex \u003d endIndex;\n          written \u003d 0;\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
        "newPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/HadoopArchives.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void reduce(IntWritable key, Iterator\u003cText\u003e values,\n        OutputCollector\u003cText, Text\u003e out,\n        Reporter reporter) throws IOException {\n      keyVal \u003d key.get();\n      while(values.hasNext()) {\n        Text value \u003d values.next();\n        String towrite \u003d value.toString() + \"\\n\";\n        indexStream.write(towrite.getBytes());\n        written++;\n        if (written \u003e numIndexes -1) {\n          // every 1000 indexes we report status\n          reporter.setStatus(\"Creating index for archives\");\n          reporter.progress();\n          endIndex \u003d keyVal;\n          String masterWrite \u003d startIndex + \" \" + endIndex + \" \" + startPos \n                              +  \" \" + indexStream.getPos() + \" \\n\" ;\n          outStream.write(masterWrite.getBytes());\n          startPos \u003d indexStream.getPos();\n          startIndex \u003d endIndex;\n          written \u003d 0;\n        }\n      }\n    }",
      "path": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
        "newPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,23 @@\n+    public void reduce(IntWritable key, Iterator\u003cText\u003e values,\n+        OutputCollector\u003cText, Text\u003e out,\n+        Reporter reporter) throws IOException {\n+      keyVal \u003d key.get();\n+      while(values.hasNext()) {\n+        Text value \u003d values.next();\n+        String towrite \u003d value.toString() + \"\\n\";\n+        indexStream.write(towrite.getBytes());\n+        written++;\n+        if (written \u003e numIndexes -1) {\n+          // every 1000 indexes we report status\n+          reporter.setStatus(\"Creating index for archives\");\n+          reporter.progress();\n+          endIndex \u003d keyVal;\n+          String masterWrite \u003d startIndex + \" \" + endIndex + \" \" + startPos \n+                              +  \" \" + indexStream.getPos() + \" \\n\" ;\n+          outStream.write(masterWrite.getBytes());\n+          startPos \u003d indexStream.getPos();\n+          startIndex \u003d endIndex;\n+          written \u003d 0;\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public void reduce(IntWritable key, Iterator\u003cText\u003e values,\n        OutputCollector\u003cText, Text\u003e out,\n        Reporter reporter) throws IOException {\n      keyVal \u003d key.get();\n      while(values.hasNext()) {\n        Text value \u003d values.next();\n        String towrite \u003d value.toString() + \"\\n\";\n        indexStream.write(towrite.getBytes());\n        written++;\n        if (written \u003e numIndexes -1) {\n          // every 1000 indexes we report status\n          reporter.setStatus(\"Creating index for archives\");\n          reporter.progress();\n          endIndex \u003d keyVal;\n          String masterWrite \u003d startIndex + \" \" + endIndex + \" \" + startPos \n                              +  \" \" + indexStream.getPos() + \" \\n\" ;\n          outStream.write(masterWrite.getBytes());\n          startPos \u003d indexStream.getPos();\n          startIndex \u003d endIndex;\n          written \u003d 0;\n        }\n      }\n    }",
      "path": "mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java"
    }
  }
}