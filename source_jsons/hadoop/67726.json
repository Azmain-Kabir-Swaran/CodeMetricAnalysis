{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CopyCommitter.java",
  "functionName": "concatFileChunks",
  "functionId": "concatFileChunks___conf-Configuration__sourceFile-Path__targetFile-Path__allChunkPaths-LinkedList__Path____srcFileStatus-CopyListingFileStatus",
  "sourceFilePath": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
  "functionStartLine": 611,
  "functionEndLine": 644,
  "numCommitsSeen": 66,
  "timeTaken": 3933,
  "changeHistory": [
    "51c64b357d4bd1a0038e61df3d4b8ea0a3ad7449",
    "c765584eb231f8482f5b90b7e8f61f9f7a931d09",
    "bf3fb585aaf2b179836e139c041fc87920a3c886"
  ],
  "changeHistoryShort": {
    "51c64b357d4bd1a0038e61df3d4b8ea0a3ad7449": "Ymultichange(Yparameterchange,Ybodychange)",
    "c765584eb231f8482f5b90b7e8f61f9f7a931d09": "Ymultichange(Yparameterchange,Ybodychange)",
    "bf3fb585aaf2b179836e139c041fc87920a3c886": "Yintroduced"
  },
  "changeHistoryDetails": {
    "51c64b357d4bd1a0038e61df3d4b8ea0a3ad7449": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13660. DistCp job fails when new data is appended in the file while the DistCp copy job is running\n\nThis uses the length of the file known at the start of the copy to determine the amount of data to copy.\n\n* If a file is appended to during the copy, the original bytes are copied.\n* If a file is truncated during a copy, or the attempt to read the data fails with a truncated stream,\n  distcp will now fail. Until now these failures were not detected.\n\nContributed by Mukund Thakur.\n\nChange-Id: I576a49d951fa48d37a45a7e4c82c47488aa8e884\n",
      "commitDate": "24/09/19 3:23 AM",
      "commitName": "51c64b357d4bd1a0038e61df3d4b8ea0a3ad7449",
      "commitAuthor": "Mukund Thakur",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13660. DistCp job fails when new data is appended in the file while the DistCp copy job is running\n\nThis uses the length of the file known at the start of the copy to determine the amount of data to copy.\n\n* If a file is appended to during the copy, the original bytes are copied.\n* If a file is truncated during a copy, or the attempt to read the data fails with a truncated stream,\n  distcp will now fail. Until now these failures were not detected.\n\nContributed by Mukund Thakur.\n\nChange-Id: I576a49d951fa48d37a45a7e4c82c47488aa8e884\n",
          "commitDate": "24/09/19 3:23 AM",
          "commitName": "51c64b357d4bd1a0038e61df3d4b8ea0a3ad7449",
          "commitAuthor": "Mukund Thakur",
          "commitDateOld": "18/08/19 6:46 PM",
          "commitNameOld": "c765584eb231f8482f5b90b7e8f61f9f7a931d09",
          "commitAuthorOld": "KAI XIE",
          "daysBetweenCommits": 36.36,
          "commitsBetweenForRepo": 313,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,32 +1,34 @@\n   private void concatFileChunks(Configuration conf, Path sourceFile,\n-                                Path targetFile, LinkedList\u003cPath\u003e allChunkPaths)\n+                                Path targetFile, LinkedList\u003cPath\u003e allChunkPaths,\n+                                CopyListingFileStatus srcFileStatus)\n       throws IOException {\n     if (allChunkPaths.size() \u003d\u003d 1) {\n       return;\n     }\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \"\n           + allChunkPaths.size());\n     }\n     FileSystem dstfs \u003d targetFile.getFileSystem(conf);\n     FileSystem srcfs \u003d sourceFile.getFileSystem(conf);\n \n     Path firstChunkFile \u003d allChunkPaths.removeFirst();\n     Path[] restChunkFiles \u003d new Path[allChunkPaths.size()];\n     allChunkPaths.toArray(restChunkFiles);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n       int i \u003d 0;\n       for (Path f : restChunkFiles) {\n         LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n         ++i;\n       }\n     }\n     dstfs.concat(firstChunkFile, restChunkFiles);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n     }\n     rename(dstfs, firstChunkFile, targetFile);\n-    DistCpUtils.compareFileLengthsAndChecksums(\n-        srcfs, sourceFile, null, dstfs, targetFile, skipCrc);\n+    DistCpUtils.compareFileLengthsAndChecksums(srcFileStatus.getLen(),\n+        srcfs, sourceFile, null, dstfs,\n+            targetFile, skipCrc, srcFileStatus.getLen());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void concatFileChunks(Configuration conf, Path sourceFile,\n                                Path targetFile, LinkedList\u003cPath\u003e allChunkPaths,\n                                CopyListingFileStatus srcFileStatus)\n      throws IOException {\n    if (allChunkPaths.size() \u003d\u003d 1) {\n      return;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \"\n          + allChunkPaths.size());\n    }\n    FileSystem dstfs \u003d targetFile.getFileSystem(conf);\n    FileSystem srcfs \u003d sourceFile.getFileSystem(conf);\n\n    Path firstChunkFile \u003d allChunkPaths.removeFirst();\n    Path[] restChunkFiles \u003d new Path[allChunkPaths.size()];\n    allChunkPaths.toArray(restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n      int i \u003d 0;\n      for (Path f : restChunkFiles) {\n        LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n        ++i;\n      }\n    }\n    dstfs.concat(firstChunkFile, restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n    }\n    rename(dstfs, firstChunkFile, targetFile);\n    DistCpUtils.compareFileLengthsAndChecksums(srcFileStatus.getLen(),\n        srcfs, sourceFile, null, dstfs,\n            targetFile, skipCrc, srcFileStatus.getLen());\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
          "extendedDetails": {
            "oldValue": "[conf-Configuration, sourceFile-Path, targetFile-Path, allChunkPaths-LinkedList\u003cPath\u003e]",
            "newValue": "[conf-Configuration, sourceFile-Path, targetFile-Path, allChunkPaths-LinkedList\u003cPath\u003e, srcFileStatus-CopyListingFileStatus]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13660. DistCp job fails when new data is appended in the file while the DistCp copy job is running\n\nThis uses the length of the file known at the start of the copy to determine the amount of data to copy.\n\n* If a file is appended to during the copy, the original bytes are copied.\n* If a file is truncated during a copy, or the attempt to read the data fails with a truncated stream,\n  distcp will now fail. Until now these failures were not detected.\n\nContributed by Mukund Thakur.\n\nChange-Id: I576a49d951fa48d37a45a7e4c82c47488aa8e884\n",
          "commitDate": "24/09/19 3:23 AM",
          "commitName": "51c64b357d4bd1a0038e61df3d4b8ea0a3ad7449",
          "commitAuthor": "Mukund Thakur",
          "commitDateOld": "18/08/19 6:46 PM",
          "commitNameOld": "c765584eb231f8482f5b90b7e8f61f9f7a931d09",
          "commitAuthorOld": "KAI XIE",
          "daysBetweenCommits": 36.36,
          "commitsBetweenForRepo": 313,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,32 +1,34 @@\n   private void concatFileChunks(Configuration conf, Path sourceFile,\n-                                Path targetFile, LinkedList\u003cPath\u003e allChunkPaths)\n+                                Path targetFile, LinkedList\u003cPath\u003e allChunkPaths,\n+                                CopyListingFileStatus srcFileStatus)\n       throws IOException {\n     if (allChunkPaths.size() \u003d\u003d 1) {\n       return;\n     }\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \"\n           + allChunkPaths.size());\n     }\n     FileSystem dstfs \u003d targetFile.getFileSystem(conf);\n     FileSystem srcfs \u003d sourceFile.getFileSystem(conf);\n \n     Path firstChunkFile \u003d allChunkPaths.removeFirst();\n     Path[] restChunkFiles \u003d new Path[allChunkPaths.size()];\n     allChunkPaths.toArray(restChunkFiles);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n       int i \u003d 0;\n       for (Path f : restChunkFiles) {\n         LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n         ++i;\n       }\n     }\n     dstfs.concat(firstChunkFile, restChunkFiles);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n     }\n     rename(dstfs, firstChunkFile, targetFile);\n-    DistCpUtils.compareFileLengthsAndChecksums(\n-        srcfs, sourceFile, null, dstfs, targetFile, skipCrc);\n+    DistCpUtils.compareFileLengthsAndChecksums(srcFileStatus.getLen(),\n+        srcfs, sourceFile, null, dstfs,\n+            targetFile, skipCrc, srcFileStatus.getLen());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void concatFileChunks(Configuration conf, Path sourceFile,\n                                Path targetFile, LinkedList\u003cPath\u003e allChunkPaths,\n                                CopyListingFileStatus srcFileStatus)\n      throws IOException {\n    if (allChunkPaths.size() \u003d\u003d 1) {\n      return;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \"\n          + allChunkPaths.size());\n    }\n    FileSystem dstfs \u003d targetFile.getFileSystem(conf);\n    FileSystem srcfs \u003d sourceFile.getFileSystem(conf);\n\n    Path firstChunkFile \u003d allChunkPaths.removeFirst();\n    Path[] restChunkFiles \u003d new Path[allChunkPaths.size()];\n    allChunkPaths.toArray(restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n      int i \u003d 0;\n      for (Path f : restChunkFiles) {\n        LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n        ++i;\n      }\n    }\n    dstfs.concat(firstChunkFile, restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n    }\n    rename(dstfs, firstChunkFile, targetFile);\n    DistCpUtils.compareFileLengthsAndChecksums(srcFileStatus.getLen(),\n        srcfs, sourceFile, null, dstfs,\n            targetFile, skipCrc, srcFileStatus.getLen());\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
          "extendedDetails": {}
        }
      ]
    },
    "c765584eb231f8482f5b90b7e8f61f9f7a931d09": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-16158. DistCp to support checksum validation when copy blocks in parallel (#919)\n\n* DistCp to support checksum validation when copy blocks in parallel\r\n\r\n* address review comments\r\n\r\n* add checksums comparison test for combine mode\r\n",
      "commitDate": "18/08/19 6:46 PM",
      "commitName": "c765584eb231f8482f5b90b7e8f61f9f7a931d09",
      "commitAuthor": "KAI XIE",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-16158. DistCp to support checksum validation when copy blocks in parallel (#919)\n\n* DistCp to support checksum validation when copy blocks in parallel\r\n\r\n* address review comments\r\n\r\n* add checksums comparison test for combine mode\r\n",
          "commitDate": "18/08/19 6:46 PM",
          "commitName": "c765584eb231f8482f5b90b7e8f61f9f7a931d09",
          "commitAuthor": "KAI XIE",
          "commitDateOld": "20/07/19 12:41 AM",
          "commitNameOld": "e60f5e2572532e2bce44757997f1086065b8fd80",
          "commitAuthorOld": "Ayush Saxena",
          "daysBetweenCommits": 29.75,
          "commitsBetweenForRepo": 268,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,32 @@\n-  private void concatFileChunks(Configuration conf, Path targetFile,\n-      LinkedList\u003cPath\u003e allChunkPaths) throws IOException {\n+  private void concatFileChunks(Configuration conf, Path sourceFile,\n+                                Path targetFile, LinkedList\u003cPath\u003e allChunkPaths)\n+      throws IOException {\n     if (allChunkPaths.size() \u003d\u003d 1) {\n       return;\n     }\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \"\n           + allChunkPaths.size());\n     }\n     FileSystem dstfs \u003d targetFile.getFileSystem(conf);\n+    FileSystem srcfs \u003d sourceFile.getFileSystem(conf);\n \n     Path firstChunkFile \u003d allChunkPaths.removeFirst();\n     Path[] restChunkFiles \u003d new Path[allChunkPaths.size()];\n     allChunkPaths.toArray(restChunkFiles);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n       int i \u003d 0;\n       for (Path f : restChunkFiles) {\n         LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n         ++i;\n       }\n     }\n     dstfs.concat(firstChunkFile, restChunkFiles);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n     }\n     rename(dstfs, firstChunkFile, targetFile);\n+    DistCpUtils.compareFileLengthsAndChecksums(\n+        srcfs, sourceFile, null, dstfs, targetFile, skipCrc);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void concatFileChunks(Configuration conf, Path sourceFile,\n                                Path targetFile, LinkedList\u003cPath\u003e allChunkPaths)\n      throws IOException {\n    if (allChunkPaths.size() \u003d\u003d 1) {\n      return;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \"\n          + allChunkPaths.size());\n    }\n    FileSystem dstfs \u003d targetFile.getFileSystem(conf);\n    FileSystem srcfs \u003d sourceFile.getFileSystem(conf);\n\n    Path firstChunkFile \u003d allChunkPaths.removeFirst();\n    Path[] restChunkFiles \u003d new Path[allChunkPaths.size()];\n    allChunkPaths.toArray(restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n      int i \u003d 0;\n      for (Path f : restChunkFiles) {\n        LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n        ++i;\n      }\n    }\n    dstfs.concat(firstChunkFile, restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n    }\n    rename(dstfs, firstChunkFile, targetFile);\n    DistCpUtils.compareFileLengthsAndChecksums(\n        srcfs, sourceFile, null, dstfs, targetFile, skipCrc);\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
          "extendedDetails": {
            "oldValue": "[conf-Configuration, targetFile-Path, allChunkPaths-LinkedList\u003cPath\u003e]",
            "newValue": "[conf-Configuration, sourceFile-Path, targetFile-Path, allChunkPaths-LinkedList\u003cPath\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-16158. DistCp to support checksum validation when copy blocks in parallel (#919)\n\n* DistCp to support checksum validation when copy blocks in parallel\r\n\r\n* address review comments\r\n\r\n* add checksums comparison test for combine mode\r\n",
          "commitDate": "18/08/19 6:46 PM",
          "commitName": "c765584eb231f8482f5b90b7e8f61f9f7a931d09",
          "commitAuthor": "KAI XIE",
          "commitDateOld": "20/07/19 12:41 AM",
          "commitNameOld": "e60f5e2572532e2bce44757997f1086065b8fd80",
          "commitAuthorOld": "Ayush Saxena",
          "daysBetweenCommits": 29.75,
          "commitsBetweenForRepo": 268,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,32 @@\n-  private void concatFileChunks(Configuration conf, Path targetFile,\n-      LinkedList\u003cPath\u003e allChunkPaths) throws IOException {\n+  private void concatFileChunks(Configuration conf, Path sourceFile,\n+                                Path targetFile, LinkedList\u003cPath\u003e allChunkPaths)\n+      throws IOException {\n     if (allChunkPaths.size() \u003d\u003d 1) {\n       return;\n     }\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \"\n           + allChunkPaths.size());\n     }\n     FileSystem dstfs \u003d targetFile.getFileSystem(conf);\n+    FileSystem srcfs \u003d sourceFile.getFileSystem(conf);\n \n     Path firstChunkFile \u003d allChunkPaths.removeFirst();\n     Path[] restChunkFiles \u003d new Path[allChunkPaths.size()];\n     allChunkPaths.toArray(restChunkFiles);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n       int i \u003d 0;\n       for (Path f : restChunkFiles) {\n         LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n         ++i;\n       }\n     }\n     dstfs.concat(firstChunkFile, restChunkFiles);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n     }\n     rename(dstfs, firstChunkFile, targetFile);\n+    DistCpUtils.compareFileLengthsAndChecksums(\n+        srcfs, sourceFile, null, dstfs, targetFile, skipCrc);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void concatFileChunks(Configuration conf, Path sourceFile,\n                                Path targetFile, LinkedList\u003cPath\u003e allChunkPaths)\n      throws IOException {\n    if (allChunkPaths.size() \u003d\u003d 1) {\n      return;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \"\n          + allChunkPaths.size());\n    }\n    FileSystem dstfs \u003d targetFile.getFileSystem(conf);\n    FileSystem srcfs \u003d sourceFile.getFileSystem(conf);\n\n    Path firstChunkFile \u003d allChunkPaths.removeFirst();\n    Path[] restChunkFiles \u003d new Path[allChunkPaths.size()];\n    allChunkPaths.toArray(restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n      int i \u003d 0;\n      for (Path f : restChunkFiles) {\n        LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n        ++i;\n      }\n    }\n    dstfs.concat(firstChunkFile, restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n    }\n    rename(dstfs, firstChunkFile, targetFile);\n    DistCpUtils.compareFileLengthsAndChecksums(\n        srcfs, sourceFile, null, dstfs, targetFile, skipCrc);\n  }",
          "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
          "extendedDetails": {}
        }
      ]
    },
    "bf3fb585aaf2b179836e139c041fc87920a3c886": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-11794. Enable distcp to copy blocks in parallel. Contributed by Yongjun Zhang, Wei-Chiu Chuang, Xiao Chen, Rosie Li.\n",
      "commitDate": "30/03/17 5:38 PM",
      "commitName": "bf3fb585aaf2b179836e139c041fc87920a3c886",
      "commitAuthor": "Yongjun Zhang",
      "diff": "@@ -0,0 +1,28 @@\n+  private void concatFileChunks(Configuration conf, Path targetFile,\n+      LinkedList\u003cPath\u003e allChunkPaths) throws IOException {\n+    if (allChunkPaths.size() \u003d\u003d 1) {\n+      return;\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \"\n+          + allChunkPaths.size());\n+    }\n+    FileSystem dstfs \u003d targetFile.getFileSystem(conf);\n+\n+    Path firstChunkFile \u003d allChunkPaths.removeFirst();\n+    Path[] restChunkFiles \u003d new Path[allChunkPaths.size()];\n+    allChunkPaths.toArray(restChunkFiles);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n+      int i \u003d 0;\n+      for (Path f : restChunkFiles) {\n+        LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n+        ++i;\n+      }\n+    }\n+    dstfs.concat(firstChunkFile, restChunkFiles);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n+    }\n+    rename(dstfs, firstChunkFile, targetFile);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void concatFileChunks(Configuration conf, Path targetFile,\n      LinkedList\u003cPath\u003e allChunkPaths) throws IOException {\n    if (allChunkPaths.size() \u003d\u003d 1) {\n      return;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \"\n          + allChunkPaths.size());\n    }\n    FileSystem dstfs \u003d targetFile.getFileSystem(conf);\n\n    Path firstChunkFile \u003d allChunkPaths.removeFirst();\n    Path[] restChunkFiles \u003d new Path[allChunkPaths.size()];\n    allChunkPaths.toArray(restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\n      int i \u003d 0;\n      for (Path f : restChunkFiles) {\n        LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\n        ++i;\n      }\n    }\n    dstfs.concat(firstChunkFile, restChunkFiles);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\n    }\n    rename(dstfs, firstChunkFile, targetFile);\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java"
    }
  }
}