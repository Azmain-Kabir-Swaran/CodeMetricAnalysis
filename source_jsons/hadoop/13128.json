{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CacheReplicationMonitor.java",
  "functionName": "rescanFile",
  "functionId": "rescanFile___directive-CacheDirective__file-INodeFile",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
  "functionStartLine": 370,
  "functionEndLine": 452,
  "numCommitsSeen": 41,
  "timeTaken": 4122,
  "changeHistory": [
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "93e23a99157c30b51752fc49748c3c210745a187",
    "bab90b2222abb41d0e3382a92c2f9a8dc568f0e0",
    "07e4fb1455abc33584fc666ef745abe256ebd7d1",
    "991c453ca3ac141a3f286f74af8401f83c38b230",
    "55e5b0653c34a5f4146ce5a97a5b4a88a976d88a",
    "13edb391d06c479720202eb5ac81f1c71fe64748",
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
    "916ab9286b6006571649d21c74d9ae70273a3ddc",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a"
  ],
  "changeHistoryShort": {
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "93e23a99157c30b51752fc49748c3c210745a187": "Ybodychange",
    "bab90b2222abb41d0e3382a92c2f9a8dc568f0e0": "Ybodychange",
    "07e4fb1455abc33584fc666ef745abe256ebd7d1": "Ybodychange",
    "991c453ca3ac141a3f286f74af8401f83c38b230": "Ybodychange",
    "55e5b0653c34a5f4146ce5a97a5b4a88a976d88a": "Ymultichange(Yparameterchange,Ybodychange)",
    "13edb391d06c479720202eb5ac81f1c71fe64748": "Ybodychange",
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde": "Ymultichange(Yparameterchange,Ybodychange)",
    "916ab9286b6006571649d21c74d9ae70273a3ddc": "Ybodychange",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": "Yintroduced"
  },
  "changeHistoryDetails": {
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "08/02/15 11:51 AM",
      "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 108.12,
      "commitsBetweenForRepo": 1040,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,83 @@\n   private void rescanFile(CacheDirective directive, INodeFile file) {\n-    BlockInfoContiguous[] blockInfos \u003d file.getBlocks();\n+    BlockInfo[] blockInfos \u003d file.getBlocks();\n \n     // Increment the \"needed\" statistics\n     directive.addFilesNeeded(1);\n     // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n     long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n         directive.getReplication();\n     directive.addBytesNeeded(neededTotal);\n \n     // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n     // thus far plus the demand of this file would exceed the pool\u0027s limit,\n     // do not cache this file.\n     CachePool pool \u003d directive.getPool();\n     if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n       LOG.debug(\"Directive {}: not scanning file {} because \" +\n           \"bytesNeeded for pool {} is {}, but the pool\u0027s limit is {}\",\n           directive.getId(),\n           file.getFullPathName(),\n           pool.getPoolName(),\n           pool.getBytesNeeded(),\n           pool.getLimit());\n       return;\n     }\n \n     long cachedTotal \u003d 0;\n-    for (BlockInfoContiguous blockInfo : blockInfos) {\n+    for (BlockInfo blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n         LOG.trace(\"Directive {}: can\u0027t cache block {} because it is in state \"\n                 + \"{}, not COMPLETE.\", directive.getId(), blockInfo,\n             blockInfo.getBlockUCState()\n         );\n         continue;\n       }\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n           directive.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n         ocblock \u003d ncblock;\n       } else {\n         // Update bytesUsed using the current replication levels.\n         // Assumptions: we assume that all the blocks are the same length\n         // on each datanode.  We can assume this because we\u0027re only caching\n         // blocks in state COMPLETE.\n         // Note that if two directives are caching the same block(s), they will\n         // both get them added to their bytesCached.\n         List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n             ocblock.getDatanodes(Type.CACHED);\n         long cachedByBlock \u003d Math.min(cachedOn.size(),\n             directive.getReplication()) * blockInfo.getNumBytes();\n         cachedTotal +\u003d cachedByBlock;\n \n         if ((mark !\u003d ocblock.getMark()) ||\n             (ocblock.getReplication() \u003c directive.getReplication())) {\n           //\n           // Overwrite the block\u0027s replication and mark in two cases:\n           //\n           // 1. If the mark on the CachedBlock is different from the mark for\n           // this scan, that means the block hasn\u0027t been updated during this\n           // scan, and we should overwrite whatever is there, since it is no\n           // longer valid.\n           //\n           // 2. If the replication in the CachedBlock is less than what the\n           // directive asks for, we want to increase the block\u0027s replication\n           // field to what the directive asks for.\n           //\n           ocblock.setReplicationAndMark(directive.getReplication(), mark);\n         }\n       }\n       LOG.trace(\"Directive {}: setting replication for block {} to {}\",\n           directive.getId(), blockInfo, ocblock.getReplication());\n     }\n     // Increment the \"cached\" statistics\n     directive.addBytesCached(cachedTotal);\n     if (cachedTotal \u003d\u003d neededTotal) {\n       directive.addFilesCached(1);\n     }\n     LOG.debug(\"Directive {}: caching {}: {}/{} bytes\", directive.getId(),\n         file.getFullPathName(), cachedTotal, neededTotal);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void rescanFile(CacheDirective directive, INodeFile file) {\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n\n    // Increment the \"needed\" statistics\n    directive.addFilesNeeded(1);\n    // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n    long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n        directive.getReplication();\n    directive.addBytesNeeded(neededTotal);\n\n    // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n    // thus far plus the demand of this file would exceed the pool\u0027s limit,\n    // do not cache this file.\n    CachePool pool \u003d directive.getPool();\n    if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n      LOG.debug(\"Directive {}: not scanning file {} because \" +\n          \"bytesNeeded for pool {} is {}, but the pool\u0027s limit is {}\",\n          directive.getId(),\n          file.getFullPathName(),\n          pool.getPoolName(),\n          pool.getBytesNeeded(),\n          pool.getLimit());\n      return;\n    }\n\n    long cachedTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        LOG.trace(\"Directive {}: can\u0027t cache block {} because it is in state \"\n                + \"{}, not COMPLETE.\", directive.getId(), blockInfo,\n            blockInfo.getBlockUCState()\n        );\n        continue;\n      }\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          directive.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n        ocblock \u003d ncblock;\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMPLETE.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(),\n            directive.getReplication()) * blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if ((mark !\u003d ocblock.getMark()) ||\n            (ocblock.getReplication() \u003c directive.getReplication())) {\n          //\n          // Overwrite the block\u0027s replication and mark in two cases:\n          //\n          // 1. If the mark on the CachedBlock is different from the mark for\n          // this scan, that means the block hasn\u0027t been updated during this\n          // scan, and we should overwrite whatever is there, since it is no\n          // longer valid.\n          //\n          // 2. If the replication in the CachedBlock is less than what the\n          // directive asks for, we want to increase the block\u0027s replication\n          // field to what the directive asks for.\n          //\n          ocblock.setReplicationAndMark(directive.getReplication(), mark);\n        }\n      }\n      LOG.trace(\"Directive {}: setting replication for block {} to {}\",\n          directive.getId(), blockInfo, ocblock.getReplication());\n    }\n    // Increment the \"cached\" statistics\n    directive.addBytesCached(cachedTotal);\n    if (cachedTotal \u003d\u003d neededTotal) {\n      directive.addFilesCached(1);\n    }\n    LOG.debug(\"Directive {}: caching {}: {}/{} bytes\", directive.getId(),\n        file.getFullPathName(), cachedTotal, neededTotal);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "23/10/14 11:58 PM",
      "commitNameOld": "0942c99eba12f6baf5609c9621cd07b09618a97e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 107.54,
      "commitsBetweenForRepo": 791,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,83 @@\n   private void rescanFile(CacheDirective directive, INodeFile file) {\n-    BlockInfo[] blockInfos \u003d file.getBlocks();\n+    BlockInfoContiguous[] blockInfos \u003d file.getBlocks();\n \n     // Increment the \"needed\" statistics\n     directive.addFilesNeeded(1);\n     // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n     long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n         directive.getReplication();\n     directive.addBytesNeeded(neededTotal);\n \n     // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n     // thus far plus the demand of this file would exceed the pool\u0027s limit,\n     // do not cache this file.\n     CachePool pool \u003d directive.getPool();\n     if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n       LOG.debug(\"Directive {}: not scanning file {} because \" +\n           \"bytesNeeded for pool {} is {}, but the pool\u0027s limit is {}\",\n           directive.getId(),\n           file.getFullPathName(),\n           pool.getPoolName(),\n           pool.getBytesNeeded(),\n           pool.getLimit());\n       return;\n     }\n \n     long cachedTotal \u003d 0;\n-    for (BlockInfo blockInfo : blockInfos) {\n+    for (BlockInfoContiguous blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n         LOG.trace(\"Directive {}: can\u0027t cache block {} because it is in state \"\n                 + \"{}, not COMPLETE.\", directive.getId(), blockInfo,\n             blockInfo.getBlockUCState()\n         );\n         continue;\n       }\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n           directive.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n         ocblock \u003d ncblock;\n       } else {\n         // Update bytesUsed using the current replication levels.\n         // Assumptions: we assume that all the blocks are the same length\n         // on each datanode.  We can assume this because we\u0027re only caching\n         // blocks in state COMPLETE.\n         // Note that if two directives are caching the same block(s), they will\n         // both get them added to their bytesCached.\n         List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n             ocblock.getDatanodes(Type.CACHED);\n         long cachedByBlock \u003d Math.min(cachedOn.size(),\n             directive.getReplication()) * blockInfo.getNumBytes();\n         cachedTotal +\u003d cachedByBlock;\n \n         if ((mark !\u003d ocblock.getMark()) ||\n             (ocblock.getReplication() \u003c directive.getReplication())) {\n           //\n           // Overwrite the block\u0027s replication and mark in two cases:\n           //\n           // 1. If the mark on the CachedBlock is different from the mark for\n           // this scan, that means the block hasn\u0027t been updated during this\n           // scan, and we should overwrite whatever is there, since it is no\n           // longer valid.\n           //\n           // 2. If the replication in the CachedBlock is less than what the\n           // directive asks for, we want to increase the block\u0027s replication\n           // field to what the directive asks for.\n           //\n           ocblock.setReplicationAndMark(directive.getReplication(), mark);\n         }\n       }\n       LOG.trace(\"Directive {}: setting replication for block {} to {}\",\n           directive.getId(), blockInfo, ocblock.getReplication());\n     }\n     // Increment the \"cached\" statistics\n     directive.addBytesCached(cachedTotal);\n     if (cachedTotal \u003d\u003d neededTotal) {\n       directive.addFilesCached(1);\n     }\n     LOG.debug(\"Directive {}: caching {}: {}/{} bytes\", directive.getId(),\n         file.getFullPathName(), cachedTotal, neededTotal);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void rescanFile(CacheDirective directive, INodeFile file) {\n    BlockInfoContiguous[] blockInfos \u003d file.getBlocks();\n\n    // Increment the \"needed\" statistics\n    directive.addFilesNeeded(1);\n    // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n    long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n        directive.getReplication();\n    directive.addBytesNeeded(neededTotal);\n\n    // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n    // thus far plus the demand of this file would exceed the pool\u0027s limit,\n    // do not cache this file.\n    CachePool pool \u003d directive.getPool();\n    if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n      LOG.debug(\"Directive {}: not scanning file {} because \" +\n          \"bytesNeeded for pool {} is {}, but the pool\u0027s limit is {}\",\n          directive.getId(),\n          file.getFullPathName(),\n          pool.getPoolName(),\n          pool.getBytesNeeded(),\n          pool.getLimit());\n      return;\n    }\n\n    long cachedTotal \u003d 0;\n    for (BlockInfoContiguous blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        LOG.trace(\"Directive {}: can\u0027t cache block {} because it is in state \"\n                + \"{}, not COMPLETE.\", directive.getId(), blockInfo,\n            blockInfo.getBlockUCState()\n        );\n        continue;\n      }\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          directive.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n        ocblock \u003d ncblock;\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMPLETE.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(),\n            directive.getReplication()) * blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if ((mark !\u003d ocblock.getMark()) ||\n            (ocblock.getReplication() \u003c directive.getReplication())) {\n          //\n          // Overwrite the block\u0027s replication and mark in two cases:\n          //\n          // 1. If the mark on the CachedBlock is different from the mark for\n          // this scan, that means the block hasn\u0027t been updated during this\n          // scan, and we should overwrite whatever is there, since it is no\n          // longer valid.\n          //\n          // 2. If the replication in the CachedBlock is less than what the\n          // directive asks for, we want to increase the block\u0027s replication\n          // field to what the directive asks for.\n          //\n          ocblock.setReplicationAndMark(directive.getReplication(), mark);\n        }\n      }\n      LOG.trace(\"Directive {}: setting replication for block {} to {}\",\n          directive.getId(), blockInfo, ocblock.getReplication());\n    }\n    // Increment the \"cached\" statistics\n    directive.addBytesCached(cachedTotal);\n    if (cachedTotal \u003d\u003d neededTotal) {\n      directive.addFilesCached(1);\n    }\n    LOG.debug(\"Directive {}: caching {}: {}/{} bytes\", directive.getId(),\n        file.getFullPathName(), cachedTotal, neededTotal);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "93e23a99157c30b51752fc49748c3c210745a187": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6613. Improve logging in caching classes. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1607697 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/07/14 10:13 AM",
      "commitName": "93e23a99157c30b51752fc49748c3c210745a187",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "10/03/14 11:24 PM",
      "commitNameOld": "bab90b2222abb41d0e3382a92c2f9a8dc568f0e0",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 114.45,
      "commitsBetweenForRepo": 724,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,83 @@\n   private void rescanFile(CacheDirective directive, INodeFile file) {\n     BlockInfo[] blockInfos \u003d file.getBlocks();\n \n     // Increment the \"needed\" statistics\n     directive.addFilesNeeded(1);\n     // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n     long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n         directive.getReplication();\n     directive.addBytesNeeded(neededTotal);\n \n     // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n     // thus far plus the demand of this file would exceed the pool\u0027s limit,\n     // do not cache this file.\n     CachePool pool \u003d directive.getPool();\n     if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(String.format(\"Directive %d: not scanning file %s because \" +\n-            \"bytesNeeded for pool %s is %d, but the pool\u0027s limit is %d\",\n-            directive.getId(),\n-            file.getFullPathName(),\n-            pool.getPoolName(),\n-            pool.getBytesNeeded(),\n-            pool.getLimit()));\n-      }\n+      LOG.debug(\"Directive {}: not scanning file {} because \" +\n+          \"bytesNeeded for pool {} is {}, but the pool\u0027s limit is {}\",\n+          directive.getId(),\n+          file.getFullPathName(),\n+          pool.getPoolName(),\n+          pool.getBytesNeeded(),\n+          pool.getLimit());\n       return;\n     }\n \n     long cachedTotal \u003d 0;\n     for (BlockInfo blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n-        if (LOG.isTraceEnabled()) {\n-          LOG.trace(\"Directive \" + directive.getId() + \": can\u0027t cache \" +\n-              \"block \" + blockInfo + \" because it is in state \" +\n-              blockInfo.getBlockUCState() + \", not COMPLETE.\");\n-        }\n+        LOG.trace(\"Directive {}: can\u0027t cache block {} because it is in state \"\n+                + \"{}, not COMPLETE.\", directive.getId(), blockInfo,\n+            blockInfo.getBlockUCState()\n+        );\n         continue;\n       }\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n           directive.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n         ocblock \u003d ncblock;\n       } else {\n         // Update bytesUsed using the current replication levels.\n         // Assumptions: we assume that all the blocks are the same length\n         // on each datanode.  We can assume this because we\u0027re only caching\n-        // blocks in state COMMITTED.\n+        // blocks in state COMPLETE.\n         // Note that if two directives are caching the same block(s), they will\n         // both get them added to their bytesCached.\n         List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n             ocblock.getDatanodes(Type.CACHED);\n         long cachedByBlock \u003d Math.min(cachedOn.size(),\n             directive.getReplication()) * blockInfo.getNumBytes();\n         cachedTotal +\u003d cachedByBlock;\n \n         if ((mark !\u003d ocblock.getMark()) ||\n             (ocblock.getReplication() \u003c directive.getReplication())) {\n           //\n           // Overwrite the block\u0027s replication and mark in two cases:\n           //\n           // 1. If the mark on the CachedBlock is different from the mark for\n           // this scan, that means the block hasn\u0027t been updated during this\n           // scan, and we should overwrite whatever is there, since it is no\n           // longer valid.\n           //\n           // 2. If the replication in the CachedBlock is less than what the\n           // directive asks for, we want to increase the block\u0027s replication\n           // field to what the directive asks for.\n           //\n           ocblock.setReplicationAndMark(directive.getReplication(), mark);\n         }\n       }\n-      if (LOG.isTraceEnabled()) {\n-        LOG.trace(\"Directive \" + directive.getId() + \": setting replication \" +\n-                \"for block \" + blockInfo + \" to \" + ocblock.getReplication());\n-      }\n+      LOG.trace(\"Directive {}: setting replication for block {} to {}\",\n+          directive.getId(), blockInfo, ocblock.getReplication());\n     }\n     // Increment the \"cached\" statistics\n     directive.addBytesCached(cachedTotal);\n     if (cachedTotal \u003d\u003d neededTotal) {\n       directive.addFilesCached(1);\n     }\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Directive \" + directive.getId() + \": caching \" +\n-          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal +\n-          \" bytes\");\n-    }\n+    LOG.debug(\"Directive {}: caching {}: {}/{} bytes\", directive.getId(),\n+        file.getFullPathName(), cachedTotal, neededTotal);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void rescanFile(CacheDirective directive, INodeFile file) {\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n\n    // Increment the \"needed\" statistics\n    directive.addFilesNeeded(1);\n    // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n    long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n        directive.getReplication();\n    directive.addBytesNeeded(neededTotal);\n\n    // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n    // thus far plus the demand of this file would exceed the pool\u0027s limit,\n    // do not cache this file.\n    CachePool pool \u003d directive.getPool();\n    if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n      LOG.debug(\"Directive {}: not scanning file {} because \" +\n          \"bytesNeeded for pool {} is {}, but the pool\u0027s limit is {}\",\n          directive.getId(),\n          file.getFullPathName(),\n          pool.getPoolName(),\n          pool.getBytesNeeded(),\n          pool.getLimit());\n      return;\n    }\n\n    long cachedTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        LOG.trace(\"Directive {}: can\u0027t cache block {} because it is in state \"\n                + \"{}, not COMPLETE.\", directive.getId(), blockInfo,\n            blockInfo.getBlockUCState()\n        );\n        continue;\n      }\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          directive.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n        ocblock \u003d ncblock;\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMPLETE.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(),\n            directive.getReplication()) * blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if ((mark !\u003d ocblock.getMark()) ||\n            (ocblock.getReplication() \u003c directive.getReplication())) {\n          //\n          // Overwrite the block\u0027s replication and mark in two cases:\n          //\n          // 1. If the mark on the CachedBlock is different from the mark for\n          // this scan, that means the block hasn\u0027t been updated during this\n          // scan, and we should overwrite whatever is there, since it is no\n          // longer valid.\n          //\n          // 2. If the replication in the CachedBlock is less than what the\n          // directive asks for, we want to increase the block\u0027s replication\n          // field to what the directive asks for.\n          //\n          ocblock.setReplicationAndMark(directive.getReplication(), mark);\n        }\n      }\n      LOG.trace(\"Directive {}: setting replication for block {} to {}\",\n          directive.getId(), blockInfo, ocblock.getReplication());\n    }\n    // Increment the \"cached\" statistics\n    directive.addBytesCached(cachedTotal);\n    if (cachedTotal \u003d\u003d neededTotal) {\n      directive.addFilesCached(1);\n    }\n    LOG.debug(\"Directive {}: caching {}: {}/{} bytes\", directive.getId(),\n        file.getFullPathName(), cachedTotal, neededTotal);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "bab90b2222abb41d0e3382a92c2f9a8dc568f0e0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6085. Improve CacheReplicationMonitor log messages a bit (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1576194 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/03/14 11:24 PM",
      "commitName": "bab90b2222abb41d0e3382a92c2f9a8dc568f0e0",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "07/01/14 12:52 PM",
      "commitNameOld": "70cff9e2f0c8f78c1dc54a064182971bb2106795",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 62.4,
      "commitsBetweenForRepo": 529,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,81 +1,91 @@\n   private void rescanFile(CacheDirective directive, INodeFile file) {\n     BlockInfo[] blockInfos \u003d file.getBlocks();\n \n     // Increment the \"needed\" statistics\n     directive.addFilesNeeded(1);\n     // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n     long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n         directive.getReplication();\n     directive.addBytesNeeded(neededTotal);\n \n     // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n     // thus far plus the demand of this file would exceed the pool\u0027s limit,\n     // do not cache this file.\n     CachePool pool \u003d directive.getPool();\n     if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(String.format(\"Skipping directive id %d file %s because \"\n-            + \"limit of pool %s would be exceeded (%d \u003e %d)\",\n+        LOG.debug(String.format(\"Directive %d: not scanning file %s because \" +\n+            \"bytesNeeded for pool %s is %d, but the pool\u0027s limit is %d\",\n             directive.getId(),\n             file.getFullPathName(),\n             pool.getPoolName(),\n             pool.getBytesNeeded(),\n             pool.getLimit()));\n       }\n       return;\n     }\n \n     long cachedTotal \u003d 0;\n     for (BlockInfo blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"Directive \" + directive.getId() + \": can\u0027t cache \" +\n+              \"block \" + blockInfo + \" because it is in state \" +\n+              blockInfo.getBlockUCState() + \", not COMPLETE.\");\n+        }\n         continue;\n       }\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n           directive.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n+        ocblock \u003d ncblock;\n       } else {\n         // Update bytesUsed using the current replication levels.\n         // Assumptions: we assume that all the blocks are the same length\n         // on each datanode.  We can assume this because we\u0027re only caching\n         // blocks in state COMMITTED.\n         // Note that if two directives are caching the same block(s), they will\n         // both get them added to their bytesCached.\n         List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n             ocblock.getDatanodes(Type.CACHED);\n         long cachedByBlock \u003d Math.min(cachedOn.size(),\n             directive.getReplication()) * blockInfo.getNumBytes();\n         cachedTotal +\u003d cachedByBlock;\n \n         if ((mark !\u003d ocblock.getMark()) ||\n             (ocblock.getReplication() \u003c directive.getReplication())) {\n           //\n           // Overwrite the block\u0027s replication and mark in two cases:\n           //\n           // 1. If the mark on the CachedBlock is different from the mark for\n           // this scan, that means the block hasn\u0027t been updated during this\n           // scan, and we should overwrite whatever is there, since it is no\n           // longer valid.\n           //\n           // 2. If the replication in the CachedBlock is less than what the\n           // directive asks for, we want to increase the block\u0027s replication\n           // field to what the directive asks for.\n           //\n           ocblock.setReplicationAndMark(directive.getReplication(), mark);\n         }\n       }\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(\"Directive \" + directive.getId() + \": setting replication \" +\n+                \"for block \" + blockInfo + \" to \" + ocblock.getReplication());\n+      }\n     }\n     // Increment the \"cached\" statistics\n     directive.addBytesCached(cachedTotal);\n     if (cachedTotal \u003d\u003d neededTotal) {\n       directive.addFilesCached(1);\n     }\n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"Directive \" + directive.getId() + \" is caching \" +\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Directive \" + directive.getId() + \": caching \" +\n           file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal +\n           \" bytes\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void rescanFile(CacheDirective directive, INodeFile file) {\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n\n    // Increment the \"needed\" statistics\n    directive.addFilesNeeded(1);\n    // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n    long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n        directive.getReplication();\n    directive.addBytesNeeded(neededTotal);\n\n    // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n    // thus far plus the demand of this file would exceed the pool\u0027s limit,\n    // do not cache this file.\n    CachePool pool \u003d directive.getPool();\n    if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(String.format(\"Directive %d: not scanning file %s because \" +\n            \"bytesNeeded for pool %s is %d, but the pool\u0027s limit is %d\",\n            directive.getId(),\n            file.getFullPathName(),\n            pool.getPoolName(),\n            pool.getBytesNeeded(),\n            pool.getLimit()));\n      }\n      return;\n    }\n\n    long cachedTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Directive \" + directive.getId() + \": can\u0027t cache \" +\n              \"block \" + blockInfo + \" because it is in state \" +\n              blockInfo.getBlockUCState() + \", not COMPLETE.\");\n        }\n        continue;\n      }\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          directive.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n        ocblock \u003d ncblock;\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMMITTED.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(),\n            directive.getReplication()) * blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if ((mark !\u003d ocblock.getMark()) ||\n            (ocblock.getReplication() \u003c directive.getReplication())) {\n          //\n          // Overwrite the block\u0027s replication and mark in two cases:\n          //\n          // 1. If the mark on the CachedBlock is different from the mark for\n          // this scan, that means the block hasn\u0027t been updated during this\n          // scan, and we should overwrite whatever is there, since it is no\n          // longer valid.\n          //\n          // 2. If the replication in the CachedBlock is less than what the\n          // directive asks for, we want to increase the block\u0027s replication\n          // field to what the directive asks for.\n          //\n          ocblock.setReplicationAndMark(directive.getReplication(), mark);\n        }\n      }\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"Directive \" + directive.getId() + \": setting replication \" +\n                \"for block \" + blockInfo + \" to \" + ocblock.getReplication());\n      }\n    }\n    // Increment the \"cached\" statistics\n    directive.addBytesCached(cachedTotal);\n    if (cachedTotal \u003d\u003d neededTotal) {\n      directive.addFilesCached(1);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Directive \" + directive.getId() + \": caching \" +\n          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal +\n          \" bytes\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "07e4fb1455abc33584fc666ef745abe256ebd7d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5708. The CacheManager throws a NPE in the DataNode logs when processing cache reports that refer to a block not known to the BlockManager. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1554594 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/12/13 4:01 PM",
      "commitName": "07e4fb1455abc33584fc666ef745abe256ebd7d1",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "20/12/13 3:27 PM",
      "commitNameOld": "b9ae3087c0f83bfeeea47ded8e19932b46fd2350",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 11.02,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,81 @@\n   private void rescanFile(CacheDirective directive, INodeFile file) {\n     BlockInfo[] blockInfos \u003d file.getBlocks();\n \n     // Increment the \"needed\" statistics\n     directive.addFilesNeeded(1);\n     // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n     long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n         directive.getReplication();\n     directive.addBytesNeeded(neededTotal);\n \n     // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n     // thus far plus the demand of this file would exceed the pool\u0027s limit,\n     // do not cache this file.\n     CachePool pool \u003d directive.getPool();\n     if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(String.format(\"Skipping directive id %d file %s because \"\n             + \"limit of pool %s would be exceeded (%d \u003e %d)\",\n             directive.getId(),\n             file.getFullPathName(),\n             pool.getPoolName(),\n             pool.getBytesNeeded(),\n             pool.getLimit()));\n       }\n       return;\n     }\n \n     long cachedTotal \u003d 0;\n     for (BlockInfo blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n         continue;\n       }\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n           directive.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n       } else {\n         // Update bytesUsed using the current replication levels.\n         // Assumptions: we assume that all the blocks are the same length\n         // on each datanode.  We can assume this because we\u0027re only caching\n         // blocks in state COMMITTED.\n         // Note that if two directives are caching the same block(s), they will\n         // both get them added to their bytesCached.\n         List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n             ocblock.getDatanodes(Type.CACHED);\n         long cachedByBlock \u003d Math.min(cachedOn.size(),\n             directive.getReplication()) * blockInfo.getNumBytes();\n         cachedTotal +\u003d cachedByBlock;\n \n-        if (mark !\u003d ocblock.getMark()) {\n-          // Mark hasn\u0027t been set in this scan, so update replication and mark.\n+        if ((mark !\u003d ocblock.getMark()) ||\n+            (ocblock.getReplication() \u003c directive.getReplication())) {\n+          //\n+          // Overwrite the block\u0027s replication and mark in two cases:\n+          //\n+          // 1. If the mark on the CachedBlock is different from the mark for\n+          // this scan, that means the block hasn\u0027t been updated during this\n+          // scan, and we should overwrite whatever is there, since it is no\n+          // longer valid.\n+          //\n+          // 2. If the replication in the CachedBlock is less than what the\n+          // directive asks for, we want to increase the block\u0027s replication\n+          // field to what the directive asks for.\n+          //\n           ocblock.setReplicationAndMark(directive.getReplication(), mark);\n-        } else {\n-          // Mark already set in this scan.  Set replication to highest value in\n-          // any CacheDirective that covers this file.\n-          ocblock.setReplicationAndMark((short)Math.max(\n-              directive.getReplication(), ocblock.getReplication()), mark);\n         }\n       }\n     }\n     // Increment the \"cached\" statistics\n     directive.addBytesCached(cachedTotal);\n     if (cachedTotal \u003d\u003d neededTotal) {\n       directive.addFilesCached(1);\n     }\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Directive \" + directive.getId() + \" is caching \" +\n           file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal +\n           \" bytes\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void rescanFile(CacheDirective directive, INodeFile file) {\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n\n    // Increment the \"needed\" statistics\n    directive.addFilesNeeded(1);\n    // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n    long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n        directive.getReplication();\n    directive.addBytesNeeded(neededTotal);\n\n    // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n    // thus far plus the demand of this file would exceed the pool\u0027s limit,\n    // do not cache this file.\n    CachePool pool \u003d directive.getPool();\n    if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(String.format(\"Skipping directive id %d file %s because \"\n            + \"limit of pool %s would be exceeded (%d \u003e %d)\",\n            directive.getId(),\n            file.getFullPathName(),\n            pool.getPoolName(),\n            pool.getBytesNeeded(),\n            pool.getLimit()));\n      }\n      return;\n    }\n\n    long cachedTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        continue;\n      }\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          directive.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMMITTED.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(),\n            directive.getReplication()) * blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if ((mark !\u003d ocblock.getMark()) ||\n            (ocblock.getReplication() \u003c directive.getReplication())) {\n          //\n          // Overwrite the block\u0027s replication and mark in two cases:\n          //\n          // 1. If the mark on the CachedBlock is different from the mark for\n          // this scan, that means the block hasn\u0027t been updated during this\n          // scan, and we should overwrite whatever is there, since it is no\n          // longer valid.\n          //\n          // 2. If the replication in the CachedBlock is less than what the\n          // directive asks for, we want to increase the block\u0027s replication\n          // field to what the directive asks for.\n          //\n          ocblock.setReplicationAndMark(directive.getReplication(), mark);\n        }\n      }\n    }\n    // Increment the \"cached\" statistics\n    directive.addBytesCached(cachedTotal);\n    if (cachedTotal \u003d\u003d neededTotal) {\n      directive.addFilesCached(1);\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Directive \" + directive.getId() + \" is caching \" +\n          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal +\n          \" bytes\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "991c453ca3ac141a3f286f74af8401f83c38b230": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 10:47 AM",
      "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "05/12/13 1:09 PM",
      "commitNameOld": "55e5b0653c34a5f4146ce5a97a5b4a88a976d88a",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 11.9,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,74 @@\n   private void rescanFile(CacheDirective directive, INodeFile file) {\n     BlockInfo[] blockInfos \u003d file.getBlocks();\n \n     // Increment the \"needed\" statistics\n     directive.addFilesNeeded(1);\n-    long neededTotal \u003d 0;\n-    for (BlockInfo blockInfo : blockInfos) {\n-      long neededByBlock \u003d \n-          directive.getReplication() * blockInfo.getNumBytes();\n-       neededTotal +\u003d neededByBlock;\n-    }\n+    // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n+    long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n+        directive.getReplication();\n     directive.addBytesNeeded(neededTotal);\n \n-    // TODO: Enforce per-pool quotas\n+    // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n+    // thus far plus the demand of this file would exceed the pool\u0027s limit,\n+    // do not cache this file.\n+    CachePool pool \u003d directive.getPool();\n+    if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(String.format(\"Skipping directive id %d file %s because \"\n+            + \"limit of pool %s would be exceeded (%d \u003e %d)\",\n+            directive.getId(),\n+            file.getFullPathName(),\n+            pool.getPoolName(),\n+            pool.getBytesNeeded(),\n+            pool.getLimit()));\n+      }\n+      return;\n+    }\n \n     long cachedTotal \u003d 0;\n     for (BlockInfo blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n         continue;\n       }\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n           directive.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n       } else {\n         // Update bytesUsed using the current replication levels.\n         // Assumptions: we assume that all the blocks are the same length\n         // on each datanode.  We can assume this because we\u0027re only caching\n         // blocks in state COMMITTED.\n         // Note that if two directives are caching the same block(s), they will\n         // both get them added to their bytesCached.\n         List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n             ocblock.getDatanodes(Type.CACHED);\n         long cachedByBlock \u003d Math.min(cachedOn.size(),\n             directive.getReplication()) * blockInfo.getNumBytes();\n         cachedTotal +\u003d cachedByBlock;\n \n         if (mark !\u003d ocblock.getMark()) {\n           // Mark hasn\u0027t been set in this scan, so update replication and mark.\n           ocblock.setReplicationAndMark(directive.getReplication(), mark);\n         } else {\n           // Mark already set in this scan.  Set replication to highest value in\n           // any CacheDirective that covers this file.\n           ocblock.setReplicationAndMark((short)Math.max(\n               directive.getReplication(), ocblock.getReplication()), mark);\n         }\n       }\n     }\n     // Increment the \"cached\" statistics\n     directive.addBytesCached(cachedTotal);\n     if (cachedTotal \u003d\u003d neededTotal) {\n       directive.addFilesCached(1);\n     }\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Directive \" + directive.getId() + \" is caching \" +\n           file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal +\n           \" bytes\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void rescanFile(CacheDirective directive, INodeFile file) {\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n\n    // Increment the \"needed\" statistics\n    directive.addFilesNeeded(1);\n    // We don\u0027t cache UC blocks, don\u0027t add them to the total here\n    long neededTotal \u003d file.computeFileSizeNotIncludingLastUcBlock() *\n        directive.getReplication();\n    directive.addBytesNeeded(neededTotal);\n\n    // The pool\u0027s bytesNeeded is incremented as we scan. If the demand\n    // thus far plus the demand of this file would exceed the pool\u0027s limit,\n    // do not cache this file.\n    CachePool pool \u003d directive.getPool();\n    if (pool.getBytesNeeded() \u003e pool.getLimit()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(String.format(\"Skipping directive id %d file %s because \"\n            + \"limit of pool %s would be exceeded (%d \u003e %d)\",\n            directive.getId(),\n            file.getFullPathName(),\n            pool.getPoolName(),\n            pool.getBytesNeeded(),\n            pool.getLimit()));\n      }\n      return;\n    }\n\n    long cachedTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        continue;\n      }\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          directive.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMMITTED.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(),\n            directive.getReplication()) * blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if (mark !\u003d ocblock.getMark()) {\n          // Mark hasn\u0027t been set in this scan, so update replication and mark.\n          ocblock.setReplicationAndMark(directive.getReplication(), mark);\n        } else {\n          // Mark already set in this scan.  Set replication to highest value in\n          // any CacheDirective that covers this file.\n          ocblock.setReplicationAndMark((short)Math.max(\n              directive.getReplication(), ocblock.getReplication()), mark);\n        }\n      }\n    }\n    // Increment the \"cached\" statistics\n    directive.addBytesCached(cachedTotal);\n    if (cachedTotal \u003d\u003d neededTotal) {\n      directive.addFilesCached(1);\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Directive \" + directive.getId() + \" is caching \" +\n          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal +\n          \" bytes\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "55e5b0653c34a5f4146ce5a97a5b4a88a976d88a": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5630. Hook up cache directive and pool usage statistics. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548309 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/12/13 1:09 PM",
      "commitName": "55e5b0653c34a5f4146ce5a97a5b4a88a976d88a",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5630. Hook up cache directive and pool usage statistics. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548309 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/12/13 1:09 PM",
          "commitName": "55e5b0653c34a5f4146ce5a97a5b4a88a976d88a",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "27/11/13 11:20 PM",
          "commitNameOld": "9da451cac57f3cd64c2c047675e5b60ca88ecf83",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 7.58,
          "commitsBetweenForRepo": 36,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,62 @@\n-  private void rescanFile(CacheDirective pce, INodeFile file) {\n-    pce.incrementFilesAffected();\n+  private void rescanFile(CacheDirective directive, INodeFile file) {\n     BlockInfo[] blockInfos \u003d file.getBlocks();\n-    long cachedTotal \u003d 0;\n+\n+    // Increment the \"needed\" statistics\n+    directive.addFilesNeeded(1);\n     long neededTotal \u003d 0;\n     for (BlockInfo blockInfo : blockInfos) {\n+      long neededByBlock \u003d \n+          directive.getReplication() * blockInfo.getNumBytes();\n+       neededTotal +\u003d neededByBlock;\n+    }\n+    directive.addBytesNeeded(neededTotal);\n+\n+    // TODO: Enforce per-pool quotas\n+\n+    long cachedTotal \u003d 0;\n+    for (BlockInfo blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n         continue;\n       }\n-      long neededByBlock \u003d \n-         pce.getReplication() * blockInfo.getNumBytes();\n-      neededTotal +\u003d neededByBlock;\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n-          pce.getReplication(), mark);\n+          directive.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n       } else {\n         // Update bytesUsed using the current replication levels.\n         // Assumptions: we assume that all the blocks are the same length\n         // on each datanode.  We can assume this because we\u0027re only caching\n         // blocks in state COMMITTED.\n         // Note that if two directives are caching the same block(s), they will\n         // both get them added to their bytesCached.\n         List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n             ocblock.getDatanodes(Type.CACHED);\n-        long cachedByBlock \u003d Math.min(cachedOn.size(), pce.getReplication()) *\n-            blockInfo.getNumBytes();\n+        long cachedByBlock \u003d Math.min(cachedOn.size(),\n+            directive.getReplication()) * blockInfo.getNumBytes();\n         cachedTotal +\u003d cachedByBlock;\n \n         if (mark !\u003d ocblock.getMark()) {\n           // Mark hasn\u0027t been set in this scan, so update replication and mark.\n-          ocblock.setReplicationAndMark(pce.getReplication(), mark);\n+          ocblock.setReplicationAndMark(directive.getReplication(), mark);\n         } else {\n           // Mark already set in this scan.  Set replication to highest value in\n           // any CacheDirective that covers this file.\n           ocblock.setReplicationAndMark((short)Math.max(\n-              pce.getReplication(), ocblock.getReplication()), mark);\n+              directive.getReplication(), ocblock.getReplication()), mark);\n         }\n       }\n     }\n-    pce.addBytesNeeded(neededTotal);\n-    pce.addBytesCached(cachedTotal);\n+    // Increment the \"cached\" statistics\n+    directive.addBytesCached(cachedTotal);\n+    if (cachedTotal \u003d\u003d neededTotal) {\n+      directive.addFilesCached(1);\n+    }\n     if (LOG.isTraceEnabled()) {\n-      LOG.debug(\"Directive \" + pce.getId() + \" is caching \" +\n-          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal);\n+      LOG.trace(\"Directive \" + directive.getId() + \" is caching \" +\n+          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal +\n+          \" bytes\");\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void rescanFile(CacheDirective directive, INodeFile file) {\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n\n    // Increment the \"needed\" statistics\n    directive.addFilesNeeded(1);\n    long neededTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      long neededByBlock \u003d \n          directive.getReplication() * blockInfo.getNumBytes();\n       neededTotal +\u003d neededByBlock;\n    }\n    directive.addBytesNeeded(neededTotal);\n\n    // TODO: Enforce per-pool quotas\n\n    long cachedTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        continue;\n      }\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          directive.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMMITTED.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(),\n            directive.getReplication()) * blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if (mark !\u003d ocblock.getMark()) {\n          // Mark hasn\u0027t been set in this scan, so update replication and mark.\n          ocblock.setReplicationAndMark(directive.getReplication(), mark);\n        } else {\n          // Mark already set in this scan.  Set replication to highest value in\n          // any CacheDirective that covers this file.\n          ocblock.setReplicationAndMark((short)Math.max(\n              directive.getReplication(), ocblock.getReplication()), mark);\n        }\n      }\n    }\n    // Increment the \"cached\" statistics\n    directive.addBytesCached(cachedTotal);\n    if (cachedTotal \u003d\u003d neededTotal) {\n      directive.addFilesCached(1);\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Directive \" + directive.getId() + \" is caching \" +\n          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal +\n          \" bytes\");\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
          "extendedDetails": {
            "oldValue": "[pce-CacheDirective, file-INodeFile]",
            "newValue": "[directive-CacheDirective, file-INodeFile]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5630. Hook up cache directive and pool usage statistics. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548309 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/12/13 1:09 PM",
          "commitName": "55e5b0653c34a5f4146ce5a97a5b4a88a976d88a",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "27/11/13 11:20 PM",
          "commitNameOld": "9da451cac57f3cd64c2c047675e5b60ca88ecf83",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 7.58,
          "commitsBetweenForRepo": 36,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,62 @@\n-  private void rescanFile(CacheDirective pce, INodeFile file) {\n-    pce.incrementFilesAffected();\n+  private void rescanFile(CacheDirective directive, INodeFile file) {\n     BlockInfo[] blockInfos \u003d file.getBlocks();\n-    long cachedTotal \u003d 0;\n+\n+    // Increment the \"needed\" statistics\n+    directive.addFilesNeeded(1);\n     long neededTotal \u003d 0;\n     for (BlockInfo blockInfo : blockInfos) {\n+      long neededByBlock \u003d \n+          directive.getReplication() * blockInfo.getNumBytes();\n+       neededTotal +\u003d neededByBlock;\n+    }\n+    directive.addBytesNeeded(neededTotal);\n+\n+    // TODO: Enforce per-pool quotas\n+\n+    long cachedTotal \u003d 0;\n+    for (BlockInfo blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n         continue;\n       }\n-      long neededByBlock \u003d \n-         pce.getReplication() * blockInfo.getNumBytes();\n-      neededTotal +\u003d neededByBlock;\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n-          pce.getReplication(), mark);\n+          directive.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n       } else {\n         // Update bytesUsed using the current replication levels.\n         // Assumptions: we assume that all the blocks are the same length\n         // on each datanode.  We can assume this because we\u0027re only caching\n         // blocks in state COMMITTED.\n         // Note that if two directives are caching the same block(s), they will\n         // both get them added to their bytesCached.\n         List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n             ocblock.getDatanodes(Type.CACHED);\n-        long cachedByBlock \u003d Math.min(cachedOn.size(), pce.getReplication()) *\n-            blockInfo.getNumBytes();\n+        long cachedByBlock \u003d Math.min(cachedOn.size(),\n+            directive.getReplication()) * blockInfo.getNumBytes();\n         cachedTotal +\u003d cachedByBlock;\n \n         if (mark !\u003d ocblock.getMark()) {\n           // Mark hasn\u0027t been set in this scan, so update replication and mark.\n-          ocblock.setReplicationAndMark(pce.getReplication(), mark);\n+          ocblock.setReplicationAndMark(directive.getReplication(), mark);\n         } else {\n           // Mark already set in this scan.  Set replication to highest value in\n           // any CacheDirective that covers this file.\n           ocblock.setReplicationAndMark((short)Math.max(\n-              pce.getReplication(), ocblock.getReplication()), mark);\n+              directive.getReplication(), ocblock.getReplication()), mark);\n         }\n       }\n     }\n-    pce.addBytesNeeded(neededTotal);\n-    pce.addBytesCached(cachedTotal);\n+    // Increment the \"cached\" statistics\n+    directive.addBytesCached(cachedTotal);\n+    if (cachedTotal \u003d\u003d neededTotal) {\n+      directive.addFilesCached(1);\n+    }\n     if (LOG.isTraceEnabled()) {\n-      LOG.debug(\"Directive \" + pce.getId() + \" is caching \" +\n-          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal);\n+      LOG.trace(\"Directive \" + directive.getId() + \" is caching \" +\n+          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal +\n+          \" bytes\");\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void rescanFile(CacheDirective directive, INodeFile file) {\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n\n    // Increment the \"needed\" statistics\n    directive.addFilesNeeded(1);\n    long neededTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      long neededByBlock \u003d \n          directive.getReplication() * blockInfo.getNumBytes();\n       neededTotal +\u003d neededByBlock;\n    }\n    directive.addBytesNeeded(neededTotal);\n\n    // TODO: Enforce per-pool quotas\n\n    long cachedTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        continue;\n      }\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          directive.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMMITTED.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(),\n            directive.getReplication()) * blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if (mark !\u003d ocblock.getMark()) {\n          // Mark hasn\u0027t been set in this scan, so update replication and mark.\n          ocblock.setReplicationAndMark(directive.getReplication(), mark);\n        } else {\n          // Mark already set in this scan.  Set replication to highest value in\n          // any CacheDirective that covers this file.\n          ocblock.setReplicationAndMark((short)Math.max(\n              directive.getReplication(), ocblock.getReplication()), mark);\n        }\n      }\n    }\n    // Increment the \"cached\" statistics\n    directive.addBytesCached(cachedTotal);\n    if (cachedTotal \u003d\u003d neededTotal) {\n      directive.addFilesCached(1);\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Directive \" + directive.getId() + \" is caching \" +\n          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal +\n          \" bytes\");\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
          "extendedDetails": {}
        }
      ]
    },
    "13edb391d06c479720202eb5ac81f1c71fe64748": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 9:55 AM",
      "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "21/11/13 9:12 AM",
      "commitNameOld": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 6.03,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,50 @@\n   private void rescanFile(CacheDirective pce, INodeFile file) {\n     pce.incrementFilesAffected();\n     BlockInfo[] blockInfos \u003d file.getBlocks();\n     long cachedTotal \u003d 0;\n     long neededTotal \u003d 0;\n     for (BlockInfo blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n         continue;\n       }\n       long neededByBlock \u003d \n          pce.getReplication() * blockInfo.getNumBytes();\n       neededTotal +\u003d neededByBlock;\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n           pce.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n       } else {\n         // Update bytesUsed using the current replication levels.\n         // Assumptions: we assume that all the blocks are the same length\n         // on each datanode.  We can assume this because we\u0027re only caching\n         // blocks in state COMMITTED.\n         // Note that if two directives are caching the same block(s), they will\n         // both get them added to their bytesCached.\n         List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n             ocblock.getDatanodes(Type.CACHED);\n         long cachedByBlock \u003d Math.min(cachedOn.size(), pce.getReplication()) *\n             blockInfo.getNumBytes();\n         cachedTotal +\u003d cachedByBlock;\n \n         if (mark !\u003d ocblock.getMark()) {\n           // Mark hasn\u0027t been set in this scan, so update replication and mark.\n           ocblock.setReplicationAndMark(pce.getReplication(), mark);\n         } else {\n           // Mark already set in this scan.  Set replication to highest value in\n           // any CacheDirective that covers this file.\n           ocblock.setReplicationAndMark((short)Math.max(\n               pce.getReplication(), ocblock.getReplication()), mark);\n         }\n       }\n     }\n     pce.addBytesNeeded(neededTotal);\n     pce.addBytesCached(cachedTotal);\n     if (LOG.isTraceEnabled()) {\n-      LOG.debug(\"Directive \" + pce.getEntryId() + \" is caching \" +\n+      LOG.debug(\"Directive \" + pce.getId() + \" is caching \" +\n           file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void rescanFile(CacheDirective pce, INodeFile file) {\n    pce.incrementFilesAffected();\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n    long cachedTotal \u003d 0;\n    long neededTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        continue;\n      }\n      long neededByBlock \u003d \n         pce.getReplication() * blockInfo.getNumBytes();\n      neededTotal +\u003d neededByBlock;\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          pce.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMMITTED.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(), pce.getReplication()) *\n            blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if (mark !\u003d ocblock.getMark()) {\n          // Mark hasn\u0027t been set in this scan, so update replication and mark.\n          ocblock.setReplicationAndMark(pce.getReplication(), mark);\n        } else {\n          // Mark already set in this scan.  Set replication to highest value in\n          // any CacheDirective that covers this file.\n          ocblock.setReplicationAndMark((short)Math.max(\n              pce.getReplication(), ocblock.getReplication()), mark);\n        }\n      }\n    }\n    pce.addBytesNeeded(neededTotal);\n    pce.addBytesCached(cachedTotal);\n    if (LOG.isTraceEnabled()) {\n      LOG.debug(\"Directive \" + pce.getId() + \" is caching \" +\n          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5473. Consistent naming of user-visible caching classes and methods (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544252 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/13 9:12 AM",
      "commitName": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5473. Consistent naming of user-visible caching classes and methods (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544252 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/11/13 9:12 AM",
          "commitName": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "20/11/13 1:31 PM",
          "commitNameOld": "916ab9286b6006571649d21c74d9ae70273a3ddc",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.82,
          "commitsBetweenForRepo": 10,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,50 @@\n-  private void rescanFile(PathBasedCacheEntry pce, INodeFile file) {\n+  private void rescanFile(CacheDirective pce, INodeFile file) {\n     pce.incrementFilesAffected();\n     BlockInfo[] blockInfos \u003d file.getBlocks();\n     long cachedTotal \u003d 0;\n     long neededTotal \u003d 0;\n     for (BlockInfo blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n         continue;\n       }\n       long neededByBlock \u003d \n          pce.getReplication() * blockInfo.getNumBytes();\n       neededTotal +\u003d neededByBlock;\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n           pce.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n       } else {\n         // Update bytesUsed using the current replication levels.\n         // Assumptions: we assume that all the blocks are the same length\n         // on each datanode.  We can assume this because we\u0027re only caching\n         // blocks in state COMMITTED.\n         // Note that if two directives are caching the same block(s), they will\n         // both get them added to their bytesCached.\n         List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n             ocblock.getDatanodes(Type.CACHED);\n         long cachedByBlock \u003d Math.min(cachedOn.size(), pce.getReplication()) *\n             blockInfo.getNumBytes();\n         cachedTotal +\u003d cachedByBlock;\n \n         if (mark !\u003d ocblock.getMark()) {\n           // Mark hasn\u0027t been set in this scan, so update replication and mark.\n           ocblock.setReplicationAndMark(pce.getReplication(), mark);\n         } else {\n           // Mark already set in this scan.  Set replication to highest value in\n-          // any PathBasedCacheEntry that covers this file.\n+          // any CacheDirective that covers this file.\n           ocblock.setReplicationAndMark((short)Math.max(\n               pce.getReplication(), ocblock.getReplication()), mark);\n         }\n       }\n     }\n     pce.addBytesNeeded(neededTotal);\n     pce.addBytesCached(cachedTotal);\n     if (LOG.isTraceEnabled()) {\n       LOG.debug(\"Directive \" + pce.getEntryId() + \" is caching \" +\n           file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void rescanFile(CacheDirective pce, INodeFile file) {\n    pce.incrementFilesAffected();\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n    long cachedTotal \u003d 0;\n    long neededTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        continue;\n      }\n      long neededByBlock \u003d \n         pce.getReplication() * blockInfo.getNumBytes();\n      neededTotal +\u003d neededByBlock;\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          pce.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMMITTED.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(), pce.getReplication()) *\n            blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if (mark !\u003d ocblock.getMark()) {\n          // Mark hasn\u0027t been set in this scan, so update replication and mark.\n          ocblock.setReplicationAndMark(pce.getReplication(), mark);\n        } else {\n          // Mark already set in this scan.  Set replication to highest value in\n          // any CacheDirective that covers this file.\n          ocblock.setReplicationAndMark((short)Math.max(\n              pce.getReplication(), ocblock.getReplication()), mark);\n        }\n      }\n    }\n    pce.addBytesNeeded(neededTotal);\n    pce.addBytesCached(cachedTotal);\n    if (LOG.isTraceEnabled()) {\n      LOG.debug(\"Directive \" + pce.getEntryId() + \" is caching \" +\n          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
          "extendedDetails": {
            "oldValue": "[pce-PathBasedCacheEntry, file-INodeFile]",
            "newValue": "[pce-CacheDirective, file-INodeFile]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5473. Consistent naming of user-visible caching classes and methods (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544252 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/11/13 9:12 AM",
          "commitName": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "20/11/13 1:31 PM",
          "commitNameOld": "916ab9286b6006571649d21c74d9ae70273a3ddc",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.82,
          "commitsBetweenForRepo": 10,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,50 @@\n-  private void rescanFile(PathBasedCacheEntry pce, INodeFile file) {\n+  private void rescanFile(CacheDirective pce, INodeFile file) {\n     pce.incrementFilesAffected();\n     BlockInfo[] blockInfos \u003d file.getBlocks();\n     long cachedTotal \u003d 0;\n     long neededTotal \u003d 0;\n     for (BlockInfo blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n         continue;\n       }\n       long neededByBlock \u003d \n          pce.getReplication() * blockInfo.getNumBytes();\n       neededTotal +\u003d neededByBlock;\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n           pce.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n       } else {\n         // Update bytesUsed using the current replication levels.\n         // Assumptions: we assume that all the blocks are the same length\n         // on each datanode.  We can assume this because we\u0027re only caching\n         // blocks in state COMMITTED.\n         // Note that if two directives are caching the same block(s), they will\n         // both get them added to their bytesCached.\n         List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n             ocblock.getDatanodes(Type.CACHED);\n         long cachedByBlock \u003d Math.min(cachedOn.size(), pce.getReplication()) *\n             blockInfo.getNumBytes();\n         cachedTotal +\u003d cachedByBlock;\n \n         if (mark !\u003d ocblock.getMark()) {\n           // Mark hasn\u0027t been set in this scan, so update replication and mark.\n           ocblock.setReplicationAndMark(pce.getReplication(), mark);\n         } else {\n           // Mark already set in this scan.  Set replication to highest value in\n-          // any PathBasedCacheEntry that covers this file.\n+          // any CacheDirective that covers this file.\n           ocblock.setReplicationAndMark((short)Math.max(\n               pce.getReplication(), ocblock.getReplication()), mark);\n         }\n       }\n     }\n     pce.addBytesNeeded(neededTotal);\n     pce.addBytesCached(cachedTotal);\n     if (LOG.isTraceEnabled()) {\n       LOG.debug(\"Directive \" + pce.getEntryId() + \" is caching \" +\n           file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void rescanFile(CacheDirective pce, INodeFile file) {\n    pce.incrementFilesAffected();\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n    long cachedTotal \u003d 0;\n    long neededTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        continue;\n      }\n      long neededByBlock \u003d \n         pce.getReplication() * blockInfo.getNumBytes();\n      neededTotal +\u003d neededByBlock;\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          pce.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMMITTED.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(), pce.getReplication()) *\n            blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if (mark !\u003d ocblock.getMark()) {\n          // Mark hasn\u0027t been set in this scan, so update replication and mark.\n          ocblock.setReplicationAndMark(pce.getReplication(), mark);\n        } else {\n          // Mark already set in this scan.  Set replication to highest value in\n          // any CacheDirective that covers this file.\n          ocblock.setReplicationAndMark((short)Math.max(\n              pce.getReplication(), ocblock.getReplication()), mark);\n        }\n      }\n    }\n    pce.addBytesNeeded(neededTotal);\n    pce.addBytesCached(cachedTotal);\n    if (LOG.isTraceEnabled()) {\n      LOG.debug(\"Directive \" + pce.getEntryId() + \" is caching \" +\n          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
          "extendedDetails": {}
        }
      ]
    },
    "916ab9286b6006571649d21c74d9ae70273a3ddc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5451. Add byte and file statistics to PathBasedCacheEntry. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543958 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/11/13 1:31 PM",
      "commitName": "916ab9286b6006571649d21c74d9ae70273a3ddc",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "13/11/13 10:18 AM",
      "commitNameOld": "3c591aa442d342bdd4a0c4abe9a43c64d8ef3e65",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 7.13,
      "commitsBetweenForRepo": 62,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,50 @@\n   private void rescanFile(PathBasedCacheEntry pce, INodeFile file) {\n+    pce.incrementFilesAffected();\n     BlockInfo[] blockInfos \u003d file.getBlocks();\n+    long cachedTotal \u003d 0;\n+    long neededTotal \u003d 0;\n     for (BlockInfo blockInfo : blockInfos) {\n       if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n         // We don\u0027t try to cache blocks that are under construction.\n         continue;\n       }\n+      long neededByBlock \u003d \n+         pce.getReplication() * blockInfo.getNumBytes();\n+      neededTotal +\u003d neededByBlock;\n       Block block \u003d new Block(blockInfo.getBlockId());\n       CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n           pce.getReplication(), mark);\n       CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n       if (ocblock \u003d\u003d null) {\n         cachedBlocks.put(ncblock);\n       } else {\n+        // Update bytesUsed using the current replication levels.\n+        // Assumptions: we assume that all the blocks are the same length\n+        // on each datanode.  We can assume this because we\u0027re only caching\n+        // blocks in state COMMITTED.\n+        // Note that if two directives are caching the same block(s), they will\n+        // both get them added to their bytesCached.\n+        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n+            ocblock.getDatanodes(Type.CACHED);\n+        long cachedByBlock \u003d Math.min(cachedOn.size(), pce.getReplication()) *\n+            blockInfo.getNumBytes();\n+        cachedTotal +\u003d cachedByBlock;\n+\n         if (mark !\u003d ocblock.getMark()) {\n           // Mark hasn\u0027t been set in this scan, so update replication and mark.\n           ocblock.setReplicationAndMark(pce.getReplication(), mark);\n         } else {\n           // Mark already set in this scan.  Set replication to highest value in\n           // any PathBasedCacheEntry that covers this file.\n           ocblock.setReplicationAndMark((short)Math.max(\n               pce.getReplication(), ocblock.getReplication()), mark);\n         }\n       }\n     }\n+    pce.addBytesNeeded(neededTotal);\n+    pce.addBytesCached(cachedTotal);\n+    if (LOG.isTraceEnabled()) {\n+      LOG.debug(\"Directive \" + pce.getEntryId() + \" is caching \" +\n+          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void rescanFile(PathBasedCacheEntry pce, INodeFile file) {\n    pce.incrementFilesAffected();\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n    long cachedTotal \u003d 0;\n    long neededTotal \u003d 0;\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        continue;\n      }\n      long neededByBlock \u003d \n         pce.getReplication() * blockInfo.getNumBytes();\n      neededTotal +\u003d neededByBlock;\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          pce.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n      } else {\n        // Update bytesUsed using the current replication levels.\n        // Assumptions: we assume that all the blocks are the same length\n        // on each datanode.  We can assume this because we\u0027re only caching\n        // blocks in state COMMITTED.\n        // Note that if two directives are caching the same block(s), they will\n        // both get them added to their bytesCached.\n        List\u003cDatanodeDescriptor\u003e cachedOn \u003d\n            ocblock.getDatanodes(Type.CACHED);\n        long cachedByBlock \u003d Math.min(cachedOn.size(), pce.getReplication()) *\n            blockInfo.getNumBytes();\n        cachedTotal +\u003d cachedByBlock;\n\n        if (mark !\u003d ocblock.getMark()) {\n          // Mark hasn\u0027t been set in this scan, so update replication and mark.\n          ocblock.setReplicationAndMark(pce.getReplication(), mark);\n        } else {\n          // Mark already set in this scan.  Set replication to highest value in\n          // any PathBasedCacheEntry that covers this file.\n          ocblock.setReplicationAndMark((short)Math.max(\n              pce.getReplication(), ocblock.getReplication()), mark);\n        }\n      }\n    }\n    pce.addBytesNeeded(neededTotal);\n    pce.addBytesCached(cachedTotal);\n    if (LOG.isTraceEnabled()) {\n      LOG.debug(\"Directive \" + pce.getEntryId() + \" is caching \" +\n          file.getFullPathName() + \": \" + cachedTotal + \"/\" + neededTotal);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
      "extendedDetails": {}
    },
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 3:15 PM",
      "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,26 @@\n+  private void rescanFile(PathBasedCacheEntry pce, INodeFile file) {\n+    BlockInfo[] blockInfos \u003d file.getBlocks();\n+    for (BlockInfo blockInfo : blockInfos) {\n+      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n+        // We don\u0027t try to cache blocks that are under construction.\n+        continue;\n+      }\n+      Block block \u003d new Block(blockInfo.getBlockId());\n+      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n+          pce.getReplication(), mark);\n+      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n+      if (ocblock \u003d\u003d null) {\n+        cachedBlocks.put(ncblock);\n+      } else {\n+        if (mark !\u003d ocblock.getMark()) {\n+          // Mark hasn\u0027t been set in this scan, so update replication and mark.\n+          ocblock.setReplicationAndMark(pce.getReplication(), mark);\n+        } else {\n+          // Mark already set in this scan.  Set replication to highest value in\n+          // any PathBasedCacheEntry that covers this file.\n+          ocblock.setReplicationAndMark((short)Math.max(\n+              pce.getReplication(), ocblock.getReplication()), mark);\n+        }\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void rescanFile(PathBasedCacheEntry pce, INodeFile file) {\n    BlockInfo[] blockInfos \u003d file.getBlocks();\n    for (BlockInfo blockInfo : blockInfos) {\n      if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {\n        // We don\u0027t try to cache blocks that are under construction.\n        continue;\n      }\n      Block block \u003d new Block(blockInfo.getBlockId());\n      CachedBlock ncblock \u003d new CachedBlock(block.getBlockId(),\n          pce.getReplication(), mark);\n      CachedBlock ocblock \u003d cachedBlocks.get(ncblock);\n      if (ocblock \u003d\u003d null) {\n        cachedBlocks.put(ncblock);\n      } else {\n        if (mark !\u003d ocblock.getMark()) {\n          // Mark hasn\u0027t been set in this scan, so update replication and mark.\n          ocblock.setReplicationAndMark(pce.getReplication(), mark);\n        } else {\n          // Mark already set in this scan.  Set replication to highest value in\n          // any PathBasedCacheEntry that covers this file.\n          ocblock.setReplicationAndMark((short)Math.max(\n              pce.getReplication(), ocblock.getReplication()), mark);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java"
    }
  }
}