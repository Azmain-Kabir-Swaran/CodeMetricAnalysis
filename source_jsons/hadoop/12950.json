{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeAdminDefaultMonitor.java",
  "functionName": "check",
  "functionId": "check",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java",
  "functionStartLine": 183,
  "functionEndLine": 287,
  "numCommitsSeen": 50,
  "timeTaken": 11476,
  "changeHistory": [
    "c93cb6790e0f1c64efd03d859f907a0522010894",
    "5747f6cff54f79de0e6439d6c77c2ed437989f10",
    "eccc9a40deda212cb367627f6f4cc35f5c619941",
    "6f81cc0beea00843b44424417f09d8ee12cd7bae",
    "79df1e750ef558afed6d166ce225a23061b36aed",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "332a61fd74fd2a9874319232c583ab5d2c53ff03",
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978",
    "796a676d18bd7cd3ed4113d002e0e69cf261d6d1",
    "a3990ca41415515b986a41dacefceee1f05622f8",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "abf833a7b228fff2bca4f69cd9df99d532380038",
    "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a",
    "9692cfc9ae2ac86c9a2d2b3ac9ca8f8b3bfc7c42",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "7fac946ac983e31613fd62836c8ac9c4a579210a",
    "969a263188f7015261719fe45fa1505121ebb80e",
    "89537b7710b23db7abcd2a77f03818c06a5f5fa7",
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
    "b9189f7b158f9c50566a1b591e25e3d76c3b0917",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "c93cb6790e0f1c64efd03d859f907a0522010894": "Ymultichange(Ymovefromfile,Ybodychange)",
    "5747f6cff54f79de0e6439d6c77c2ed437989f10": "Ybodychange",
    "eccc9a40deda212cb367627f6f4cc35f5c619941": "Ybodychange",
    "6f81cc0beea00843b44424417f09d8ee12cd7bae": "Ybodychange",
    "79df1e750ef558afed6d166ce225a23061b36aed": "Ymultichange(Yfilerename,Ybodychange)",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "332a61fd74fd2a9874319232c583ab5d2c53ff03": "Ybodychange",
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978": "Ybodychange",
    "796a676d18bd7cd3ed4113d002e0e69cf261d6d1": "Ybodychange",
    "a3990ca41415515b986a41dacefceee1f05622f8": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "abf833a7b228fff2bca4f69cd9df99d532380038": "Ybodychange",
    "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a": "Ybodychange",
    "9692cfc9ae2ac86c9a2d2b3ac9ca8f8b3bfc7c42": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "7fac946ac983e31613fd62836c8ac9c4a579210a": "Ybodychange",
    "969a263188f7015261719fe45fa1505121ebb80e": "Ybodychange",
    "89537b7710b23db7abcd2a77f03818c06a5f5fa7": "Ybodychange",
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9": "Ybodychange",
    "b9189f7b158f9c50566a1b591e25e3d76c3b0917": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c93cb6790e0f1c64efd03d859f907a0522010894": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-14854. Create improved decommission monitor implementation. Contributed by Stephen O\u0027Donnell.\n\nReviewed-by: Inigo Goiri \u003cinigoiri@apache.org\u003e\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "10/12/19 5:16 PM",
      "commitName": "c93cb6790e0f1c64efd03d859f907a0522010894",
      "commitAuthor": "Stephen O\u0027Donnell",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-14854. Create improved decommission monitor implementation. Contributed by Stephen O\u0027Donnell.\n\nReviewed-by: Inigo Goiri \u003cinigoiri@apache.org\u003e\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
          "commitDate": "10/12/19 5:16 PM",
          "commitName": "c93cb6790e0f1c64efd03d859f907a0522010894",
          "commitAuthor": "Stephen O\u0027Donnell",
          "commitDateOld": "10/12/19 6:51 AM",
          "commitNameOld": "875a3e97dd4a26fe224a1858c54d1b4512db6be3",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 0.43,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,105 +1,105 @@\n-    private void check() {\n-      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n-          it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n-              iterkey).iterator();\n-      final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n+  private void check() {\n+    final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n+        it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n+        iterkey).iterator();\n+    final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n \n-      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n-          .isRunning()) {\n-        numNodesChecked++;\n-        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n-            entry \u003d it.next();\n-        final DatanodeDescriptor dn \u003d entry.getKey();\n-        try {\n-          AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n-          boolean fullScan \u003d false;\n-          if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n-            // If maintenance expires, stop tracking it.\n-            stopMaintenance(dn);\n-            toRemove.add(dn);\n-            continue;\n-          }\n-          if (dn.isInMaintenance()) {\n-            // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n-            continue;\n-          }\n-          if (blocks \u003d\u003d null) {\n-            // This is a newly added datanode, run through its list to schedule\n-            // under-replicated blocks for replication and collect the blocks\n-            // that are insufficiently replicated for further tracking\n-            LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n-                \"insufficiently-replicated blocks.\", dn);\n+    while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n+        .isRunning()) {\n+      numNodesChecked++;\n+      final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n+          entry \u003d it.next();\n+      final DatanodeDescriptor dn \u003d entry.getKey();\n+      try {\n+        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n+        boolean fullScan \u003d false;\n+        if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n+          // If maintenance expires, stop tracking it.\n+          dnAdmin.stopMaintenance(dn);\n+          toRemove.add(dn);\n+          continue;\n+        }\n+        if (dn.isInMaintenance()) {\n+          // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n+          continue;\n+        }\n+        if (blocks \u003d\u003d null) {\n+          // This is a newly added datanode, run through its list to schedule\n+          // under-replicated blocks for replication and collect the blocks\n+          // that are insufficiently replicated for further tracking\n+          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n+              \"insufficiently-replicated blocks.\", dn);\n+          blocks \u003d handleInsufficientlyStored(dn);\n+          outOfServiceNodeBlocks.put(dn, blocks);\n+          fullScan \u003d true;\n+        } else {\n+          // This is a known datanode, check if its # of insufficiently\n+          // replicated blocks has dropped to zero and if it can move\n+          // to the next state.\n+          LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n+          pruneReliableBlocks(dn, blocks);\n+        }\n+        if (blocks.size() \u003d\u003d 0) {\n+          if (!fullScan) {\n+            // If we didn\u0027t just do a full scan, need to re-check with the\n+            // full block map.\n+            //\n+            // We\u0027ve replicated all the known insufficiently replicated\n+            // blocks. Re-check with the full block map before finally\n+            // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n+            LOG.debug(\"Node {} has finished replicating current set of \"\n+                + \"blocks, checking with the full block map.\", dn);\n             blocks \u003d handleInsufficientlyStored(dn);\n             outOfServiceNodeBlocks.put(dn, blocks);\n-            fullScan \u003d true;\n-          } else {\n-            // This is a known datanode, check if its # of insufficiently\n-            // replicated blocks has dropped to zero and if it can move\n-            // to the next state.\n-            LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n-            pruneReliableBlocks(dn, blocks);\n           }\n-          if (blocks.size() \u003d\u003d 0) {\n-            if (!fullScan) {\n-              // If we didn\u0027t just do a full scan, need to re-check with the\n-              // full block map.\n-              //\n-              // We\u0027ve replicated all the known insufficiently replicated\n-              // blocks. Re-check with the full block map before finally\n-              // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n-              LOG.debug(\"Node {} has finished replicating current set of \"\n-                  + \"blocks, checking with the full block map.\", dn);\n-              blocks \u003d handleInsufficientlyStored(dn);\n-              outOfServiceNodeBlocks.put(dn, blocks);\n-            }\n-            // If the full scan is clean AND the node liveness is okay,\n-            // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n-            final boolean isHealthy \u003d\n-                blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n-            if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n-              if (dn.isDecommissionInProgress()) {\n-                setDecommissioned(dn);\n-                toRemove.add(dn);\n-              } else if (dn.isEnteringMaintenance()) {\n-                // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n-                // to track maintenance expiration.\n-                setInMaintenance(dn);\n-              } else {\n-                Preconditions.checkState(false,\n-                    \"Node %s is in an invalid state! \"\n-                      + \"Invalid state: %s %s blocks are on this dn.\",\n-                        dn, dn.getAdminState(), blocks.size());\n-              }\n-              LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n-                  + \"marked as {}.\", dn, dn.getAdminState());\n+          // If the full scan is clean AND the node liveness is okay,\n+          // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n+          final boolean isHealthy \u003d\n+              blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n+          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n+            if (dn.isDecommissionInProgress()) {\n+              dnAdmin.setDecommissioned(dn);\n+              toRemove.add(dn);\n+            } else if (dn.isEnteringMaintenance()) {\n+              // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n+              // to track maintenance expiration.\n+              dnAdmin.setInMaintenance(dn);\n             } else {\n-              LOG.info(\"Node {} {} healthy.\"\n-                  + \" It needs to replicate {} more blocks.\"\n-                  + \" {} is still in progress.\", dn,\n-                  isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n+              Preconditions.checkState(false,\n+                  \"Node %s is in an invalid state! \"\n+                      + \"Invalid state: %s %s blocks are on this dn.\",\n+                  dn, dn.getAdminState(), blocks.size());\n             }\n+            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n+                + \"marked as {}.\", dn, dn.getAdminState());\n           } else {\n-            LOG.info(\"Node {} still has {} blocks to replicate \"\n-                    + \"before it is a candidate to finish {}.\",\n-                dn, blocks.size(), dn.getAdminState());\n+            LOG.info(\"Node {} {} healthy.\"\n+                    + \" It needs to replicate {} more blocks.\"\n+                    + \" {} is still in progress.\", dn,\n+                isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n           }\n-        } catch (Exception e) {\n-          // Log and postpone to process node when meet exception since it is in\n-          // an invalid state.\n-          LOG.warn(\"DatanodeAdminMonitor caught exception when processing node \"\n-              + \"{}.\", dn, e);\n-          pendingNodes.add(dn);\n-          toRemove.add(dn);\n-        } finally {\n-          iterkey \u003d dn;\n+        } else {\n+          LOG.info(\"Node {} still has {} blocks to replicate \"\n+                  + \"before it is a candidate to finish {}.\",\n+              dn, blocks.size(), dn.getAdminState());\n         }\n+      } catch (Exception e) {\n+        // Log and postpone to process node when meet exception since it is in\n+        // an invalid state.\n+        LOG.warn(\"DatanodeAdminMonitor caught exception when processing node \"\n+            + \"{}.\", dn, e);\n+        pendingNodes.add(dn);\n+        toRemove.add(dn);\n+      } finally {\n+        iterkey \u003d dn;\n       }\n-      // Remove the datanodes that are DECOMMISSIONED or in service after\n-      // maintenance expiration.\n-      for (DatanodeDescriptor dn : toRemove) {\n-        Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n-            \"Removing node %s that is not yet decommissioned or in service!\",\n-                dn);\n-        outOfServiceNodeBlocks.remove(dn);\n-      }\n-    }\n\\ No newline at end of file\n+    }\n+    // Remove the datanodes that are DECOMMISSIONED or in service after\n+    // maintenance expiration.\n+    for (DatanodeDescriptor dn : toRemove) {\n+      Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n+          \"Removing node %s that is not yet decommissioned or in service!\",\n+          dn);\n+      outOfServiceNodeBlocks.remove(dn);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void check() {\n    final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n        it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n        iterkey).iterator();\n    final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n\n    while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n        .isRunning()) {\n      numNodesChecked++;\n      final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n          entry \u003d it.next();\n      final DatanodeDescriptor dn \u003d entry.getKey();\n      try {\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n          // If maintenance expires, stop tracking it.\n          dnAdmin.stopMaintenance(dn);\n          toRemove.add(dn);\n          continue;\n        }\n        if (dn.isInMaintenance()) {\n          // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n          continue;\n        }\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule\n          // under-replicated blocks for replication and collect the blocks\n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyStored(dn);\n          outOfServiceNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently\n          // replicated blocks has dropped to zero and if it can move\n          // to the next state.\n          LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n          pruneReliableBlocks(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the\n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated\n            // blocks. Re-check with the full block map before finally\n            // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            outOfServiceNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay,\n          // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            if (dn.isDecommissionInProgress()) {\n              dnAdmin.setDecommissioned(dn);\n              toRemove.add(dn);\n            } else if (dn.isEnteringMaintenance()) {\n              // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n              // to track maintenance expiration.\n              dnAdmin.setInMaintenance(dn);\n            } else {\n              Preconditions.checkState(false,\n                  \"Node %s is in an invalid state! \"\n                      + \"Invalid state: %s %s blocks are on this dn.\",\n                  dn, dn.getAdminState(), blocks.size());\n            }\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as {}.\", dn, dn.getAdminState());\n          } else {\n            LOG.info(\"Node {} {} healthy.\"\n                    + \" It needs to replicate {} more blocks.\"\n                    + \" {} is still in progress.\", dn,\n                isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n          }\n        } else {\n          LOG.info(\"Node {} still has {} blocks to replicate \"\n                  + \"before it is a candidate to finish {}.\",\n              dn, blocks.size(), dn.getAdminState());\n        }\n      } catch (Exception e) {\n        // Log and postpone to process node when meet exception since it is in\n        // an invalid state.\n        LOG.warn(\"DatanodeAdminMonitor caught exception when processing node \"\n            + \"{}.\", dn, e);\n        pendingNodes.add(dn);\n        toRemove.add(dn);\n      } finally {\n        iterkey \u003d dn;\n      }\n    }\n    // Remove the datanodes that are DECOMMISSIONED or in service after\n    // maintenance expiration.\n    for (DatanodeDescriptor dn : toRemove) {\n      Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n          \"Removing node %s that is not yet decommissioned or in service!\",\n          dn);\n      outOfServiceNodeBlocks.remove(dn);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java",
            "oldMethodName": "check",
            "newMethodName": "check"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-14854. Create improved decommission monitor implementation. Contributed by Stephen O\u0027Donnell.\n\nReviewed-by: Inigo Goiri \u003cinigoiri@apache.org\u003e\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
          "commitDate": "10/12/19 5:16 PM",
          "commitName": "c93cb6790e0f1c64efd03d859f907a0522010894",
          "commitAuthor": "Stephen O\u0027Donnell",
          "commitDateOld": "10/12/19 6:51 AM",
          "commitNameOld": "875a3e97dd4a26fe224a1858c54d1b4512db6be3",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 0.43,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,105 +1,105 @@\n-    private void check() {\n-      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n-          it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n-              iterkey).iterator();\n-      final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n+  private void check() {\n+    final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n+        it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n+        iterkey).iterator();\n+    final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n \n-      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n-          .isRunning()) {\n-        numNodesChecked++;\n-        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n-            entry \u003d it.next();\n-        final DatanodeDescriptor dn \u003d entry.getKey();\n-        try {\n-          AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n-          boolean fullScan \u003d false;\n-          if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n-            // If maintenance expires, stop tracking it.\n-            stopMaintenance(dn);\n-            toRemove.add(dn);\n-            continue;\n-          }\n-          if (dn.isInMaintenance()) {\n-            // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n-            continue;\n-          }\n-          if (blocks \u003d\u003d null) {\n-            // This is a newly added datanode, run through its list to schedule\n-            // under-replicated blocks for replication and collect the blocks\n-            // that are insufficiently replicated for further tracking\n-            LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n-                \"insufficiently-replicated blocks.\", dn);\n+    while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n+        .isRunning()) {\n+      numNodesChecked++;\n+      final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n+          entry \u003d it.next();\n+      final DatanodeDescriptor dn \u003d entry.getKey();\n+      try {\n+        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n+        boolean fullScan \u003d false;\n+        if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n+          // If maintenance expires, stop tracking it.\n+          dnAdmin.stopMaintenance(dn);\n+          toRemove.add(dn);\n+          continue;\n+        }\n+        if (dn.isInMaintenance()) {\n+          // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n+          continue;\n+        }\n+        if (blocks \u003d\u003d null) {\n+          // This is a newly added datanode, run through its list to schedule\n+          // under-replicated blocks for replication and collect the blocks\n+          // that are insufficiently replicated for further tracking\n+          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n+              \"insufficiently-replicated blocks.\", dn);\n+          blocks \u003d handleInsufficientlyStored(dn);\n+          outOfServiceNodeBlocks.put(dn, blocks);\n+          fullScan \u003d true;\n+        } else {\n+          // This is a known datanode, check if its # of insufficiently\n+          // replicated blocks has dropped to zero and if it can move\n+          // to the next state.\n+          LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n+          pruneReliableBlocks(dn, blocks);\n+        }\n+        if (blocks.size() \u003d\u003d 0) {\n+          if (!fullScan) {\n+            // If we didn\u0027t just do a full scan, need to re-check with the\n+            // full block map.\n+            //\n+            // We\u0027ve replicated all the known insufficiently replicated\n+            // blocks. Re-check with the full block map before finally\n+            // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n+            LOG.debug(\"Node {} has finished replicating current set of \"\n+                + \"blocks, checking with the full block map.\", dn);\n             blocks \u003d handleInsufficientlyStored(dn);\n             outOfServiceNodeBlocks.put(dn, blocks);\n-            fullScan \u003d true;\n-          } else {\n-            // This is a known datanode, check if its # of insufficiently\n-            // replicated blocks has dropped to zero and if it can move\n-            // to the next state.\n-            LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n-            pruneReliableBlocks(dn, blocks);\n           }\n-          if (blocks.size() \u003d\u003d 0) {\n-            if (!fullScan) {\n-              // If we didn\u0027t just do a full scan, need to re-check with the\n-              // full block map.\n-              //\n-              // We\u0027ve replicated all the known insufficiently replicated\n-              // blocks. Re-check with the full block map before finally\n-              // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n-              LOG.debug(\"Node {} has finished replicating current set of \"\n-                  + \"blocks, checking with the full block map.\", dn);\n-              blocks \u003d handleInsufficientlyStored(dn);\n-              outOfServiceNodeBlocks.put(dn, blocks);\n-            }\n-            // If the full scan is clean AND the node liveness is okay,\n-            // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n-            final boolean isHealthy \u003d\n-                blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n-            if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n-              if (dn.isDecommissionInProgress()) {\n-                setDecommissioned(dn);\n-                toRemove.add(dn);\n-              } else if (dn.isEnteringMaintenance()) {\n-                // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n-                // to track maintenance expiration.\n-                setInMaintenance(dn);\n-              } else {\n-                Preconditions.checkState(false,\n-                    \"Node %s is in an invalid state! \"\n-                      + \"Invalid state: %s %s blocks are on this dn.\",\n-                        dn, dn.getAdminState(), blocks.size());\n-              }\n-              LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n-                  + \"marked as {}.\", dn, dn.getAdminState());\n+          // If the full scan is clean AND the node liveness is okay,\n+          // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n+          final boolean isHealthy \u003d\n+              blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n+          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n+            if (dn.isDecommissionInProgress()) {\n+              dnAdmin.setDecommissioned(dn);\n+              toRemove.add(dn);\n+            } else if (dn.isEnteringMaintenance()) {\n+              // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n+              // to track maintenance expiration.\n+              dnAdmin.setInMaintenance(dn);\n             } else {\n-              LOG.info(\"Node {} {} healthy.\"\n-                  + \" It needs to replicate {} more blocks.\"\n-                  + \" {} is still in progress.\", dn,\n-                  isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n+              Preconditions.checkState(false,\n+                  \"Node %s is in an invalid state! \"\n+                      + \"Invalid state: %s %s blocks are on this dn.\",\n+                  dn, dn.getAdminState(), blocks.size());\n             }\n+            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n+                + \"marked as {}.\", dn, dn.getAdminState());\n           } else {\n-            LOG.info(\"Node {} still has {} blocks to replicate \"\n-                    + \"before it is a candidate to finish {}.\",\n-                dn, blocks.size(), dn.getAdminState());\n+            LOG.info(\"Node {} {} healthy.\"\n+                    + \" It needs to replicate {} more blocks.\"\n+                    + \" {} is still in progress.\", dn,\n+                isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n           }\n-        } catch (Exception e) {\n-          // Log and postpone to process node when meet exception since it is in\n-          // an invalid state.\n-          LOG.warn(\"DatanodeAdminMonitor caught exception when processing node \"\n-              + \"{}.\", dn, e);\n-          pendingNodes.add(dn);\n-          toRemove.add(dn);\n-        } finally {\n-          iterkey \u003d dn;\n+        } else {\n+          LOG.info(\"Node {} still has {} blocks to replicate \"\n+                  + \"before it is a candidate to finish {}.\",\n+              dn, blocks.size(), dn.getAdminState());\n         }\n+      } catch (Exception e) {\n+        // Log and postpone to process node when meet exception since it is in\n+        // an invalid state.\n+        LOG.warn(\"DatanodeAdminMonitor caught exception when processing node \"\n+            + \"{}.\", dn, e);\n+        pendingNodes.add(dn);\n+        toRemove.add(dn);\n+      } finally {\n+        iterkey \u003d dn;\n       }\n-      // Remove the datanodes that are DECOMMISSIONED or in service after\n-      // maintenance expiration.\n-      for (DatanodeDescriptor dn : toRemove) {\n-        Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n-            \"Removing node %s that is not yet decommissioned or in service!\",\n-                dn);\n-        outOfServiceNodeBlocks.remove(dn);\n-      }\n-    }\n\\ No newline at end of file\n+    }\n+    // Remove the datanodes that are DECOMMISSIONED or in service after\n+    // maintenance expiration.\n+    for (DatanodeDescriptor dn : toRemove) {\n+      Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n+          \"Removing node %s that is not yet decommissioned or in service!\",\n+          dn);\n+      outOfServiceNodeBlocks.remove(dn);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void check() {\n    final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n        it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n        iterkey).iterator();\n    final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n\n    while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n        .isRunning()) {\n      numNodesChecked++;\n      final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n          entry \u003d it.next();\n      final DatanodeDescriptor dn \u003d entry.getKey();\n      try {\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n          // If maintenance expires, stop tracking it.\n          dnAdmin.stopMaintenance(dn);\n          toRemove.add(dn);\n          continue;\n        }\n        if (dn.isInMaintenance()) {\n          // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n          continue;\n        }\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule\n          // under-replicated blocks for replication and collect the blocks\n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyStored(dn);\n          outOfServiceNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently\n          // replicated blocks has dropped to zero and if it can move\n          // to the next state.\n          LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n          pruneReliableBlocks(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the\n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated\n            // blocks. Re-check with the full block map before finally\n            // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            outOfServiceNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay,\n          // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            if (dn.isDecommissionInProgress()) {\n              dnAdmin.setDecommissioned(dn);\n              toRemove.add(dn);\n            } else if (dn.isEnteringMaintenance()) {\n              // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n              // to track maintenance expiration.\n              dnAdmin.setInMaintenance(dn);\n            } else {\n              Preconditions.checkState(false,\n                  \"Node %s is in an invalid state! \"\n                      + \"Invalid state: %s %s blocks are on this dn.\",\n                  dn, dn.getAdminState(), blocks.size());\n            }\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as {}.\", dn, dn.getAdminState());\n          } else {\n            LOG.info(\"Node {} {} healthy.\"\n                    + \" It needs to replicate {} more blocks.\"\n                    + \" {} is still in progress.\", dn,\n                isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n          }\n        } else {\n          LOG.info(\"Node {} still has {} blocks to replicate \"\n                  + \"before it is a candidate to finish {}.\",\n              dn, blocks.size(), dn.getAdminState());\n        }\n      } catch (Exception e) {\n        // Log and postpone to process node when meet exception since it is in\n        // an invalid state.\n        LOG.warn(\"DatanodeAdminMonitor caught exception when processing node \"\n            + \"{}.\", dn, e);\n        pendingNodes.add(dn);\n        toRemove.add(dn);\n      } finally {\n        iterkey \u003d dn;\n      }\n    }\n    // Remove the datanodes that are DECOMMISSIONED or in service after\n    // maintenance expiration.\n    for (DatanodeDescriptor dn : toRemove) {\n      Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n          \"Removing node %s that is not yet decommissioned or in service!\",\n          dn);\n      outOfServiceNodeBlocks.remove(dn);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java",
          "extendedDetails": {}
        }
      ]
    },
    "5747f6cff54f79de0e6439d6c77c2ed437989f10": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14624. When decommissioning a node, log remaining blocks to replicate periodically. Contributed by Stephen O\u0027Donnell.\n",
      "commitDate": "11/07/19 8:55 AM",
      "commitName": "5747f6cff54f79de0e6439d6c77c2ed437989f10",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "10/07/19 11:11 AM",
      "commitNameOld": "eccc9a40deda212cb367627f6f4cc35f5c619941",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 0.91,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,105 +1,105 @@\n     private void check() {\n       final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n           it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n               iterkey).iterator();\n       final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n \n       while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n           .isRunning()) {\n         numNodesChecked++;\n         final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n         try {\n           AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n           boolean fullScan \u003d false;\n           if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n             // If maintenance expires, stop tracking it.\n             stopMaintenance(dn);\n             toRemove.add(dn);\n             continue;\n           }\n           if (dn.isInMaintenance()) {\n             // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n             continue;\n           }\n           if (blocks \u003d\u003d null) {\n             // This is a newly added datanode, run through its list to schedule\n             // under-replicated blocks for replication and collect the blocks\n             // that are insufficiently replicated for further tracking\n             LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n                 \"insufficiently-replicated blocks.\", dn);\n             blocks \u003d handleInsufficientlyStored(dn);\n             outOfServiceNodeBlocks.put(dn, blocks);\n             fullScan \u003d true;\n           } else {\n             // This is a known datanode, check if its # of insufficiently\n             // replicated blocks has dropped to zero and if it can move\n             // to the next state.\n             LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n             pruneReliableBlocks(dn, blocks);\n           }\n           if (blocks.size() \u003d\u003d 0) {\n             if (!fullScan) {\n               // If we didn\u0027t just do a full scan, need to re-check with the\n               // full block map.\n               //\n               // We\u0027ve replicated all the known insufficiently replicated\n               // blocks. Re-check with the full block map before finally\n               // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n               LOG.debug(\"Node {} has finished replicating current set of \"\n                   + \"blocks, checking with the full block map.\", dn);\n               blocks \u003d handleInsufficientlyStored(dn);\n               outOfServiceNodeBlocks.put(dn, blocks);\n             }\n             // If the full scan is clean AND the node liveness is okay,\n             // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n             final boolean isHealthy \u003d\n                 blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n             if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n               if (dn.isDecommissionInProgress()) {\n                 setDecommissioned(dn);\n                 toRemove.add(dn);\n               } else if (dn.isEnteringMaintenance()) {\n                 // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n                 // to track maintenance expiration.\n                 setInMaintenance(dn);\n               } else {\n                 Preconditions.checkState(false,\n                     \"Node %s is in an invalid state! \"\n                       + \"Invalid state: %s %s blocks are on this dn.\",\n                         dn, dn.getAdminState(), blocks.size());\n               }\n               LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                   + \"marked as {}.\", dn, dn.getAdminState());\n             } else {\n-              LOG.debug(\"Node {} {} healthy.\"\n+              LOG.info(\"Node {} {} healthy.\"\n                   + \" It needs to replicate {} more blocks.\"\n                   + \" {} is still in progress.\", dn,\n                   isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n             }\n           } else {\n-            LOG.debug(\"Node {} still has {} blocks to replicate \"\n-                + \"before it is a candidate to finish {}.\",\n+            LOG.info(\"Node {} still has {} blocks to replicate \"\n+                    + \"before it is a candidate to finish {}.\",\n                 dn, blocks.size(), dn.getAdminState());\n           }\n         } catch (Exception e) {\n           // Log and postpone to process node when meet exception since it is in\n           // an invalid state.\n           LOG.warn(\"DatanodeAdminMonitor caught exception when processing node \"\n               + \"{}.\", dn, e);\n           pendingNodes.add(dn);\n           toRemove.add(dn);\n         } finally {\n           iterkey \u003d dn;\n         }\n       }\n       // Remove the datanodes that are DECOMMISSIONED or in service after\n       // maintenance expiration.\n       for (DatanodeDescriptor dn : toRemove) {\n         Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n             \"Removing node %s that is not yet decommissioned or in service!\",\n                 dn);\n         outOfServiceNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n              iterkey).iterator();\n      final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n\n      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n          .isRunning()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        try {\n          AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n          boolean fullScan \u003d false;\n          if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n            // If maintenance expires, stop tracking it.\n            stopMaintenance(dn);\n            toRemove.add(dn);\n            continue;\n          }\n          if (dn.isInMaintenance()) {\n            // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n            continue;\n          }\n          if (blocks \u003d\u003d null) {\n            // This is a newly added datanode, run through its list to schedule\n            // under-replicated blocks for replication and collect the blocks\n            // that are insufficiently replicated for further tracking\n            LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n                \"insufficiently-replicated blocks.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            outOfServiceNodeBlocks.put(dn, blocks);\n            fullScan \u003d true;\n          } else {\n            // This is a known datanode, check if its # of insufficiently\n            // replicated blocks has dropped to zero and if it can move\n            // to the next state.\n            LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n            pruneReliableBlocks(dn, blocks);\n          }\n          if (blocks.size() \u003d\u003d 0) {\n            if (!fullScan) {\n              // If we didn\u0027t just do a full scan, need to re-check with the\n              // full block map.\n              //\n              // We\u0027ve replicated all the known insufficiently replicated\n              // blocks. Re-check with the full block map before finally\n              // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n              LOG.debug(\"Node {} has finished replicating current set of \"\n                  + \"blocks, checking with the full block map.\", dn);\n              blocks \u003d handleInsufficientlyStored(dn);\n              outOfServiceNodeBlocks.put(dn, blocks);\n            }\n            // If the full scan is clean AND the node liveness is okay,\n            // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n            final boolean isHealthy \u003d\n                blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n            if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n              if (dn.isDecommissionInProgress()) {\n                setDecommissioned(dn);\n                toRemove.add(dn);\n              } else if (dn.isEnteringMaintenance()) {\n                // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n                // to track maintenance expiration.\n                setInMaintenance(dn);\n              } else {\n                Preconditions.checkState(false,\n                    \"Node %s is in an invalid state! \"\n                      + \"Invalid state: %s %s blocks are on this dn.\",\n                        dn, dn.getAdminState(), blocks.size());\n              }\n              LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                  + \"marked as {}.\", dn, dn.getAdminState());\n            } else {\n              LOG.info(\"Node {} {} healthy.\"\n                  + \" It needs to replicate {} more blocks.\"\n                  + \" {} is still in progress.\", dn,\n                  isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n            }\n          } else {\n            LOG.info(\"Node {} still has {} blocks to replicate \"\n                    + \"before it is a candidate to finish {}.\",\n                dn, blocks.size(), dn.getAdminState());\n          }\n        } catch (Exception e) {\n          // Log and postpone to process node when meet exception since it is in\n          // an invalid state.\n          LOG.warn(\"DatanodeAdminMonitor caught exception when processing node \"\n              + \"{}.\", dn, e);\n          pendingNodes.add(dn);\n          toRemove.add(dn);\n        } finally {\n          iterkey \u003d dn;\n        }\n      }\n      // Remove the datanodes that are DECOMMISSIONED or in service after\n      // maintenance expiration.\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n            \"Removing node %s that is not yet decommissioned or in service!\",\n                dn);\n        outOfServiceNodeBlocks.remove(dn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java",
      "extendedDetails": {}
    },
    "eccc9a40deda212cb367627f6f4cc35f5c619941": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12703. Exceptions are fatal to decommissioning monitor. Contributed by He Xiaoqiao.\n",
      "commitDate": "10/07/19 11:11 AM",
      "commitName": "eccc9a40deda212cb367627f6f4cc35f5c619941",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "17/06/19 3:18 PM",
      "commitNameOld": "f9a7b442fdd7855e3c7b28e19a12580df48d92bf",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 22.83,
      "commitsBetweenForRepo": 209,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,105 @@\n     private void check() {\n       final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n           it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n               iterkey).iterator();\n       final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n \n       while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n           .isRunning()) {\n         numNodesChecked++;\n         final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n-        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n-        boolean fullScan \u003d false;\n-        if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n-          // If maintenance expires, stop tracking it.\n-          stopMaintenance(dn);\n-          toRemove.add(dn);\n-          continue;\n-        }\n-        if (dn.isInMaintenance()) {\n-          // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n-          continue;\n-        }\n-        if (blocks \u003d\u003d null) {\n-          // This is a newly added datanode, run through its list to schedule\n-          // under-replicated blocks for replication and collect the blocks\n-          // that are insufficiently replicated for further tracking\n-          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n-              \"insufficiently-replicated blocks.\", dn);\n-          blocks \u003d handleInsufficientlyStored(dn);\n-          outOfServiceNodeBlocks.put(dn, blocks);\n-          fullScan \u003d true;\n-        } else {\n-          // This is a known datanode, check if its # of insufficiently\n-          // replicated blocks has dropped to zero and if it can move\n-          // to the next state.\n-          LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n-          pruneReliableBlocks(dn, blocks);\n-        }\n-        if (blocks.size() \u003d\u003d 0) {\n-          if (!fullScan) {\n-            // If we didn\u0027t just do a full scan, need to re-check with the\n-            // full block map.\n-            //\n-            // We\u0027ve replicated all the known insufficiently replicated\n-            // blocks. Re-check with the full block map before finally\n-            // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n-            LOG.debug(\"Node {} has finished replicating current set of \"\n-                + \"blocks, checking with the full block map.\", dn);\n+        try {\n+          AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n+          boolean fullScan \u003d false;\n+          if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n+            // If maintenance expires, stop tracking it.\n+            stopMaintenance(dn);\n+            toRemove.add(dn);\n+            continue;\n+          }\n+          if (dn.isInMaintenance()) {\n+            // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n+            continue;\n+          }\n+          if (blocks \u003d\u003d null) {\n+            // This is a newly added datanode, run through its list to schedule\n+            // under-replicated blocks for replication and collect the blocks\n+            // that are insufficiently replicated for further tracking\n+            LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n+                \"insufficiently-replicated blocks.\", dn);\n             blocks \u003d handleInsufficientlyStored(dn);\n             outOfServiceNodeBlocks.put(dn, blocks);\n-          }\n-          // If the full scan is clean AND the node liveness is okay,\n-          // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n-          final boolean isHealthy \u003d\n-              blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n-          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n-            if (dn.isDecommissionInProgress()) {\n-              setDecommissioned(dn);\n-              toRemove.add(dn);\n-            } else if (dn.isEnteringMaintenance()) {\n-              // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n-              // to track maintenance expiration.\n-              setInMaintenance(dn);\n-            } else {\n-              Preconditions.checkState(false,\n-                  \"A node is in an invalid state!\");\n-            }\n-            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n-                + \"marked as {}.\", dn, dn.getAdminState());\n+            fullScan \u003d true;\n           } else {\n-            LOG.debug(\"Node {} {} healthy.\"\n-                + \" It needs to replicate {} more blocks.\"\n-                + \" {} is still in progress.\", dn,\n-                isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n+            // This is a known datanode, check if its # of insufficiently\n+            // replicated blocks has dropped to zero and if it can move\n+            // to the next state.\n+            LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n+            pruneReliableBlocks(dn, blocks);\n           }\n-        } else {\n-          LOG.debug(\"Node {} still has {} blocks to replicate \"\n-              + \"before it is a candidate to finish {}.\",\n-              dn, blocks.size(), dn.getAdminState());\n+          if (blocks.size() \u003d\u003d 0) {\n+            if (!fullScan) {\n+              // If we didn\u0027t just do a full scan, need to re-check with the\n+              // full block map.\n+              //\n+              // We\u0027ve replicated all the known insufficiently replicated\n+              // blocks. Re-check with the full block map before finally\n+              // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n+              LOG.debug(\"Node {} has finished replicating current set of \"\n+                  + \"blocks, checking with the full block map.\", dn);\n+              blocks \u003d handleInsufficientlyStored(dn);\n+              outOfServiceNodeBlocks.put(dn, blocks);\n+            }\n+            // If the full scan is clean AND the node liveness is okay,\n+            // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n+            final boolean isHealthy \u003d\n+                blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n+            if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n+              if (dn.isDecommissionInProgress()) {\n+                setDecommissioned(dn);\n+                toRemove.add(dn);\n+              } else if (dn.isEnteringMaintenance()) {\n+                // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n+                // to track maintenance expiration.\n+                setInMaintenance(dn);\n+              } else {\n+                Preconditions.checkState(false,\n+                    \"Node %s is in an invalid state! \"\n+                      + \"Invalid state: %s %s blocks are on this dn.\",\n+                        dn, dn.getAdminState(), blocks.size());\n+              }\n+              LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n+                  + \"marked as {}.\", dn, dn.getAdminState());\n+            } else {\n+              LOG.debug(\"Node {} {} healthy.\"\n+                  + \" It needs to replicate {} more blocks.\"\n+                  + \" {} is still in progress.\", dn,\n+                  isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n+            }\n+          } else {\n+            LOG.debug(\"Node {} still has {} blocks to replicate \"\n+                + \"before it is a candidate to finish {}.\",\n+                dn, blocks.size(), dn.getAdminState());\n+          }\n+        } catch (Exception e) {\n+          // Log and postpone to process node when meet exception since it is in\n+          // an invalid state.\n+          LOG.warn(\"DatanodeAdminMonitor caught exception when processing node \"\n+              + \"{}.\", dn, e);\n+          pendingNodes.add(dn);\n+          toRemove.add(dn);\n+        } finally {\n+          iterkey \u003d dn;\n         }\n-        iterkey \u003d dn;\n       }\n       // Remove the datanodes that are DECOMMISSIONED or in service after\n       // maintenance expiration.\n       for (DatanodeDescriptor dn : toRemove) {\n         Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n-            \"Removing a node that is not yet decommissioned or in service!\");\n+            \"Removing node %s that is not yet decommissioned or in service!\",\n+                dn);\n         outOfServiceNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n              iterkey).iterator();\n      final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n\n      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n          .isRunning()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        try {\n          AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n          boolean fullScan \u003d false;\n          if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n            // If maintenance expires, stop tracking it.\n            stopMaintenance(dn);\n            toRemove.add(dn);\n            continue;\n          }\n          if (dn.isInMaintenance()) {\n            // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n            continue;\n          }\n          if (blocks \u003d\u003d null) {\n            // This is a newly added datanode, run through its list to schedule\n            // under-replicated blocks for replication and collect the blocks\n            // that are insufficiently replicated for further tracking\n            LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n                \"insufficiently-replicated blocks.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            outOfServiceNodeBlocks.put(dn, blocks);\n            fullScan \u003d true;\n          } else {\n            // This is a known datanode, check if its # of insufficiently\n            // replicated blocks has dropped to zero and if it can move\n            // to the next state.\n            LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n            pruneReliableBlocks(dn, blocks);\n          }\n          if (blocks.size() \u003d\u003d 0) {\n            if (!fullScan) {\n              // If we didn\u0027t just do a full scan, need to re-check with the\n              // full block map.\n              //\n              // We\u0027ve replicated all the known insufficiently replicated\n              // blocks. Re-check with the full block map before finally\n              // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n              LOG.debug(\"Node {} has finished replicating current set of \"\n                  + \"blocks, checking with the full block map.\", dn);\n              blocks \u003d handleInsufficientlyStored(dn);\n              outOfServiceNodeBlocks.put(dn, blocks);\n            }\n            // If the full scan is clean AND the node liveness is okay,\n            // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n            final boolean isHealthy \u003d\n                blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n            if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n              if (dn.isDecommissionInProgress()) {\n                setDecommissioned(dn);\n                toRemove.add(dn);\n              } else if (dn.isEnteringMaintenance()) {\n                // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n                // to track maintenance expiration.\n                setInMaintenance(dn);\n              } else {\n                Preconditions.checkState(false,\n                    \"Node %s is in an invalid state! \"\n                      + \"Invalid state: %s %s blocks are on this dn.\",\n                        dn, dn.getAdminState(), blocks.size());\n              }\n              LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                  + \"marked as {}.\", dn, dn.getAdminState());\n            } else {\n              LOG.debug(\"Node {} {} healthy.\"\n                  + \" It needs to replicate {} more blocks.\"\n                  + \" {} is still in progress.\", dn,\n                  isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n            }\n          } else {\n            LOG.debug(\"Node {} still has {} blocks to replicate \"\n                + \"before it is a candidate to finish {}.\",\n                dn, blocks.size(), dn.getAdminState());\n          }\n        } catch (Exception e) {\n          // Log and postpone to process node when meet exception since it is in\n          // an invalid state.\n          LOG.warn(\"DatanodeAdminMonitor caught exception when processing node \"\n              + \"{}.\", dn, e);\n          pendingNodes.add(dn);\n          toRemove.add(dn);\n        } finally {\n          iterkey \u003d dn;\n        }\n      }\n      // Remove the datanodes that are DECOMMISSIONED or in service after\n      // maintenance expiration.\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n            \"Removing node %s that is not yet decommissioned or in service!\",\n                dn);\n        outOfServiceNodeBlocks.remove(dn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java",
      "extendedDetails": {}
    },
    "6f81cc0beea00843b44424417f09d8ee12cd7bae": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13167. DatanodeAdminManager Improvements. Contributed by BELUGA BEHR.\n",
      "commitDate": "20/02/18 3:18 PM",
      "commitName": "6f81cc0beea00843b44424417f09d8ee12cd7bae",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "02/01/18 2:59 PM",
      "commitNameOld": "42a1c98597e6dba2e371510a6b2b6b1fb94e4090",
      "commitAuthorOld": "Manoj Govindassamy",
      "daysBetweenCommits": 49.01,
      "commitsBetweenForRepo": 297,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,92 @@\n     private void check() {\n       final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n           it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n               iterkey).iterator();\n-      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n+      final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n \n       while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n           .isRunning()) {\n         numNodesChecked++;\n         final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n         AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n         boolean fullScan \u003d false;\n         if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n           // If maintenance expires, stop tracking it.\n           stopMaintenance(dn);\n           toRemove.add(dn);\n           continue;\n         }\n         if (dn.isInMaintenance()) {\n           // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n           continue;\n         }\n         if (blocks \u003d\u003d null) {\n           // This is a newly added datanode, run through its list to schedule\n           // under-replicated blocks for replication and collect the blocks\n           // that are insufficiently replicated for further tracking\n           LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n               \"insufficiently-replicated blocks.\", dn);\n           blocks \u003d handleInsufficientlyStored(dn);\n           outOfServiceNodeBlocks.put(dn, blocks);\n           fullScan \u003d true;\n         } else {\n           // This is a known datanode, check if its # of insufficiently\n           // replicated blocks has dropped to zero and if it can move\n           // to the next state.\n           LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n           pruneReliableBlocks(dn, blocks);\n         }\n         if (blocks.size() \u003d\u003d 0) {\n           if (!fullScan) {\n             // If we didn\u0027t just do a full scan, need to re-check with the\n             // full block map.\n             //\n             // We\u0027ve replicated all the known insufficiently replicated\n             // blocks. Re-check with the full block map before finally\n             // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n             LOG.debug(\"Node {} has finished replicating current set of \"\n                 + \"blocks, checking with the full block map.\", dn);\n             blocks \u003d handleInsufficientlyStored(dn);\n             outOfServiceNodeBlocks.put(dn, blocks);\n           }\n           // If the full scan is clean AND the node liveness is okay,\n           // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n           final boolean isHealthy \u003d\n               blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n           if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n             if (dn.isDecommissionInProgress()) {\n               setDecommissioned(dn);\n               toRemove.add(dn);\n             } else if (dn.isEnteringMaintenance()) {\n               // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n               // to track maintenance expiration.\n               setInMaintenance(dn);\n             } else {\n               Preconditions.checkState(false,\n                   \"A node is in an invalid state!\");\n             }\n             LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n-                + \"marked as {}.\", dn.getAdminState());\n+                + \"marked as {}.\", dn, dn.getAdminState());\n           } else {\n             LOG.debug(\"Node {} {} healthy.\"\n                 + \" It needs to replicate {} more blocks.\"\n                 + \" {} is still in progress.\", dn,\n-                isHealthy? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n+                isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n           }\n         } else {\n           LOG.debug(\"Node {} still has {} blocks to replicate \"\n               + \"before it is a candidate to finish {}.\",\n               dn, blocks.size(), dn.getAdminState());\n         }\n         iterkey \u003d dn;\n       }\n       // Remove the datanodes that are DECOMMISSIONED or in service after\n       // maintenance expiration.\n       for (DatanodeDescriptor dn : toRemove) {\n         Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n             \"Removing a node that is not yet decommissioned or in service!\");\n         outOfServiceNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n              iterkey).iterator();\n      final List\u003cDatanodeDescriptor\u003e toRemove \u003d new ArrayList\u003c\u003e();\n\n      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n          .isRunning()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n          // If maintenance expires, stop tracking it.\n          stopMaintenance(dn);\n          toRemove.add(dn);\n          continue;\n        }\n        if (dn.isInMaintenance()) {\n          // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n          continue;\n        }\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule\n          // under-replicated blocks for replication and collect the blocks\n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyStored(dn);\n          outOfServiceNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently\n          // replicated blocks has dropped to zero and if it can move\n          // to the next state.\n          LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n          pruneReliableBlocks(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the\n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated\n            // blocks. Re-check with the full block map before finally\n            // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            outOfServiceNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay,\n          // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            if (dn.isDecommissionInProgress()) {\n              setDecommissioned(dn);\n              toRemove.add(dn);\n            } else if (dn.isEnteringMaintenance()) {\n              // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n              // to track maintenance expiration.\n              setInMaintenance(dn);\n            } else {\n              Preconditions.checkState(false,\n                  \"A node is in an invalid state!\");\n            }\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as {}.\", dn, dn.getAdminState());\n          } else {\n            LOG.debug(\"Node {} {} healthy.\"\n                + \" It needs to replicate {} more blocks.\"\n                + \" {} is still in progress.\", dn,\n                isHealthy ? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n          }\n        } else {\n          LOG.debug(\"Node {} still has {} blocks to replicate \"\n              + \"before it is a candidate to finish {}.\",\n              dn, blocks.size(), dn.getAdminState());\n        }\n        iterkey \u003d dn;\n      }\n      // Remove the datanodes that are DECOMMISSIONED or in service after\n      // maintenance expiration.\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n            \"Removing a node that is not yet decommissioned or in service!\");\n        outOfServiceNodeBlocks.remove(dn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java",
      "extendedDetails": {}
    },
    "79df1e750ef558afed6d166ce225a23061b36aed": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HDFS-9388. Decommission related code to support Maintenance State for datanodes.\n",
      "commitDate": "02/08/17 2:22 PM",
      "commitName": "79df1e750ef558afed6d166ce225a23061b36aed",
      "commitAuthor": "Manoj Govindassamy",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-9388. Decommission related code to support Maintenance State for datanodes.\n",
          "commitDate": "02/08/17 2:22 PM",
          "commitName": "79df1e750ef558afed6d166ce225a23061b36aed",
          "commitAuthor": "Manoj Govindassamy",
          "commitDateOld": "02/08/17 12:12 PM",
          "commitNameOld": "12e44e7bdaf53d3720a89d32f0cc2717241bd6b2",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,91 +1,92 @@\n     private void check() {\n       final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n           it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n               iterkey).iterator();\n       final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n \n       while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n           .isRunning()) {\n         numNodesChecked++;\n         final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n         AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n         boolean fullScan \u003d false;\n         if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n           // If maintenance expires, stop tracking it.\n           stopMaintenance(dn);\n           toRemove.add(dn);\n           continue;\n         }\n         if (dn.isInMaintenance()) {\n           // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n           continue;\n         }\n         if (blocks \u003d\u003d null) {\n-          // This is a newly added datanode, run through its list to schedule \n-          // under-replicated blocks for replication and collect the blocks \n+          // This is a newly added datanode, run through its list to schedule\n+          // under-replicated blocks for replication and collect the blocks\n           // that are insufficiently replicated for further tracking\n           LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n               \"insufficiently-replicated blocks.\", dn);\n           blocks \u003d handleInsufficientlyStored(dn);\n           outOfServiceNodeBlocks.put(dn, blocks);\n           fullScan \u003d true;\n         } else {\n-          // This is a known datanode, check if its # of insufficiently \n-          // replicated blocks has dropped to zero and if it can be decommed\n+          // This is a known datanode, check if its # of insufficiently\n+          // replicated blocks has dropped to zero and if it can move\n+          // to the next state.\n           LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n           pruneReliableBlocks(dn, blocks);\n         }\n         if (blocks.size() \u003d\u003d 0) {\n           if (!fullScan) {\n-            // If we didn\u0027t just do a full scan, need to re-check with the \n+            // If we didn\u0027t just do a full scan, need to re-check with the\n             // full block map.\n             //\n-            // We\u0027ve replicated all the known insufficiently replicated \n-            // blocks. Re-check with the full block map before finally \n-            // marking the datanode as decommissioned \n+            // We\u0027ve replicated all the known insufficiently replicated\n+            // blocks. Re-check with the full block map before finally\n+            // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n             LOG.debug(\"Node {} has finished replicating current set of \"\n                 + \"blocks, checking with the full block map.\", dn);\n             blocks \u003d handleInsufficientlyStored(dn);\n             outOfServiceNodeBlocks.put(dn, blocks);\n           }\n-          // If the full scan is clean AND the node liveness is okay, \n-          // we can finally mark as decommissioned.\n+          // If the full scan is clean AND the node liveness is okay,\n+          // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n           final boolean isHealthy \u003d\n               blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n           if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n             if (dn.isDecommissionInProgress()) {\n               setDecommissioned(dn);\n               toRemove.add(dn);\n             } else if (dn.isEnteringMaintenance()) {\n               // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n               // to track maintenance expiration.\n               setInMaintenance(dn);\n             } else {\n               Preconditions.checkState(false,\n                   \"A node is in an invalid state!\");\n             }\n             LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                 + \"marked as {}.\", dn.getAdminState());\n           } else {\n             LOG.debug(\"Node {} {} healthy.\"\n                 + \" It needs to replicate {} more blocks.\"\n                 + \" {} is still in progress.\", dn,\n                 isHealthy? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n           }\n         } else {\n           LOG.debug(\"Node {} still has {} blocks to replicate \"\n               + \"before it is a candidate to finish {}.\",\n               dn, blocks.size(), dn.getAdminState());\n         }\n         iterkey \u003d dn;\n       }\n-      // Remove the datanodes that are decommissioned or in service after\n+      // Remove the datanodes that are DECOMMISSIONED or in service after\n       // maintenance expiration.\n       for (DatanodeDescriptor dn : toRemove) {\n         Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n             \"Removing a node that is not yet decommissioned or in service!\");\n         outOfServiceNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n              iterkey).iterator();\n      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n\n      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n          .isRunning()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n          // If maintenance expires, stop tracking it.\n          stopMaintenance(dn);\n          toRemove.add(dn);\n          continue;\n        }\n        if (dn.isInMaintenance()) {\n          // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n          continue;\n        }\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule\n          // under-replicated blocks for replication and collect the blocks\n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyStored(dn);\n          outOfServiceNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently\n          // replicated blocks has dropped to zero and if it can move\n          // to the next state.\n          LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n          pruneReliableBlocks(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the\n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated\n            // blocks. Re-check with the full block map before finally\n            // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            outOfServiceNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay,\n          // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            if (dn.isDecommissionInProgress()) {\n              setDecommissioned(dn);\n              toRemove.add(dn);\n            } else if (dn.isEnteringMaintenance()) {\n              // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n              // to track maintenance expiration.\n              setInMaintenance(dn);\n            } else {\n              Preconditions.checkState(false,\n                  \"A node is in an invalid state!\");\n            }\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as {}.\", dn.getAdminState());\n          } else {\n            LOG.debug(\"Node {} {} healthy.\"\n                + \" It needs to replicate {} more blocks.\"\n                + \" {} is still in progress.\", dn,\n                isHealthy? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n          }\n        } else {\n          LOG.debug(\"Node {} still has {} blocks to replicate \"\n              + \"before it is a candidate to finish {}.\",\n              dn, blocks.size(), dn.getAdminState());\n        }\n        iterkey \u003d dn;\n      }\n      // Remove the datanodes that are DECOMMISSIONED or in service after\n      // maintenance expiration.\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n            \"Removing a node that is not yet decommissioned or in service!\");\n        outOfServiceNodeBlocks.remove(dn);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9388. Decommission related code to support Maintenance State for datanodes.\n",
          "commitDate": "02/08/17 2:22 PM",
          "commitName": "79df1e750ef558afed6d166ce225a23061b36aed",
          "commitAuthor": "Manoj Govindassamy",
          "commitDateOld": "02/08/17 12:12 PM",
          "commitNameOld": "12e44e7bdaf53d3720a89d32f0cc2717241bd6b2",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,91 +1,92 @@\n     private void check() {\n       final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n           it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n               iterkey).iterator();\n       final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n \n       while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n           .isRunning()) {\n         numNodesChecked++;\n         final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n         AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n         boolean fullScan \u003d false;\n         if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n           // If maintenance expires, stop tracking it.\n           stopMaintenance(dn);\n           toRemove.add(dn);\n           continue;\n         }\n         if (dn.isInMaintenance()) {\n           // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n           continue;\n         }\n         if (blocks \u003d\u003d null) {\n-          // This is a newly added datanode, run through its list to schedule \n-          // under-replicated blocks for replication and collect the blocks \n+          // This is a newly added datanode, run through its list to schedule\n+          // under-replicated blocks for replication and collect the blocks\n           // that are insufficiently replicated for further tracking\n           LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n               \"insufficiently-replicated blocks.\", dn);\n           blocks \u003d handleInsufficientlyStored(dn);\n           outOfServiceNodeBlocks.put(dn, blocks);\n           fullScan \u003d true;\n         } else {\n-          // This is a known datanode, check if its # of insufficiently \n-          // replicated blocks has dropped to zero and if it can be decommed\n+          // This is a known datanode, check if its # of insufficiently\n+          // replicated blocks has dropped to zero and if it can move\n+          // to the next state.\n           LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n           pruneReliableBlocks(dn, blocks);\n         }\n         if (blocks.size() \u003d\u003d 0) {\n           if (!fullScan) {\n-            // If we didn\u0027t just do a full scan, need to re-check with the \n+            // If we didn\u0027t just do a full scan, need to re-check with the\n             // full block map.\n             //\n-            // We\u0027ve replicated all the known insufficiently replicated \n-            // blocks. Re-check with the full block map before finally \n-            // marking the datanode as decommissioned \n+            // We\u0027ve replicated all the known insufficiently replicated\n+            // blocks. Re-check with the full block map before finally\n+            // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n             LOG.debug(\"Node {} has finished replicating current set of \"\n                 + \"blocks, checking with the full block map.\", dn);\n             blocks \u003d handleInsufficientlyStored(dn);\n             outOfServiceNodeBlocks.put(dn, blocks);\n           }\n-          // If the full scan is clean AND the node liveness is okay, \n-          // we can finally mark as decommissioned.\n+          // If the full scan is clean AND the node liveness is okay,\n+          // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n           final boolean isHealthy \u003d\n               blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n           if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n             if (dn.isDecommissionInProgress()) {\n               setDecommissioned(dn);\n               toRemove.add(dn);\n             } else if (dn.isEnteringMaintenance()) {\n               // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n               // to track maintenance expiration.\n               setInMaintenance(dn);\n             } else {\n               Preconditions.checkState(false,\n                   \"A node is in an invalid state!\");\n             }\n             LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                 + \"marked as {}.\", dn.getAdminState());\n           } else {\n             LOG.debug(\"Node {} {} healthy.\"\n                 + \" It needs to replicate {} more blocks.\"\n                 + \" {} is still in progress.\", dn,\n                 isHealthy? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n           }\n         } else {\n           LOG.debug(\"Node {} still has {} blocks to replicate \"\n               + \"before it is a candidate to finish {}.\",\n               dn, blocks.size(), dn.getAdminState());\n         }\n         iterkey \u003d dn;\n       }\n-      // Remove the datanodes that are decommissioned or in service after\n+      // Remove the datanodes that are DECOMMISSIONED or in service after\n       // maintenance expiration.\n       for (DatanodeDescriptor dn : toRemove) {\n         Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n             \"Removing a node that is not yet decommissioned or in service!\");\n         outOfServiceNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n              iterkey).iterator();\n      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n\n      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n          .isRunning()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n          // If maintenance expires, stop tracking it.\n          stopMaintenance(dn);\n          toRemove.add(dn);\n          continue;\n        }\n        if (dn.isInMaintenance()) {\n          // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n          continue;\n        }\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule\n          // under-replicated blocks for replication and collect the blocks\n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyStored(dn);\n          outOfServiceNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently\n          // replicated blocks has dropped to zero and if it can move\n          // to the next state.\n          LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n          pruneReliableBlocks(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the\n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated\n            // blocks. Re-check with the full block map before finally\n            // marking the datanode as DECOMMISSIONED or IN_MAINTENANCE.\n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            outOfServiceNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay,\n          // we can finally mark as DECOMMISSIONED or IN_MAINTENANCE.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            if (dn.isDecommissionInProgress()) {\n              setDecommissioned(dn);\n              toRemove.add(dn);\n            } else if (dn.isEnteringMaintenance()) {\n              // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n              // to track maintenance expiration.\n              setInMaintenance(dn);\n            } else {\n              Preconditions.checkState(false,\n                  \"A node is in an invalid state!\");\n            }\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as {}.\", dn.getAdminState());\n          } else {\n            LOG.debug(\"Node {} {} healthy.\"\n                + \" It needs to replicate {} more blocks.\"\n                + \" {} is still in progress.\", dn,\n                isHealthy? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n          }\n        } else {\n          LOG.debug(\"Node {} still has {} blocks to replicate \"\n              + \"before it is a candidate to finish {}.\",\n              dn, blocks.size(), dn.getAdminState());\n        }\n        iterkey \u003d dn;\n      }\n      // Remove the datanodes that are DECOMMISSIONED or in service after\n      // maintenance expiration.\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n            \"Removing a node that is not yet decommissioned or in service!\");\n        outOfServiceNodeBlocks.remove(dn);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "13/10/16 11:52 AM",
      "commitNameOld": "332a61fd74fd2a9874319232c583ab5d2c53ff03",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 4.25,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,91 @@\n     private void check() {\n       final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n           it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n               iterkey).iterator();\n       final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n \n       while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n           .isRunning()) {\n         numNodesChecked++;\n         final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n         AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n         boolean fullScan \u003d false;\n-        if (dn.isMaintenance()) {\n-          // TODO HDFS-9390 make sure blocks are minimally replicated\n-          // before transitioning the node to IN_MAINTENANCE state.\n-\n+        if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n           // If maintenance expires, stop tracking it.\n-          if (dn.maintenanceExpired()) {\n-            stopMaintenance(dn);\n-            toRemove.add(dn);\n-          }\n+          stopMaintenance(dn);\n+          toRemove.add(dn);\n+          continue;\n+        }\n+        if (dn.isInMaintenance()) {\n+          // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n           continue;\n         }\n         if (blocks \u003d\u003d null) {\n           // This is a newly added datanode, run through its list to schedule \n           // under-replicated blocks for replication and collect the blocks \n           // that are insufficiently replicated for further tracking\n           LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n               \"insufficiently-replicated blocks.\", dn);\n           blocks \u003d handleInsufficientlyStored(dn);\n           outOfServiceNodeBlocks.put(dn, blocks);\n           fullScan \u003d true;\n         } else {\n           // This is a known datanode, check if its # of insufficiently \n           // replicated blocks has dropped to zero and if it can be decommed\n-          LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n+          LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n           pruneReliableBlocks(dn, blocks);\n         }\n         if (blocks.size() \u003d\u003d 0) {\n           if (!fullScan) {\n             // If we didn\u0027t just do a full scan, need to re-check with the \n             // full block map.\n             //\n             // We\u0027ve replicated all the known insufficiently replicated \n             // blocks. Re-check with the full block map before finally \n             // marking the datanode as decommissioned \n             LOG.debug(\"Node {} has finished replicating current set of \"\n                 + \"blocks, checking with the full block map.\", dn);\n             blocks \u003d handleInsufficientlyStored(dn);\n             outOfServiceNodeBlocks.put(dn, blocks);\n           }\n           // If the full scan is clean AND the node liveness is okay, \n           // we can finally mark as decommissioned.\n           final boolean isHealthy \u003d\n-              blockManager.isNodeHealthyForDecommission(dn);\n+              blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n           if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n-            setDecommissioned(dn);\n-            toRemove.add(dn);\n+            if (dn.isDecommissionInProgress()) {\n+              setDecommissioned(dn);\n+              toRemove.add(dn);\n+            } else if (dn.isEnteringMaintenance()) {\n+              // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n+              // to track maintenance expiration.\n+              setInMaintenance(dn);\n+            } else {\n+              Preconditions.checkState(false,\n+                  \"A node is in an invalid state!\");\n+            }\n             LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n-                + \"marked as decommissioned.\", dn);\n+                + \"marked as {}.\", dn.getAdminState());\n           } else {\n             LOG.debug(\"Node {} {} healthy.\"\n                 + \" It needs to replicate {} more blocks.\"\n-                + \" Decommissioning is still in progress.\",\n-                dn, isHealthy? \"is\": \"isn\u0027t\", blocks.size());\n+                + \" {} is still in progress.\", dn,\n+                isHealthy? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n           }\n         } else {\n           LOG.debug(\"Node {} still has {} blocks to replicate \"\n-                  + \"before it is a candidate to finish decommissioning.\",\n-              dn, blocks.size());\n+              + \"before it is a candidate to finish {}.\",\n+              dn, blocks.size(), dn.getAdminState());\n         }\n         iterkey \u003d dn;\n       }\n       // Remove the datanodes that are decommissioned or in service after\n       // maintenance expiration.\n       for (DatanodeDescriptor dn : toRemove) {\n         Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n             \"Removing a node that is not yet decommissioned or in service!\");\n         outOfServiceNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n              iterkey).iterator();\n      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n\n      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n          .isRunning()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (dn.isMaintenance() \u0026\u0026 dn.maintenanceExpired()) {\n          // If maintenance expires, stop tracking it.\n          stopMaintenance(dn);\n          toRemove.add(dn);\n          continue;\n        }\n        if (dn.isInMaintenance()) {\n          // The dn is IN_MAINTENANCE and the maintenance hasn\u0027t expired yet.\n          continue;\n        }\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule \n          // under-replicated blocks for replication and collect the blocks \n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyStored(dn);\n          outOfServiceNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently \n          // replicated blocks has dropped to zero and if it can be decommed\n          LOG.debug(\"Processing {} node {}\", dn.getAdminState(), dn);\n          pruneReliableBlocks(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the \n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated \n            // blocks. Re-check with the full block map before finally \n            // marking the datanode as decommissioned \n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            outOfServiceNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay, \n          // we can finally mark as decommissioned.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            if (dn.isDecommissionInProgress()) {\n              setDecommissioned(dn);\n              toRemove.add(dn);\n            } else if (dn.isEnteringMaintenance()) {\n              // IN_MAINTENANCE node remains in the outOfServiceNodeBlocks to\n              // to track maintenance expiration.\n              setInMaintenance(dn);\n            } else {\n              Preconditions.checkState(false,\n                  \"A node is in an invalid state!\");\n            }\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as {}.\", dn.getAdminState());\n          } else {\n            LOG.debug(\"Node {} {} healthy.\"\n                + \" It needs to replicate {} more blocks.\"\n                + \" {} is still in progress.\", dn,\n                isHealthy? \"is\": \"isn\u0027t\", blocks.size(), dn.getAdminState());\n          }\n        } else {\n          LOG.debug(\"Node {} still has {} blocks to replicate \"\n              + \"before it is a candidate to finish {}.\",\n              dn, blocks.size(), dn.getAdminState());\n        }\n        iterkey \u003d dn;\n      }\n      // Remove the datanodes that are decommissioned or in service after\n      // maintenance expiration.\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n            \"Removing a node that is not yet decommissioned or in service!\");\n        outOfServiceNodeBlocks.remove(dn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "332a61fd74fd2a9874319232c583ab5d2c53ff03": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10987. Make Decommission less expensive when lot of blocks present. Contributed by Brahma Reddy Battula.\n",
      "commitDate": "13/10/16 11:52 AM",
      "commitName": "332a61fd74fd2a9874319232c583ab5d2c53ff03",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "06/09/16 10:38 AM",
      "commitNameOld": "d37dc5d1b8e022a7085118a2e7066623483c293f",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 37.05,
      "commitsBetweenForRepo": 251,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,83 @@\n     private void check() {\n       final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n           it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n               iterkey).iterator();\n       final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n \n-      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck()) {\n+      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n+          .isRunning()) {\n         numNodesChecked++;\n         final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n         AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n         boolean fullScan \u003d false;\n         if (dn.isMaintenance()) {\n           // TODO HDFS-9390 make sure blocks are minimally replicated\n           // before transitioning the node to IN_MAINTENANCE state.\n \n           // If maintenance expires, stop tracking it.\n           if (dn.maintenanceExpired()) {\n             stopMaintenance(dn);\n             toRemove.add(dn);\n           }\n           continue;\n         }\n         if (blocks \u003d\u003d null) {\n           // This is a newly added datanode, run through its list to schedule \n           // under-replicated blocks for replication and collect the blocks \n           // that are insufficiently replicated for further tracking\n           LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n               \"insufficiently-replicated blocks.\", dn);\n           blocks \u003d handleInsufficientlyStored(dn);\n           outOfServiceNodeBlocks.put(dn, blocks);\n           fullScan \u003d true;\n         } else {\n           // This is a known datanode, check if its # of insufficiently \n           // replicated blocks has dropped to zero and if it can be decommed\n           LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n           pruneReliableBlocks(dn, blocks);\n         }\n         if (blocks.size() \u003d\u003d 0) {\n           if (!fullScan) {\n             // If we didn\u0027t just do a full scan, need to re-check with the \n             // full block map.\n             //\n             // We\u0027ve replicated all the known insufficiently replicated \n             // blocks. Re-check with the full block map before finally \n             // marking the datanode as decommissioned \n             LOG.debug(\"Node {} has finished replicating current set of \"\n                 + \"blocks, checking with the full block map.\", dn);\n             blocks \u003d handleInsufficientlyStored(dn);\n             outOfServiceNodeBlocks.put(dn, blocks);\n           }\n           // If the full scan is clean AND the node liveness is okay, \n           // we can finally mark as decommissioned.\n           final boolean isHealthy \u003d\n               blockManager.isNodeHealthyForDecommission(dn);\n           if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n             setDecommissioned(dn);\n             toRemove.add(dn);\n             LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                 + \"marked as decommissioned.\", dn);\n           } else {\n             LOG.debug(\"Node {} {} healthy.\"\n                 + \" It needs to replicate {} more blocks.\"\n                 + \" Decommissioning is still in progress.\",\n                 dn, isHealthy? \"is\": \"isn\u0027t\", blocks.size());\n           }\n         } else {\n           LOG.debug(\"Node {} still has {} blocks to replicate \"\n                   + \"before it is a candidate to finish decommissioning.\",\n               dn, blocks.size());\n         }\n         iterkey \u003d dn;\n       }\n       // Remove the datanodes that are decommissioned or in service after\n       // maintenance expiration.\n       for (DatanodeDescriptor dn : toRemove) {\n         Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n             \"Removing a node that is not yet decommissioned or in service!\");\n         outOfServiceNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n              iterkey).iterator();\n      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n\n      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck() \u0026\u0026 namesystem\n          .isRunning()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (dn.isMaintenance()) {\n          // TODO HDFS-9390 make sure blocks are minimally replicated\n          // before transitioning the node to IN_MAINTENANCE state.\n\n          // If maintenance expires, stop tracking it.\n          if (dn.maintenanceExpired()) {\n            stopMaintenance(dn);\n            toRemove.add(dn);\n          }\n          continue;\n        }\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule \n          // under-replicated blocks for replication and collect the blocks \n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyStored(dn);\n          outOfServiceNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently \n          // replicated blocks has dropped to zero and if it can be decommed\n          LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n          pruneReliableBlocks(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the \n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated \n            // blocks. Re-check with the full block map before finally \n            // marking the datanode as decommissioned \n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            outOfServiceNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay, \n          // we can finally mark as decommissioned.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommission(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            setDecommissioned(dn);\n            toRemove.add(dn);\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as decommissioned.\", dn);\n          } else {\n            LOG.debug(\"Node {} {} healthy.\"\n                + \" It needs to replicate {} more blocks.\"\n                + \" Decommissioning is still in progress.\",\n                dn, isHealthy? \"is\": \"isn\u0027t\", blocks.size());\n          }\n        } else {\n          LOG.debug(\"Node {} still has {} blocks to replicate \"\n                  + \"before it is a candidate to finish decommissioning.\",\n              dn, blocks.size());\n        }\n        iterkey \u003d dn;\n      }\n      // Remove the datanodes that are decommissioned or in service after\n      // maintenance expiration.\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n            \"Removing a node that is not yet decommissioned or in service!\");\n        outOfServiceNodeBlocks.remove(dn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9392. Admins support for maintenance state. Contributed by Ming Ma.\n",
      "commitDate": "30/08/16 2:00 PM",
      "commitName": "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "26/05/16 4:50 PM",
      "commitNameOld": "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 95.88,
      "commitsBetweenForRepo": 766,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,82 @@\n     private void check() {\n       final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n-          it \u003d new CyclicIteration\u003c\u003e(decomNodeBlocks, iterkey).iterator();\n+          it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n+              iterkey).iterator();\n       final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n \n       while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck()) {\n         numNodesChecked++;\n         final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n         AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n         boolean fullScan \u003d false;\n+        if (dn.isMaintenance()) {\n+          // TODO HDFS-9390 make sure blocks are minimally replicated\n+          // before transitioning the node to IN_MAINTENANCE state.\n+\n+          // If maintenance expires, stop tracking it.\n+          if (dn.maintenanceExpired()) {\n+            stopMaintenance(dn);\n+            toRemove.add(dn);\n+          }\n+          continue;\n+        }\n         if (blocks \u003d\u003d null) {\n           // This is a newly added datanode, run through its list to schedule \n           // under-replicated blocks for replication and collect the blocks \n           // that are insufficiently replicated for further tracking\n           LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n               \"insufficiently-replicated blocks.\", dn);\n           blocks \u003d handleInsufficientlyStored(dn);\n-          decomNodeBlocks.put(dn, blocks);\n+          outOfServiceNodeBlocks.put(dn, blocks);\n           fullScan \u003d true;\n         } else {\n           // This is a known datanode, check if its # of insufficiently \n           // replicated blocks has dropped to zero and if it can be decommed\n           LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n           pruneReliableBlocks(dn, blocks);\n         }\n         if (blocks.size() \u003d\u003d 0) {\n           if (!fullScan) {\n             // If we didn\u0027t just do a full scan, need to re-check with the \n             // full block map.\n             //\n             // We\u0027ve replicated all the known insufficiently replicated \n             // blocks. Re-check with the full block map before finally \n             // marking the datanode as decommissioned \n             LOG.debug(\"Node {} has finished replicating current set of \"\n                 + \"blocks, checking with the full block map.\", dn);\n             blocks \u003d handleInsufficientlyStored(dn);\n-            decomNodeBlocks.put(dn, blocks);\n+            outOfServiceNodeBlocks.put(dn, blocks);\n           }\n           // If the full scan is clean AND the node liveness is okay, \n           // we can finally mark as decommissioned.\n           final boolean isHealthy \u003d\n               blockManager.isNodeHealthyForDecommission(dn);\n           if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n             setDecommissioned(dn);\n             toRemove.add(dn);\n             LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                 + \"marked as decommissioned.\", dn);\n           } else {\n             LOG.debug(\"Node {} {} healthy.\"\n                 + \" It needs to replicate {} more blocks.\"\n                 + \" Decommissioning is still in progress.\",\n                 dn, isHealthy? \"is\": \"isn\u0027t\", blocks.size());\n           }\n         } else {\n           LOG.debug(\"Node {} still has {} blocks to replicate \"\n                   + \"before it is a candidate to finish decommissioning.\",\n               dn, blocks.size());\n         }\n         iterkey \u003d dn;\n       }\n-      // Remove the datanodes that are decommissioned\n+      // Remove the datanodes that are decommissioned or in service after\n+      // maintenance expiration.\n       for (DatanodeDescriptor dn : toRemove) {\n-        Preconditions.checkState(dn.isDecommissioned(),\n-            \"Removing a node that is not yet decommissioned!\");\n-        decomNodeBlocks.remove(dn);\n+        Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n+            \"Removing a node that is not yet decommissioned or in service!\");\n+        outOfServiceNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(outOfServiceNodeBlocks,\n              iterkey).iterator();\n      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n\n      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (dn.isMaintenance()) {\n          // TODO HDFS-9390 make sure blocks are minimally replicated\n          // before transitioning the node to IN_MAINTENANCE state.\n\n          // If maintenance expires, stop tracking it.\n          if (dn.maintenanceExpired()) {\n            stopMaintenance(dn);\n            toRemove.add(dn);\n          }\n          continue;\n        }\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule \n          // under-replicated blocks for replication and collect the blocks \n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyStored(dn);\n          outOfServiceNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently \n          // replicated blocks has dropped to zero and if it can be decommed\n          LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n          pruneReliableBlocks(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the \n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated \n            // blocks. Re-check with the full block map before finally \n            // marking the datanode as decommissioned \n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            outOfServiceNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay, \n          // we can finally mark as decommissioned.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommission(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            setDecommissioned(dn);\n            toRemove.add(dn);\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as decommissioned.\", dn);\n          } else {\n            LOG.debug(\"Node {} {} healthy.\"\n                + \" It needs to replicate {} more blocks.\"\n                + \" Decommissioning is still in progress.\",\n                dn, isHealthy? \"is\": \"isn\u0027t\", blocks.size());\n          }\n        } else {\n          LOG.debug(\"Node {} still has {} blocks to replicate \"\n                  + \"before it is a candidate to finish decommissioning.\",\n              dn, blocks.size());\n        }\n        iterkey \u003d dn;\n      }\n      // Remove the datanodes that are decommissioned or in service after\n      // maintenance expiration.\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned() || dn.isInService(),\n            \"Removing a node that is not yet decommissioned or in service!\");\n        outOfServiceNodeBlocks.remove(dn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "796a676d18bd7cd3ed4113d002e0e69cf261d6d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9528. Cleanup namenode audit/log/exception messages. (szetszwo via umamahesh)\n",
      "commitDate": "11/12/15 5:57 PM",
      "commitName": "796a676d18bd7cd3ed4113d002e0e69cf261d6d1",
      "commitAuthor": "Uma Mahesh",
      "commitDateOld": "09/12/15 5:55 PM",
      "commitNameOld": "132478e805ba0f955345217b8ad87c2d17cccb2d",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 2.0,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,69 @@\n     private void check() {\n       final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n           it \u003d new CyclicIteration\u003c\u003e(decomNodeBlocks, iterkey).iterator();\n       final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n \n       while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck()) {\n         numNodesChecked++;\n         final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n         AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n         boolean fullScan \u003d false;\n         if (blocks \u003d\u003d null) {\n           // This is a newly added datanode, run through its list to schedule \n           // under-replicated blocks for replication and collect the blocks \n           // that are insufficiently replicated for further tracking\n           LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n               \"insufficiently-replicated blocks.\", dn);\n           blocks \u003d handleInsufficientlyStored(dn);\n           decomNodeBlocks.put(dn, blocks);\n           fullScan \u003d true;\n         } else {\n           // This is a known datanode, check if its # of insufficiently \n           // replicated blocks has dropped to zero and if it can be decommed\n           LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n           pruneReliableBlocks(dn, blocks);\n         }\n         if (blocks.size() \u003d\u003d 0) {\n           if (!fullScan) {\n             // If we didn\u0027t just do a full scan, need to re-check with the \n             // full block map.\n             //\n             // We\u0027ve replicated all the known insufficiently replicated \n             // blocks. Re-check with the full block map before finally \n             // marking the datanode as decommissioned \n             LOG.debug(\"Node {} has finished replicating current set of \"\n                 + \"blocks, checking with the full block map.\", dn);\n             blocks \u003d handleInsufficientlyStored(dn);\n             decomNodeBlocks.put(dn, blocks);\n           }\n           // If the full scan is clean AND the node liveness is okay, \n           // we can finally mark as decommissioned.\n           final boolean isHealthy \u003d\n               blockManager.isNodeHealthyForDecommission(dn);\n           if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n             setDecommissioned(dn);\n             toRemove.add(dn);\n             LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                 + \"marked as decommissioned.\", dn);\n           } else {\n-            if (LOG.isDebugEnabled()) {\n-              StringBuilder b \u003d new StringBuilder(\"Node {} \");\n-              if (isHealthy) {\n-                b.append(\"is \");\n-              } else {\n-                b.append(\"isn\u0027t \");\n-              }\n-              b.append(\"healthy and still needs to replicate {} more blocks,\" +\n-                  \" decommissioning is still in progress.\");\n-              LOG.debug(b.toString(), dn, blocks.size());\n-            }\n+            LOG.debug(\"Node {} {} healthy.\"\n+                + \" It needs to replicate {} more blocks.\"\n+                + \" Decommissioning is still in progress.\",\n+                dn, isHealthy? \"is\": \"isn\u0027t\", blocks.size());\n           }\n         } else {\n           LOG.debug(\"Node {} still has {} blocks to replicate \"\n                   + \"before it is a candidate to finish decommissioning.\",\n               dn, blocks.size());\n         }\n         iterkey \u003d dn;\n       }\n       // Remove the datanodes that are decommissioned\n       for (DatanodeDescriptor dn : toRemove) {\n         Preconditions.checkState(dn.isDecommissioned(),\n             \"Removing a node that is not yet decommissioned!\");\n         decomNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(decomNodeBlocks, iterkey).iterator();\n      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n\n      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule \n          // under-replicated blocks for replication and collect the blocks \n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyStored(dn);\n          decomNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently \n          // replicated blocks has dropped to zero and if it can be decommed\n          LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n          pruneReliableBlocks(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the \n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated \n            // blocks. Re-check with the full block map before finally \n            // marking the datanode as decommissioned \n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            decomNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay, \n          // we can finally mark as decommissioned.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommission(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            setDecommissioned(dn);\n            toRemove.add(dn);\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as decommissioned.\", dn);\n          } else {\n            LOG.debug(\"Node {} {} healthy.\"\n                + \" It needs to replicate {} more blocks.\"\n                + \" Decommissioning is still in progress.\",\n                dn, isHealthy? \"is\": \"isn\u0027t\", blocks.size());\n          }\n        } else {\n          LOG.debug(\"Node {} still has {} blocks to replicate \"\n                  + \"before it is a candidate to finish decommissioning.\",\n              dn, blocks.size());\n        }\n        iterkey \u003d dn;\n      }\n      // Remove the datanodes that are decommissioned\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned(),\n            \"Removing a node that is not yet decommissioned!\");\n        decomNodeBlocks.remove(dn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "a3990ca41415515b986a41dacefceee1f05622f8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8591. Remove support for deprecated configuration key dfs.namenode.decommission.nodes.per.interval.\n",
      "commitDate": "16/06/15 10:03 AM",
      "commitName": "a3990ca41415515b986a41dacefceee1f05622f8",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/05/15 3:42 PM",
      "commitNameOld": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 19.76,
      "commitsBetweenForRepo": 139,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,76 @@\n     private void check() {\n       final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n           it \u003d new CyclicIteration\u003c\u003e(decomNodeBlocks, iterkey).iterator();\n       final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n \n-      while (it.hasNext()\n-          \u0026\u0026 !exceededNumBlocksPerCheck()\n-          \u0026\u0026 !exceededNumNodesPerCheck()) {\n+      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck()) {\n         numNodesChecked++;\n         final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n         AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n         boolean fullScan \u003d false;\n         if (blocks \u003d\u003d null) {\n           // This is a newly added datanode, run through its list to schedule \n           // under-replicated blocks for replication and collect the blocks \n           // that are insufficiently replicated for further tracking\n           LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n               \"insufficiently-replicated blocks.\", dn);\n           blocks \u003d handleInsufficientlyReplicated(dn);\n           decomNodeBlocks.put(dn, blocks);\n           fullScan \u003d true;\n         } else {\n           // This is a known datanode, check if its # of insufficiently \n           // replicated blocks has dropped to zero and if it can be decommed\n           LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n           pruneSufficientlyReplicated(dn, blocks);\n         }\n         if (blocks.size() \u003d\u003d 0) {\n           if (!fullScan) {\n             // If we didn\u0027t just do a full scan, need to re-check with the \n             // full block map.\n             //\n             // We\u0027ve replicated all the known insufficiently replicated \n             // blocks. Re-check with the full block map before finally \n             // marking the datanode as decommissioned \n             LOG.debug(\"Node {} has finished replicating current set of \"\n                 + \"blocks, checking with the full block map.\", dn);\n             blocks \u003d handleInsufficientlyReplicated(dn);\n             decomNodeBlocks.put(dn, blocks);\n           }\n           // If the full scan is clean AND the node liveness is okay, \n           // we can finally mark as decommissioned.\n           final boolean isHealthy \u003d\n               blockManager.isNodeHealthyForDecommission(dn);\n           if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n             setDecommissioned(dn);\n             toRemove.add(dn);\n             LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                 + \"marked as decommissioned.\", dn);\n           } else {\n             if (LOG.isDebugEnabled()) {\n               StringBuilder b \u003d new StringBuilder(\"Node {} \");\n               if (isHealthy) {\n                 b.append(\"is \");\n               } else {\n                 b.append(\"isn\u0027t \");\n               }\n               b.append(\"healthy and still needs to replicate {} more blocks,\" +\n                   \" decommissioning is still in progress.\");\n               LOG.debug(b.toString(), dn, blocks.size());\n             }\n           }\n         } else {\n           LOG.debug(\"Node {} still has {} blocks to replicate \"\n                   + \"before it is a candidate to finish decommissioning.\",\n               dn, blocks.size());\n         }\n         iterkey \u003d dn;\n       }\n       // Remove the datanodes that are decommissioned\n       for (DatanodeDescriptor dn : toRemove) {\n         Preconditions.checkState(dn.isDecommissioned(),\n             \"Removing a node that is not yet decommissioned!\");\n         decomNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(decomNodeBlocks, iterkey).iterator();\n      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n\n      while (it.hasNext() \u0026\u0026 !exceededNumBlocksPerCheck()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule \n          // under-replicated blocks for replication and collect the blocks \n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyReplicated(dn);\n          decomNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently \n          // replicated blocks has dropped to zero and if it can be decommed\n          LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n          pruneSufficientlyReplicated(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the \n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated \n            // blocks. Re-check with the full block map before finally \n            // marking the datanode as decommissioned \n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyReplicated(dn);\n            decomNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay, \n          // we can finally mark as decommissioned.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommission(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            setDecommissioned(dn);\n            toRemove.add(dn);\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as decommissioned.\", dn);\n          } else {\n            if (LOG.isDebugEnabled()) {\n              StringBuilder b \u003d new StringBuilder(\"Node {} \");\n              if (isHealthy) {\n                b.append(\"is \");\n              } else {\n                b.append(\"isn\u0027t \");\n              }\n              b.append(\"healthy and still needs to replicate {} more blocks,\" +\n                  \" decommissioning is still in progress.\");\n              LOG.debug(b.toString(), dn, blocks.size());\n            }\n          }\n        } else {\n          LOG.debug(\"Node {} still has {} blocks to replicate \"\n                  + \"before it is a candidate to finish decommissioning.\",\n              dn, blocks.size());\n        }\n        iterkey \u003d dn;\n      }\n      // Remove the datanodes that are decommissioned\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned(),\n            \"Removing a node that is not yet decommissioned!\");\n        decomNodeBlocks.remove(dn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "12/05/15 6:29 AM",
      "commitNameOld": "6d5da9484185ca9f585195d6da069b9cd5be4044",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 15.38,
      "commitsBetweenForRepo": 120,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,78 @@\n     private void check() {\n-      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfoContiguous\u003e\u003e\u003e\n+      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n           it \u003d new CyclicIteration\u003c\u003e(decomNodeBlocks, iterkey).iterator();\n       final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n \n       while (it.hasNext()\n           \u0026\u0026 !exceededNumBlocksPerCheck()\n           \u0026\u0026 !exceededNumNodesPerCheck()) {\n         numNodesChecked++;\n-        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfoContiguous\u003e\u003e\n+        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n-        AbstractList\u003cBlockInfoContiguous\u003e blocks \u003d entry.getValue();\n+        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n         boolean fullScan \u003d false;\n         if (blocks \u003d\u003d null) {\n           // This is a newly added datanode, run through its list to schedule \n           // under-replicated blocks for replication and collect the blocks \n           // that are insufficiently replicated for further tracking\n           LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n               \"insufficiently-replicated blocks.\", dn);\n           blocks \u003d handleInsufficientlyReplicated(dn);\n           decomNodeBlocks.put(dn, blocks);\n           fullScan \u003d true;\n         } else {\n           // This is a known datanode, check if its # of insufficiently \n           // replicated blocks has dropped to zero and if it can be decommed\n           LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n           pruneSufficientlyReplicated(dn, blocks);\n         }\n         if (blocks.size() \u003d\u003d 0) {\n           if (!fullScan) {\n             // If we didn\u0027t just do a full scan, need to re-check with the \n             // full block map.\n             //\n             // We\u0027ve replicated all the known insufficiently replicated \n             // blocks. Re-check with the full block map before finally \n             // marking the datanode as decommissioned \n             LOG.debug(\"Node {} has finished replicating current set of \"\n                 + \"blocks, checking with the full block map.\", dn);\n             blocks \u003d handleInsufficientlyReplicated(dn);\n             decomNodeBlocks.put(dn, blocks);\n           }\n           // If the full scan is clean AND the node liveness is okay, \n           // we can finally mark as decommissioned.\n           final boolean isHealthy \u003d\n               blockManager.isNodeHealthyForDecommission(dn);\n           if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n             setDecommissioned(dn);\n             toRemove.add(dn);\n             LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                 + \"marked as decommissioned.\", dn);\n           } else {\n             if (LOG.isDebugEnabled()) {\n               StringBuilder b \u003d new StringBuilder(\"Node {} \");\n               if (isHealthy) {\n                 b.append(\"is \");\n               } else {\n                 b.append(\"isn\u0027t \");\n               }\n               b.append(\"healthy and still needs to replicate {} more blocks,\" +\n                   \" decommissioning is still in progress.\");\n               LOG.debug(b.toString(), dn, blocks.size());\n             }\n           }\n         } else {\n           LOG.debug(\"Node {} still has {} blocks to replicate \"\n                   + \"before it is a candidate to finish decommissioning.\",\n               dn, blocks.size());\n         }\n         iterkey \u003d dn;\n       }\n       // Remove the datanodes that are decommissioned\n       for (DatanodeDescriptor dn : toRemove) {\n         Preconditions.checkState(dn.isDecommissioned(),\n             \"Removing a node that is not yet decommissioned!\");\n         decomNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(decomNodeBlocks, iterkey).iterator();\n      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n\n      while (it.hasNext()\n          \u0026\u0026 !exceededNumBlocksPerCheck()\n          \u0026\u0026 !exceededNumNodesPerCheck()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule \n          // under-replicated blocks for replication and collect the blocks \n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyReplicated(dn);\n          decomNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently \n          // replicated blocks has dropped to zero and if it can be decommed\n          LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n          pruneSufficientlyReplicated(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the \n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated \n            // blocks. Re-check with the full block map before finally \n            // marking the datanode as decommissioned \n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyReplicated(dn);\n            decomNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay, \n          // we can finally mark as decommissioned.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommission(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            setDecommissioned(dn);\n            toRemove.add(dn);\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as decommissioned.\", dn);\n          } else {\n            if (LOG.isDebugEnabled()) {\n              StringBuilder b \u003d new StringBuilder(\"Node {} \");\n              if (isHealthy) {\n                b.append(\"is \");\n              } else {\n                b.append(\"isn\u0027t \");\n              }\n              b.append(\"healthy and still needs to replicate {} more blocks,\" +\n                  \" decommissioning is still in progress.\");\n              LOG.debug(b.toString(), dn, blocks.size());\n            }\n          }\n        } else {\n          LOG.debug(\"Node {} still has {} blocks to replicate \"\n                  + \"before it is a candidate to finish decommissioning.\",\n              dn, blocks.size());\n        }\n        iterkey \u003d dn;\n      }\n      // Remove the datanodes that are decommissioned\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned(),\n            \"Removing a node that is not yet decommissioned!\");\n        decomNodeBlocks.remove(dn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "abf833a7b228fff2bca4f69cd9df99d532380038": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7907. Erasure Coding: track invalid, corrupt, and under-recovery striped blocks in NameNode. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:43 AM",
      "commitName": "abf833a7b228fff2bca4f69cd9df99d532380038",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:32 AM",
      "commitNameOld": "11585883a9eb30ba080b9aa49dba42cb0a797d75",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,78 @@\n     private void check() {\n-      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfoContiguous\u003e\u003e\u003e\n+      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n           it \u003d new CyclicIteration\u003c\u003e(decomNodeBlocks, iterkey).iterator();\n       final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n \n       while (it.hasNext()\n           \u0026\u0026 !exceededNumBlocksPerCheck()\n           \u0026\u0026 !exceededNumNodesPerCheck()) {\n         numNodesChecked++;\n-        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfoContiguous\u003e\u003e\n+        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n             entry \u003d it.next();\n         final DatanodeDescriptor dn \u003d entry.getKey();\n-        AbstractList\u003cBlockInfoContiguous\u003e blocks \u003d entry.getValue();\n+        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n         boolean fullScan \u003d false;\n         if (blocks \u003d\u003d null) {\n           // This is a newly added datanode, run through its list to schedule \n           // under-replicated blocks for replication and collect the blocks \n           // that are insufficiently replicated for further tracking\n           LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n               \"insufficiently-replicated blocks.\", dn);\n-          blocks \u003d handleInsufficientlyReplicated(dn);\n+          blocks \u003d handleInsufficientlyStored(dn);\n           decomNodeBlocks.put(dn, blocks);\n           fullScan \u003d true;\n         } else {\n           // This is a known datanode, check if its # of insufficiently \n           // replicated blocks has dropped to zero and if it can be decommed\n           LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n-          pruneSufficientlyReplicated(dn, blocks);\n+          pruneReliableBlocks(dn, blocks);\n         }\n         if (blocks.size() \u003d\u003d 0) {\n           if (!fullScan) {\n             // If we didn\u0027t just do a full scan, need to re-check with the \n             // full block map.\n             //\n             // We\u0027ve replicated all the known insufficiently replicated \n             // blocks. Re-check with the full block map before finally \n             // marking the datanode as decommissioned \n             LOG.debug(\"Node {} has finished replicating current set of \"\n                 + \"blocks, checking with the full block map.\", dn);\n-            blocks \u003d handleInsufficientlyReplicated(dn);\n+            blocks \u003d handleInsufficientlyStored(dn);\n             decomNodeBlocks.put(dn, blocks);\n           }\n           // If the full scan is clean AND the node liveness is okay, \n           // we can finally mark as decommissioned.\n           final boolean isHealthy \u003d\n               blockManager.isNodeHealthyForDecommission(dn);\n           if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n             setDecommissioned(dn);\n             toRemove.add(dn);\n             LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                 + \"marked as decommissioned.\", dn);\n           } else {\n             if (LOG.isDebugEnabled()) {\n               StringBuilder b \u003d new StringBuilder(\"Node {} \");\n               if (isHealthy) {\n                 b.append(\"is \");\n               } else {\n                 b.append(\"isn\u0027t \");\n               }\n               b.append(\"healthy and still needs to replicate {} more blocks,\" +\n                   \" decommissioning is still in progress.\");\n               LOG.debug(b.toString(), dn, blocks.size());\n             }\n           }\n         } else {\n           LOG.debug(\"Node {} still has {} blocks to replicate \"\n                   + \"before it is a candidate to finish decommissioning.\",\n               dn, blocks.size());\n         }\n         iterkey \u003d dn;\n       }\n       // Remove the datanodes that are decommissioned\n       for (DatanodeDescriptor dn : toRemove) {\n         Preconditions.checkState(dn.isDecommissioned(),\n             \"Removing a node that is not yet decommissioned!\");\n         decomNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(decomNodeBlocks, iterkey).iterator();\n      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n\n      while (it.hasNext()\n          \u0026\u0026 !exceededNumBlocksPerCheck()\n          \u0026\u0026 !exceededNumNodesPerCheck()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfo\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        AbstractList\u003cBlockInfo\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule \n          // under-replicated blocks for replication and collect the blocks \n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyStored(dn);\n          decomNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently \n          // replicated blocks has dropped to zero and if it can be decommed\n          LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n          pruneReliableBlocks(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the \n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated \n            // blocks. Re-check with the full block map before finally \n            // marking the datanode as decommissioned \n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyStored(dn);\n            decomNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay, \n          // we can finally mark as decommissioned.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommission(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            setDecommissioned(dn);\n            toRemove.add(dn);\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as decommissioned.\", dn);\n          } else {\n            if (LOG.isDebugEnabled()) {\n              StringBuilder b \u003d new StringBuilder(\"Node {} \");\n              if (isHealthy) {\n                b.append(\"is \");\n              } else {\n                b.append(\"isn\u0027t \");\n              }\n              b.append(\"healthy and still needs to replicate {} more blocks,\" +\n                  \" decommissioning is still in progress.\");\n              LOG.debug(b.toString(), dn, blocks.size());\n            }\n          }\n        } else {\n          LOG.debug(\"Node {} still has {} blocks to replicate \"\n                  + \"before it is a candidate to finish decommissioning.\",\n              dn, blocks.size());\n        }\n        iterkey \u003d dn;\n      }\n      // Remove the datanodes that are decommissioned\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned(),\n            \"Removing a node that is not yet decommissioned!\");\n        decomNodeBlocks.remove(dn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7411. Change decommission logic to throttle by blocks rather\nthan nodes in each interval. Contributed by Andrew Wang\n",
      "commitDate": "08/03/15 6:31 PM",
      "commitName": "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "28/10/11 11:28 AM",
      "commitNameOld": "9692cfc9ae2ac86c9a2d2b3ac9ca8f8b3bfc7c42",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1227.29,
      "commitsBetweenForRepo": 8325,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,78 @@\n     private void check() {\n-      final DatanodeManager dm \u003d blockmanager.getDatanodeManager();\n-      int count \u003d 0;\n-      for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n-          : dm.getDatanodeCyclicIteration(firstkey)) {\n-        final DatanodeDescriptor d \u003d entry.getValue();\n-        firstkey \u003d entry.getKey();\n+      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfoContiguous\u003e\u003e\u003e\n+          it \u003d new CyclicIteration\u003c\u003e(decomNodeBlocks, iterkey).iterator();\n+      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n \n-        if (d.isDecommissionInProgress()) {\n-          try {\n-            dm.checkDecommissionState(d);\n-          } catch(Exception e) {\n-            LOG.warn(\"entry\u003d\" + entry, e);\n-          }\n-          if (++count \u003d\u003d numNodesPerCheck) {\n-            return;\n-          }\n+      while (it.hasNext()\n+          \u0026\u0026 !exceededNumBlocksPerCheck()\n+          \u0026\u0026 !exceededNumNodesPerCheck()) {\n+        numNodesChecked++;\n+        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfoContiguous\u003e\u003e\n+            entry \u003d it.next();\n+        final DatanodeDescriptor dn \u003d entry.getKey();\n+        AbstractList\u003cBlockInfoContiguous\u003e blocks \u003d entry.getValue();\n+        boolean fullScan \u003d false;\n+        if (blocks \u003d\u003d null) {\n+          // This is a newly added datanode, run through its list to schedule \n+          // under-replicated blocks for replication and collect the blocks \n+          // that are insufficiently replicated for further tracking\n+          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n+              \"insufficiently-replicated blocks.\", dn);\n+          blocks \u003d handleInsufficientlyReplicated(dn);\n+          decomNodeBlocks.put(dn, blocks);\n+          fullScan \u003d true;\n+        } else {\n+          // This is a known datanode, check if its # of insufficiently \n+          // replicated blocks has dropped to zero and if it can be decommed\n+          LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n+          pruneSufficientlyReplicated(dn, blocks);\n         }\n+        if (blocks.size() \u003d\u003d 0) {\n+          if (!fullScan) {\n+            // If we didn\u0027t just do a full scan, need to re-check with the \n+            // full block map.\n+            //\n+            // We\u0027ve replicated all the known insufficiently replicated \n+            // blocks. Re-check with the full block map before finally \n+            // marking the datanode as decommissioned \n+            LOG.debug(\"Node {} has finished replicating current set of \"\n+                + \"blocks, checking with the full block map.\", dn);\n+            blocks \u003d handleInsufficientlyReplicated(dn);\n+            decomNodeBlocks.put(dn, blocks);\n+          }\n+          // If the full scan is clean AND the node liveness is okay, \n+          // we can finally mark as decommissioned.\n+          final boolean isHealthy \u003d\n+              blockManager.isNodeHealthyForDecommission(dn);\n+          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n+            setDecommissioned(dn);\n+            toRemove.add(dn);\n+            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n+                + \"marked as decommissioned.\", dn);\n+          } else {\n+            if (LOG.isDebugEnabled()) {\n+              StringBuilder b \u003d new StringBuilder(\"Node {} \");\n+              if (isHealthy) {\n+                b.append(\"is \");\n+              } else {\n+                b.append(\"isn\u0027t \");\n+              }\n+              b.append(\"healthy and still needs to replicate {} more blocks,\" +\n+                  \" decommissioning is still in progress.\");\n+              LOG.debug(b.toString(), dn, blocks.size());\n+            }\n+          }\n+        } else {\n+          LOG.debug(\"Node {} still has {} blocks to replicate \"\n+                  + \"before it is a candidate to finish decommissioning.\",\n+              dn, blocks.size());\n+        }\n+        iterkey \u003d dn;\n+      }\n+      // Remove the datanodes that are decommissioned\n+      for (DatanodeDescriptor dn : toRemove) {\n+        Preconditions.checkState(dn.isDecommissioned(),\n+            \"Removing a node that is not yet decommissioned!\");\n+        decomNodeBlocks.remove(dn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final Iterator\u003cMap.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfoContiguous\u003e\u003e\u003e\n          it \u003d new CyclicIteration\u003c\u003e(decomNodeBlocks, iterkey).iterator();\n      final LinkedList\u003cDatanodeDescriptor\u003e toRemove \u003d new LinkedList\u003c\u003e();\n\n      while (it.hasNext()\n          \u0026\u0026 !exceededNumBlocksPerCheck()\n          \u0026\u0026 !exceededNumNodesPerCheck()) {\n        numNodesChecked++;\n        final Map.Entry\u003cDatanodeDescriptor, AbstractList\u003cBlockInfoContiguous\u003e\u003e\n            entry \u003d it.next();\n        final DatanodeDescriptor dn \u003d entry.getKey();\n        AbstractList\u003cBlockInfoContiguous\u003e blocks \u003d entry.getValue();\n        boolean fullScan \u003d false;\n        if (blocks \u003d\u003d null) {\n          // This is a newly added datanode, run through its list to schedule \n          // under-replicated blocks for replication and collect the blocks \n          // that are insufficiently replicated for further tracking\n          LOG.debug(\"Newly-added node {}, doing full scan to find \" +\n              \"insufficiently-replicated blocks.\", dn);\n          blocks \u003d handleInsufficientlyReplicated(dn);\n          decomNodeBlocks.put(dn, blocks);\n          fullScan \u003d true;\n        } else {\n          // This is a known datanode, check if its # of insufficiently \n          // replicated blocks has dropped to zero and if it can be decommed\n          LOG.debug(\"Processing decommission-in-progress node {}\", dn);\n          pruneSufficientlyReplicated(dn, blocks);\n        }\n        if (blocks.size() \u003d\u003d 0) {\n          if (!fullScan) {\n            // If we didn\u0027t just do a full scan, need to re-check with the \n            // full block map.\n            //\n            // We\u0027ve replicated all the known insufficiently replicated \n            // blocks. Re-check with the full block map before finally \n            // marking the datanode as decommissioned \n            LOG.debug(\"Node {} has finished replicating current set of \"\n                + \"blocks, checking with the full block map.\", dn);\n            blocks \u003d handleInsufficientlyReplicated(dn);\n            decomNodeBlocks.put(dn, blocks);\n          }\n          // If the full scan is clean AND the node liveness is okay, \n          // we can finally mark as decommissioned.\n          final boolean isHealthy \u003d\n              blockManager.isNodeHealthyForDecommission(dn);\n          if (blocks.size() \u003d\u003d 0 \u0026\u0026 isHealthy) {\n            setDecommissioned(dn);\n            toRemove.add(dn);\n            LOG.debug(\"Node {} is sufficiently replicated and healthy, \"\n                + \"marked as decommissioned.\", dn);\n          } else {\n            if (LOG.isDebugEnabled()) {\n              StringBuilder b \u003d new StringBuilder(\"Node {} \");\n              if (isHealthy) {\n                b.append(\"is \");\n              } else {\n                b.append(\"isn\u0027t \");\n              }\n              b.append(\"healthy and still needs to replicate {} more blocks,\" +\n                  \" decommissioning is still in progress.\");\n              LOG.debug(b.toString(), dn, blocks.size());\n            }\n          }\n        } else {\n          LOG.debug(\"Node {} still has {} blocks to replicate \"\n                  + \"before it is a candidate to finish decommissioning.\",\n              dn, blocks.size());\n        }\n        iterkey \u003d dn;\n      }\n      // Remove the datanodes that are decommissioned\n      for (DatanodeDescriptor dn : toRemove) {\n        Preconditions.checkState(dn.isDecommissioned(),\n            \"Removing a node that is not yet decommissioned!\");\n        decomNodeBlocks.remove(dn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "9692cfc9ae2ac86c9a2d2b3ac9ca8f8b3bfc7c42": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2493. Remove reference to FSNamesystem in blockmanagement classes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190491 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/10/11 11:28 AM",
      "commitName": "9692cfc9ae2ac86c9a2d2b3ac9ca8f8b3bfc7c42",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 64.76,
      "commitsBetweenForRepo": 485,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n     private void check() {\n-      final DatanodeManager dm \u003d fsnamesystem.getBlockManager().getDatanodeManager();\n+      final DatanodeManager dm \u003d blockmanager.getDatanodeManager();\n       int count \u003d 0;\n       for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n           : dm.getDatanodeCyclicIteration(firstkey)) {\n         final DatanodeDescriptor d \u003d entry.getValue();\n         firstkey \u003d entry.getKey();\n \n         if (d.isDecommissionInProgress()) {\n           try {\n             dm.checkDecommissionState(d);\n           } catch(Exception e) {\n             LOG.warn(\"entry\u003d\" + entry, e);\n           }\n           if (++count \u003d\u003d numNodesPerCheck) {\n             return;\n           }\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final DatanodeManager dm \u003d blockmanager.getDatanodeManager();\n      int count \u003d 0;\n      for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n          : dm.getDatanodeCyclicIteration(firstkey)) {\n        final DatanodeDescriptor d \u003d entry.getValue();\n        firstkey \u003d entry.getKey();\n\n        if (d.isDecommissionInProgress()) {\n          try {\n            dm.checkDecommissionState(d);\n          } catch(Exception e) {\n            LOG.warn(\"entry\u003d\" + entry, e);\n          }\n          if (++count \u003d\u003d numNodesPerCheck) {\n            return;\n          }\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void check() {\n      final DatanodeManager dm \u003d fsnamesystem.getBlockManager().getDatanodeManager();\n      int count \u003d 0;\n      for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n          : dm.getDatanodeCyclicIteration(firstkey)) {\n        final DatanodeDescriptor d \u003d entry.getValue();\n        firstkey \u003d entry.getKey();\n\n        if (d.isDecommissionInProgress()) {\n          try {\n            dm.checkDecommissionState(d);\n          } catch(Exception e) {\n            LOG.warn(\"entry\u003d\" + entry, e);\n          }\n          if (++count \u003d\u003d numNodesPerCheck) {\n            return;\n          }\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void check() {\n      final DatanodeManager dm \u003d fsnamesystem.getBlockManager().getDatanodeManager();\n      int count \u003d 0;\n      for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n          : dm.getDatanodeCyclicIteration(firstkey)) {\n        final DatanodeDescriptor d \u003d entry.getValue();\n        firstkey \u003d entry.getKey();\n\n        if (d.isDecommissionInProgress()) {\n          try {\n            dm.checkDecommissionState(d);\n          } catch(Exception e) {\n            LOG.warn(\"entry\u003d\" + entry, e);\n          }\n          if (++count \u003d\u003d numNodesPerCheck) {\n            return;\n          }\n        }\n      }\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java"
      }
    },
    "7fac946ac983e31613fd62836c8ac9c4a579210a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2108. Move datanode heartbeat handling from namenode package to blockmanagement package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154042 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/08/11 3:55 PM",
      "commitName": "7fac946ac983e31613fd62836c8ac9c4a579210a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "26/07/11 10:46 PM",
      "commitNameOld": "969a263188f7015261719fe45fa1505121ebb80e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 8.71,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n     private void check() {\n+      final DatanodeManager dm \u003d fsnamesystem.getBlockManager().getDatanodeManager();\n       int count \u003d 0;\n       for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n-          : blockManager.getDatanodeManager().getDatanodeCyclicIteration(\n-              firstkey)) {\n+          : dm.getDatanodeCyclicIteration(firstkey)) {\n         final DatanodeDescriptor d \u003d entry.getValue();\n         firstkey \u003d entry.getKey();\n \n         if (d.isDecommissionInProgress()) {\n           try {\n-            blockManager.checkDecommissionStateInternal(d);\n+            dm.checkDecommissionState(d);\n           } catch(Exception e) {\n             LOG.warn(\"entry\u003d\" + entry, e);\n           }\n           if (++count \u003d\u003d numNodesPerCheck) {\n             return;\n           }\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      final DatanodeManager dm \u003d fsnamesystem.getBlockManager().getDatanodeManager();\n      int count \u003d 0;\n      for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n          : dm.getDatanodeCyclicIteration(firstkey)) {\n        final DatanodeDescriptor d \u003d entry.getValue();\n        firstkey \u003d entry.getKey();\n\n        if (d.isDecommissionInProgress()) {\n          try {\n            dm.checkDecommissionState(d);\n          } catch(Exception e) {\n            LOG.warn(\"entry\u003d\" + entry, e);\n          }\n          if (++count \u003d\u003d numNodesPerCheck) {\n            return;\n          }\n        }\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "969a263188f7015261719fe45fa1505121ebb80e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2191.  Move datanodeMap from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151339 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/11 10:46 PM",
      "commitName": "969a263188f7015261719fe45fa1505121ebb80e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "22/07/11 6:01 PM",
      "commitNameOld": "89537b7710b23db7abcd2a77f03818c06a5f5fa7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.2,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n     private void check() {\n       int count \u003d 0;\n       for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n-          : new CyclicIteration\u003cString, DatanodeDescriptor\u003e(\n-              fsnamesystem.datanodeMap, firstkey)) {\n+          : blockManager.getDatanodeManager().getDatanodeCyclicIteration(\n+              firstkey)) {\n         final DatanodeDescriptor d \u003d entry.getValue();\n         firstkey \u003d entry.getKey();\n \n         if (d.isDecommissionInProgress()) {\n           try {\n             blockManager.checkDecommissionStateInternal(d);\n           } catch(Exception e) {\n             LOG.warn(\"entry\u003d\" + entry, e);\n           }\n           if (++count \u003d\u003d numNodesPerCheck) {\n             return;\n           }\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      int count \u003d 0;\n      for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n          : blockManager.getDatanodeManager().getDatanodeCyclicIteration(\n              firstkey)) {\n        final DatanodeDescriptor d \u003d entry.getValue();\n        firstkey \u003d entry.getKey();\n\n        if (d.isDecommissionInProgress()) {\n          try {\n            blockManager.checkDecommissionStateInternal(d);\n          } catch(Exception e) {\n            LOG.warn(\"entry\u003d\" + entry, e);\n          }\n          if (++count \u003d\u003d numNodesPerCheck) {\n            return;\n          }\n        }\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "89537b7710b23db7abcd2a77f03818c06a5f5fa7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2112.  Move ReplicationMonitor to block management.  Contributed by Uma Maheswara Rao G\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149771 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/11 6:01 PM",
      "commitName": "89537b7710b23db7abcd2a77f03818c06a5f5fa7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "21/07/11 9:20 PM",
      "commitNameOld": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.86,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n     private void check() {\n       int count \u003d 0;\n       for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n           : new CyclicIteration\u003cString, DatanodeDescriptor\u003e(\n               fsnamesystem.datanodeMap, firstkey)) {\n         final DatanodeDescriptor d \u003d entry.getValue();\n         firstkey \u003d entry.getKey();\n \n         if (d.isDecommissionInProgress()) {\n           try {\n-            blockmanager.checkDecommissionStateInternal(d);\n+            blockManager.checkDecommissionStateInternal(d);\n           } catch(Exception e) {\n             LOG.warn(\"entry\u003d\" + entry, e);\n           }\n           if (++count \u003d\u003d numNodesPerCheck) {\n             return;\n           }\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      int count \u003d 0;\n      for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n          : new CyclicIteration\u003cString, DatanodeDescriptor\u003e(\n              fsnamesystem.datanodeMap, firstkey)) {\n        final DatanodeDescriptor d \u003d entry.getValue();\n        firstkey \u003d entry.getKey();\n\n        if (d.isDecommissionInProgress()) {\n          try {\n            blockManager.checkDecommissionStateInternal(d);\n          } catch(Exception e) {\n            LOG.warn(\"entry\u003d\" + entry, e);\n          }\n          if (++count \u003d\u003d numNodesPerCheck) {\n            return;\n          }\n        }\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/07/11 9:20 PM",
      "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "11/07/11 4:06 PM",
      "commitNameOld": "b9189f7b158f9c50566a1b591e25e3d76c3b0917",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 10.22,
      "commitsBetweenForRepo": 48,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n     private void check() {\n       int count \u003d 0;\n       for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n           : new CyclicIteration\u003cString, DatanodeDescriptor\u003e(\n               fsnamesystem.datanodeMap, firstkey)) {\n         final DatanodeDescriptor d \u003d entry.getValue();\n         firstkey \u003d entry.getKey();\n \n         if (d.isDecommissionInProgress()) {\n           try {\n-            fsnamesystem.checkDecommissionStateInternal(d);\n+            blockmanager.checkDecommissionStateInternal(d);\n           } catch(Exception e) {\n             LOG.warn(\"entry\u003d\" + entry, e);\n           }\n           if (++count \u003d\u003d numNodesPerCheck) {\n             return;\n           }\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      int count \u003d 0;\n      for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n          : new CyclicIteration\u003cString, DatanodeDescriptor\u003e(\n              fsnamesystem.datanodeMap, firstkey)) {\n        final DatanodeDescriptor d \u003d entry.getValue();\n        firstkey \u003d entry.getKey();\n\n        if (d.isDecommissionInProgress()) {\n          try {\n            blockmanager.checkDecommissionStateInternal(d);\n          } catch(Exception e) {\n            LOG.warn(\"entry\u003d\" + entry, e);\n          }\n          if (++count \u003d\u003d numNodesPerCheck) {\n            return;\n          }\n        }\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "b9189f7b158f9c50566a1b591e25e3d76c3b0917": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2134. Move DecommissionManager to the blockmanagement package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1145393 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/07/11 4:06 PM",
      "commitName": "b9189f7b158f9c50566a1b591e25e3d76c3b0917",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "10/07/11 6:28 AM",
      "commitNameOld": "e8eed98feb5aa482abf9cec156e5b87022769604",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.4,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void check() {\n      int count \u003d 0;\n      for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n          : new CyclicIteration\u003cString, DatanodeDescriptor\u003e(\n              fsnamesystem.datanodeMap, firstkey)) {\n        final DatanodeDescriptor d \u003d entry.getValue();\n        firstkey \u003d entry.getKey();\n\n        if (d.isDecommissionInProgress()) {\n          try {\n            fsnamesystem.checkDecommissionStateInternal(d);\n          } catch(Exception e) {\n            LOG.warn(\"entry\u003d\" + entry, e);\n          }\n          if (++count \u003d\u003d numNodesPerCheck) {\n            return;\n          }\n        }\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/DecommissionManager.java",
        "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,20 @@\n+    private void check() {\n+      int count \u003d 0;\n+      for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n+          : new CyclicIteration\u003cString, DatanodeDescriptor\u003e(\n+              fsnamesystem.datanodeMap, firstkey)) {\n+        final DatanodeDescriptor d \u003d entry.getValue();\n+        firstkey \u003d entry.getKey();\n+\n+        if (d.isDecommissionInProgress()) {\n+          try {\n+            fsnamesystem.checkDecommissionStateInternal(d);\n+          } catch(Exception e) {\n+            LOG.warn(\"entry\u003d\" + entry, e);\n+          }\n+          if (++count \u003d\u003d numNodesPerCheck) {\n+            return;\n+          }\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void check() {\n      int count \u003d 0;\n      for(Map.Entry\u003cString, DatanodeDescriptor\u003e entry\n          : new CyclicIteration\u003cString, DatanodeDescriptor\u003e(\n              fsnamesystem.datanodeMap, firstkey)) {\n        final DatanodeDescriptor d \u003d entry.getValue();\n        firstkey \u003d entry.getKey();\n\n        if (d.isDecommissionInProgress()) {\n          try {\n            fsnamesystem.checkDecommissionStateInternal(d);\n          } catch(Exception e) {\n            LOG.warn(\"entry\u003d\" + entry, e);\n          }\n          if (++count \u003d\u003d numNodesPerCheck) {\n            return;\n          }\n        }\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/DecommissionManager.java"
    }
  }
}