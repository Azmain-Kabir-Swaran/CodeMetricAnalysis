{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "waitAndQueuePacket",
  "functionId": "waitAndQueuePacket___packet-DFSPacket",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 912,
  "functionEndLine": 953,
  "numCommitsSeen": 156,
  "timeTaken": 10060,
  "changeHistory": [
    "49b02d4a9bf9beac19f716488348ea4e30563ff4",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb",
    "2e140523d3ccb27809cde4a55e95f7e0006c028f",
    "631ccbdd2031a8387d4c2b743a4fc64c990391ce",
    "f2f5cdb5554d294a29ebf465101c5607fd56e244",
    "1c309f763be3dd2e3d7d1616d2c960ff80cf9b03",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "49b02d4a9bf9beac19f716488348ea4e30563ff4": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yrename,Yparameterchange)",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": "Ybodychange",
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb": "Ybodychange",
    "2e140523d3ccb27809cde4a55e95f7e0006c028f": "Ybodychange",
    "631ccbdd2031a8387d4c2b743a4fc64c990391ce": "Ybodychange",
    "f2f5cdb5554d294a29ebf465101c5607fd56e244": "Ybodychange",
    "1c309f763be3dd2e3d7d1616d2c960ff80cf9b03": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "49b02d4a9bf9beac19f716488348ea4e30563ff4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14395. Remove WARN Logging From Interrupts. Contributed by David Mollitor.\n",
      "commitDate": "28/03/19 11:16 AM",
      "commitName": "49b02d4a9bf9beac19f716488348ea4e30563ff4",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "06/11/18 11:18 AM",
      "commitNameOld": "887244de4adebe27693ed4ad3296a6f700cfa8c1",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 141.96,
      "commitsBetweenForRepo": 1021,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,42 @@\n   void waitAndQueuePacket(DFSPacket packet) throws IOException {\n     synchronized (dataQueue) {\n       try {\n         // If queue is full, then wait till we have enough space\n         boolean firstWait \u003d true;\n         try {\n           while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n               dfsClient.getConf().getWriteMaxPackets()) {\n             if (firstWait) {\n               Span span \u003d Tracer.getCurrentSpan();\n               if (span !\u003d null) {\n                 span.addTimelineAnnotation(\"dataQueue.wait\");\n               }\n               firstWait \u003d false;\n             }\n             try {\n               dataQueue.wait();\n             } catch (InterruptedException e) {\n               // If we get interrupted while waiting to queue data, we still need to get rid\n               // of the current packet. This is because we have an invariant that if\n               // currentPacket gets full, it will get queued before the next writeChunk.\n               //\n               // Rather than wait around for space in the queue, we should instead try to\n               // return to the caller as soon as possible, even though we slightly overrun\n               // the MAX_PACKETS length.\n               Thread.currentThread().interrupt();\n               break;\n             }\n           }\n         } finally {\n           Span span \u003d Tracer.getCurrentSpan();\n           if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n             span.addTimelineAnnotation(\"end.wait\");\n           }\n         }\n         checkClosed();\n         queuePacket(packet);\n-      } catch (ClosedChannelException ignored) {\n+      } catch (ClosedChannelException cce) {\n+        LOG.debug(\"Closed channel exception\", cce);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n    synchronized (dataQueue) {\n      try {\n        // If queue is full, then wait till we have enough space\n        boolean firstWait \u003d true;\n        try {\n          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n              dfsClient.getConf().getWriteMaxPackets()) {\n            if (firstWait) {\n              Span span \u003d Tracer.getCurrentSpan();\n              if (span !\u003d null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait \u003d false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();\n              break;\n            }\n          }\n        } finally {\n          Span span \u003d Tracer.getCurrentSpan();\n          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n        checkClosed();\n        queuePacket(packet);\n      } catch (ClosedChannelException cce) {\n        LOG.debug(\"Closed channel exception\", cce);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   void waitAndQueuePacket(DFSPacket packet) throws IOException {\n     synchronized (dataQueue) {\n       try {\n         // If queue is full, then wait till we have enough space\n         boolean firstWait \u003d true;\n         try {\n           while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n               dfsClient.getConf().getWriteMaxPackets()) {\n             if (firstWait) {\n               Span span \u003d Tracer.getCurrentSpan();\n               if (span !\u003d null) {\n                 span.addTimelineAnnotation(\"dataQueue.wait\");\n               }\n               firstWait \u003d false;\n             }\n             try {\n               dataQueue.wait();\n             } catch (InterruptedException e) {\n               // If we get interrupted while waiting to queue data, we still need to get rid\n               // of the current packet. This is because we have an invariant that if\n               // currentPacket gets full, it will get queued before the next writeChunk.\n               //\n               // Rather than wait around for space in the queue, we should instead try to\n               // return to the caller as soon as possible, even though we slightly overrun\n               // the MAX_PACKETS length.\n               Thread.currentThread().interrupt();\n               break;\n             }\n           }\n         } finally {\n           Span span \u003d Tracer.getCurrentSpan();\n           if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n             span.addTimelineAnnotation(\"end.wait\");\n           }\n         }\n         checkClosed();\n         queuePacket(packet);\n-      } catch (ClosedChannelException e) {\n+      } catch (ClosedChannelException ignored) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n    synchronized (dataQueue) {\n      try {\n        // If queue is full, then wait till we have enough space\n        boolean firstWait \u003d true;\n        try {\n          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n              dfsClient.getConf().getWriteMaxPackets()) {\n            if (firstWait) {\n              Span span \u003d Tracer.getCurrentSpan();\n              if (span !\u003d null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait \u003d false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();\n              break;\n            }\n          }\n        } finally {\n          Span span \u003d Tracer.getCurrentSpan();\n          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n        checkClosed();\n        queuePacket(packet);\n      } catch (ClosedChannelException ignored) {\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/09/15 11:08 AM",
      "commitNameOld": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.86,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   void waitAndQueuePacket(DFSPacket packet) throws IOException {\n     synchronized (dataQueue) {\n       try {\n         // If queue is full, then wait till we have enough space\n         boolean firstWait \u003d true;\n         try {\n           while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n               dfsClient.getConf().getWriteMaxPackets()) {\n             if (firstWait) {\n-              Span span \u003d Trace.currentSpan();\n+              Span span \u003d Tracer.getCurrentSpan();\n               if (span !\u003d null) {\n                 span.addTimelineAnnotation(\"dataQueue.wait\");\n               }\n               firstWait \u003d false;\n             }\n             try {\n               dataQueue.wait();\n             } catch (InterruptedException e) {\n               // If we get interrupted while waiting to queue data, we still need to get rid\n               // of the current packet. This is because we have an invariant that if\n               // currentPacket gets full, it will get queued before the next writeChunk.\n               //\n               // Rather than wait around for space in the queue, we should instead try to\n               // return to the caller as soon as possible, even though we slightly overrun\n               // the MAX_PACKETS length.\n               Thread.currentThread().interrupt();\n               break;\n             }\n           }\n         } finally {\n-          Span span \u003d Trace.currentSpan();\n+          Span span \u003d Tracer.getCurrentSpan();\n           if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n             span.addTimelineAnnotation(\"end.wait\");\n           }\n         }\n         checkClosed();\n         queuePacket(packet);\n       } catch (ClosedChannelException e) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n    synchronized (dataQueue) {\n      try {\n        // If queue is full, then wait till we have enough space\n        boolean firstWait \u003d true;\n        try {\n          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n              dfsClient.getConf().getWriteMaxPackets()) {\n            if (firstWait) {\n              Span span \u003d Tracer.getCurrentSpan();\n              if (span !\u003d null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait \u003d false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();\n              break;\n            }\n          }\n        } finally {\n          Span span \u003d Tracer.getCurrentSpan();\n          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n        checkClosed();\n        queuePacket(packet);\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n    synchronized (dataQueue) {\n      try {\n        // If queue is full, then wait till we have enough space\n        boolean firstWait \u003d true;\n        try {\n          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n              dfsClient.getConf().getWriteMaxPackets()) {\n            if (firstWait) {\n              Span span \u003d Trace.currentSpan();\n              if (span !\u003d null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait \u003d false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();\n              break;\n            }\n          }\n        } finally {\n          Span span \u003d Trace.currentSpan();\n          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n        checkClosed();\n        queuePacket(packet);\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
      }
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "07/04/15 1:59 PM",
      "commitNameOld": "571a1ce9d037d99e7c9042bcb77ae7a2c4daf6d3",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 3.03,
      "commitsBetweenForRepo": 48,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   void waitAndQueuePacket(DFSPacket packet) throws IOException {\n     synchronized (dataQueue) {\n       try {\n         // If queue is full, then wait till we have enough space\n         boolean firstWait \u003d true;\n         try {\n           while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n-              dfsClient.getConf().writeMaxPackets) {\n+              dfsClient.getConf().getWriteMaxPackets()) {\n             if (firstWait) {\n               Span span \u003d Trace.currentSpan();\n               if (span !\u003d null) {\n                 span.addTimelineAnnotation(\"dataQueue.wait\");\n               }\n               firstWait \u003d false;\n             }\n             try {\n               dataQueue.wait();\n             } catch (InterruptedException e) {\n               // If we get interrupted while waiting to queue data, we still need to get rid\n               // of the current packet. This is because we have an invariant that if\n               // currentPacket gets full, it will get queued before the next writeChunk.\n               //\n               // Rather than wait around for space in the queue, we should instead try to\n               // return to the caller as soon as possible, even though we slightly overrun\n               // the MAX_PACKETS length.\n               Thread.currentThread().interrupt();\n               break;\n             }\n           }\n         } finally {\n           Span span \u003d Trace.currentSpan();\n           if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n             span.addTimelineAnnotation(\"end.wait\");\n           }\n         }\n         checkClosed();\n         queuePacket(packet);\n       } catch (ClosedChannelException e) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n    synchronized (dataQueue) {\n      try {\n        // If queue is full, then wait till we have enough space\n        boolean firstWait \u003d true;\n        try {\n          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n              dfsClient.getConf().getWriteMaxPackets()) {\n            if (firstWait) {\n              Span span \u003d Trace.currentSpan();\n              if (span !\u003d null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait \u003d false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();\n              break;\n            }\n          }\n        } finally {\n          Span span \u003d Trace.currentSpan();\n          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n        checkClosed();\n        queuePacket(packet);\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "24/03/15 11:06 AM",
      "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,41 @@\n-  private void waitAndQueueCurrentPacket() throws IOException {\n+  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n     synchronized (dataQueue) {\n       try {\n-      // If queue is full, then wait till we have enough space\n+        // If queue is full, then wait till we have enough space\n         boolean firstWait \u003d true;\n         try {\n-          while (!isClosed() \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n+          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n               dfsClient.getConf().writeMaxPackets) {\n             if (firstWait) {\n               Span span \u003d Trace.currentSpan();\n               if (span !\u003d null) {\n                 span.addTimelineAnnotation(\"dataQueue.wait\");\n               }\n               firstWait \u003d false;\n             }\n             try {\n               dataQueue.wait();\n             } catch (InterruptedException e) {\n               // If we get interrupted while waiting to queue data, we still need to get rid\n               // of the current packet. This is because we have an invariant that if\n               // currentPacket gets full, it will get queued before the next writeChunk.\n               //\n               // Rather than wait around for space in the queue, we should instead try to\n               // return to the caller as soon as possible, even though we slightly overrun\n               // the MAX_PACKETS length.\n               Thread.currentThread().interrupt();\n               break;\n             }\n           }\n         } finally {\n           Span span \u003d Trace.currentSpan();\n           if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n             span.addTimelineAnnotation(\"end.wait\");\n           }\n         }\n         checkClosed();\n-        queueCurrentPacket();\n+        queuePacket(packet);\n       } catch (ClosedChannelException e) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n    synchronized (dataQueue) {\n      try {\n        // If queue is full, then wait till we have enough space\n        boolean firstWait \u003d true;\n        try {\n          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n              dfsClient.getConf().writeMaxPackets) {\n            if (firstWait) {\n              Span span \u003d Trace.currentSpan();\n              if (span !\u003d null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait \u003d false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();\n              break;\n            }\n          }\n        } finally {\n          Span span \u003d Trace.currentSpan();\n          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n        checkClosed();\n        queuePacket(packet);\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
            "oldMethodName": "waitAndQueueCurrentPacket",
            "newMethodName": "waitAndQueuePacket"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,41 @@\n-  private void waitAndQueueCurrentPacket() throws IOException {\n+  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n     synchronized (dataQueue) {\n       try {\n-      // If queue is full, then wait till we have enough space\n+        // If queue is full, then wait till we have enough space\n         boolean firstWait \u003d true;\n         try {\n-          while (!isClosed() \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n+          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n               dfsClient.getConf().writeMaxPackets) {\n             if (firstWait) {\n               Span span \u003d Trace.currentSpan();\n               if (span !\u003d null) {\n                 span.addTimelineAnnotation(\"dataQueue.wait\");\n               }\n               firstWait \u003d false;\n             }\n             try {\n               dataQueue.wait();\n             } catch (InterruptedException e) {\n               // If we get interrupted while waiting to queue data, we still need to get rid\n               // of the current packet. This is because we have an invariant that if\n               // currentPacket gets full, it will get queued before the next writeChunk.\n               //\n               // Rather than wait around for space in the queue, we should instead try to\n               // return to the caller as soon as possible, even though we slightly overrun\n               // the MAX_PACKETS length.\n               Thread.currentThread().interrupt();\n               break;\n             }\n           }\n         } finally {\n           Span span \u003d Trace.currentSpan();\n           if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n             span.addTimelineAnnotation(\"end.wait\");\n           }\n         }\n         checkClosed();\n-        queueCurrentPacket();\n+        queuePacket(packet);\n       } catch (ClosedChannelException e) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n    synchronized (dataQueue) {\n      try {\n        // If queue is full, then wait till we have enough space\n        boolean firstWait \u003d true;\n        try {\n          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n              dfsClient.getConf().writeMaxPackets) {\n            if (firstWait) {\n              Span span \u003d Trace.currentSpan();\n              if (span !\u003d null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait \u003d false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();\n              break;\n            }\n          }\n        } finally {\n          Span span \u003d Trace.currentSpan();\n          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n        checkClosed();\n        queuePacket(packet);\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,41 @@\n-  private void waitAndQueueCurrentPacket() throws IOException {\n+  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n     synchronized (dataQueue) {\n       try {\n-      // If queue is full, then wait till we have enough space\n+        // If queue is full, then wait till we have enough space\n         boolean firstWait \u003d true;\n         try {\n-          while (!isClosed() \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n+          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n               dfsClient.getConf().writeMaxPackets) {\n             if (firstWait) {\n               Span span \u003d Trace.currentSpan();\n               if (span !\u003d null) {\n                 span.addTimelineAnnotation(\"dataQueue.wait\");\n               }\n               firstWait \u003d false;\n             }\n             try {\n               dataQueue.wait();\n             } catch (InterruptedException e) {\n               // If we get interrupted while waiting to queue data, we still need to get rid\n               // of the current packet. This is because we have an invariant that if\n               // currentPacket gets full, it will get queued before the next writeChunk.\n               //\n               // Rather than wait around for space in the queue, we should instead try to\n               // return to the caller as soon as possible, even though we slightly overrun\n               // the MAX_PACKETS length.\n               Thread.currentThread().interrupt();\n               break;\n             }\n           }\n         } finally {\n           Span span \u003d Trace.currentSpan();\n           if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n             span.addTimelineAnnotation(\"end.wait\");\n           }\n         }\n         checkClosed();\n-        queueCurrentPacket();\n+        queuePacket(packet);\n       } catch (ClosedChannelException e) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n    synchronized (dataQueue) {\n      try {\n        // If queue is full, then wait till we have enough space\n        boolean firstWait \u003d true;\n        try {\n          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n              dfsClient.getConf().writeMaxPackets) {\n            if (firstWait) {\n              Span span \u003d Trace.currentSpan();\n              if (span !\u003d null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait \u003d false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();\n              break;\n            }\n          }\n        } finally {\n          Span span \u003d Trace.currentSpan();\n          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n        checkClosed();\n        queuePacket(packet);\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,41 @@\n-  private void waitAndQueueCurrentPacket() throws IOException {\n+  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n     synchronized (dataQueue) {\n       try {\n-      // If queue is full, then wait till we have enough space\n+        // If queue is full, then wait till we have enough space\n         boolean firstWait \u003d true;\n         try {\n-          while (!isClosed() \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n+          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n               dfsClient.getConf().writeMaxPackets) {\n             if (firstWait) {\n               Span span \u003d Trace.currentSpan();\n               if (span !\u003d null) {\n                 span.addTimelineAnnotation(\"dataQueue.wait\");\n               }\n               firstWait \u003d false;\n             }\n             try {\n               dataQueue.wait();\n             } catch (InterruptedException e) {\n               // If we get interrupted while waiting to queue data, we still need to get rid\n               // of the current packet. This is because we have an invariant that if\n               // currentPacket gets full, it will get queued before the next writeChunk.\n               //\n               // Rather than wait around for space in the queue, we should instead try to\n               // return to the caller as soon as possible, even though we slightly overrun\n               // the MAX_PACKETS length.\n               Thread.currentThread().interrupt();\n               break;\n             }\n           }\n         } finally {\n           Span span \u003d Trace.currentSpan();\n           if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n             span.addTimelineAnnotation(\"end.wait\");\n           }\n         }\n         checkClosed();\n-        queueCurrentPacket();\n+        queuePacket(packet);\n       } catch (ClosedChannelException e) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n    synchronized (dataQueue) {\n      try {\n        // If queue is full, then wait till we have enough space\n        boolean firstWait \u003d true;\n        try {\n          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n              dfsClient.getConf().writeMaxPackets) {\n            if (firstWait) {\n              Span span \u003d Trace.currentSpan();\n              if (span !\u003d null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait \u003d false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();\n              break;\n            }\n          }\n        } finally {\n          Span span \u003d Trace.currentSpan();\n          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n        checkClosed();\n        queuePacket(packet);\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldValue": "waitAndQueueCurrentPacket",
            "newValue": "waitAndQueuePacket"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,41 @@\n-  private void waitAndQueueCurrentPacket() throws IOException {\n+  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n     synchronized (dataQueue) {\n       try {\n-      // If queue is full, then wait till we have enough space\n+        // If queue is full, then wait till we have enough space\n         boolean firstWait \u003d true;\n         try {\n-          while (!isClosed() \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n+          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n               dfsClient.getConf().writeMaxPackets) {\n             if (firstWait) {\n               Span span \u003d Trace.currentSpan();\n               if (span !\u003d null) {\n                 span.addTimelineAnnotation(\"dataQueue.wait\");\n               }\n               firstWait \u003d false;\n             }\n             try {\n               dataQueue.wait();\n             } catch (InterruptedException e) {\n               // If we get interrupted while waiting to queue data, we still need to get rid\n               // of the current packet. This is because we have an invariant that if\n               // currentPacket gets full, it will get queued before the next writeChunk.\n               //\n               // Rather than wait around for space in the queue, we should instead try to\n               // return to the caller as soon as possible, even though we slightly overrun\n               // the MAX_PACKETS length.\n               Thread.currentThread().interrupt();\n               break;\n             }\n           }\n         } finally {\n           Span span \u003d Trace.currentSpan();\n           if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n             span.addTimelineAnnotation(\"end.wait\");\n           }\n         }\n         checkClosed();\n-        queueCurrentPacket();\n+        queuePacket(packet);\n       } catch (ClosedChannelException e) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void waitAndQueuePacket(DFSPacket packet) throws IOException {\n    synchronized (dataQueue) {\n      try {\n        // If queue is full, then wait till we have enough space\n        boolean firstWait \u003d true;\n        try {\n          while (!streamerClosed \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n              dfsClient.getConf().writeMaxPackets) {\n            if (firstWait) {\n              Span span \u003d Trace.currentSpan();\n              if (span !\u003d null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait \u003d false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();\n              break;\n            }\n          }\n        } finally {\n          Span span \u003d Trace.currentSpan();\n          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n        checkClosed();\n        queuePacket(packet);\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[packet-DFSPacket]"
          }
        }
      ]
    },
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7054. Make DFSOutputStream tracing more fine-grained (cmccabe)\n",
      "commitDate": "18/03/15 6:14 PM",
      "commitName": "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "16/03/15 9:58 PM",
      "commitNameOld": "046521cd6511b7fc6d9478cb2bed90d8e75fca20",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,41 @@\n   private void waitAndQueueCurrentPacket() throws IOException {\n     synchronized (dataQueue) {\n       try {\n       // If queue is full, then wait till we have enough space\n-      while (!isClosed() \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e dfsClient.getConf().writeMaxPackets) {\n+        boolean firstWait \u003d true;\n         try {\n-          dataQueue.wait();\n-        } catch (InterruptedException e) {\n-          // If we get interrupted while waiting to queue data, we still need to get rid\n-          // of the current packet. This is because we have an invariant that if\n-          // currentPacket gets full, it will get queued before the next writeChunk.\n-          //\n-          // Rather than wait around for space in the queue, we should instead try to\n-          // return to the caller as soon as possible, even though we slightly overrun\n-          // the MAX_PACKETS length.\n-          Thread.currentThread().interrupt();\n-          break;\n+          while (!isClosed() \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n+              dfsClient.getConf().writeMaxPackets) {\n+            if (firstWait) {\n+              Span span \u003d Trace.currentSpan();\n+              if (span !\u003d null) {\n+                span.addTimelineAnnotation(\"dataQueue.wait\");\n+              }\n+              firstWait \u003d false;\n+            }\n+            try {\n+              dataQueue.wait();\n+            } catch (InterruptedException e) {\n+              // If we get interrupted while waiting to queue data, we still need to get rid\n+              // of the current packet. This is because we have an invariant that if\n+              // currentPacket gets full, it will get queued before the next writeChunk.\n+              //\n+              // Rather than wait around for space in the queue, we should instead try to\n+              // return to the caller as soon as possible, even though we slightly overrun\n+              // the MAX_PACKETS length.\n+              Thread.currentThread().interrupt();\n+              break;\n+            }\n+          }\n+        } finally {\n+          Span span \u003d Trace.currentSpan();\n+          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n+            span.addTimelineAnnotation(\"end.wait\");\n+          }\n         }\n-      }\n-      checkClosed();\n-      queueCurrentPacket();\n+        checkClosed();\n+        queueCurrentPacket();\n       } catch (ClosedChannelException e) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void waitAndQueueCurrentPacket() throws IOException {\n    synchronized (dataQueue) {\n      try {\n      // If queue is full, then wait till we have enough space\n        boolean firstWait \u003d true;\n        try {\n          while (!isClosed() \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e\n              dfsClient.getConf().writeMaxPackets) {\n            if (firstWait) {\n              Span span \u003d Trace.currentSpan();\n              if (span !\u003d null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait \u003d false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();\n              break;\n            }\n          }\n        } finally {\n          Span span \u003d Trace.currentSpan();\n          if ((span !\u003d null) \u0026\u0026 (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n        checkClosed();\n        queueCurrentPacket();\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7358. Clients may get stuck waiting when using ByteArrayManager.\n",
      "commitDate": "13/11/14 12:28 PM",
      "commitName": "394ba94c5d2801fbc5d95c7872eeeede28eed1eb",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "05/11/14 10:51 AM",
      "commitNameOld": "56257fab1d5a7f66bebd9149c7df0436c0a57adb",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 8.07,
      "commitsBetweenForRepo": 93,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   private void waitAndQueueCurrentPacket() throws IOException {\n     synchronized (dataQueue) {\n       try {\n       // If queue is full, then wait till we have enough space\n-      while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e dfsClient.getConf().writeMaxPackets) {\n+      while (!isClosed() \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e dfsClient.getConf().writeMaxPackets) {\n         try {\n           dataQueue.wait();\n         } catch (InterruptedException e) {\n           // If we get interrupted while waiting to queue data, we still need to get rid\n           // of the current packet. This is because we have an invariant that if\n           // currentPacket gets full, it will get queued before the next writeChunk.\n           //\n           // Rather than wait around for space in the queue, we should instead try to\n           // return to the caller as soon as possible, even though we slightly overrun\n           // the MAX_PACKETS length.\n           Thread.currentThread().interrupt();\n           break;\n         }\n       }\n       checkClosed();\n       queueCurrentPacket();\n       } catch (ClosedChannelException e) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void waitAndQueueCurrentPacket() throws IOException {\n    synchronized (dataQueue) {\n      try {\n      // If queue is full, then wait till we have enough space\n      while (!isClosed() \u0026\u0026 dataQueue.size() + ackQueue.size() \u003e dfsClient.getConf().writeMaxPackets) {\n        try {\n          dataQueue.wait();\n        } catch (InterruptedException e) {\n          // If we get interrupted while waiting to queue data, we still need to get rid\n          // of the current packet. This is because we have an invariant that if\n          // currentPacket gets full, it will get queued before the next writeChunk.\n          //\n          // Rather than wait around for space in the queue, we should instead try to\n          // return to the caller as soon as possible, even though we slightly overrun\n          // the MAX_PACKETS length.\n          Thread.currentThread().interrupt();\n          break;\n        }\n      }\n      checkClosed();\n      queueCurrentPacket();\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "2e140523d3ccb27809cde4a55e95f7e0006c028f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7260. Change DFSOutputStream.MAX_PACKETS to be configurable.\n",
      "commitDate": "17/10/14 6:30 PM",
      "commitName": "2e140523d3ccb27809cde4a55e95f7e0006c028f",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "14/10/14 10:22 AM",
      "commitNameOld": "7dcad84143a9eef059688570cd0f9cf73747f2de",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 3.34,
      "commitsBetweenForRepo": 28,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   private void waitAndQueueCurrentPacket() throws IOException {\n     synchronized (dataQueue) {\n       try {\n       // If queue is full, then wait till we have enough space\n-      while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e MAX_PACKETS) {\n+      while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e dfsClient.getConf().writeMaxPackets) {\n         try {\n           dataQueue.wait();\n         } catch (InterruptedException e) {\n           // If we get interrupted while waiting to queue data, we still need to get rid\n           // of the current packet. This is because we have an invariant that if\n           // currentPacket gets full, it will get queued before the next writeChunk.\n           //\n           // Rather than wait around for space in the queue, we should instead try to\n           // return to the caller as soon as possible, even though we slightly overrun\n           // the MAX_PACKETS length.\n           Thread.currentThread().interrupt();\n           break;\n         }\n       }\n       checkClosed();\n       queueCurrentPacket();\n       } catch (ClosedChannelException e) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void waitAndQueueCurrentPacket() throws IOException {\n    synchronized (dataQueue) {\n      try {\n      // If queue is full, then wait till we have enough space\n      while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e dfsClient.getConf().writeMaxPackets) {\n        try {\n          dataQueue.wait();\n        } catch (InterruptedException e) {\n          // If we get interrupted while waiting to queue data, we still need to get rid\n          // of the current packet. This is because we have an invariant that if\n          // currentPacket gets full, it will get queued before the next writeChunk.\n          //\n          // Rather than wait around for space in the queue, we should instead try to\n          // return to the caller as soon as possible, even though we slightly overrun\n          // the MAX_PACKETS length.\n          Thread.currentThread().interrupt();\n          break;\n        }\n      }\n      checkClosed();\n      queueCurrentPacket();\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "631ccbdd2031a8387d4c2b743a4fc64c990391ce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5374. Remove deadcode in DFSOutputStream. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1533258 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/10/13 1:47 PM",
      "commitName": "631ccbdd2031a8387d4c2b743a4fc64c990391ce",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "10/10/13 4:58 PM",
      "commitNameOld": "f2f5cdb5554d294a29ebf465101c5607fd56e244",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 6.87,
      "commitsBetweenForRepo": 49,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   private void waitAndQueueCurrentPacket() throws IOException {\n     synchronized (dataQueue) {\n       try {\n       // If queue is full, then wait till we have enough space\n       while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e MAX_PACKETS) {\n         try {\n           dataQueue.wait();\n         } catch (InterruptedException e) {\n           // If we get interrupted while waiting to queue data, we still need to get rid\n           // of the current packet. This is because we have an invariant that if\n           // currentPacket gets full, it will get queued before the next writeChunk.\n           //\n           // Rather than wait around for space in the queue, we should instead try to\n           // return to the caller as soon as possible, even though we slightly overrun\n-          // the MAX_PACKETS iength.\n+          // the MAX_PACKETS length.\n           Thread.currentThread().interrupt();\n           break;\n         }\n       }\n       checkClosed();\n       queueCurrentPacket();\n       } catch (ClosedChannelException e) {\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void waitAndQueueCurrentPacket() throws IOException {\n    synchronized (dataQueue) {\n      try {\n      // If queue is full, then wait till we have enough space\n      while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e MAX_PACKETS) {\n        try {\n          dataQueue.wait();\n        } catch (InterruptedException e) {\n          // If we get interrupted while waiting to queue data, we still need to get rid\n          // of the current packet. This is because we have an invariant that if\n          // currentPacket gets full, it will get queued before the next writeChunk.\n          //\n          // Rather than wait around for space in the queue, we should instead try to\n          // return to the caller as soon as possible, even though we slightly overrun\n          // the MAX_PACKETS length.\n          Thread.currentThread().interrupt();\n          break;\n        }\n      }\n      checkClosed();\n      queueCurrentPacket();\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "f2f5cdb5554d294a29ebf465101c5607fd56e244": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5335. Hive query failed with possible race in dfs output stream. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1531152 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/10/13 4:58 PM",
      "commitName": "f2f5cdb5554d294a29ebf465101c5607fd56e244",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "22/07/13 11:15 AM",
      "commitNameOld": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 80.24,
      "commitsBetweenForRepo": 499,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,25 @@\n   private void waitAndQueueCurrentPacket() throws IOException {\n     synchronized (dataQueue) {\n+      try {\n       // If queue is full, then wait till we have enough space\n       while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e MAX_PACKETS) {\n         try {\n           dataQueue.wait();\n         } catch (InterruptedException e) {\n           // If we get interrupted while waiting to queue data, we still need to get rid\n           // of the current packet. This is because we have an invariant that if\n           // currentPacket gets full, it will get queued before the next writeChunk.\n           //\n           // Rather than wait around for space in the queue, we should instead try to\n           // return to the caller as soon as possible, even though we slightly overrun\n           // the MAX_PACKETS iength.\n           Thread.currentThread().interrupt();\n           break;\n         }\n       }\n       checkClosed();\n       queueCurrentPacket();\n+      } catch (ClosedChannelException e) {\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void waitAndQueueCurrentPacket() throws IOException {\n    synchronized (dataQueue) {\n      try {\n      // If queue is full, then wait till we have enough space\n      while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e MAX_PACKETS) {\n        try {\n          dataQueue.wait();\n        } catch (InterruptedException e) {\n          // If we get interrupted while waiting to queue data, we still need to get rid\n          // of the current packet. This is because we have an invariant that if\n          // currentPacket gets full, it will get queued before the next writeChunk.\n          //\n          // Rather than wait around for space in the queue, we should instead try to\n          // return to the caller as soon as possible, even though we slightly overrun\n          // the MAX_PACKETS iength.\n          Thread.currentThread().interrupt();\n          break;\n        }\n      }\n      checkClosed();\n      queueCurrentPacket();\n      } catch (ClosedChannelException e) {\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "1c309f763be3dd2e3d7d1616d2c960ff80cf9b03": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4906. HDFS Output streams should not accept writes after being closed. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1494303 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/06/13 2:05 PM",
      "commitName": "1c309f763be3dd2e3d7d1616d2c960ff80cf9b03",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "23/05/13 2:35 PM",
      "commitNameOld": "2ad7397c49844b5c12e122779c8760f51fa3a998",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 25.98,
      "commitsBetweenForRepo": 200,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   private void waitAndQueueCurrentPacket() throws IOException {\n     synchronized (dataQueue) {\n       // If queue is full, then wait till we have enough space\n       while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e MAX_PACKETS) {\n         try {\n           dataQueue.wait();\n         } catch (InterruptedException e) {\n           // If we get interrupted while waiting to queue data, we still need to get rid\n           // of the current packet. This is because we have an invariant that if\n           // currentPacket gets full, it will get queued before the next writeChunk.\n           //\n           // Rather than wait around for space in the queue, we should instead try to\n           // return to the caller as soon as possible, even though we slightly overrun\n           // the MAX_PACKETS iength.\n           Thread.currentThread().interrupt();\n           break;\n         }\n       }\n-      isClosed();\n+      checkClosed();\n       queueCurrentPacket();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void waitAndQueueCurrentPacket() throws IOException {\n    synchronized (dataQueue) {\n      // If queue is full, then wait till we have enough space\n      while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e MAX_PACKETS) {\n        try {\n          dataQueue.wait();\n        } catch (InterruptedException e) {\n          // If we get interrupted while waiting to queue data, we still need to get rid\n          // of the current packet. This is because we have an invariant that if\n          // currentPacket gets full, it will get queued before the next writeChunk.\n          //\n          // Rather than wait around for space in the queue, we should instead try to\n          // return to the caller as soon as possible, even though we slightly overrun\n          // the MAX_PACKETS iength.\n          Thread.currentThread().interrupt();\n          break;\n        }\n      }\n      checkClosed();\n      queueCurrentPacket();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void waitAndQueueCurrentPacket() throws IOException {\n    synchronized (dataQueue) {\n      // If queue is full, then wait till we have enough space\n      while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e MAX_PACKETS) {\n        try {\n          dataQueue.wait();\n        } catch (InterruptedException e) {\n          // If we get interrupted while waiting to queue data, we still need to get rid\n          // of the current packet. This is because we have an invariant that if\n          // currentPacket gets full, it will get queued before the next writeChunk.\n          //\n          // Rather than wait around for space in the queue, we should instead try to\n          // return to the caller as soon as possible, even though we slightly overrun\n          // the MAX_PACKETS iength.\n          Thread.currentThread().interrupt();\n          break;\n        }\n      }\n      isClosed();\n      queueCurrentPacket();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void waitAndQueueCurrentPacket() throws IOException {\n    synchronized (dataQueue) {\n      // If queue is full, then wait till we have enough space\n      while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e MAX_PACKETS) {\n        try {\n          dataQueue.wait();\n        } catch (InterruptedException e) {\n          // If we get interrupted while waiting to queue data, we still need to get rid\n          // of the current packet. This is because we have an invariant that if\n          // currentPacket gets full, it will get queued before the next writeChunk.\n          //\n          // Rather than wait around for space in the queue, we should instead try to\n          // return to the caller as soon as possible, even though we slightly overrun\n          // the MAX_PACKETS iength.\n          Thread.currentThread().interrupt();\n          break;\n        }\n      }\n      isClosed();\n      queueCurrentPacket();\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,22 @@\n+  private void waitAndQueueCurrentPacket() throws IOException {\n+    synchronized (dataQueue) {\n+      // If queue is full, then wait till we have enough space\n+      while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e MAX_PACKETS) {\n+        try {\n+          dataQueue.wait();\n+        } catch (InterruptedException e) {\n+          // If we get interrupted while waiting to queue data, we still need to get rid\n+          // of the current packet. This is because we have an invariant that if\n+          // currentPacket gets full, it will get queued before the next writeChunk.\n+          //\n+          // Rather than wait around for space in the queue, we should instead try to\n+          // return to the caller as soon as possible, even though we slightly overrun\n+          // the MAX_PACKETS iength.\n+          Thread.currentThread().interrupt();\n+          break;\n+        }\n+      }\n+      isClosed();\n+      queueCurrentPacket();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void waitAndQueueCurrentPacket() throws IOException {\n    synchronized (dataQueue) {\n      // If queue is full, then wait till we have enough space\n      while (!closed \u0026\u0026 dataQueue.size() + ackQueue.size()  \u003e MAX_PACKETS) {\n        try {\n          dataQueue.wait();\n        } catch (InterruptedException e) {\n          // If we get interrupted while waiting to queue data, we still need to get rid\n          // of the current packet. This is because we have an invariant that if\n          // currentPacket gets full, it will get queued before the next writeChunk.\n          //\n          // Rather than wait around for space in the queue, we should instead try to\n          // return to the caller as soon as possible, even though we slightly overrun\n          // the MAX_PACKETS iength.\n          Thread.currentThread().interrupt();\n          break;\n        }\n      }\n      isClosed();\n      queueCurrentPacket();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}