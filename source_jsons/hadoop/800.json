{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StripeReader.java",
  "functionName": "getReadStrategies",
  "functionId": "getReadStrategies___chunk-StripingChunk",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
  "functionStartLine": 214,
  "functionEndLine": 231,
  "numCommitsSeen": 19,
  "timeTaken": 1793,
  "changeHistory": [
    "734d54c1a8950446e68098f62d8964e02ecc2890",
    "401db4fc65140979fe7665983e36905e886df971",
    "793447f79924c97c2b562d5e41fa85adf19673fe"
  ],
  "changeHistoryShort": {
    "734d54c1a8950446e68098f62d8964e02ecc2890": "Ymultichange(Ymovefromfile,Ybodychange)",
    "401db4fc65140979fe7665983e36905e886df971": "Ybodychange",
    "793447f79924c97c2b562d5e41fa85adf19673fe": "Ybodychange"
  },
  "changeHistoryDetails": {
    "734d54c1a8950446e68098f62d8964e02ecc2890": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
      "commitDate": "21/09/16 6:34 AM",
      "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "20/09/16 12:03 AM",
          "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,18 @@\n-    private ByteBufferStrategy[] getReadStrategies(StripingChunk chunk) {\n-      if (chunk.useByteBuffer()) {\n-        ByteBufferStrategy strategy \u003d new ByteBufferStrategy(\n-            chunk.getByteBuffer(), readStatistics, dfsClient);\n-        return new ByteBufferStrategy[]{strategy};\n-      } else {\n-        ByteBufferStrategy[] strategies \u003d\n-            new ByteBufferStrategy[chunk.getChunkBuffer().getSlices().size()];\n-        for (int i \u003d 0; i \u003c strategies.length; i++) {\n-          ByteBuffer buffer \u003d chunk.getChunkBuffer().getSlice(i);\n-          strategies[i] \u003d\n-              new ByteBufferStrategy(buffer, readStatistics, dfsClient);\n-        }\n-        return strategies;\n-      }\n-    }\n\\ No newline at end of file\n+  private ByteBufferStrategy[] getReadStrategies(StripingChunk chunk) {\n+    if (chunk.useByteBuffer()) {\n+      ByteBufferStrategy strategy \u003d new ByteBufferStrategy(\n+          chunk.getByteBuffer(), dfsStripedInputStream.getReadStatistics(),\n+          dfsStripedInputStream.getDFSClient());\n+      return new ByteBufferStrategy[]{strategy};\n+    }\n+\n+    ByteBufferStrategy[] strategies \u003d\n+        new ByteBufferStrategy[chunk.getChunkBuffer().getSlices().size()];\n+    for (int i \u003d 0; i \u003c strategies.length; i++) {\n+      ByteBuffer buffer \u003d chunk.getChunkBuffer().getSlice(i);\n+      strategies[i] \u003d new ByteBufferStrategy(buffer,\n+              dfsStripedInputStream.getReadStatistics(),\n+              dfsStripedInputStream.getDFSClient());\n+    }\n+    return strategies;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private ByteBufferStrategy[] getReadStrategies(StripingChunk chunk) {\n    if (chunk.useByteBuffer()) {\n      ByteBufferStrategy strategy \u003d new ByteBufferStrategy(\n          chunk.getByteBuffer(), dfsStripedInputStream.getReadStatistics(),\n          dfsStripedInputStream.getDFSClient());\n      return new ByteBufferStrategy[]{strategy};\n    }\n\n    ByteBufferStrategy[] strategies \u003d\n        new ByteBufferStrategy[chunk.getChunkBuffer().getSlices().size()];\n    for (int i \u003d 0; i \u003c strategies.length; i++) {\n      ByteBuffer buffer \u003d chunk.getChunkBuffer().getSlice(i);\n      strategies[i] \u003d new ByteBufferStrategy(buffer,\n              dfsStripedInputStream.getReadStatistics(),\n              dfsStripedInputStream.getDFSClient());\n    }\n    return strategies;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
            "oldMethodName": "getReadStrategies",
            "newMethodName": "getReadStrategies"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "20/09/16 12:03 AM",
          "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,18 @@\n-    private ByteBufferStrategy[] getReadStrategies(StripingChunk chunk) {\n-      if (chunk.useByteBuffer()) {\n-        ByteBufferStrategy strategy \u003d new ByteBufferStrategy(\n-            chunk.getByteBuffer(), readStatistics, dfsClient);\n-        return new ByteBufferStrategy[]{strategy};\n-      } else {\n-        ByteBufferStrategy[] strategies \u003d\n-            new ByteBufferStrategy[chunk.getChunkBuffer().getSlices().size()];\n-        for (int i \u003d 0; i \u003c strategies.length; i++) {\n-          ByteBuffer buffer \u003d chunk.getChunkBuffer().getSlice(i);\n-          strategies[i] \u003d\n-              new ByteBufferStrategy(buffer, readStatistics, dfsClient);\n-        }\n-        return strategies;\n-      }\n-    }\n\\ No newline at end of file\n+  private ByteBufferStrategy[] getReadStrategies(StripingChunk chunk) {\n+    if (chunk.useByteBuffer()) {\n+      ByteBufferStrategy strategy \u003d new ByteBufferStrategy(\n+          chunk.getByteBuffer(), dfsStripedInputStream.getReadStatistics(),\n+          dfsStripedInputStream.getDFSClient());\n+      return new ByteBufferStrategy[]{strategy};\n+    }\n+\n+    ByteBufferStrategy[] strategies \u003d\n+        new ByteBufferStrategy[chunk.getChunkBuffer().getSlices().size()];\n+    for (int i \u003d 0; i \u003c strategies.length; i++) {\n+      ByteBuffer buffer \u003d chunk.getChunkBuffer().getSlice(i);\n+      strategies[i] \u003d new ByteBufferStrategy(buffer,\n+              dfsStripedInputStream.getReadStatistics(),\n+              dfsStripedInputStream.getDFSClient());\n+    }\n+    return strategies;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private ByteBufferStrategy[] getReadStrategies(StripingChunk chunk) {\n    if (chunk.useByteBuffer()) {\n      ByteBufferStrategy strategy \u003d new ByteBufferStrategy(\n          chunk.getByteBuffer(), dfsStripedInputStream.getReadStatistics(),\n          dfsStripedInputStream.getDFSClient());\n      return new ByteBufferStrategy[]{strategy};\n    }\n\n    ByteBufferStrategy[] strategies \u003d\n        new ByteBufferStrategy[chunk.getChunkBuffer().getSlices().size()];\n    for (int i \u003d 0; i \u003c strategies.length; i++) {\n      ByteBuffer buffer \u003d chunk.getChunkBuffer().getSlice(i);\n      strategies[i] \u003d new ByteBufferStrategy(buffer,\n              dfsStripedInputStream.getReadStatistics(),\n              dfsStripedInputStream.getDFSClient());\n    }\n    return strategies;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java",
          "extendedDetails": {}
        }
      ]
    },
    "401db4fc65140979fe7665983e36905e886df971": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
      "commitDate": "08/09/16 11:54 AM",
      "commitName": "401db4fc65140979fe7665983e36905e886df971",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "24/08/16 6:57 AM",
      "commitNameOld": "793447f79924c97c2b562d5e41fa85adf19673fe",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 15.21,
      "commitsBetweenForRepo": 83,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,16 @@\n     private ByteBufferStrategy[] getReadStrategies(StripingChunk chunk) {\n-      if (chunk.byteBuffer !\u003d null) {\n-        ByteBufferStrategy strategy \u003d\n-            new ByteBufferStrategy(chunk.byteBuffer, readStatistics, dfsClient);\n+      if (chunk.useByteBuffer()) {\n+        ByteBufferStrategy strategy \u003d new ByteBufferStrategy(\n+            chunk.getByteBuffer(), readStatistics, dfsClient);\n         return new ByteBufferStrategy[]{strategy};\n       } else {\n         ByteBufferStrategy[] strategies \u003d\n-            new ByteBufferStrategy[chunk.byteArray.getOffsets().length];\n+            new ByteBufferStrategy[chunk.getChunkBuffer().getSlices().size()];\n         for (int i \u003d 0; i \u003c strategies.length; i++) {\n-          ByteBuffer buffer \u003d ByteBuffer.wrap(chunk.byteArray.buf(),\n-              chunk.byteArray.getOffsets()[i], chunk.byteArray.getLengths()[i]);\n+          ByteBuffer buffer \u003d chunk.getChunkBuffer().getSlice(i);\n           strategies[i] \u003d\n               new ByteBufferStrategy(buffer, readStatistics, dfsClient);\n         }\n         return strategies;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private ByteBufferStrategy[] getReadStrategies(StripingChunk chunk) {\n      if (chunk.useByteBuffer()) {\n        ByteBufferStrategy strategy \u003d new ByteBufferStrategy(\n            chunk.getByteBuffer(), readStatistics, dfsClient);\n        return new ByteBufferStrategy[]{strategy};\n      } else {\n        ByteBufferStrategy[] strategies \u003d\n            new ByteBufferStrategy[chunk.getChunkBuffer().getSlices().size()];\n        for (int i \u003d 0; i \u003c strategies.length; i++) {\n          ByteBuffer buffer \u003d chunk.getChunkBuffer().getSlice(i);\n          strategies[i] \u003d\n              new ByteBufferStrategy(buffer, readStatistics, dfsClient);\n        }\n        return strategies;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
      "extendedDetails": {}
    },
    "793447f79924c97c2b562d5e41fa85adf19673fe": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8905. Refactor DFSInputStream#ReaderStrategy. Contributed by Kai Zheng and Sammi Chen\n",
      "commitDate": "24/08/16 6:57 AM",
      "commitName": "793447f79924c97c2b562d5e41fa85adf19673fe",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "12/08/16 10:52 PM",
      "commitNameOld": "b5af9be72c72734d668f817c99d889031922a951",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 11.34,
      "commitsBetweenForRepo": 76,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,17 @@\n     private ByteBufferStrategy[] getReadStrategies(StripingChunk chunk) {\n       if (chunk.byteBuffer !\u003d null) {\n-        ByteBufferStrategy strategy \u003d new ByteBufferStrategy(chunk.byteBuffer);\n+        ByteBufferStrategy strategy \u003d\n+            new ByteBufferStrategy(chunk.byteBuffer, readStatistics, dfsClient);\n         return new ByteBufferStrategy[]{strategy};\n       } else {\n         ByteBufferStrategy[] strategies \u003d\n             new ByteBufferStrategy[chunk.byteArray.getOffsets().length];\n         for (int i \u003d 0; i \u003c strategies.length; i++) {\n           ByteBuffer buffer \u003d ByteBuffer.wrap(chunk.byteArray.buf(),\n               chunk.byteArray.getOffsets()[i], chunk.byteArray.getLengths()[i]);\n-          strategies[i] \u003d new ByteBufferStrategy(buffer);\n+          strategies[i] \u003d\n+              new ByteBufferStrategy(buffer, readStatistics, dfsClient);\n         }\n         return strategies;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private ByteBufferStrategy[] getReadStrategies(StripingChunk chunk) {\n      if (chunk.byteBuffer !\u003d null) {\n        ByteBufferStrategy strategy \u003d\n            new ByteBufferStrategy(chunk.byteBuffer, readStatistics, dfsClient);\n        return new ByteBufferStrategy[]{strategy};\n      } else {\n        ByteBufferStrategy[] strategies \u003d\n            new ByteBufferStrategy[chunk.byteArray.getOffsets().length];\n        for (int i \u003d 0; i \u003c strategies.length; i++) {\n          ByteBuffer buffer \u003d ByteBuffer.wrap(chunk.byteArray.buf(),\n              chunk.byteArray.getOffsets()[i], chunk.byteArray.getLengths()[i]);\n          strategies[i] \u003d\n              new ByteBufferStrategy(buffer, readStatistics, dfsClient);\n        }\n        return strategies;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
      "extendedDetails": {}
    }
  }
}