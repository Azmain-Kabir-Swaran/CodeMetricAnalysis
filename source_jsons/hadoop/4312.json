{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "SaslDataTransferServer.java",
  "functionName": "getEncryptionKeyFromUserName",
  "functionId": "getEncryptionKeyFromUserName___userName-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java",
  "functionStartLine": 265,
  "functionEndLine": 277,
  "numCommitsSeen": 16,
  "timeTaken": 2935,
  "changeHistory": [
    "3b54223c0f32d42a84436c670d80b791a8e9696d",
    "9b4a7900c7dfc0590316eedaa97144f938885651"
  ],
  "changeHistoryShort": {
    "3b54223c0f32d42a84436c670d80b791a8e9696d": "Ymultichange(Ymovefromfile,Ymodifierchange,Yparameterchange)",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3b54223c0f32d42a84436c670d80b791a8e9696d": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Yparameterchange)",
      "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/07/14 11:10 AM",
      "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthor": "Chris Nauroth",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/07/14 11:10 AM",
          "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthor": "Chris Nauroth",
          "commitDateOld": "14/07/14 10:51 AM",
          "commitNameOld": "425616861bd7e801fdcf0a113606ad81015b1861",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,13 @@\n-  private static byte[] getEncryptionKeyFromUserName(\n-      BlockPoolTokenSecretManager blockPoolTokenSecretManager, String userName)\n+  private byte[] getEncryptionKeyFromUserName(String userName)\n       throws IOException {\n     String[] nameComponents \u003d userName.split(NAME_DELIMITER);\n     if (nameComponents.length !\u003d 3) {\n       throw new IOException(\"Provided name \u0027\" + userName + \"\u0027 has \" +\n           nameComponents.length + \" components instead of the expected 3.\");\n     }\n     int keyId \u003d Integer.parseInt(nameComponents[0]);\n     String blockPoolId \u003d nameComponents[1];\n     byte[] nonce \u003d Base64.decodeBase64(nameComponents[2]);\n     return blockPoolTokenSecretManager.retrieveDataEncryptionKey(keyId,\n         blockPoolId, nonce);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private byte[] getEncryptionKeyFromUserName(String userName)\n      throws IOException {\n    String[] nameComponents \u003d userName.split(NAME_DELIMITER);\n    if (nameComponents.length !\u003d 3) {\n      throw new IOException(\"Provided name \u0027\" + userName + \"\u0027 has \" +\n          nameComponents.length + \" components instead of the expected 3.\");\n    }\n    int keyId \u003d Integer.parseInt(nameComponents[0]);\n    String blockPoolId \u003d nameComponents[1];\n    byte[] nonce \u003d Base64.decodeBase64(nameComponents[2]);\n    return blockPoolTokenSecretManager.retrieveDataEncryptionKey(keyId,\n        blockPoolId, nonce);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferEncryptor.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java",
            "oldMethodName": "getEncryptionKeyFromUserName",
            "newMethodName": "getEncryptionKeyFromUserName"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/07/14 11:10 AM",
          "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthor": "Chris Nauroth",
          "commitDateOld": "14/07/14 10:51 AM",
          "commitNameOld": "425616861bd7e801fdcf0a113606ad81015b1861",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,13 @@\n-  private static byte[] getEncryptionKeyFromUserName(\n-      BlockPoolTokenSecretManager blockPoolTokenSecretManager, String userName)\n+  private byte[] getEncryptionKeyFromUserName(String userName)\n       throws IOException {\n     String[] nameComponents \u003d userName.split(NAME_DELIMITER);\n     if (nameComponents.length !\u003d 3) {\n       throw new IOException(\"Provided name \u0027\" + userName + \"\u0027 has \" +\n           nameComponents.length + \" components instead of the expected 3.\");\n     }\n     int keyId \u003d Integer.parseInt(nameComponents[0]);\n     String blockPoolId \u003d nameComponents[1];\n     byte[] nonce \u003d Base64.decodeBase64(nameComponents[2]);\n     return blockPoolTokenSecretManager.retrieveDataEncryptionKey(keyId,\n         blockPoolId, nonce);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private byte[] getEncryptionKeyFromUserName(String userName)\n      throws IOException {\n    String[] nameComponents \u003d userName.split(NAME_DELIMITER);\n    if (nameComponents.length !\u003d 3) {\n      throw new IOException(\"Provided name \u0027\" + userName + \"\u0027 has \" +\n          nameComponents.length + \" components instead of the expected 3.\");\n    }\n    int keyId \u003d Integer.parseInt(nameComponents[0]);\n    String blockPoolId \u003d nameComponents[1];\n    byte[] nonce \u003d Base64.decodeBase64(nameComponents[2]);\n    return blockPoolTokenSecretManager.retrieveDataEncryptionKey(keyId,\n        blockPoolId, nonce);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java",
          "extendedDetails": {
            "oldValue": "[private, static]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/07/14 11:10 AM",
          "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthor": "Chris Nauroth",
          "commitDateOld": "14/07/14 10:51 AM",
          "commitNameOld": "425616861bd7e801fdcf0a113606ad81015b1861",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,13 @@\n-  private static byte[] getEncryptionKeyFromUserName(\n-      BlockPoolTokenSecretManager blockPoolTokenSecretManager, String userName)\n+  private byte[] getEncryptionKeyFromUserName(String userName)\n       throws IOException {\n     String[] nameComponents \u003d userName.split(NAME_DELIMITER);\n     if (nameComponents.length !\u003d 3) {\n       throw new IOException(\"Provided name \u0027\" + userName + \"\u0027 has \" +\n           nameComponents.length + \" components instead of the expected 3.\");\n     }\n     int keyId \u003d Integer.parseInt(nameComponents[0]);\n     String blockPoolId \u003d nameComponents[1];\n     byte[] nonce \u003d Base64.decodeBase64(nameComponents[2]);\n     return blockPoolTokenSecretManager.retrieveDataEncryptionKey(keyId,\n         blockPoolId, nonce);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private byte[] getEncryptionKeyFromUserName(String userName)\n      throws IOException {\n    String[] nameComponents \u003d userName.split(NAME_DELIMITER);\n    if (nameComponents.length !\u003d 3) {\n      throw new IOException(\"Provided name \u0027\" + userName + \"\u0027 has \" +\n          nameComponents.length + \" components instead of the expected 3.\");\n    }\n    int keyId \u003d Integer.parseInt(nameComponents[0]);\n    String blockPoolId \u003d nameComponents[1];\n    byte[] nonce \u003d Base64.decodeBase64(nameComponents[2]);\n    return blockPoolTokenSecretManager.retrieveDataEncryptionKey(keyId,\n        blockPoolId, nonce);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java",
          "extendedDetails": {
            "oldValue": "[blockPoolTokenSecretManager-BlockPoolTokenSecretManager, userName-String]",
            "newValue": "[userName-String]"
          }
        }
      ]
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "diff": "@@ -0,0 +1,14 @@\n+  private static byte[] getEncryptionKeyFromUserName(\n+      BlockPoolTokenSecretManager blockPoolTokenSecretManager, String userName)\n+      throws IOException {\n+    String[] nameComponents \u003d userName.split(NAME_DELIMITER);\n+    if (nameComponents.length !\u003d 3) {\n+      throw new IOException(\"Provided name \u0027\" + userName + \"\u0027 has \" +\n+          nameComponents.length + \" components instead of the expected 3.\");\n+    }\n+    int keyId \u003d Integer.parseInt(nameComponents[0]);\n+    String blockPoolId \u003d nameComponents[1];\n+    byte[] nonce \u003d Base64.decodeBase64(nameComponents[2]);\n+    return blockPoolTokenSecretManager.retrieveDataEncryptionKey(keyId,\n+        blockPoolId, nonce);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static byte[] getEncryptionKeyFromUserName(\n      BlockPoolTokenSecretManager blockPoolTokenSecretManager, String userName)\n      throws IOException {\n    String[] nameComponents \u003d userName.split(NAME_DELIMITER);\n    if (nameComponents.length !\u003d 3) {\n      throw new IOException(\"Provided name \u0027\" + userName + \"\u0027 has \" +\n          nameComponents.length + \" components instead of the expected 3.\");\n    }\n    int keyId \u003d Integer.parseInt(nameComponents[0]);\n    String blockPoolId \u003d nameComponents[1];\n    byte[] nonce \u003d Base64.decodeBase64(nameComponents[2]);\n    return blockPoolTokenSecretManager.retrieveDataEncryptionKey(keyId,\n        blockPoolId, nonce);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferEncryptor.java"
    }
  }
}