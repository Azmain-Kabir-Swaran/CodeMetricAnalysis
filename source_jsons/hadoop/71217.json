{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "GenerateBlockImagesDriver.java",
  "functionName": "run",
  "functionId": "run___args-String[]",
  "sourceFilePath": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-blockgen/src/main/java/org/apache/hadoop/tools/dynamometer/blockgenerator/GenerateBlockImagesDriver.java",
  "functionStartLine": 65,
  "functionEndLine": 125,
  "numCommitsSeen": 1,
  "timeTaken": 346,
  "changeHistory": [
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91"
  ],
  "changeHistoryShort": {
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": "Yintroduced"
  },
  "changeHistoryDetails": {
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-12345 Add Dynamometer to hadoop-tools, a tool for scale testing the HDFS NameNode with real metadata and workloads. Contributed by Erik Krogen.\n",
      "commitDate": "25/06/19 8:07 AM",
      "commitName": "ab0b180ddb5d0775a2452d5eeb7badd252aadb91",
      "commitAuthor": "Erik Krogen",
      "diff": "@@ -0,0 +1,61 @@\n+  public int run(String[] args) throws Exception {\n+    Options options \u003d new Options();\n+    options.addOption(\"h\", \"help\", false, \"Shows this message\");\n+    options.addOption(OptionBuilder.withArgName(\"Input path of the XML fsImage\")\n+        .hasArg().isRequired(true)\n+        .withDescription(\"Input path to the Hadoop fsImage XML file (required)\")\n+        .create(FSIMAGE_INPUT_PATH_ARG));\n+    options.addOption(OptionBuilder.withArgName(\"BlockImage output directory\")\n+        .hasArg().isRequired(true)\n+        .withDescription(\"Directory where the generated files containing the \"\n+            + \"block listing for each DataNode should be stored (required)\")\n+        .create(BLOCK_IMAGE_OUTPUT_ARG));\n+    options.addOption(OptionBuilder.withArgName(\"Number of reducers\").hasArg()\n+        .isRequired(false)\n+        .withDescription(\n+            \"Number of reducers for this job (defaults to number of datanodes)\")\n+        .create(NUM_REDUCERS_ARG));\n+    options.addOption(OptionBuilder.withArgName(\"Number of datanodes\").hasArg()\n+        .isRequired(true)\n+        .withDescription(\"Number of DataNodes to create blocks for (required)\")\n+        .create(NUM_DATANODES_ARG));\n+\n+    CommandLineParser parser \u003d new PosixParser();\n+    CommandLine cli \u003d parser.parse(options, args);\n+    if (cli.hasOption(\"h\")) {\n+      HelpFormatter formatter \u003d new HelpFormatter();\n+      formatter.printHelp(200, \"hadoop jar dynamometer-*.jar \"\n+              + \"org.apache.hadoop.tools.dynamometer.blockgenerator.\"\n+              + \"GenerateBlockImagesDriver [options]\",\n+          null, options, null);\n+      return 0;\n+    }\n+\n+    String fsImageInputPath \u003d cli.getOptionValue(FSIMAGE_INPUT_PATH_ARG);\n+    String blockImageOutputDir \u003d cli.getOptionValue(BLOCK_IMAGE_OUTPUT_ARG);\n+    int numDataNodes \u003d Integer.parseInt(cli.getOptionValue(NUM_DATANODES_ARG));\n+    int numReducers \u003d Integer.parseInt(\n+        cli.getOptionValue(NUM_REDUCERS_ARG, String.valueOf(numDataNodes)));\n+\n+    FileSystem fs \u003d FileSystem.get(new URI(blockImageOutputDir), getConf());\n+    Job job \u003d Job.getInstance(getConf(), \"Create blocksImages for Dynamometer\");\n+    FileInputFormat.setInputPaths(job, new Path(fsImageInputPath));\n+    Path blockImagesDir \u003d new Path(blockImageOutputDir);\n+    fs.delete(blockImagesDir, true);\n+    FileOutputFormat.setOutputPath(job, blockImagesDir);\n+    job.getConfiguration().setInt(NUM_DATANODES_KEY, numDataNodes);\n+\n+    job.setJarByClass(GenerateBlockImagesDriver.class);\n+    job.setInputFormatClass(NoSplitTextInputFormat.class);\n+    job.setNumReduceTasks(numReducers);\n+    LazyOutputFormat.setOutputFormatClass(job, TextOutputFormat.class);\n+    job.setMapperClass(XMLParserMapper.class);\n+    job.setReducerClass(GenerateDNBlockInfosReducer.class);\n+    job.setMapOutputKeyClass(IntWritable.class);\n+    job.setMapOutputValueClass(BlockInfo.class);\n+    job.setOutputKeyClass(NullWritable.class);\n+    job.setOutputValueClass(Text.class);\n+\n+    boolean success \u003d job.waitForCompletion(true);\n+    return success ? 0 : 1;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public int run(String[] args) throws Exception {\n    Options options \u003d new Options();\n    options.addOption(\"h\", \"help\", false, \"Shows this message\");\n    options.addOption(OptionBuilder.withArgName(\"Input path of the XML fsImage\")\n        .hasArg().isRequired(true)\n        .withDescription(\"Input path to the Hadoop fsImage XML file (required)\")\n        .create(FSIMAGE_INPUT_PATH_ARG));\n    options.addOption(OptionBuilder.withArgName(\"BlockImage output directory\")\n        .hasArg().isRequired(true)\n        .withDescription(\"Directory where the generated files containing the \"\n            + \"block listing for each DataNode should be stored (required)\")\n        .create(BLOCK_IMAGE_OUTPUT_ARG));\n    options.addOption(OptionBuilder.withArgName(\"Number of reducers\").hasArg()\n        .isRequired(false)\n        .withDescription(\n            \"Number of reducers for this job (defaults to number of datanodes)\")\n        .create(NUM_REDUCERS_ARG));\n    options.addOption(OptionBuilder.withArgName(\"Number of datanodes\").hasArg()\n        .isRequired(true)\n        .withDescription(\"Number of DataNodes to create blocks for (required)\")\n        .create(NUM_DATANODES_ARG));\n\n    CommandLineParser parser \u003d new PosixParser();\n    CommandLine cli \u003d parser.parse(options, args);\n    if (cli.hasOption(\"h\")) {\n      HelpFormatter formatter \u003d new HelpFormatter();\n      formatter.printHelp(200, \"hadoop jar dynamometer-*.jar \"\n              + \"org.apache.hadoop.tools.dynamometer.blockgenerator.\"\n              + \"GenerateBlockImagesDriver [options]\",\n          null, options, null);\n      return 0;\n    }\n\n    String fsImageInputPath \u003d cli.getOptionValue(FSIMAGE_INPUT_PATH_ARG);\n    String blockImageOutputDir \u003d cli.getOptionValue(BLOCK_IMAGE_OUTPUT_ARG);\n    int numDataNodes \u003d Integer.parseInt(cli.getOptionValue(NUM_DATANODES_ARG));\n    int numReducers \u003d Integer.parseInt(\n        cli.getOptionValue(NUM_REDUCERS_ARG, String.valueOf(numDataNodes)));\n\n    FileSystem fs \u003d FileSystem.get(new URI(blockImageOutputDir), getConf());\n    Job job \u003d Job.getInstance(getConf(), \"Create blocksImages for Dynamometer\");\n    FileInputFormat.setInputPaths(job, new Path(fsImageInputPath));\n    Path blockImagesDir \u003d new Path(blockImageOutputDir);\n    fs.delete(blockImagesDir, true);\n    FileOutputFormat.setOutputPath(job, blockImagesDir);\n    job.getConfiguration().setInt(NUM_DATANODES_KEY, numDataNodes);\n\n    job.setJarByClass(GenerateBlockImagesDriver.class);\n    job.setInputFormatClass(NoSplitTextInputFormat.class);\n    job.setNumReduceTasks(numReducers);\n    LazyOutputFormat.setOutputFormatClass(job, TextOutputFormat.class);\n    job.setMapperClass(XMLParserMapper.class);\n    job.setReducerClass(GenerateDNBlockInfosReducer.class);\n    job.setMapOutputKeyClass(IntWritable.class);\n    job.setMapOutputValueClass(BlockInfo.class);\n    job.setOutputKeyClass(NullWritable.class);\n    job.setOutputValueClass(Text.class);\n\n    boolean success \u003d job.waitForCompletion(true);\n    return success ? 0 : 1;\n  }",
      "path": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-blockgen/src/main/java/org/apache/hadoop/tools/dynamometer/blockgenerator/GenerateBlockImagesDriver.java"
    }
  }
}