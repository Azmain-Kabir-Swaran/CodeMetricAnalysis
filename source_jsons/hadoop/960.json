{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSStripedOutputStream.java",
  "functionName": "updatePipeline",
  "functionId": "updatePipeline___newBG-ExtendedBlock",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
  "functionStartLine": 826,
  "functionEndLine": 855,
  "numCommitsSeen": 38,
  "timeTaken": 1861,
  "changeHistory": [
    "ccd2ac60ecc5fccce56debf21a068e663c1d5f11",
    "200b11368d3954138a9bce128c8fa763b4a503a1",
    "a9a3d219fed2dd9d7bb84c228f6b8d97eadbe1f6",
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d"
  ],
  "changeHistoryShort": {
    "ccd2ac60ecc5fccce56debf21a068e663c1d5f11": "Ybodychange",
    "200b11368d3954138a9bce128c8fa763b4a503a1": "Ybodychange",
    "a9a3d219fed2dd9d7bb84c228f6b8d97eadbe1f6": "Ybodychange",
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d": "Ybodychange"
  },
  "changeHistoryDetails": {
    "ccd2ac60ecc5fccce56debf21a068e663c1d5f11": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11882. Precisely calculate acked length of striped block groups in updatePipeline.\n",
      "commitDate": "05/09/17 2:16 PM",
      "commitName": "ccd2ac60ecc5fccce56debf21a068e663c1d5f11",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "05/09/17 2:46 AM",
      "commitNameOld": "5dba54596a1587e0ba5f9f02f40483e597b0df64",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 0.48,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,30 @@\n   private void updatePipeline(ExtendedBlock newBG) throws IOException {\n     final DatanodeInfo[] newNodes \u003d new DatanodeInfo[numAllBlocks];\n     final String[] newStorageIDs \u003d new String[numAllBlocks];\n     for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n       final StripedDataStreamer streamer \u003d getStripedDataStreamer(i);\n       final DatanodeInfo[] nodes \u003d streamer.getNodes();\n       final String[] storageIDs \u003d streamer.getStorageIDs();\n       if (streamer.isHealthy() \u0026\u0026 nodes !\u003d null \u0026\u0026 storageIDs !\u003d null) {\n         newNodes[i] \u003d nodes[0];\n         newStorageIDs[i] \u003d storageIDs[0];\n       } else {\n         newNodes[i] \u003d new DatanodeInfoBuilder()\n             .setNodeID(DatanodeID.EMPTY_DATANODE_ID).build();\n         newStorageIDs[i] \u003d \"\";\n       }\n     }\n \n-    // should update the block group length based on the acked length\n+    // Update the NameNode with the acked length of the block group\n+    // Save and restore the unacked length\n     final long sentBytes \u003d currentBlockGroup.getNumBytes();\n-    final long ackedBytes \u003d getNumAckedStripes() * cellSize * numDataBlocks;\n+    final long ackedBytes \u003d getAckedLength();\n     Preconditions.checkState(ackedBytes \u003c\u003d sentBytes,\n         \"Acked:\" + ackedBytes + \", Sent:\" + sentBytes);\n     currentBlockGroup.setNumBytes(ackedBytes);\n     newBG.setNumBytes(ackedBytes);\n     dfsClient.namenode.updatePipeline(dfsClient.clientName, currentBlockGroup,\n         newBG, newNodes, newStorageIDs);\n     currentBlockGroup \u003d newBG;\n     currentBlockGroup.setNumBytes(sentBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updatePipeline(ExtendedBlock newBG) throws IOException {\n    final DatanodeInfo[] newNodes \u003d new DatanodeInfo[numAllBlocks];\n    final String[] newStorageIDs \u003d new String[numAllBlocks];\n    for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n      final StripedDataStreamer streamer \u003d getStripedDataStreamer(i);\n      final DatanodeInfo[] nodes \u003d streamer.getNodes();\n      final String[] storageIDs \u003d streamer.getStorageIDs();\n      if (streamer.isHealthy() \u0026\u0026 nodes !\u003d null \u0026\u0026 storageIDs !\u003d null) {\n        newNodes[i] \u003d nodes[0];\n        newStorageIDs[i] \u003d storageIDs[0];\n      } else {\n        newNodes[i] \u003d new DatanodeInfoBuilder()\n            .setNodeID(DatanodeID.EMPTY_DATANODE_ID).build();\n        newStorageIDs[i] \u003d \"\";\n      }\n    }\n\n    // Update the NameNode with the acked length of the block group\n    // Save and restore the unacked length\n    final long sentBytes \u003d currentBlockGroup.getNumBytes();\n    final long ackedBytes \u003d getAckedLength();\n    Preconditions.checkState(ackedBytes \u003c\u003d sentBytes,\n        \"Acked:\" + ackedBytes + \", Sent:\" + sentBytes);\n    currentBlockGroup.setNumBytes(ackedBytes);\n    newBG.setNumBytes(ackedBytes);\n    dfsClient.namenode.updatePipeline(dfsClient.clientName, currentBlockGroup,\n        newBG, newNodes, newStorageIDs);\n    currentBlockGroup \u003d newBG;\n    currentBlockGroup.setNumBytes(sentBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "200b11368d3954138a9bce128c8fa763b4a503a1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12258. ec -listPolicies should list all policies in system, no matter it\u0027s enabled or disabled. Contributed by Wei Zhou.\n",
      "commitDate": "30/08/17 12:28 AM",
      "commitName": "200b11368d3954138a9bce128c8fa763b4a503a1",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "08/05/17 9:59 PM",
      "commitNameOld": "54fd0e44b76c4b982dcfb47932b6159851f14136",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 113.1,
      "commitsBetweenForRepo": 706,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,29 @@\n   private void updatePipeline(ExtendedBlock newBG) throws IOException {\n     final DatanodeInfo[] newNodes \u003d new DatanodeInfo[numAllBlocks];\n     final String[] newStorageIDs \u003d new String[numAllBlocks];\n     for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n       final StripedDataStreamer streamer \u003d getStripedDataStreamer(i);\n       final DatanodeInfo[] nodes \u003d streamer.getNodes();\n       final String[] storageIDs \u003d streamer.getStorageIDs();\n       if (streamer.isHealthy() \u0026\u0026 nodes !\u003d null \u0026\u0026 storageIDs !\u003d null) {\n         newNodes[i] \u003d nodes[0];\n         newStorageIDs[i] \u003d storageIDs[0];\n       } else {\n         newNodes[i] \u003d new DatanodeInfoBuilder()\n             .setNodeID(DatanodeID.EMPTY_DATANODE_ID).build();\n         newStorageIDs[i] \u003d \"\";\n       }\n     }\n \n     // should update the block group length based on the acked length\n     final long sentBytes \u003d currentBlockGroup.getNumBytes();\n     final long ackedBytes \u003d getNumAckedStripes() * cellSize * numDataBlocks;\n-    Preconditions.checkState(ackedBytes \u003c\u003d sentBytes);\n+    Preconditions.checkState(ackedBytes \u003c\u003d sentBytes,\n+        \"Acked:\" + ackedBytes + \", Sent:\" + sentBytes);\n     currentBlockGroup.setNumBytes(ackedBytes);\n     newBG.setNumBytes(ackedBytes);\n     dfsClient.namenode.updatePipeline(dfsClient.clientName, currentBlockGroup,\n         newBG, newNodes, newStorageIDs);\n     currentBlockGroup \u003d newBG;\n     currentBlockGroup.setNumBytes(sentBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updatePipeline(ExtendedBlock newBG) throws IOException {\n    final DatanodeInfo[] newNodes \u003d new DatanodeInfo[numAllBlocks];\n    final String[] newStorageIDs \u003d new String[numAllBlocks];\n    for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n      final StripedDataStreamer streamer \u003d getStripedDataStreamer(i);\n      final DatanodeInfo[] nodes \u003d streamer.getNodes();\n      final String[] storageIDs \u003d streamer.getStorageIDs();\n      if (streamer.isHealthy() \u0026\u0026 nodes !\u003d null \u0026\u0026 storageIDs !\u003d null) {\n        newNodes[i] \u003d nodes[0];\n        newStorageIDs[i] \u003d storageIDs[0];\n      } else {\n        newNodes[i] \u003d new DatanodeInfoBuilder()\n            .setNodeID(DatanodeID.EMPTY_DATANODE_ID).build();\n        newStorageIDs[i] \u003d \"\";\n      }\n    }\n\n    // should update the block group length based on the acked length\n    final long sentBytes \u003d currentBlockGroup.getNumBytes();\n    final long ackedBytes \u003d getNumAckedStripes() * cellSize * numDataBlocks;\n    Preconditions.checkState(ackedBytes \u003c\u003d sentBytes,\n        \"Acked:\" + ackedBytes + \", Sent:\" + sentBytes);\n    currentBlockGroup.setNumBytes(ackedBytes);\n    newBG.setNumBytes(ackedBytes);\n    dfsClient.namenode.updatePipeline(dfsClient.clientName, currentBlockGroup,\n        newBG, newNodes, newStorageIDs);\n    currentBlockGroup \u003d newBG;\n    currentBlockGroup.setNumBytes(sentBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "a9a3d219fed2dd9d7bb84c228f6b8d97eadbe1f6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9342. Erasure coding: client should update and commit block based on acknowledged size. Contributed by SammiChen.\n",
      "commitDate": "07/05/17 2:45 PM",
      "commitName": "a9a3d219fed2dd9d7bb84c228f6b8d97eadbe1f6",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "28/04/17 5:06 PM",
      "commitNameOld": "19a7e94ee47f81557f0db6fb76bdf6bc49944dd0",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 8.9,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,28 @@\n   private void updatePipeline(ExtendedBlock newBG) throws IOException {\n     final DatanodeInfo[] newNodes \u003d new DatanodeInfo[numAllBlocks];\n     final String[] newStorageIDs \u003d new String[numAllBlocks];\n     for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n       final StripedDataStreamer streamer \u003d getStripedDataStreamer(i);\n       final DatanodeInfo[] nodes \u003d streamer.getNodes();\n       final String[] storageIDs \u003d streamer.getStorageIDs();\n       if (streamer.isHealthy() \u0026\u0026 nodes !\u003d null \u0026\u0026 storageIDs !\u003d null) {\n         newNodes[i] \u003d nodes[0];\n         newStorageIDs[i] \u003d storageIDs[0];\n       } else {\n         newNodes[i] \u003d new DatanodeInfoBuilder()\n             .setNodeID(DatanodeID.EMPTY_DATANODE_ID).build();\n         newStorageIDs[i] \u003d \"\";\n       }\n     }\n+\n+    // should update the block group length based on the acked length\n+    final long sentBytes \u003d currentBlockGroup.getNumBytes();\n+    final long ackedBytes \u003d getNumAckedStripes() * cellSize * numDataBlocks;\n+    Preconditions.checkState(ackedBytes \u003c\u003d sentBytes);\n+    currentBlockGroup.setNumBytes(ackedBytes);\n+    newBG.setNumBytes(ackedBytes);\n     dfsClient.namenode.updatePipeline(dfsClient.clientName, currentBlockGroup,\n         newBG, newNodes, newStorageIDs);\n     currentBlockGroup \u003d newBG;\n+    currentBlockGroup.setNumBytes(sentBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updatePipeline(ExtendedBlock newBG) throws IOException {\n    final DatanodeInfo[] newNodes \u003d new DatanodeInfo[numAllBlocks];\n    final String[] newStorageIDs \u003d new String[numAllBlocks];\n    for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n      final StripedDataStreamer streamer \u003d getStripedDataStreamer(i);\n      final DatanodeInfo[] nodes \u003d streamer.getNodes();\n      final String[] storageIDs \u003d streamer.getStorageIDs();\n      if (streamer.isHealthy() \u0026\u0026 nodes !\u003d null \u0026\u0026 storageIDs !\u003d null) {\n        newNodes[i] \u003d nodes[0];\n        newStorageIDs[i] \u003d storageIDs[0];\n      } else {\n        newNodes[i] \u003d new DatanodeInfoBuilder()\n            .setNodeID(DatanodeID.EMPTY_DATANODE_ID).build();\n        newStorageIDs[i] \u003d \"\";\n      }\n    }\n\n    // should update the block group length based on the acked length\n    final long sentBytes \u003d currentBlockGroup.getNumBytes();\n    final long ackedBytes \u003d getNumAckedStripes() * cellSize * numDataBlocks;\n    Preconditions.checkState(ackedBytes \u003c\u003d sentBytes);\n    currentBlockGroup.setNumBytes(ackedBytes);\n    newBG.setNumBytes(ackedBytes);\n    dfsClient.namenode.updatePipeline(dfsClient.clientName, currentBlockGroup,\n        newBG, newNodes, newStorageIDs);\n    currentBlockGroup \u003d newBG;\n    currentBlockGroup.setNumBytes(sentBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9482. Replace DatanodeInfo constructors with a builder pattern. Contributed by Brahma Reddy Battula.\n",
      "commitDate": "08/11/16 6:17 PM",
      "commitName": "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
      "commitAuthor": "Brahma Reddy Battula",
      "commitDateOld": "17/08/16 3:52 PM",
      "commitNameOld": "2aa5e2c40364cf1e90e6af7851801f5eda759002",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 83.14,
      "commitsBetweenForRepo": 628,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,20 @@\n   private void updatePipeline(ExtendedBlock newBG) throws IOException {\n     final DatanodeInfo[] newNodes \u003d new DatanodeInfo[numAllBlocks];\n     final String[] newStorageIDs \u003d new String[numAllBlocks];\n     for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n       final StripedDataStreamer streamer \u003d getStripedDataStreamer(i);\n       final DatanodeInfo[] nodes \u003d streamer.getNodes();\n       final String[] storageIDs \u003d streamer.getStorageIDs();\n       if (streamer.isHealthy() \u0026\u0026 nodes !\u003d null \u0026\u0026 storageIDs !\u003d null) {\n         newNodes[i] \u003d nodes[0];\n         newStorageIDs[i] \u003d storageIDs[0];\n       } else {\n-        newNodes[i] \u003d new DatanodeInfo(DatanodeID.EMPTY_DATANODE_ID);\n+        newNodes[i] \u003d new DatanodeInfoBuilder()\n+            .setNodeID(DatanodeID.EMPTY_DATANODE_ID).build();\n         newStorageIDs[i] \u003d \"\";\n       }\n     }\n     dfsClient.namenode.updatePipeline(dfsClient.clientName, currentBlockGroup,\n         newBG, newNodes, newStorageIDs);\n     currentBlockGroup \u003d newBG;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updatePipeline(ExtendedBlock newBG) throws IOException {\n    final DatanodeInfo[] newNodes \u003d new DatanodeInfo[numAllBlocks];\n    final String[] newStorageIDs \u003d new String[numAllBlocks];\n    for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n      final StripedDataStreamer streamer \u003d getStripedDataStreamer(i);\n      final DatanodeInfo[] nodes \u003d streamer.getNodes();\n      final String[] storageIDs \u003d streamer.getStorageIDs();\n      if (streamer.isHealthy() \u0026\u0026 nodes !\u003d null \u0026\u0026 storageIDs !\u003d null) {\n        newNodes[i] \u003d nodes[0];\n        newStorageIDs[i] \u003d storageIDs[0];\n      } else {\n        newNodes[i] \u003d new DatanodeInfoBuilder()\n            .setNodeID(DatanodeID.EMPTY_DATANODE_ID).build();\n        newStorageIDs[i] \u003d \"\";\n      }\n    }\n    dfsClient.namenode.updatePipeline(dfsClient.clientName, currentBlockGroup,\n        newBG, newNodes, newStorageIDs);\n    currentBlockGroup \u003d newBG;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    }
  }
}