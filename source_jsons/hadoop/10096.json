{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataNodeDiskMetrics.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java",
  "functionStartLine": 76,
  "functionEndLine": 123,
  "numCommitsSeen": 5,
  "timeTaken": 2063,
  "changeHistory": [
    "41e18feda3f5ff924c87c4bed5b5cbbaecb19ae1",
    "b3ec531f400dd0a6506dc71233d38ae57b764a43"
  ],
  "changeHistoryShort": {
    "41e18feda3f5ff924c87c4bed5b5cbbaecb19ae1": "Ybodychange",
    "b3ec531f400dd0a6506dc71233d38ae57b764a43": "Yintroduced"
  },
  "changeHistoryDetails": {
    "41e18feda3f5ff924c87c4bed5b5cbbaecb19ae1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14235. Handle ArrayIndexOutOfBoundsException in DataNodeDiskMetrics#slowDiskDetectionDaemon. Contributed by Ranith Sardar.\n",
      "commitDate": "20/02/19 3:26 AM",
      "commitName": "41e18feda3f5ff924c87c4bed5b5cbbaecb19ae1",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "30/03/17 10:41 PM",
      "commitNameOld": "28cdc5a8dc37ade1f45bda3aede589ee8593945e",
      "commitAuthorOld": "Hanisha Koneru",
      "daysBetweenCommits": 691.24,
      "commitsBetweenForRepo": 5538,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,48 @@\n       public void run() {\n         while (shouldRun) {\n-          Map\u003cString, Double\u003e metadataOpStats \u003d Maps.newHashMap();\n-          Map\u003cString, Double\u003e readIoStats \u003d Maps.newHashMap();\n-          Map\u003cString, Double\u003e writeIoStats \u003d Maps.newHashMap();\n-          FsDatasetSpi.FsVolumeReferences fsVolumeReferences \u003d null;\n-          try {\n-            fsVolumeReferences \u003d dn.getFSDataset().getFsVolumeReferences();\n-            Iterator\u003cFsVolumeSpi\u003e volumeIterator \u003d fsVolumeReferences\n-                .iterator();\n-            while (volumeIterator.hasNext()) {\n-              FsVolumeSpi volume \u003d volumeIterator.next();\n-              DataNodeVolumeMetrics metrics \u003d volumeIterator.next().getMetrics();\n-              String volumeName \u003d volume.getBaseURI().getPath();\n+          if (dn.getFSDataset() !\u003d null) {\n+            Map\u003cString, Double\u003e metadataOpStats \u003d Maps.newHashMap();\n+            Map\u003cString, Double\u003e readIoStats \u003d Maps.newHashMap();\n+            Map\u003cString, Double\u003e writeIoStats \u003d Maps.newHashMap();\n+            FsDatasetSpi.FsVolumeReferences fsVolumeReferences \u003d null;\n+            try {\n+              fsVolumeReferences \u003d dn.getFSDataset().getFsVolumeReferences();\n+              Iterator\u003cFsVolumeSpi\u003e volumeIterator \u003d fsVolumeReferences\n+                  .iterator();\n+              while (volumeIterator.hasNext()) {\n+                FsVolumeSpi volume \u003d volumeIterator.next();\n+                DataNodeVolumeMetrics metrics \u003d volume.getMetrics();\n+                String volumeName \u003d volume.getBaseURI().getPath();\n \n-              metadataOpStats.put(volumeName,\n-                  metrics.getMetadataOperationMean());\n-              readIoStats.put(volumeName, metrics.getReadIoMean());\n-              writeIoStats.put(volumeName, metrics.getWriteIoMean());\n-            }\n-          } finally {\n-            if (fsVolumeReferences !\u003d null) {\n-              try {\n-                fsVolumeReferences.close();\n-              } catch (IOException e) {\n-                LOG.error(\"Error in releasing FS Volume references\", e);\n+                metadataOpStats.put(volumeName,\n+                    metrics.getMetadataOperationMean());\n+                readIoStats.put(volumeName, metrics.getReadIoMean());\n+                writeIoStats.put(volumeName, metrics.getWriteIoMean());\n+              }\n+            } finally {\n+              if (fsVolumeReferences !\u003d null) {\n+                try {\n+                  fsVolumeReferences.close();\n+                } catch (IOException e) {\n+                  LOG.error(\"Error in releasing FS Volume references\", e);\n+                }\n               }\n             }\n-          }\n-          if (metadataOpStats.isEmpty() \u0026\u0026 readIoStats.isEmpty() \u0026\u0026\n-              writeIoStats.isEmpty()) {\n-            LOG.debug(\"No disk stats available for detecting outliers.\");\n-            return;\n-          }\n+            if (metadataOpStats.isEmpty() \u0026\u0026 readIoStats.isEmpty()\n+                \u0026\u0026 writeIoStats.isEmpty()) {\n+              LOG.debug(\"No disk stats available for detecting outliers.\");\n+              continue;\n+            }\n \n-          detectAndUpdateDiskOutliers(metadataOpStats, readIoStats,\n-              writeIoStats);\n+            detectAndUpdateDiskOutliers(metadataOpStats, readIoStats,\n+                writeIoStats);\n+          }\n \n           try {\n             Thread.sleep(detectionInterval);\n           } catch (InterruptedException e) {\n             LOG.error(\"Disk Outlier Detection thread interrupted\", e);\n             Thread.currentThread().interrupt();\n           }\n         }\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public void run() {\n        while (shouldRun) {\n          if (dn.getFSDataset() !\u003d null) {\n            Map\u003cString, Double\u003e metadataOpStats \u003d Maps.newHashMap();\n            Map\u003cString, Double\u003e readIoStats \u003d Maps.newHashMap();\n            Map\u003cString, Double\u003e writeIoStats \u003d Maps.newHashMap();\n            FsDatasetSpi.FsVolumeReferences fsVolumeReferences \u003d null;\n            try {\n              fsVolumeReferences \u003d dn.getFSDataset().getFsVolumeReferences();\n              Iterator\u003cFsVolumeSpi\u003e volumeIterator \u003d fsVolumeReferences\n                  .iterator();\n              while (volumeIterator.hasNext()) {\n                FsVolumeSpi volume \u003d volumeIterator.next();\n                DataNodeVolumeMetrics metrics \u003d volume.getMetrics();\n                String volumeName \u003d volume.getBaseURI().getPath();\n\n                metadataOpStats.put(volumeName,\n                    metrics.getMetadataOperationMean());\n                readIoStats.put(volumeName, metrics.getReadIoMean());\n                writeIoStats.put(volumeName, metrics.getWriteIoMean());\n              }\n            } finally {\n              if (fsVolumeReferences !\u003d null) {\n                try {\n                  fsVolumeReferences.close();\n                } catch (IOException e) {\n                  LOG.error(\"Error in releasing FS Volume references\", e);\n                }\n              }\n            }\n            if (metadataOpStats.isEmpty() \u0026\u0026 readIoStats.isEmpty()\n                \u0026\u0026 writeIoStats.isEmpty()) {\n              LOG.debug(\"No disk stats available for detecting outliers.\");\n              continue;\n            }\n\n            detectAndUpdateDiskOutliers(metadataOpStats, readIoStats,\n                writeIoStats);\n          }\n\n          try {\n            Thread.sleep(detectionInterval);\n          } catch (InterruptedException e) {\n            LOG.error(\"Disk Outlier Detection thread interrupted\", e);\n            Thread.currentThread().interrupt();\n          }\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java",
      "extendedDetails": {}
    },
    "b3ec531f400dd0a6506dc71233d38ae57b764a43": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11461. DataNode Disk Outlier Detection. Contributed by Hanisha Koneru.\n",
      "commitDate": "02/03/17 12:45 PM",
      "commitName": "b3ec531f400dd0a6506dc71233d38ae57b764a43",
      "commitAuthor": "Arpit Agarwal",
      "diff": "@@ -0,0 +1,46 @@\n+      public void run() {\n+        while (shouldRun) {\n+          Map\u003cString, Double\u003e metadataOpStats \u003d Maps.newHashMap();\n+          Map\u003cString, Double\u003e readIoStats \u003d Maps.newHashMap();\n+          Map\u003cString, Double\u003e writeIoStats \u003d Maps.newHashMap();\n+          FsDatasetSpi.FsVolumeReferences fsVolumeReferences \u003d null;\n+          try {\n+            fsVolumeReferences \u003d dn.getFSDataset().getFsVolumeReferences();\n+            Iterator\u003cFsVolumeSpi\u003e volumeIterator \u003d fsVolumeReferences\n+                .iterator();\n+            while (volumeIterator.hasNext()) {\n+              FsVolumeSpi volume \u003d volumeIterator.next();\n+              DataNodeVolumeMetrics metrics \u003d volumeIterator.next().getMetrics();\n+              String volumeName \u003d volume.getBaseURI().getPath();\n+\n+              metadataOpStats.put(volumeName,\n+                  metrics.getMetadataOperationMean());\n+              readIoStats.put(volumeName, metrics.getReadIoMean());\n+              writeIoStats.put(volumeName, metrics.getWriteIoMean());\n+            }\n+          } finally {\n+            if (fsVolumeReferences !\u003d null) {\n+              try {\n+                fsVolumeReferences.close();\n+              } catch (IOException e) {\n+                LOG.error(\"Error in releasing FS Volume references\", e);\n+              }\n+            }\n+          }\n+          if (metadataOpStats.isEmpty() \u0026\u0026 readIoStats.isEmpty() \u0026\u0026\n+              writeIoStats.isEmpty()) {\n+            LOG.debug(\"No disk stats available for detecting outliers.\");\n+            return;\n+          }\n+\n+          detectAndUpdateDiskOutliers(metadataOpStats, readIoStats,\n+              writeIoStats);\n+\n+          try {\n+            Thread.sleep(detectionInterval);\n+          } catch (InterruptedException e) {\n+            LOG.error(\"Disk Outlier Detection thread interrupted\", e);\n+            Thread.currentThread().interrupt();\n+          }\n+        }\n+      }\n\\ No newline at end of file\n",
      "actualSource": "      public void run() {\n        while (shouldRun) {\n          Map\u003cString, Double\u003e metadataOpStats \u003d Maps.newHashMap();\n          Map\u003cString, Double\u003e readIoStats \u003d Maps.newHashMap();\n          Map\u003cString, Double\u003e writeIoStats \u003d Maps.newHashMap();\n          FsDatasetSpi.FsVolumeReferences fsVolumeReferences \u003d null;\n          try {\n            fsVolumeReferences \u003d dn.getFSDataset().getFsVolumeReferences();\n            Iterator\u003cFsVolumeSpi\u003e volumeIterator \u003d fsVolumeReferences\n                .iterator();\n            while (volumeIterator.hasNext()) {\n              FsVolumeSpi volume \u003d volumeIterator.next();\n              DataNodeVolumeMetrics metrics \u003d volumeIterator.next().getMetrics();\n              String volumeName \u003d volume.getBaseURI().getPath();\n\n              metadataOpStats.put(volumeName,\n                  metrics.getMetadataOperationMean());\n              readIoStats.put(volumeName, metrics.getReadIoMean());\n              writeIoStats.put(volumeName, metrics.getWriteIoMean());\n            }\n          } finally {\n            if (fsVolumeReferences !\u003d null) {\n              try {\n                fsVolumeReferences.close();\n              } catch (IOException e) {\n                LOG.error(\"Error in releasing FS Volume references\", e);\n              }\n            }\n          }\n          if (metadataOpStats.isEmpty() \u0026\u0026 readIoStats.isEmpty() \u0026\u0026\n              writeIoStats.isEmpty()) {\n            LOG.debug(\"No disk stats available for detecting outliers.\");\n            return;\n          }\n\n          detectAndUpdateDiskOutliers(metadataOpStats, readIoStats,\n              writeIoStats);\n\n          try {\n            Thread.sleep(detectionInterval);\n          } catch (InterruptedException e) {\n            LOG.error(\"Disk Outlier Detection thread interrupted\", e);\n            Thread.currentThread().interrupt();\n          }\n        }\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java"
    }
  }
}