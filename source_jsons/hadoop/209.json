{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "OpenFileCtx.java",
  "functionName": "checkCommitInternal",
  "functionId": "checkCommitInternal___commitOffset-long__channel-Channel__xid-int__preOpAttr-Nfs3FileAttributes__fromRead-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
  "functionStartLine": 840,
  "functionEndLine": 933,
  "numCommitsSeen": 61,
  "timeTaken": 2925,
  "changeHistory": [
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
    "99d9d0c2d19b9f161b765947f3fb64619ea58090",
    "b6f9d5538cf2b425652687e99503f3d566b2056a",
    "9ff3836a367737d6dfcb12f50c8bd2f1b2233e37",
    "5ea533c2bfc72fd3adbfd972d18806fbc397e0f8",
    "a2200a64175867a1c66cf8338f536ccaaaa36508",
    "5c02d2f6225144772dcb975d3144b057b71d6476",
    "28e3d09230971b32f74284311931525cb7ad1b7c",
    "c9b89de0eacf15f21faa3a7ba30d4773f571c9a4",
    "37f587563a943a827fbff865f5302bac6d202415"
  ],
  "changeHistoryShort": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": "Ybodychange",
    "99d9d0c2d19b9f161b765947f3fb64619ea58090": "Ybodychange",
    "b6f9d5538cf2b425652687e99503f3d566b2056a": "Ybodychange",
    "9ff3836a367737d6dfcb12f50c8bd2f1b2233e37": "Ybodychange",
    "5ea533c2bfc72fd3adbfd972d18806fbc397e0f8": "Ymultichange(Yparameterchange,Ybodychange)",
    "a2200a64175867a1c66cf8338f536ccaaaa36508": "Ymodifierchange",
    "5c02d2f6225144772dcb975d3144b057b71d6476": "Ymultichange(Yparameterchange,Yreturntypechange,Ymodifierchange,Ybodychange)",
    "28e3d09230971b32f74284311931525cb7ad1b7c": "Ybodychange",
    "c9b89de0eacf15f21faa3a7ba30d4773f571c9a4": "Ybodychange",
    "37f587563a943a827fbff865f5302bac6d202415": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10571. Use Log.*(Object, Throwable) overload to log exceptions.\nContributed by Andras Bokor.\n",
      "commitDate": "14/02/18 8:20 AM",
      "commitName": "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "10/10/17 10:38 AM",
      "commitNameOld": "d6602b5f39833611b4afa4581552f6c4c37e23a8",
      "commitAuthorOld": "Jitendra Pandey",
      "daysBetweenCommits": 126.95,
      "commitsBetweenForRepo": 833,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,102 +1,94 @@\n   synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n       Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n     if (!activeState) {\n       if (pendingWrites.isEmpty()) {\n         return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n       } else {\n         // TODO: return success if already committed\n         return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n       }\n     }\n     \n     long flushed \u003d getFlushedOffset();\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset\n-          + \"nextOffset\u003d\" + nextOffset.get());\n-    }\n+    LOG.debug(\"getFlushedOffset\u003d{} commitOffset\u003d{} nextOffset\u003d{}\",\n+        flushed, commitOffset, nextOffset.get());\n     \n     if (pendingWrites.isEmpty()) {\n       if (aixCompatMode) {\n         // Note that, there is no guarantee data is synced. Caller should still\n         // do a sync here though the output stream might be closed.\n         return COMMIT_STATUS.COMMIT_FINISHED;\n       } else {\n         if (flushed \u003c nextOffset.get()) {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"get commit while still writing to the requested offset,\"\n-                + \" with empty queue\");\n-          }\n+          LOG.debug(\"get commit while still writing to the requested offset,\"\n+              + \" with empty queue\");\n           return handleSpecialWait(fromRead, nextOffset.get(), channel, xid,\n               preOpAttr);\n         } else {\n           return COMMIT_STATUS.COMMIT_FINISHED;\n         }\n       }\n     }\n     \n     Preconditions.checkState(flushed \u003c\u003d nextOffset.get(), \"flushed \" + flushed\n         + \" is larger than nextOffset \" + nextOffset.get());\n     // Handle large file upload\n     if (uploadLargeFile \u0026\u0026 !aixCompatMode) {\n       long co \u003d (commitOffset \u003e 0) ? commitOffset : pendingWrites.firstEntry()\n           .getKey().getMax() - 1;\n \n       if (co \u003c\u003d flushed) {\n         return COMMIT_STATUS.COMMIT_DO_SYNC;\n       } else if (co \u003c nextOffset.get()) {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"get commit while still writing to the requested offset\");\n-        }\n+        LOG.debug(\"get commit while still writing to the requested offset\");\n         return handleSpecialWait(fromRead, co, channel, xid, preOpAttr);\n       } else {\n         // co \u003e\u003d nextOffset\n         if (checkSequential(co, nextOffset.get())) {\n           return handleSpecialWait(fromRead, co, channel, xid, preOpAttr);\n         } else {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"return COMMIT_SPECIAL_SUCCESS\");\n-          }\n+          LOG.debug(\"return COMMIT_SPECIAL_SUCCESS\");\n           return COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS;\n         }\n       }\n     }\n     \n     if (commitOffset \u003e 0) {\n       if (aixCompatMode) {\n         // The AIX NFS client misinterprets RFC-1813 and will always send 4096\n         // for the commitOffset even if fewer bytes than that have ever (or will\n         // ever) be sent by the client. So, if in AIX compatibility mode, we\n         // will always DO_SYNC if the number of bytes to commit have already all\n         // been flushed, else we will fall through to the logic below which\n         // checks for pending writes in the case that we\u0027re being asked to\n         // commit more bytes than have so far been flushed. See HDFS-6549 for\n         // more info.\n         if (commitOffset \u003c\u003d flushed) {\n           return COMMIT_STATUS.COMMIT_DO_SYNC;\n         }\n       } else {\n         if (commitOffset \u003e flushed) {\n           if (!fromRead) {\n             CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n                 preOpAttr);\n             pendingCommits.put(commitOffset, commitCtx);\n           }\n           return COMMIT_STATUS.COMMIT_WAIT;\n         } else {\n           return COMMIT_STATUS.COMMIT_DO_SYNC;\n         } \n       }\n     }\n \n     Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n \n     // Commit whole file, commitOffset \u003d\u003d 0\n     if (!fromRead) {\n       // Insert commit\n       long maxOffset \u003d key.getKey().getMax() - 1;\n       Preconditions.checkState(maxOffset \u003e 0);\n       CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n       pendingCommits.put(maxOffset, commitCtx);\n     }\n     return COMMIT_STATUS.COMMIT_WAIT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n    \n    long flushed \u003d getFlushedOffset();\n    LOG.debug(\"getFlushedOffset\u003d{} commitOffset\u003d{} nextOffset\u003d{}\",\n        flushed, commitOffset, nextOffset.get());\n    \n    if (pendingWrites.isEmpty()) {\n      if (aixCompatMode) {\n        // Note that, there is no guarantee data is synced. Caller should still\n        // do a sync here though the output stream might be closed.\n        return COMMIT_STATUS.COMMIT_FINISHED;\n      } else {\n        if (flushed \u003c nextOffset.get()) {\n          LOG.debug(\"get commit while still writing to the requested offset,\"\n              + \" with empty queue\");\n          return handleSpecialWait(fromRead, nextOffset.get(), channel, xid,\n              preOpAttr);\n        } else {\n          return COMMIT_STATUS.COMMIT_FINISHED;\n        }\n      }\n    }\n    \n    Preconditions.checkState(flushed \u003c\u003d nextOffset.get(), \"flushed \" + flushed\n        + \" is larger than nextOffset \" + nextOffset.get());\n    // Handle large file upload\n    if (uploadLargeFile \u0026\u0026 !aixCompatMode) {\n      long co \u003d (commitOffset \u003e 0) ? commitOffset : pendingWrites.firstEntry()\n          .getKey().getMax() - 1;\n\n      if (co \u003c\u003d flushed) {\n        return COMMIT_STATUS.COMMIT_DO_SYNC;\n      } else if (co \u003c nextOffset.get()) {\n        LOG.debug(\"get commit while still writing to the requested offset\");\n        return handleSpecialWait(fromRead, co, channel, xid, preOpAttr);\n      } else {\n        // co \u003e\u003d nextOffset\n        if (checkSequential(co, nextOffset.get())) {\n          return handleSpecialWait(fromRead, co, channel, xid, preOpAttr);\n        } else {\n          LOG.debug(\"return COMMIT_SPECIAL_SUCCESS\");\n          return COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS;\n        }\n      }\n    }\n    \n    if (commitOffset \u003e 0) {\n      if (aixCompatMode) {\n        // The AIX NFS client misinterprets RFC-1813 and will always send 4096\n        // for the commitOffset even if fewer bytes than that have ever (or will\n        // ever) be sent by the client. So, if in AIX compatibility mode, we\n        // will always DO_SYNC if the number of bytes to commit have already all\n        // been flushed, else we will fall through to the logic below which\n        // checks for pending writes in the case that we\u0027re being asked to\n        // commit more bytes than have so far been flushed. See HDFS-6549 for\n        // more info.\n        if (commitOffset \u003c\u003d flushed) {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        }\n      } else {\n        if (commitOffset \u003e flushed) {\n          if (!fromRead) {\n            CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n                preOpAttr);\n            pendingCommits.put(commitOffset, commitCtx);\n          }\n          return COMMIT_STATUS.COMMIT_WAIT;\n        } else {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        } \n      }\n    }\n\n    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset \u003d\u003d 0\n    if (!fromRead) {\n      // Insert commit\n      long maxOffset \u003d key.getKey().getMax() - 1;\n      Preconditions.checkState(maxOffset \u003e 0);\n      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n      pendingCommits.put(maxOffset, commitCtx);\n    }\n    return COMMIT_STATUS.COMMIT_WAIT;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "99d9d0c2d19b9f161b765947f3fb64619ea58090": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7387. NFS may only do partial commit due to a race between COMMIT and write. Contributed by Brandon Li\n",
      "commitDate": "11/11/14 1:03 PM",
      "commitName": "99d9d0c2d19b9f161b765947f3fb64619ea58090",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "29/10/14 11:05 AM",
      "commitNameOld": "72a556d3b0def0ab4e4509528cc513f6df06b084",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 13.12,
      "commitsBetweenForRepo": 142,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,102 @@\n   synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n       Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n     if (!activeState) {\n       if (pendingWrites.isEmpty()) {\n         return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n       } else {\n         // TODO: return success if already committed\n         return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n       }\n     }\n-    if (pendingWrites.isEmpty()) {\n-      // Note that, there is no guarantee data is synced. Caller should still\n-      // do a sync here though the output stream might be closed.\n-      return COMMIT_STATUS.COMMIT_FINISHED;\n-    }\n     \n     long flushed \u003d getFlushedOffset();\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n+      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset\n+          + \"nextOffset\u003d\" + nextOffset.get());\n     }\n-\n+    \n+    if (pendingWrites.isEmpty()) {\n+      if (aixCompatMode) {\n+        // Note that, there is no guarantee data is synced. Caller should still\n+        // do a sync here though the output stream might be closed.\n+        return COMMIT_STATUS.COMMIT_FINISHED;\n+      } else {\n+        if (flushed \u003c nextOffset.get()) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"get commit while still writing to the requested offset,\"\n+                + \" with empty queue\");\n+          }\n+          return handleSpecialWait(fromRead, nextOffset.get(), channel, xid,\n+              preOpAttr);\n+        } else {\n+          return COMMIT_STATUS.COMMIT_FINISHED;\n+        }\n+      }\n+    }\n+    \n+    Preconditions.checkState(flushed \u003c\u003d nextOffset.get(), \"flushed \" + flushed\n+        + \" is larger than nextOffset \" + nextOffset.get());\n     // Handle large file upload\n     if (uploadLargeFile \u0026\u0026 !aixCompatMode) {\n       long co \u003d (commitOffset \u003e 0) ? commitOffset : pendingWrites.firstEntry()\n           .getKey().getMax() - 1;\n \n       if (co \u003c\u003d flushed) {\n         return COMMIT_STATUS.COMMIT_DO_SYNC;\n       } else if (co \u003c nextOffset.get()) {\n-        if (!fromRead) {\n-          // let client retry the same request, add pending commit to sync later\n-          CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n-              preOpAttr);\n-          pendingCommits.put(commitOffset, commitCtx);\n-        }\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"return COMMIT_SPECIAL_WAIT\");\n+          LOG.debug(\"get commit while still writing to the requested offset\");\n         }\n-        return COMMIT_STATUS.COMMIT_SPECIAL_WAIT;\n+        return handleSpecialWait(fromRead, co, channel, xid, preOpAttr);\n       } else {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"return COMMIT_SPECIAL_SUCCESS\");\n+        // co \u003e\u003d nextOffset\n+        if (checkSequential(co, nextOffset.get())) {\n+          return handleSpecialWait(fromRead, co, channel, xid, preOpAttr);\n+        } else {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"return COMMIT_SPECIAL_SUCCESS\");\n+          }\n+          return COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS;\n         }\n-        return COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS;\n       }\n     }\n     \n     if (commitOffset \u003e 0) {\n       if (aixCompatMode) {\n         // The AIX NFS client misinterprets RFC-1813 and will always send 4096\n         // for the commitOffset even if fewer bytes than that have ever (or will\n         // ever) be sent by the client. So, if in AIX compatibility mode, we\n         // will always DO_SYNC if the number of bytes to commit have already all\n         // been flushed, else we will fall through to the logic below which\n         // checks for pending writes in the case that we\u0027re being asked to\n         // commit more bytes than have so far been flushed. See HDFS-6549 for\n         // more info.\n         if (commitOffset \u003c\u003d flushed) {\n           return COMMIT_STATUS.COMMIT_DO_SYNC;\n         }\n       } else {\n         if (commitOffset \u003e flushed) {\n           if (!fromRead) {\n             CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n                 preOpAttr);\n             pendingCommits.put(commitOffset, commitCtx);\n           }\n           return COMMIT_STATUS.COMMIT_WAIT;\n         } else {\n           return COMMIT_STATUS.COMMIT_DO_SYNC;\n         } \n       }\n     }\n \n     Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n \n     // Commit whole file, commitOffset \u003d\u003d 0\n     if (!fromRead) {\n       // Insert commit\n       long maxOffset \u003d key.getKey().getMax() - 1;\n       Preconditions.checkState(maxOffset \u003e 0);\n       CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n       pendingCommits.put(maxOffset, commitCtx);\n     }\n     return COMMIT_STATUS.COMMIT_WAIT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n    \n    long flushed \u003d getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset\n          + \"nextOffset\u003d\" + nextOffset.get());\n    }\n    \n    if (pendingWrites.isEmpty()) {\n      if (aixCompatMode) {\n        // Note that, there is no guarantee data is synced. Caller should still\n        // do a sync here though the output stream might be closed.\n        return COMMIT_STATUS.COMMIT_FINISHED;\n      } else {\n        if (flushed \u003c nextOffset.get()) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"get commit while still writing to the requested offset,\"\n                + \" with empty queue\");\n          }\n          return handleSpecialWait(fromRead, nextOffset.get(), channel, xid,\n              preOpAttr);\n        } else {\n          return COMMIT_STATUS.COMMIT_FINISHED;\n        }\n      }\n    }\n    \n    Preconditions.checkState(flushed \u003c\u003d nextOffset.get(), \"flushed \" + flushed\n        + \" is larger than nextOffset \" + nextOffset.get());\n    // Handle large file upload\n    if (uploadLargeFile \u0026\u0026 !aixCompatMode) {\n      long co \u003d (commitOffset \u003e 0) ? commitOffset : pendingWrites.firstEntry()\n          .getKey().getMax() - 1;\n\n      if (co \u003c\u003d flushed) {\n        return COMMIT_STATUS.COMMIT_DO_SYNC;\n      } else if (co \u003c nextOffset.get()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"get commit while still writing to the requested offset\");\n        }\n        return handleSpecialWait(fromRead, co, channel, xid, preOpAttr);\n      } else {\n        // co \u003e\u003d nextOffset\n        if (checkSequential(co, nextOffset.get())) {\n          return handleSpecialWait(fromRead, co, channel, xid, preOpAttr);\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"return COMMIT_SPECIAL_SUCCESS\");\n          }\n          return COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS;\n        }\n      }\n    }\n    \n    if (commitOffset \u003e 0) {\n      if (aixCompatMode) {\n        // The AIX NFS client misinterprets RFC-1813 and will always send 4096\n        // for the commitOffset even if fewer bytes than that have ever (or will\n        // ever) be sent by the client. So, if in AIX compatibility mode, we\n        // will always DO_SYNC if the number of bytes to commit have already all\n        // been flushed, else we will fall through to the logic below which\n        // checks for pending writes in the case that we\u0027re being asked to\n        // commit more bytes than have so far been flushed. See HDFS-6549 for\n        // more info.\n        if (commitOffset \u003c\u003d flushed) {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        }\n      } else {\n        if (commitOffset \u003e flushed) {\n          if (!fromRead) {\n            CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n                preOpAttr);\n            pendingCommits.put(commitOffset, commitCtx);\n          }\n          return COMMIT_STATUS.COMMIT_WAIT;\n        } else {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        } \n      }\n    }\n\n    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset \u003d\u003d 0\n    if (!fromRead) {\n      // Insert commit\n      long maxOffset \u003d key.getKey().getMax() - 1;\n      Preconditions.checkState(maxOffset \u003e 0);\n      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n      pendingCommits.put(maxOffset, commitCtx);\n    }\n    return COMMIT_STATUS.COMMIT_WAIT;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "b6f9d5538cf2b425652687e99503f3d566b2056a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7259. Unresponseive NFS mount point due to deferred COMMIT response. Contributed by Brandon Li\n",
      "commitDate": "21/10/14 10:20 AM",
      "commitName": "b6f9d5538cf2b425652687e99503f3d566b2056a",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "01/10/14 1:18 PM",
      "commitNameOld": "875aa797caee96572162ff59bc50cf97d1195348",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 19.88,
      "commitsBetweenForRepo": 154,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,86 @@\n   synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n       Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n     if (!activeState) {\n       if (pendingWrites.isEmpty()) {\n         return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n       } else {\n         // TODO: return success if already committed\n         return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n       }\n     }\n-\n+    if (pendingWrites.isEmpty()) {\n+      // Note that, there is no guarantee data is synced. Caller should still\n+      // do a sync here though the output stream might be closed.\n+      return COMMIT_STATUS.COMMIT_FINISHED;\n+    }\n+    \n     long flushed \u003d getFlushedOffset();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n     }\n \n+    // Handle large file upload\n+    if (uploadLargeFile \u0026\u0026 !aixCompatMode) {\n+      long co \u003d (commitOffset \u003e 0) ? commitOffset : pendingWrites.firstEntry()\n+          .getKey().getMax() - 1;\n+\n+      if (co \u003c\u003d flushed) {\n+        return COMMIT_STATUS.COMMIT_DO_SYNC;\n+      } else if (co \u003c nextOffset.get()) {\n+        if (!fromRead) {\n+          // let client retry the same request, add pending commit to sync later\n+          CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n+              preOpAttr);\n+          pendingCommits.put(commitOffset, commitCtx);\n+        }\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"return COMMIT_SPECIAL_WAIT\");\n+        }\n+        return COMMIT_STATUS.COMMIT_SPECIAL_WAIT;\n+      } else {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"return COMMIT_SPECIAL_SUCCESS\");\n+        }\n+        return COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS;\n+      }\n+    }\n+    \n     if (commitOffset \u003e 0) {\n       if (aixCompatMode) {\n         // The AIX NFS client misinterprets RFC-1813 and will always send 4096\n         // for the commitOffset even if fewer bytes than that have ever (or will\n         // ever) be sent by the client. So, if in AIX compatibility mode, we\n         // will always DO_SYNC if the number of bytes to commit have already all\n         // been flushed, else we will fall through to the logic below which\n         // checks for pending writes in the case that we\u0027re being asked to\n         // commit more bytes than have so far been flushed. See HDFS-6549 for\n         // more info.\n         if (commitOffset \u003c\u003d flushed) {\n           return COMMIT_STATUS.COMMIT_DO_SYNC;\n         }\n       } else {\n         if (commitOffset \u003e flushed) {\n           if (!fromRead) {\n             CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n                 preOpAttr);\n             pendingCommits.put(commitOffset, commitCtx);\n           }\n           return COMMIT_STATUS.COMMIT_WAIT;\n         } else {\n           return COMMIT_STATUS.COMMIT_DO_SYNC;\n         } \n       }\n     }\n \n     Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n \n     // Commit whole file, commitOffset \u003d\u003d 0\n-    if (pendingWrites.isEmpty()) {\n-      // Note that, there is no guarantee data is synced. TODO: We could still\n-      // do a sync here though the output stream might be closed.\n-      return COMMIT_STATUS.COMMIT_FINISHED;\n-    } else {\n-      if (!fromRead) {\n-        // Insert commit\n-        long maxOffset \u003d key.getKey().getMax() - 1;\n-        Preconditions.checkState(maxOffset \u003e 0);\n-        CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n-        pendingCommits.put(maxOffset, commitCtx);\n-      }\n-      return COMMIT_STATUS.COMMIT_WAIT;\n+    if (!fromRead) {\n+      // Insert commit\n+      long maxOffset \u003d key.getKey().getMax() - 1;\n+      Preconditions.checkState(maxOffset \u003e 0);\n+      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n+      pendingCommits.put(maxOffset, commitCtx);\n     }\n+    return COMMIT_STATUS.COMMIT_WAIT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n    if (pendingWrites.isEmpty()) {\n      // Note that, there is no guarantee data is synced. Caller should still\n      // do a sync here though the output stream might be closed.\n      return COMMIT_STATUS.COMMIT_FINISHED;\n    }\n    \n    long flushed \u003d getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    }\n\n    // Handle large file upload\n    if (uploadLargeFile \u0026\u0026 !aixCompatMode) {\n      long co \u003d (commitOffset \u003e 0) ? commitOffset : pendingWrites.firstEntry()\n          .getKey().getMax() - 1;\n\n      if (co \u003c\u003d flushed) {\n        return COMMIT_STATUS.COMMIT_DO_SYNC;\n      } else if (co \u003c nextOffset.get()) {\n        if (!fromRead) {\n          // let client retry the same request, add pending commit to sync later\n          CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n              preOpAttr);\n          pendingCommits.put(commitOffset, commitCtx);\n        }\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"return COMMIT_SPECIAL_WAIT\");\n        }\n        return COMMIT_STATUS.COMMIT_SPECIAL_WAIT;\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"return COMMIT_SPECIAL_SUCCESS\");\n        }\n        return COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS;\n      }\n    }\n    \n    if (commitOffset \u003e 0) {\n      if (aixCompatMode) {\n        // The AIX NFS client misinterprets RFC-1813 and will always send 4096\n        // for the commitOffset even if fewer bytes than that have ever (or will\n        // ever) be sent by the client. So, if in AIX compatibility mode, we\n        // will always DO_SYNC if the number of bytes to commit have already all\n        // been flushed, else we will fall through to the logic below which\n        // checks for pending writes in the case that we\u0027re being asked to\n        // commit more bytes than have so far been flushed. See HDFS-6549 for\n        // more info.\n        if (commitOffset \u003c\u003d flushed) {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        }\n      } else {\n        if (commitOffset \u003e flushed) {\n          if (!fromRead) {\n            CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n                preOpAttr);\n            pendingCommits.put(commitOffset, commitCtx);\n          }\n          return COMMIT_STATUS.COMMIT_WAIT;\n        } else {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        } \n      }\n    }\n\n    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset \u003d\u003d 0\n    if (!fromRead) {\n      // Insert commit\n      long maxOffset \u003d key.getKey().getMax() - 1;\n      Preconditions.checkState(maxOffset \u003e 0);\n      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n      pendingCommits.put(maxOffset, commitCtx);\n    }\n    return COMMIT_STATUS.COMMIT_WAIT;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "9ff3836a367737d6dfcb12f50c8bd2f1b2233e37": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6549. Add support for accessing the NFS gateway from the AIX NFS client. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1604022 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/06/14 12:39 PM",
      "commitName": "9ff3836a367737d6dfcb12f50c8bd2f1b2233e37",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "30/05/14 4:53 PM",
      "commitNameOld": "42391d260da400593812396c1ffd45d1a371d3cb",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 19.82,
      "commitsBetweenForRepo": 118,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,61 @@\n   synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n       Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n     if (!activeState) {\n       if (pendingWrites.isEmpty()) {\n         return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n       } else {\n         // TODO: return success if already committed\n         return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n       }\n     }\n \n     long flushed \u003d getFlushedOffset();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n     }\n \n     if (commitOffset \u003e 0) {\n-      if (commitOffset \u003e flushed) {\n-        if (!fromRead) {\n-          CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n-              preOpAttr);\n-          pendingCommits.put(commitOffset, commitCtx);\n+      if (aixCompatMode) {\n+        // The AIX NFS client misinterprets RFC-1813 and will always send 4096\n+        // for the commitOffset even if fewer bytes than that have ever (or will\n+        // ever) be sent by the client. So, if in AIX compatibility mode, we\n+        // will always DO_SYNC if the number of bytes to commit have already all\n+        // been flushed, else we will fall through to the logic below which\n+        // checks for pending writes in the case that we\u0027re being asked to\n+        // commit more bytes than have so far been flushed. See HDFS-6549 for\n+        // more info.\n+        if (commitOffset \u003c\u003d flushed) {\n+          return COMMIT_STATUS.COMMIT_DO_SYNC;\n         }\n-        return COMMIT_STATUS.COMMIT_WAIT;\n       } else {\n-        return COMMIT_STATUS.COMMIT_DO_SYNC;\n+        if (commitOffset \u003e flushed) {\n+          if (!fromRead) {\n+            CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n+                preOpAttr);\n+            pendingCommits.put(commitOffset, commitCtx);\n+          }\n+          return COMMIT_STATUS.COMMIT_WAIT;\n+        } else {\n+          return COMMIT_STATUS.COMMIT_DO_SYNC;\n+        } \n       }\n     }\n \n     Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n \n     // Commit whole file, commitOffset \u003d\u003d 0\n     if (pendingWrites.isEmpty()) {\n       // Note that, there is no guarantee data is synced. TODO: We could still\n       // do a sync here though the output stream might be closed.\n       return COMMIT_STATUS.COMMIT_FINISHED;\n     } else {\n       if (!fromRead) {\n         // Insert commit\n         long maxOffset \u003d key.getKey().getMax() - 1;\n         Preconditions.checkState(maxOffset \u003e 0);\n         CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n         pendingCommits.put(maxOffset, commitCtx);\n       }\n       return COMMIT_STATUS.COMMIT_WAIT;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n\n    long flushed \u003d getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    }\n\n    if (commitOffset \u003e 0) {\n      if (aixCompatMode) {\n        // The AIX NFS client misinterprets RFC-1813 and will always send 4096\n        // for the commitOffset even if fewer bytes than that have ever (or will\n        // ever) be sent by the client. So, if in AIX compatibility mode, we\n        // will always DO_SYNC if the number of bytes to commit have already all\n        // been flushed, else we will fall through to the logic below which\n        // checks for pending writes in the case that we\u0027re being asked to\n        // commit more bytes than have so far been flushed. See HDFS-6549 for\n        // more info.\n        if (commitOffset \u003c\u003d flushed) {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        }\n      } else {\n        if (commitOffset \u003e flushed) {\n          if (!fromRead) {\n            CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n                preOpAttr);\n            pendingCommits.put(commitOffset, commitCtx);\n          }\n          return COMMIT_STATUS.COMMIT_WAIT;\n        } else {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        } \n      }\n    }\n\n    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset \u003d\u003d 0\n    if (pendingWrites.isEmpty()) {\n      // Note that, there is no guarantee data is synced. TODO: We could still\n      // do a sync here though the output stream might be closed.\n      return COMMIT_STATUS.COMMIT_FINISHED;\n    } else {\n      if (!fromRead) {\n        // Insert commit\n        long maxOffset \u003d key.getKey().getMax() - 1;\n        Preconditions.checkState(maxOffset \u003e 0);\n        CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n        pendingCommits.put(maxOffset, commitCtx);\n      }\n      return COMMIT_STATUS.COMMIT_WAIT;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "5ea533c2bfc72fd3adbfd972d18806fbc397e0f8": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5563. NFS gateway should commit the buffered data when read request comes after write to the same file. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546233 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 3:41 PM",
      "commitName": "5ea533c2bfc72fd3adbfd972d18806fbc397e0f8",
      "commitAuthor": "Brandon Li",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5563. NFS gateway should commit the buffered data when read request comes after write to the same file. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546233 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 3:41 PM",
          "commitName": "5ea533c2bfc72fd3adbfd972d18806fbc397e0f8",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "15/11/13 4:57 PM",
          "commitNameOld": "a2200a64175867a1c66cf8338f536ccaaaa36508",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 11.95,
          "commitsBetweenForRepo": 69,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,47 @@\n   synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n-      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n+      Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n     if (!activeState) {\n       if (pendingWrites.isEmpty()) {\n         return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n       } else {\n         // TODO: return success if already committed\n         return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n       }\n     }\n \n     long flushed \u003d getFlushedOffset();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n     }\n \n     if (commitOffset \u003e 0) {\n       if (commitOffset \u003e flushed) {\n-        CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n-            preOpAttr);\n-        pendingCommits.put(commitOffset, commitCtx);\n+        if (!fromRead) {\n+          CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n+              preOpAttr);\n+          pendingCommits.put(commitOffset, commitCtx);\n+        }\n         return COMMIT_STATUS.COMMIT_WAIT;\n       } else {\n         return COMMIT_STATUS.COMMIT_DO_SYNC;\n       }\n     }\n \n     Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n \n     // Commit whole file, commitOffset \u003d\u003d 0\n     if (pendingWrites.isEmpty()) {\n       // Note that, there is no guarantee data is synced. TODO: We could still\n       // do a sync here though the output stream might be closed.\n       return COMMIT_STATUS.COMMIT_FINISHED;\n     } else {\n-      // Insert commit\n-      long maxOffset \u003d key.getKey().getMax() - 1;\n-      Preconditions.checkState(maxOffset \u003e 0);\n-      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n-      pendingCommits.put(maxOffset, commitCtx);\n+      if (!fromRead) {\n+        // Insert commit\n+        long maxOffset \u003d key.getKey().getMax() - 1;\n+        Preconditions.checkState(maxOffset \u003e 0);\n+        CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n+        pendingCommits.put(maxOffset, commitCtx);\n+      }\n       return COMMIT_STATUS.COMMIT_WAIT;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n\n    long flushed \u003d getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    }\n\n    if (commitOffset \u003e 0) {\n      if (commitOffset \u003e flushed) {\n        if (!fromRead) {\n          CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n              preOpAttr);\n          pendingCommits.put(commitOffset, commitCtx);\n        }\n        return COMMIT_STATUS.COMMIT_WAIT;\n      } else {\n        return COMMIT_STATUS.COMMIT_DO_SYNC;\n      }\n    }\n\n    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset \u003d\u003d 0\n    if (pendingWrites.isEmpty()) {\n      // Note that, there is no guarantee data is synced. TODO: We could still\n      // do a sync here though the output stream might be closed.\n      return COMMIT_STATUS.COMMIT_FINISHED;\n    } else {\n      if (!fromRead) {\n        // Insert commit\n        long maxOffset \u003d key.getKey().getMax() - 1;\n        Preconditions.checkState(maxOffset \u003e 0);\n        CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n        pendingCommits.put(maxOffset, commitCtx);\n      }\n      return COMMIT_STATUS.COMMIT_WAIT;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
          "extendedDetails": {
            "oldValue": "[commitOffset-long, channel-Channel, xid-int, preOpAttr-Nfs3FileAttributes]",
            "newValue": "[commitOffset-long, channel-Channel, xid-int, preOpAttr-Nfs3FileAttributes, fromRead-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5563. NFS gateway should commit the buffered data when read request comes after write to the same file. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546233 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 3:41 PM",
          "commitName": "5ea533c2bfc72fd3adbfd972d18806fbc397e0f8",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "15/11/13 4:57 PM",
          "commitNameOld": "a2200a64175867a1c66cf8338f536ccaaaa36508",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 11.95,
          "commitsBetweenForRepo": 69,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,47 @@\n   synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n-      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n+      Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n     if (!activeState) {\n       if (pendingWrites.isEmpty()) {\n         return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n       } else {\n         // TODO: return success if already committed\n         return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n       }\n     }\n \n     long flushed \u003d getFlushedOffset();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n     }\n \n     if (commitOffset \u003e 0) {\n       if (commitOffset \u003e flushed) {\n-        CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n-            preOpAttr);\n-        pendingCommits.put(commitOffset, commitCtx);\n+        if (!fromRead) {\n+          CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n+              preOpAttr);\n+          pendingCommits.put(commitOffset, commitCtx);\n+        }\n         return COMMIT_STATUS.COMMIT_WAIT;\n       } else {\n         return COMMIT_STATUS.COMMIT_DO_SYNC;\n       }\n     }\n \n     Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n \n     // Commit whole file, commitOffset \u003d\u003d 0\n     if (pendingWrites.isEmpty()) {\n       // Note that, there is no guarantee data is synced. TODO: We could still\n       // do a sync here though the output stream might be closed.\n       return COMMIT_STATUS.COMMIT_FINISHED;\n     } else {\n-      // Insert commit\n-      long maxOffset \u003d key.getKey().getMax() - 1;\n-      Preconditions.checkState(maxOffset \u003e 0);\n-      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n-      pendingCommits.put(maxOffset, commitCtx);\n+      if (!fromRead) {\n+        // Insert commit\n+        long maxOffset \u003d key.getKey().getMax() - 1;\n+        Preconditions.checkState(maxOffset \u003e 0);\n+        CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n+        pendingCommits.put(maxOffset, commitCtx);\n+      }\n       return COMMIT_STATUS.COMMIT_WAIT;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n\n    long flushed \u003d getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    }\n\n    if (commitOffset \u003e 0) {\n      if (commitOffset \u003e flushed) {\n        if (!fromRead) {\n          CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n              preOpAttr);\n          pendingCommits.put(commitOffset, commitCtx);\n        }\n        return COMMIT_STATUS.COMMIT_WAIT;\n      } else {\n        return COMMIT_STATUS.COMMIT_DO_SYNC;\n      }\n    }\n\n    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset \u003d\u003d 0\n    if (pendingWrites.isEmpty()) {\n      // Note that, there is no guarantee data is synced. TODO: We could still\n      // do a sync here though the output stream might be closed.\n      return COMMIT_STATUS.COMMIT_FINISHED;\n    } else {\n      if (!fromRead) {\n        // Insert commit\n        long maxOffset \u003d key.getKey().getMax() - 1;\n        Preconditions.checkState(maxOffset \u003e 0);\n        CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n        pendingCommits.put(maxOffset, commitCtx);\n      }\n      return COMMIT_STATUS.COMMIT_WAIT;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
          "extendedDetails": {}
        }
      ]
    },
    "a2200a64175867a1c66cf8338f536ccaaaa36508": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-5519. COMMIT handler should update the commit status after sync. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1542437 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/11/13 4:57 PM",
      "commitName": "a2200a64175867a1c66cf8338f536ccaaaa36508",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "07/11/13 1:49 PM",
      "commitNameOld": "3fccdec6e0a8e9305fc75921211c3745eddb9c45",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 8.13,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n-  private synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n+  synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n       Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n     if (!activeState) {\n       if (pendingWrites.isEmpty()) {\n         return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n       } else {\n         // TODO: return success if already committed\n         return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n       }\n     }\n \n     long flushed \u003d getFlushedOffset();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n     }\n \n     if (commitOffset \u003e 0) {\n       if (commitOffset \u003e flushed) {\n         CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n             preOpAttr);\n         pendingCommits.put(commitOffset, commitCtx);\n         return COMMIT_STATUS.COMMIT_WAIT;\n       } else {\n         return COMMIT_STATUS.COMMIT_DO_SYNC;\n       }\n     }\n \n     Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n \n     // Commit whole file, commitOffset \u003d\u003d 0\n     if (pendingWrites.isEmpty()) {\n       // Note that, there is no guarantee data is synced. TODO: We could still\n       // do a sync here though the output stream might be closed.\n       return COMMIT_STATUS.COMMIT_FINISHED;\n     } else {\n       // Insert commit\n       long maxOffset \u003d key.getKey().getMax() - 1;\n       Preconditions.checkState(maxOffset \u003e 0);\n       CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n       pendingCommits.put(maxOffset, commitCtx);\n       return COMMIT_STATUS.COMMIT_WAIT;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n\n    long flushed \u003d getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    }\n\n    if (commitOffset \u003e 0) {\n      if (commitOffset \u003e flushed) {\n        CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n            preOpAttr);\n        pendingCommits.put(commitOffset, commitCtx);\n        return COMMIT_STATUS.COMMIT_WAIT;\n      } else {\n        return COMMIT_STATUS.COMMIT_DO_SYNC;\n      }\n    }\n\n    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset \u003d\u003d 0\n    if (pendingWrites.isEmpty()) {\n      // Note that, there is no guarantee data is synced. TODO: We could still\n      // do a sync here though the output stream might be closed.\n      return COMMIT_STATUS.COMMIT_FINISHED;\n    } else {\n      // Insert commit\n      long maxOffset \u003d key.getKey().getMax() - 1;\n      Preconditions.checkState(maxOffset \u003e 0);\n      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n      pendingCommits.put(maxOffset, commitCtx);\n      return COMMIT_STATUS.COMMIT_WAIT;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {
        "oldValue": "[private, synchronized]",
        "newValue": "[synchronized]"
      }
    },
    "5c02d2f6225144772dcb975d3144b057b71d6476": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-5281. COMMIT request should not block. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1530461 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/10/13 4:40 PM",
      "commitName": "5c02d2f6225144772dcb975d3144b057b71d6476",
      "commitAuthor": "Brandon Li",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5281. COMMIT request should not block. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1530461 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/10/13 4:40 PM",
          "commitName": "5c02d2f6225144772dcb975d3144b057b71d6476",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "06/10/13 7:57 PM",
          "commitNameOld": "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 1.86,
          "commitsBetweenForRepo": 12,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,43 @@\n-  private int checkCommitInternal(long commitOffset) {\n-    if (commitOffset \u003d\u003d 0) {\n-      // Commit whole file\n-      commitOffset \u003d nextOffset.get();\n+  private synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n+      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n+    if (!activeState) {\n+      if (pendingWrites.isEmpty()) {\n+        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n+      } else {\n+        // TODO: return success if already committed\n+        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n+      }\n     }\n \n     long flushed \u003d getFlushedOffset();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n     }\n-    if (flushed \u003c commitOffset) {\n-      // Keep stream active\n-      updateLastAccessTime();\n-      return COMMIT_WAIT;\n-    }\n \n-    int ret \u003d COMMIT_WAIT;\n-    try {\n-      // Sync file data and length\n-      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n-      // Nothing to do for metadata since attr related change is pass-through\n-      ret \u003d COMMIT_FINISHED;\n-    } catch (ClosedChannelException cce) { \n-      ret \u003d COMMIT_INACTIVE_CTX;\n-      if (pendingWrites.isEmpty()) {\n-        ret \u003d COMMIT_INACTIVE_CTX;\n+    if (commitOffset \u003e 0) {\n+      if (commitOffset \u003e flushed) {\n+        CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n+            preOpAttr);\n+        pendingCommits.put(commitOffset, commitCtx);\n+        return COMMIT_STATUS.COMMIT_WAIT;\n       } else {\n-        ret \u003d COMMIT_INACTIVE_WITH_PENDING_WRITE;\n+        return COMMIT_STATUS.COMMIT_DO_SYNC;\n       }\n-    } catch (IOException e) {\n-      LOG.error(\"Got stream error during data sync:\" + e);\n-      // Do nothing. Stream will be closed eventually by StreamMonitor.\n-      ret \u003d COMMIT_ERROR;\n     }\n \n-    // Keep stream active\n-    updateLastAccessTime();\n-    return ret;\n+    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n+\n+    // Commit whole file, commitOffset \u003d\u003d 0\n+    if (pendingWrites.isEmpty()) {\n+      // Note that, there is no guarantee data is synced. TODO: We could still\n+      // do a sync here though the output stream might be closed.\n+      return COMMIT_STATUS.COMMIT_FINISHED;\n+    } else {\n+      // Insert commit\n+      long maxOffset \u003d key.getKey().getMax() - 1;\n+      Preconditions.checkState(maxOffset \u003e 0);\n+      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n+      pendingCommits.put(maxOffset, commitCtx);\n+      return COMMIT_STATUS.COMMIT_WAIT;\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n\n    long flushed \u003d getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    }\n\n    if (commitOffset \u003e 0) {\n      if (commitOffset \u003e flushed) {\n        CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n            preOpAttr);\n        pendingCommits.put(commitOffset, commitCtx);\n        return COMMIT_STATUS.COMMIT_WAIT;\n      } else {\n        return COMMIT_STATUS.COMMIT_DO_SYNC;\n      }\n    }\n\n    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset \u003d\u003d 0\n    if (pendingWrites.isEmpty()) {\n      // Note that, there is no guarantee data is synced. TODO: We could still\n      // do a sync here though the output stream might be closed.\n      return COMMIT_STATUS.COMMIT_FINISHED;\n    } else {\n      // Insert commit\n      long maxOffset \u003d key.getKey().getMax() - 1;\n      Preconditions.checkState(maxOffset \u003e 0);\n      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n      pendingCommits.put(maxOffset, commitCtx);\n      return COMMIT_STATUS.COMMIT_WAIT;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
          "extendedDetails": {
            "oldValue": "[commitOffset-long]",
            "newValue": "[commitOffset-long, channel-Channel, xid-int, preOpAttr-Nfs3FileAttributes]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5281. COMMIT request should not block. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1530461 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/10/13 4:40 PM",
          "commitName": "5c02d2f6225144772dcb975d3144b057b71d6476",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "06/10/13 7:57 PM",
          "commitNameOld": "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 1.86,
          "commitsBetweenForRepo": 12,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,43 @@\n-  private int checkCommitInternal(long commitOffset) {\n-    if (commitOffset \u003d\u003d 0) {\n-      // Commit whole file\n-      commitOffset \u003d nextOffset.get();\n+  private synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n+      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n+    if (!activeState) {\n+      if (pendingWrites.isEmpty()) {\n+        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n+      } else {\n+        // TODO: return success if already committed\n+        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n+      }\n     }\n \n     long flushed \u003d getFlushedOffset();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n     }\n-    if (flushed \u003c commitOffset) {\n-      // Keep stream active\n-      updateLastAccessTime();\n-      return COMMIT_WAIT;\n-    }\n \n-    int ret \u003d COMMIT_WAIT;\n-    try {\n-      // Sync file data and length\n-      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n-      // Nothing to do for metadata since attr related change is pass-through\n-      ret \u003d COMMIT_FINISHED;\n-    } catch (ClosedChannelException cce) { \n-      ret \u003d COMMIT_INACTIVE_CTX;\n-      if (pendingWrites.isEmpty()) {\n-        ret \u003d COMMIT_INACTIVE_CTX;\n+    if (commitOffset \u003e 0) {\n+      if (commitOffset \u003e flushed) {\n+        CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n+            preOpAttr);\n+        pendingCommits.put(commitOffset, commitCtx);\n+        return COMMIT_STATUS.COMMIT_WAIT;\n       } else {\n-        ret \u003d COMMIT_INACTIVE_WITH_PENDING_WRITE;\n+        return COMMIT_STATUS.COMMIT_DO_SYNC;\n       }\n-    } catch (IOException e) {\n-      LOG.error(\"Got stream error during data sync:\" + e);\n-      // Do nothing. Stream will be closed eventually by StreamMonitor.\n-      ret \u003d COMMIT_ERROR;\n     }\n \n-    // Keep stream active\n-    updateLastAccessTime();\n-    return ret;\n+    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n+\n+    // Commit whole file, commitOffset \u003d\u003d 0\n+    if (pendingWrites.isEmpty()) {\n+      // Note that, there is no guarantee data is synced. TODO: We could still\n+      // do a sync here though the output stream might be closed.\n+      return COMMIT_STATUS.COMMIT_FINISHED;\n+    } else {\n+      // Insert commit\n+      long maxOffset \u003d key.getKey().getMax() - 1;\n+      Preconditions.checkState(maxOffset \u003e 0);\n+      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n+      pendingCommits.put(maxOffset, commitCtx);\n+      return COMMIT_STATUS.COMMIT_WAIT;\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n\n    long flushed \u003d getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    }\n\n    if (commitOffset \u003e 0) {\n      if (commitOffset \u003e flushed) {\n        CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n            preOpAttr);\n        pendingCommits.put(commitOffset, commitCtx);\n        return COMMIT_STATUS.COMMIT_WAIT;\n      } else {\n        return COMMIT_STATUS.COMMIT_DO_SYNC;\n      }\n    }\n\n    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset \u003d\u003d 0\n    if (pendingWrites.isEmpty()) {\n      // Note that, there is no guarantee data is synced. TODO: We could still\n      // do a sync here though the output stream might be closed.\n      return COMMIT_STATUS.COMMIT_FINISHED;\n    } else {\n      // Insert commit\n      long maxOffset \u003d key.getKey().getMax() - 1;\n      Preconditions.checkState(maxOffset \u003e 0);\n      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n      pendingCommits.put(maxOffset, commitCtx);\n      return COMMIT_STATUS.COMMIT_WAIT;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
          "extendedDetails": {
            "oldValue": "int",
            "newValue": "COMMIT_STATUS"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5281. COMMIT request should not block. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1530461 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/10/13 4:40 PM",
          "commitName": "5c02d2f6225144772dcb975d3144b057b71d6476",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "06/10/13 7:57 PM",
          "commitNameOld": "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 1.86,
          "commitsBetweenForRepo": 12,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,43 @@\n-  private int checkCommitInternal(long commitOffset) {\n-    if (commitOffset \u003d\u003d 0) {\n-      // Commit whole file\n-      commitOffset \u003d nextOffset.get();\n+  private synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n+      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n+    if (!activeState) {\n+      if (pendingWrites.isEmpty()) {\n+        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n+      } else {\n+        // TODO: return success if already committed\n+        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n+      }\n     }\n \n     long flushed \u003d getFlushedOffset();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n     }\n-    if (flushed \u003c commitOffset) {\n-      // Keep stream active\n-      updateLastAccessTime();\n-      return COMMIT_WAIT;\n-    }\n \n-    int ret \u003d COMMIT_WAIT;\n-    try {\n-      // Sync file data and length\n-      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n-      // Nothing to do for metadata since attr related change is pass-through\n-      ret \u003d COMMIT_FINISHED;\n-    } catch (ClosedChannelException cce) { \n-      ret \u003d COMMIT_INACTIVE_CTX;\n-      if (pendingWrites.isEmpty()) {\n-        ret \u003d COMMIT_INACTIVE_CTX;\n+    if (commitOffset \u003e 0) {\n+      if (commitOffset \u003e flushed) {\n+        CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n+            preOpAttr);\n+        pendingCommits.put(commitOffset, commitCtx);\n+        return COMMIT_STATUS.COMMIT_WAIT;\n       } else {\n-        ret \u003d COMMIT_INACTIVE_WITH_PENDING_WRITE;\n+        return COMMIT_STATUS.COMMIT_DO_SYNC;\n       }\n-    } catch (IOException e) {\n-      LOG.error(\"Got stream error during data sync:\" + e);\n-      // Do nothing. Stream will be closed eventually by StreamMonitor.\n-      ret \u003d COMMIT_ERROR;\n     }\n \n-    // Keep stream active\n-    updateLastAccessTime();\n-    return ret;\n+    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n+\n+    // Commit whole file, commitOffset \u003d\u003d 0\n+    if (pendingWrites.isEmpty()) {\n+      // Note that, there is no guarantee data is synced. TODO: We could still\n+      // do a sync here though the output stream might be closed.\n+      return COMMIT_STATUS.COMMIT_FINISHED;\n+    } else {\n+      // Insert commit\n+      long maxOffset \u003d key.getKey().getMax() - 1;\n+      Preconditions.checkState(maxOffset \u003e 0);\n+      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n+      pendingCommits.put(maxOffset, commitCtx);\n+      return COMMIT_STATUS.COMMIT_WAIT;\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n\n    long flushed \u003d getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    }\n\n    if (commitOffset \u003e 0) {\n      if (commitOffset \u003e flushed) {\n        CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n            preOpAttr);\n        pendingCommits.put(commitOffset, commitCtx);\n        return COMMIT_STATUS.COMMIT_WAIT;\n      } else {\n        return COMMIT_STATUS.COMMIT_DO_SYNC;\n      }\n    }\n\n    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset \u003d\u003d 0\n    if (pendingWrites.isEmpty()) {\n      // Note that, there is no guarantee data is synced. TODO: We could still\n      // do a sync here though the output stream might be closed.\n      return COMMIT_STATUS.COMMIT_FINISHED;\n    } else {\n      // Insert commit\n      long maxOffset \u003d key.getKey().getMax() - 1;\n      Preconditions.checkState(maxOffset \u003e 0);\n      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n      pendingCommits.put(maxOffset, commitCtx);\n      return COMMIT_STATUS.COMMIT_WAIT;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[private, synchronized]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5281. COMMIT request should not block. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1530461 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/10/13 4:40 PM",
          "commitName": "5c02d2f6225144772dcb975d3144b057b71d6476",
          "commitAuthor": "Brandon Li",
          "commitDateOld": "06/10/13 7:57 PM",
          "commitNameOld": "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 1.86,
          "commitsBetweenForRepo": 12,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,43 @@\n-  private int checkCommitInternal(long commitOffset) {\n-    if (commitOffset \u003d\u003d 0) {\n-      // Commit whole file\n-      commitOffset \u003d nextOffset.get();\n+  private synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n+      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n+    if (!activeState) {\n+      if (pendingWrites.isEmpty()) {\n+        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n+      } else {\n+        // TODO: return success if already committed\n+        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n+      }\n     }\n \n     long flushed \u003d getFlushedOffset();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n     }\n-    if (flushed \u003c commitOffset) {\n-      // Keep stream active\n-      updateLastAccessTime();\n-      return COMMIT_WAIT;\n-    }\n \n-    int ret \u003d COMMIT_WAIT;\n-    try {\n-      // Sync file data and length\n-      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n-      // Nothing to do for metadata since attr related change is pass-through\n-      ret \u003d COMMIT_FINISHED;\n-    } catch (ClosedChannelException cce) { \n-      ret \u003d COMMIT_INACTIVE_CTX;\n-      if (pendingWrites.isEmpty()) {\n-        ret \u003d COMMIT_INACTIVE_CTX;\n+    if (commitOffset \u003e 0) {\n+      if (commitOffset \u003e flushed) {\n+        CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n+            preOpAttr);\n+        pendingCommits.put(commitOffset, commitCtx);\n+        return COMMIT_STATUS.COMMIT_WAIT;\n       } else {\n-        ret \u003d COMMIT_INACTIVE_WITH_PENDING_WRITE;\n+        return COMMIT_STATUS.COMMIT_DO_SYNC;\n       }\n-    } catch (IOException e) {\n-      LOG.error(\"Got stream error during data sync:\" + e);\n-      // Do nothing. Stream will be closed eventually by StreamMonitor.\n-      ret \u003d COMMIT_ERROR;\n     }\n \n-    // Keep stream active\n-    updateLastAccessTime();\n-    return ret;\n+    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n+\n+    // Commit whole file, commitOffset \u003d\u003d 0\n+    if (pendingWrites.isEmpty()) {\n+      // Note that, there is no guarantee data is synced. TODO: We could still\n+      // do a sync here though the output stream might be closed.\n+      return COMMIT_STATUS.COMMIT_FINISHED;\n+    } else {\n+      // Insert commit\n+      long maxOffset \u003d key.getKey().getMax() - 1;\n+      Preconditions.checkState(maxOffset \u003e 0);\n+      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n+      pendingCommits.put(maxOffset, commitCtx);\n+      return COMMIT_STATUS.COMMIT_WAIT;\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n\n    long flushed \u003d getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    }\n\n    if (commitOffset \u003e 0) {\n      if (commitOffset \u003e flushed) {\n        CommitCtx commitCtx \u003d new CommitCtx(commitOffset, channel, xid,\n            preOpAttr);\n        pendingCommits.put(commitOffset, commitCtx);\n        return COMMIT_STATUS.COMMIT_WAIT;\n      } else {\n        return COMMIT_STATUS.COMMIT_DO_SYNC;\n      }\n    }\n\n    Entry\u003cOffsetRange, WriteCtx\u003e key \u003d pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset \u003d\u003d 0\n    if (pendingWrites.isEmpty()) {\n      // Note that, there is no guarantee data is synced. TODO: We could still\n      // do a sync here though the output stream might be closed.\n      return COMMIT_STATUS.COMMIT_FINISHED;\n    } else {\n      // Insert commit\n      long maxOffset \u003d key.getKey().getMax() - 1;\n      Preconditions.checkState(maxOffset \u003e 0);\n      CommitCtx commitCtx \u003d new CommitCtx(maxOffset, channel, xid, preOpAttr);\n      pendingCommits.put(maxOffset, commitCtx);\n      return COMMIT_STATUS.COMMIT_WAIT;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
          "extendedDetails": {}
        }
      ]
    },
    "28e3d09230971b32f74284311931525cb7ad1b7c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4971. Move IO operations out of locking in OpenFileCtx. Contributed by Jing Zhao and Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525681 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/09/13 1:02 PM",
      "commitName": "28e3d09230971b32f74284311931525cb7ad1b7c",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "17/09/13 11:08 PM",
      "commitNameOld": "5e18410e06dd63113c49029894007e0878312903",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 5.58,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,39 @@\n   private int checkCommitInternal(long commitOffset) {\n     if (commitOffset \u003d\u003d 0) {\n       // Commit whole file\n-      commitOffset \u003d getNextOffsetUnprotected();\n+      commitOffset \u003d nextOffset.get();\n     }\n \n     long flushed \u003d getFlushedOffset();\n-    LOG.info(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n+    }\n     if (flushed \u003c commitOffset) {\n       // Keep stream active\n       updateLastAccessTime();\n       return COMMIT_WAIT;\n     }\n \n     int ret \u003d COMMIT_WAIT;\n     try {\n       // Sync file data and length\n       fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n       // Nothing to do for metadata since attr related change is pass-through\n       ret \u003d COMMIT_FINISHED;\n+    } catch (ClosedChannelException cce) { \n+      ret \u003d COMMIT_INACTIVE_CTX;\n+      if (pendingWrites.isEmpty()) {\n+        ret \u003d COMMIT_INACTIVE_CTX;\n+      } else {\n+        ret \u003d COMMIT_INACTIVE_WITH_PENDING_WRITE;\n+      }\n     } catch (IOException e) {\n       LOG.error(\"Got stream error during data sync:\" + e);\n       // Do nothing. Stream will be closed eventually by StreamMonitor.\n       ret \u003d COMMIT_ERROR;\n     }\n \n     // Keep stream active\n     updateLastAccessTime();\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private int checkCommitInternal(long commitOffset) {\n    if (commitOffset \u003d\u003d 0) {\n      // Commit whole file\n      commitOffset \u003d nextOffset.get();\n    }\n\n    long flushed \u003d getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    }\n    if (flushed \u003c commitOffset) {\n      // Keep stream active\n      updateLastAccessTime();\n      return COMMIT_WAIT;\n    }\n\n    int ret \u003d COMMIT_WAIT;\n    try {\n      // Sync file data and length\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n      // Nothing to do for metadata since attr related change is pass-through\n      ret \u003d COMMIT_FINISHED;\n    } catch (ClosedChannelException cce) { \n      ret \u003d COMMIT_INACTIVE_CTX;\n      if (pendingWrites.isEmpty()) {\n        ret \u003d COMMIT_INACTIVE_CTX;\n      } else {\n        ret \u003d COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    } catch (IOException e) {\n      LOG.error(\"Got stream error during data sync:\" + e);\n      // Do nothing. Stream will be closed eventually by StreamMonitor.\n      ret \u003d COMMIT_ERROR;\n    }\n\n    // Keep stream active\n    updateLastAccessTime();\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "c9b89de0eacf15f21faa3a7ba30d4773f571c9a4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5110 Change FSDataOutputStream to HdfsDataOutputStream for opened streams to fix type cast error. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1515624 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/13 2:54 PM",
      "commitName": "c9b89de0eacf15f21faa3a7ba30d4773f571c9a4",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "10/07/13 10:01 AM",
      "commitNameOld": "58d75576c4d2a03d4954174bc223ed0334b34fee",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 40.2,
      "commitsBetweenForRepo": 257,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n   private int checkCommitInternal(long commitOffset) {\n     if (commitOffset \u003d\u003d 0) {\n       // Commit whole file\n       commitOffset \u003d getNextOffsetUnprotected();\n     }\n \n     long flushed \u003d getFlushedOffset();\n     LOG.info(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n     if (flushed \u003c commitOffset) {\n       // Keep stream active\n       updateLastAccessTime();\n       return COMMIT_WAIT;\n     }\n \n     int ret \u003d COMMIT_WAIT;\n     try {\n       // Sync file data and length\n-      ((HdfsDataOutputStream) fos).hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n+      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n       // Nothing to do for metadata since attr related change is pass-through\n       ret \u003d COMMIT_FINISHED;\n     } catch (IOException e) {\n       LOG.error(\"Got stream error during data sync:\" + e);\n       // Do nothing. Stream will be closed eventually by StreamMonitor.\n       ret \u003d COMMIT_ERROR;\n     }\n \n     // Keep stream active\n     updateLastAccessTime();\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private int checkCommitInternal(long commitOffset) {\n    if (commitOffset \u003d\u003d 0) {\n      // Commit whole file\n      commitOffset \u003d getNextOffsetUnprotected();\n    }\n\n    long flushed \u003d getFlushedOffset();\n    LOG.info(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    if (flushed \u003c commitOffset) {\n      // Keep stream active\n      updateLastAccessTime();\n      return COMMIT_WAIT;\n    }\n\n    int ret \u003d COMMIT_WAIT;\n    try {\n      // Sync file data and length\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n      // Nothing to do for metadata since attr related change is pass-through\n      ret \u003d COMMIT_FINISHED;\n    } catch (IOException e) {\n      LOG.error(\"Got stream error during data sync:\" + e);\n      // Do nothing. Stream will be closed eventually by StreamMonitor.\n      ret \u003d COMMIT_ERROR;\n    }\n\n    // Keep stream active\n    updateLastAccessTime();\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "37f587563a943a827fbff865f5302bac6d202415": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4762 Provide HDFS based NFSv3 and Mountd implementation. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1499029 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/07/13 10:31 AM",
      "commitName": "37f587563a943a827fbff865f5302bac6d202415",
      "commitAuthor": "Brandon Li",
      "diff": "@@ -0,0 +1,30 @@\n+  private int checkCommitInternal(long commitOffset) {\n+    if (commitOffset \u003d\u003d 0) {\n+      // Commit whole file\n+      commitOffset \u003d getNextOffsetUnprotected();\n+    }\n+\n+    long flushed \u003d getFlushedOffset();\n+    LOG.info(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n+    if (flushed \u003c commitOffset) {\n+      // Keep stream active\n+      updateLastAccessTime();\n+      return COMMIT_WAIT;\n+    }\n+\n+    int ret \u003d COMMIT_WAIT;\n+    try {\n+      // Sync file data and length\n+      ((HdfsDataOutputStream) fos).hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n+      // Nothing to do for metadata since attr related change is pass-through\n+      ret \u003d COMMIT_FINISHED;\n+    } catch (IOException e) {\n+      LOG.error(\"Got stream error during data sync:\" + e);\n+      // Do nothing. Stream will be closed eventually by StreamMonitor.\n+      ret \u003d COMMIT_ERROR;\n+    }\n+\n+    // Keep stream active\n+    updateLastAccessTime();\n+    return ret;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private int checkCommitInternal(long commitOffset) {\n    if (commitOffset \u003d\u003d 0) {\n      // Commit whole file\n      commitOffset \u003d getNextOffsetUnprotected();\n    }\n\n    long flushed \u003d getFlushedOffset();\n    LOG.info(\"getFlushedOffset\u003d\" + flushed + \" commitOffset\u003d\" + commitOffset);\n    if (flushed \u003c commitOffset) {\n      // Keep stream active\n      updateLastAccessTime();\n      return COMMIT_WAIT;\n    }\n\n    int ret \u003d COMMIT_WAIT;\n    try {\n      // Sync file data and length\n      ((HdfsDataOutputStream) fos).hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n      // Nothing to do for metadata since attr related change is pass-through\n      ret \u003d COMMIT_FINISHED;\n    } catch (IOException e) {\n      LOG.error(\"Got stream error during data sync:\" + e);\n      // Do nothing. Stream will be closed eventually by StreamMonitor.\n      ret \u003d COMMIT_ERROR;\n    }\n\n    // Keep stream active\n    updateLastAccessTime();\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java"
    }
  }
}