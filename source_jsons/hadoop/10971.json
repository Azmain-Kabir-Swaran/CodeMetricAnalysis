{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockSender.java",
  "functionName": "verifyChecksum",
  "functionId": "verifyChecksum___buf-byte[](modifiers-final)__dataOffset-int(modifiers-final)__datalen-int(modifiers-final)__numChunks-int(modifiers-final)__checksumOffset-int(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
  "functionStartLine": 712,
  "functionEndLine": 736,
  "numCommitsSeen": 65,
  "timeTaken": 2548,
  "changeHistory": [
    "5d1609ddf275e4907bd224bf618e2aad4b262888",
    "e90a5b40430cc1fbce075d34b31e3cc05fd9831f"
  ],
  "changeHistoryShort": {
    "5d1609ddf275e4907bd224bf618e2aad4b262888": "Ybodychange",
    "e90a5b40430cc1fbce075d34b31e3cc05fd9831f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "5d1609ddf275e4907bd224bf618e2aad4b262888": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10625. VolumeScanner to report why a block is found bad. Contributed by Rushabh S Shah and Yiqun Lin.\n",
      "commitDate": "29/08/16 1:59 PM",
      "commitName": "5d1609ddf275e4907bd224bf618e2aad4b262888",
      "commitAuthor": "Yongjun Zhang",
      "commitDateOld": "17/08/16 4:29 PM",
      "commitNameOld": "ca13e7971d0db0705d5e36bcf03ead3cab5ab0d7",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 11.9,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,25 @@\n   public void verifyChecksum(final byte[] buf, final int dataOffset,\n       final int datalen, final int numChunks, final int checksumOffset)\n       throws ChecksumException {\n     int dOff \u003d dataOffset;\n     int cOff \u003d checksumOffset;\n     int dLeft \u003d datalen;\n \n     for (int i \u003d 0; i \u003c numChunks; i++) {\n       checksum.reset();\n       int dLen \u003d Math.min(dLeft, chunkSize);\n       checksum.update(buf, dOff, dLen);\n       if (!checksum.compare(buf, cOff)) {\n         long failedPos \u003d offset + datalen - dLeft;\n-        throw new ChecksumException(\"Checksum failed at \" + failedPos,\n-            failedPos);\n+        StringBuilder replicaInfoString \u003d new StringBuilder();\n+        if (replica !\u003d null) {\n+          replicaInfoString.append(\" for replica: \" + replica.toString());\n+        }\n+        throw new ChecksumException(\"Checksum failed at \" + failedPos\n+            + replicaInfoString, failedPos);\n       }\n       dLeft -\u003d dLen;\n       dOff +\u003d dLen;\n       cOff +\u003d checksumSize;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void verifyChecksum(final byte[] buf, final int dataOffset,\n      final int datalen, final int numChunks, final int checksumOffset)\n      throws ChecksumException {\n    int dOff \u003d dataOffset;\n    int cOff \u003d checksumOffset;\n    int dLeft \u003d datalen;\n\n    for (int i \u003d 0; i \u003c numChunks; i++) {\n      checksum.reset();\n      int dLen \u003d Math.min(dLeft, chunkSize);\n      checksum.update(buf, dOff, dLen);\n      if (!checksum.compare(buf, cOff)) {\n        long failedPos \u003d offset + datalen - dLeft;\n        StringBuilder replicaInfoString \u003d new StringBuilder();\n        if (replica !\u003d null) {\n          replicaInfoString.append(\" for replica: \" + replica.toString());\n        }\n        throw new ChecksumException(\"Checksum failed at \" + failedPos\n            + replicaInfoString, failedPos);\n      }\n      dLeft -\u003d dLen;\n      dOff +\u003d dLen;\n      cOff +\u003d checksumSize;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "e90a5b40430cc1fbce075d34b31e3cc05fd9831f": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2371. Refactor BlockSender.java for better readability. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1177161 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/09/11 9:40 PM",
      "commitName": "e90a5b40430cc1fbce075d34b31e3cc05fd9831f",
      "commitAuthor": "Suresh Srinivas",
      "diff": "@@ -0,0 +1,21 @@\n+  public void verifyChecksum(final byte[] buf, final int dataOffset,\n+      final int datalen, final int numChunks, final int checksumOffset)\n+      throws ChecksumException {\n+    int dOff \u003d dataOffset;\n+    int cOff \u003d checksumOffset;\n+    int dLeft \u003d datalen;\n+\n+    for (int i \u003d 0; i \u003c numChunks; i++) {\n+      checksum.reset();\n+      int dLen \u003d Math.min(dLeft, chunkSize);\n+      checksum.update(buf, dOff, dLen);\n+      if (!checksum.compare(buf, cOff)) {\n+        long failedPos \u003d offset + datalen - dLeft;\n+        throw new ChecksumException(\"Checksum failed at \" + failedPos,\n+            failedPos);\n+      }\n+      dLeft -\u003d dLen;\n+      dOff +\u003d dLen;\n+      cOff +\u003d checksumSize;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void verifyChecksum(final byte[] buf, final int dataOffset,\n      final int datalen, final int numChunks, final int checksumOffset)\n      throws ChecksumException {\n    int dOff \u003d dataOffset;\n    int cOff \u003d checksumOffset;\n    int dLeft \u003d datalen;\n\n    for (int i \u003d 0; i \u003c numChunks; i++) {\n      checksum.reset();\n      int dLen \u003d Math.min(dLeft, chunkSize);\n      checksum.update(buf, dOff, dLen);\n      if (!checksum.compare(buf, cOff)) {\n        long failedPos \u003d offset + datalen - dLeft;\n        throw new ChecksumException(\"Checksum failed at \" + failedPos,\n            failedPos);\n      }\n      dLeft -\u003d dLen;\n      dOff +\u003d dLen;\n      cOff +\u003d checksumSize;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java"
    }
  }
}