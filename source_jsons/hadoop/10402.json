{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataXceiver.java",
  "functionName": "transferBlock",
  "functionId": "transferBlock___blk-ExtendedBlock(modifiers-final)__blockToken-Token__BlockTokenIdentifier__(modifiers-final)__clientName-String(modifiers-final)__targets-DatanodeInfo[](modifiers-final)__targetStorageTypes-StorageType[](modifiers-final)__targetStorageIds-String[](modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
  "functionStartLine": 959,
  "functionEndLine": 985,
  "numCommitsSeen": 275,
  "timeTaken": 9739,
  "changeHistory": [
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
    "a3954ccab148bddc290cb96528e63ff19799bcc9",
    "2f73396b5901fd5fe29f6cd76fc1b3134b854b37",
    "38c4c14472996562eb3d610649246770c2888c6b",
    "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e",
    "2d4f3e567e4bb8068c028de12df118a4f3fa6343",
    "86cad007d7d6366b293bb9a073814889081c8662",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
    "837e17b2eac1471d93e2eff395272063b265fee7",
    "239b2742d0e80d13c970fd062af4930e672fe903",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "7aa2889f822a970b8b1edb8bc58aab67412877ae",
    "905a127850d5e0cba85c2e075f989fa0f5cf129a",
    "c46876982ed90d0819a94b518f6135b82334d10d",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": "Ybodychange",
    "a3954ccab148bddc290cb96528e63ff19799bcc9": "Ymultichange(Yparameterchange,Ybodychange)",
    "2f73396b5901fd5fe29f6cd76fc1b3134b854b37": "Ybodychange",
    "38c4c14472996562eb3d610649246770c2888c6b": "Ybodychange",
    "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e": "Ybodychange",
    "2d4f3e567e4bb8068c028de12df118a4f3fa6343": "Ybodychange",
    "86cad007d7d6366b293bb9a073814889081c8662": "Ybodychange",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": "Ymultichange(Yparameterchange,Ybodychange)",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": "Ybodychange",
    "837e17b2eac1471d93e2eff395272063b265fee7": "Ybodychange",
    "239b2742d0e80d13c970fd062af4930e672fe903": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ybodychange",
    "7aa2889f822a970b8b1edb8bc58aab67412877ae": "Ybodychange",
    "905a127850d5e0cba85c2e075f989fa0f5cf129a": "Ybodychange",
    "c46876982ed90d0819a94b518f6135b82334d10d": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10571. Use Log.*(Object, Throwable) overload to log exceptions.\nContributed by Andras Bokor.\n",
      "commitDate": "14/02/18 8:20 AM",
      "commitName": "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "01/11/17 1:41 AM",
      "commitNameOld": "56b88b06705441f6f171eec7fb2fa77946ca204b",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 105.32,
      "commitsBetweenForRepo": 696,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,27 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets,\n       final StorageType[] targetStorageTypes,\n       final String[] targetStorageIds) throws IOException {\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, blk, blockToken, Op.TRANSFER_BLOCK,\n         BlockTokenIdentifier.AccessMode.COPY, targetStorageTypes,\n         targetStorageIds);\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets,\n           targetStorageTypes, targetStorageIds, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } catch (IOException ioe) {\n-      LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n+      LOG.info(\"transferBlock {} received exception {}\",\n+          blk, ioe.toString());\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes,\n      final String[] targetStorageIds) throws IOException {\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, blk, blockToken, Op.TRANSFER_BLOCK,\n        BlockTokenIdentifier.AccessMode.COPY, targetStorageTypes,\n        targetStorageIds);\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets,\n          targetStorageTypes, targetStorageIds, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } catch (IOException ioe) {\n      LOG.info(\"transferBlock {} received exception {}\",\n          blk, ioe.toString());\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "a3954ccab148bddc290cb96528e63ff19799bcc9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
      "commitDate": "05/05/17 12:01 PM",
      "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthor": "Chris Douglas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "25/04/17 11:57 PM",
          "commitNameOld": "2f73396b5901fd5fe29f6cd76fc1b3134b854b37",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 9.5,
          "commitsBetweenForRepo": 64,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,26 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets,\n-      final StorageType[] targetStorageTypes) throws IOException {\n+      final StorageType[] targetStorageTypes,\n+      final String[] targetStorageIds) throws IOException {\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, blk, blockToken, Op.TRANSFER_BLOCK,\n-        BlockTokenIdentifier.AccessMode.COPY, targetStorageTypes);\n+        BlockTokenIdentifier.AccessMode.COPY, targetStorageTypes,\n+        targetStorageIds);\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets,\n-          targetStorageTypes, clientName);\n+          targetStorageTypes, targetStorageIds, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } catch (IOException ioe) {\n       LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes,\n      final String[] targetStorageIds) throws IOException {\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, blk, blockToken, Op.TRANSFER_BLOCK,\n        BlockTokenIdentifier.AccessMode.COPY, targetStorageTypes,\n        targetStorageIds);\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets,\n          targetStorageTypes, targetStorageIds, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } catch (IOException ioe) {\n      LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {
            "oldValue": "[blk-ExtendedBlock(modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final), clientName-String(modifiers-final), targets-DatanodeInfo[](modifiers-final), targetStorageTypes-StorageType[](modifiers-final)]",
            "newValue": "[blk-ExtendedBlock(modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final), clientName-String(modifiers-final), targets-DatanodeInfo[](modifiers-final), targetStorageTypes-StorageType[](modifiers-final), targetStorageIds-String[](modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "25/04/17 11:57 PM",
          "commitNameOld": "2f73396b5901fd5fe29f6cd76fc1b3134b854b37",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 9.5,
          "commitsBetweenForRepo": 64,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,26 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets,\n-      final StorageType[] targetStorageTypes) throws IOException {\n+      final StorageType[] targetStorageTypes,\n+      final String[] targetStorageIds) throws IOException {\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     checkAccess(out, true, blk, blockToken, Op.TRANSFER_BLOCK,\n-        BlockTokenIdentifier.AccessMode.COPY, targetStorageTypes);\n+        BlockTokenIdentifier.AccessMode.COPY, targetStorageTypes,\n+        targetStorageIds);\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets,\n-          targetStorageTypes, clientName);\n+          targetStorageTypes, targetStorageIds, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } catch (IOException ioe) {\n       LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes,\n      final String[] targetStorageIds) throws IOException {\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, blk, blockToken, Op.TRANSFER_BLOCK,\n        BlockTokenIdentifier.AccessMode.COPY, targetStorageTypes,\n        targetStorageIds);\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets,\n          targetStorageTypes, targetStorageIds, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } catch (IOException ioe) {\n      LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {}
        }
      ]
    },
    "2f73396b5901fd5fe29f6cd76fc1b3134b854b37": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6708. StorageType should be encoded in the block token. Contributed by Ewan Higgs\n",
      "commitDate": "25/04/17 11:57 PM",
      "commitName": "2f73396b5901fd5fe29f6cd76fc1b3134b854b37",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "12/04/17 11:40 AM",
      "commitNameOld": "abce61335678da753cd0f7965a236370274abee8",
      "commitAuthorOld": "Anu Engineer",
      "daysBetweenCommits": 13.51,
      "commitsBetweenForRepo": 57,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets,\n       final StorageType[] targetStorageTypes) throws IOException {\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n-    checkAccess(out, true, blk, blockToken,\n-        Op.TRANSFER_BLOCK, BlockTokenIdentifier.AccessMode.COPY);\n+    checkAccess(out, true, blk, blockToken, Op.TRANSFER_BLOCK,\n+        BlockTokenIdentifier.AccessMode.COPY, targetStorageTypes);\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets,\n           targetStorageTypes, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } catch (IOException ioe) {\n       LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes) throws IOException {\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, blk, blockToken, Op.TRANSFER_BLOCK,\n        BlockTokenIdentifier.AccessMode.COPY, targetStorageTypes);\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets,\n          targetStorageTypes, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } catch (IOException ioe) {\n      LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "38c4c14472996562eb3d610649246770c2888c6b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9574. Reduce client failures during datanode restart. Contributed by Kihwal Lee.\n",
      "commitDate": "08/01/16 9:13 AM",
      "commitName": "38c4c14472996562eb3d610649246770c2888c6b",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "04/01/16 2:32 PM",
      "commitNameOld": "778146eaae5b1e17928a1f26fb1e46536a6ee510",
      "commitAuthorOld": "Uma Mahesh",
      "daysBetweenCommits": 3.78,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets,\n       final StorageType[] targetStorageTypes) throws IOException {\n-    checkAccess(socketOut, true, blk, blockToken,\n-        Op.TRANSFER_BLOCK, BlockTokenIdentifier.AccessMode.COPY);\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n+    checkAccess(out, true, blk, blockToken,\n+        Op.TRANSFER_BLOCK, BlockTokenIdentifier.AccessMode.COPY);\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets,\n           targetStorageTypes, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } catch (IOException ioe) {\n       LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes) throws IOException {\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenIdentifier.AccessMode.COPY);\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets,\n          targetStorageTypes, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } catch (IOException ioe) {\n      LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8103. Move BlockTokenSecretManager.AccessMode into BlockTokenIdentifier. Contributed by Haohui Mai.\n",
      "commitDate": "10/04/15 4:36 PM",
      "commitName": "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "20/03/15 12:02 PM",
      "commitNameOld": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 21.19,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets,\n       final StorageType[] targetStorageTypes) throws IOException {\n     checkAccess(socketOut, true, blk, blockToken,\n-        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n+        Op.TRANSFER_BLOCK, BlockTokenIdentifier.AccessMode.COPY);\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets,\n           targetStorageTypes, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } catch (IOException ioe) {\n       LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n       incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes) throws IOException {\n    checkAccess(socketOut, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenIdentifier.AccessMode.COPY);\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets,\n          targetStorageTypes, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } catch (IOException ioe) {\n      LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "2d4f3e567e4bb8068c028de12df118a4f3fa6343": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7331. Add Datanode network counts to datanode jmx page. Contributed by Charles Lamb.\n",
      "commitDate": "21/11/14 4:36 PM",
      "commitName": "2d4f3e567e4bb8068c028de12df118a4f3fa6343",
      "commitAuthor": "Aaron T. Myers",
      "commitDateOld": "08/11/14 10:24 PM",
      "commitNameOld": "9ba8d8c7eb65eeb6fe673f04e493d9eedd95a822",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 12.76,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets,\n       final StorageType[] targetStorageTypes) throws IOException {\n     checkAccess(socketOut, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets,\n           targetStorageTypes, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } catch (IOException ioe) {\n       LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n-      datanode.metrics.incrDatanodeNetworkErrors();\n+      incrDatanodeNetworkErrors();\n       throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes) throws IOException {\n    checkAccess(socketOut, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets,\n          targetStorageTypes, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } catch (IOException ioe) {\n      LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "86cad007d7d6366b293bb9a073814889081c8662": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7222. Expose DataNode network errors as a metric. (Charles Lamb via wang)\n",
      "commitDate": "23/10/14 12:53 PM",
      "commitName": "86cad007d7d6366b293bb9a073814889081c8662",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/08/14 9:47 PM",
      "commitNameOld": "a317bd7b02c37bd57743bfad59593ec12f53f4ed",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 56.63,
      "commitsBetweenForRepo": 574,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,24 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets,\n       final StorageType[] targetStorageTypes) throws IOException {\n     checkAccess(socketOut, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets,\n           targetStorageTypes, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n+    } catch (IOException ioe) {\n+      LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n+      datanode.metrics.incrDatanodeNetworkErrors();\n+      throw ioe;\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes) throws IOException {\n    checkAccess(socketOut, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets,\n          targetStorageTypes, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } catch (IOException ioe) {\n      LOG.info(\"transferBlock \" + blk + \" received exception \" + ioe);\n      datanode.metrics.incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/14 12:41 AM",
      "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/14 12:41 AM",
          "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/07/14 11:10 AM",
          "commitNameOld": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 7.56,
          "commitsBetweenForRepo": 68,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,20 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n-      final DatanodeInfo[] targets) throws IOException {\n+      final DatanodeInfo[] targets,\n+      final StorageType[] targetStorageTypes) throws IOException {\n     checkAccess(socketOut, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     try {\n-      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n+      datanode.transferReplicaForPipelineRecovery(blk, targets,\n+          targetStorageTypes, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes) throws IOException {\n    checkAccess(socketOut, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets,\n          targetStorageTypes, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {
            "oldValue": "[blk-ExtendedBlock(modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final), clientName-String(modifiers-final), targets-DatanodeInfo[](modifiers-final)]",
            "newValue": "[blk-ExtendedBlock(modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final), clientName-String(modifiers-final), targets-DatanodeInfo[](modifiers-final), targetStorageTypes-StorageType[](modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/14 12:41 AM",
          "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/07/14 11:10 AM",
          "commitNameOld": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 7.56,
          "commitsBetweenForRepo": 68,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,20 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n-      final DatanodeInfo[] targets) throws IOException {\n+      final DatanodeInfo[] targets,\n+      final StorageType[] targetStorageTypes) throws IOException {\n     checkAccess(socketOut, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     try {\n-      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n+      datanode.transferReplicaForPipelineRecovery(blk, targets,\n+          targetStorageTypes, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets,\n      final StorageType[] targetStorageTypes) throws IOException {\n    checkAccess(socketOut, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets,\n          targetStorageTypes, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {}
        }
      ]
    },
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:34 PM",
      "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "08/01/13 6:39 PM",
      "commitNameOld": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets) throws IOException {\n-    checkAccess(null, true, blk, blockToken,\n+    checkAccess(socketOut, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(socketOut, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "837e17b2eac1471d93e2eff395272063b265fee7": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 6:39 PM",
      "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/01/13 12:44 PM",
      "commitNameOld": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.25,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets) throws IOException {\n-    checkAccess(socketOut, true, blk, blockToken,\n+    checkAccess(null, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "239b2742d0e80d13c970fd062af4930e672fe903": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 12:44 PM",
      "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "28/10/12 4:10 PM",
      "commitNameOld": "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 71.9,
      "commitsBetweenForRepo": 300,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets) throws IOException {\n-    checkAccess(null, true, blk, blockToken,\n+    checkAccess(socketOut, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         getOutputStream());\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(socketOut, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "15/07/12 7:58 PM",
      "commitNameOld": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 22.57,
      "commitsBetweenForRepo": 106,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets) throws IOException {\n     checkAccess(null, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n     previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n-        NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n+        getOutputStream());\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        getOutputStream());\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "7aa2889f822a970b8b1edb8bc58aab67412877ae": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3375. Put client name in DataXceiver thread name for readBlock and keepalive. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1335270 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/05/12 2:34 PM",
      "commitName": "7aa2889f822a970b8b1edb8bc58aab67412877ae",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "04/05/12 11:50 AM",
      "commitNameOld": "a701c792f880c43ba807f00a92a99dadf89eab0c",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 3.11,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets) throws IOException {\n     checkAccess(null, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n-\n+    previousOpClientName \u003d clientName;\n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n    previousOpClientName \u003d clientName;\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "905a127850d5e0cba85c2e075f989fa0f5cf129a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2562. Refactor DN configuration variables out of DataNode class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203543 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 1:04 AM",
      "commitName": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "31/10/11 10:17 PM",
      "commitNameOld": "1c940637b14eee777a65d153d0d712a1aea3866c",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 17.16,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets) throws IOException {\n     checkAccess(null, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n \n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n-        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n+        NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n       writeResponse(Status.SUCCESS, null, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, dnConf.socketWriteTimeout));\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "c46876982ed90d0819a94b518f6135b82334d10d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2512. Add textual error message to data transfer protocol responses. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195693 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/10/11 2:53 PM",
      "commitName": "c46876982ed90d0819a94b518f6135b82334d10d",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "23/10/11 1:17 PM",
      "commitNameOld": "6e0991704ffda5cf4cff758f0e7086523fa7bcb4",
      "commitAuthorOld": "Konstantin Shvachko",
      "daysBetweenCommits": 8.07,
      "commitsBetweenForRepo": 100,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public void transferBlock(final ExtendedBlock blk,\n       final Token\u003cBlockTokenIdentifier\u003e blockToken,\n       final String clientName,\n       final DatanodeInfo[] targets) throws IOException {\n     checkAccess(null, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n \n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n     try {\n       datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n-      writeResponse(Status.SUCCESS, out);\n+      writeResponse(Status.SUCCESS, null, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, null, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
      }
    },
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/06/11 4:57 PM",
      "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/06/11 4:57 PM",
          "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/06/11 10:12 AM",
          "commitNameOld": "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.28,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,18 @@\n-  protected void opTransferBlock(final DataInputStream in,\n-      final ExtendedBlock blk, final String client,\n-      final DatanodeInfo[] targets,\n-      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+  public void transferBlock(final ExtendedBlock blk,\n+      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n+      final String clientName,\n+      final DatanodeInfo[] targets) throws IOException {\n     checkAccess(null, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n \n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n     try {\n-      datanode.transferReplicaForPipelineRecovery(blk, targets, client);\n+      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n       writeResponse(Status.SUCCESS, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {
            "oldValue": "opTransferBlock",
            "newValue": "transferBlock"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/06/11 4:57 PM",
          "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/06/11 10:12 AM",
          "commitNameOld": "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.28,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,18 @@\n-  protected void opTransferBlock(final DataInputStream in,\n-      final ExtendedBlock blk, final String client,\n-      final DatanodeInfo[] targets,\n-      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+  public void transferBlock(final ExtendedBlock blk,\n+      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n+      final String clientName,\n+      final DatanodeInfo[] targets) throws IOException {\n     checkAccess(null, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n \n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n     try {\n-      datanode.transferReplicaForPipelineRecovery(blk, targets, client);\n+      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n       writeResponse(Status.SUCCESS, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {
            "oldValue": "[in-DataInputStream(modifiers-final), blk-ExtendedBlock(modifiers-final), client-String(modifiers-final), targets-DatanodeInfo[](modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]",
            "newValue": "[blk-ExtendedBlock(modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final), clientName-String(modifiers-final), targets-DatanodeInfo[](modifiers-final)]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/06/11 4:57 PM",
          "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/06/11 10:12 AM",
          "commitNameOld": "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.28,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,18 @@\n-  protected void opTransferBlock(final DataInputStream in,\n-      final ExtendedBlock blk, final String client,\n-      final DatanodeInfo[] targets,\n-      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+  public void transferBlock(final ExtendedBlock blk,\n+      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n+      final String clientName,\n+      final DatanodeInfo[] targets) throws IOException {\n     checkAccess(null, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n \n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n     try {\n-      datanode.transferReplicaForPipelineRecovery(blk, targets, client);\n+      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n       writeResponse(Status.SUCCESS, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {
            "oldValue": "[protected]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/06/11 4:57 PM",
          "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/06/11 10:12 AM",
          "commitNameOld": "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 2.28,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,18 @@\n-  protected void opTransferBlock(final DataInputStream in,\n-      final ExtendedBlock blk, final String client,\n-      final DatanodeInfo[] targets,\n-      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+  public void transferBlock(final ExtendedBlock blk,\n+      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n+      final String clientName,\n+      final DatanodeInfo[] targets) throws IOException {\n     checkAccess(null, true, blk, blockToken,\n         Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n \n     updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n \n     final DataOutputStream out \u003d new DataOutputStream(\n         NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n     try {\n-      datanode.transferReplicaForPipelineRecovery(blk, targets, client);\n+      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n       writeResponse(Status.SUCCESS, out);\n     } finally {\n       IOUtils.closeStream(out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void transferBlock(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken,\n      final String clientName,\n      final DatanodeInfo[] targets) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, clientName);\n      writeResponse(Status.SUCCESS, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
          "extendedDetails": {}
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,18 @@\n+  protected void opTransferBlock(final DataInputStream in,\n+      final ExtendedBlock blk, final String client,\n+      final DatanodeInfo[] targets,\n+      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+    checkAccess(null, true, blk, blockToken,\n+        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n+\n+    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n+\n+    final DataOutputStream out \u003d new DataOutputStream(\n+        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n+    try {\n+      datanode.transferReplicaForPipelineRecovery(blk, targets, client);\n+      writeResponse(Status.SUCCESS, out);\n+    } finally {\n+      IOUtils.closeStream(out);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void opTransferBlock(final DataInputStream in,\n      final ExtendedBlock blk, final String client,\n      final DatanodeInfo[] targets,\n      final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    checkAccess(null, true, blk, blockToken,\n        Op.TRANSFER_BLOCK, BlockTokenSecretManager.AccessMode.COPY);\n\n    updateCurrentThreadName(Op.TRANSFER_BLOCK + \" \" + blk);\n\n    final DataOutputStream out \u003d new DataOutputStream(\n        NetUtils.getOutputStream(s, datanode.socketWriteTimeout));\n    try {\n      datanode.transferReplicaForPipelineRecovery(blk, targets, client);\n      writeResponse(Status.SUCCESS, out);\n    } finally {\n      IOUtils.closeStream(out);\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
    }
  }
}