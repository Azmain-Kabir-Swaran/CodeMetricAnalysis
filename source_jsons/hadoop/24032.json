{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LocalJobRunner.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
  "functionStartLine": 524,
  "functionEndLine": 610,
  "numCommitsSeen": 22,
  "timeTaken": 5610,
  "changeHistory": [
    "a2cdffb95acbcb3625ee72ebc8aeb8bf17fa4bc7",
    "de09716fb8782596ccf44a3cfc90ac3c4f288667",
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
    "6ea797d0d0a65275aa6a194e97f8d016ac7803f3",
    "a61a18cc098591eacd998e4a2f61babe27353a31",
    "cfb6a9883d2bf02c99f258e9f19ffcd83805d077",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "a2cdffb95acbcb3625ee72ebc8aeb8bf17fa4bc7": "Ybodychange",
    "de09716fb8782596ccf44a3cfc90ac3c4f288667": "Ybodychange",
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4": "Ybodychange",
    "6ea797d0d0a65275aa6a194e97f8d016ac7803f3": "Ybodychange",
    "a61a18cc098591eacd998e4a2f61babe27353a31": "Ybodychange",
    "cfb6a9883d2bf02c99f258e9f19ffcd83805d077": "Ymultichange(Yfilerename,Ybodychange)",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "ded6f225a55517deedc2bd502f2b68f1ca2ddee8": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a2cdffb95acbcb3625ee72ebc8aeb8bf17fa4bc7": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-7094. LocalDistributedCacheManager leaves classloaders open, which leaks FDs. Contributed by Adam Szita.\n",
      "commitDate": "17/05/18 11:16 AM",
      "commitName": "a2cdffb95acbcb3625ee72ebc8aeb8bf17fa4bc7",
      "commitAuthor": "Miklos Szegedi",
      "commitDateOld": "26/03/18 2:55 PM",
      "commitNameOld": "edb202e4934be750e43103c047752b97c5eafc94",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 51.85,
      "commitsBetweenForRepo": 861,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,81 +1,87 @@\n     public void run() {\n       JobID jobId \u003d profile.getJobID();\n       JobContext jContext \u003d new JobContextImpl(job, jobId);\n       \n       org.apache.hadoop.mapreduce.OutputCommitter outputCommitter \u003d null;\n       try {\n         outputCommitter \u003d createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n       } catch (Exception e) {\n         LOG.info(\"Failed to createOutputCommitter\", e);\n         return;\n       }\n       \n       try {\n         TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n           SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n \n         int numReduceTasks \u003d job.getNumReduceTasks();\n         outputCommitter.setupJob(jContext);\n         status.setSetupProgress(1.0f);\n \n         Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n             Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n         \n         List\u003cRunnableWithThrowable\u003e mapRunnables \u003d getMapTaskRunnables(\n             taskSplitMetaInfos, jobId, mapOutputFiles);\n               \n         initCounters(mapRunnables.size(), numReduceTasks);\n         ExecutorService mapService \u003d createMapExecutor();\n         runTasks(mapRunnables, mapService, \"map\");\n \n         try {\n           if (numReduceTasks \u003e 0) {\n             List\u003cRunnableWithThrowable\u003e reduceRunnables \u003d getReduceTaskRunnables(\n                 jobId, mapOutputFiles);\n             ExecutorService reduceService \u003d createReduceExecutor();\n             runTasks(reduceRunnables, reduceService, \"reduce\");\n           }\n         } finally {\n           for (MapOutputFile output : mapOutputFiles.values()) {\n             output.removeAll();\n           }\n         }\n         // delete the temporary directory in output directory\n         outputCommitter.commitJob(jContext);\n         status.setCleanupProgress(1.0f);\n \n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.SUCCEEDED);\n         }\n \n         JobEndNotifier.localRunnerNotification(job, status);\n       } catch (Throwable t) {\n         try {\n           outputCommitter.abortJob(jContext, \n             org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n         } catch (IOException ioe) {\n           LOG.info(\"Error cleaning up job:\" + id);\n         }\n         status.setCleanupProgress(1.0f);\n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.FAILED);\n         }\n         LOG.warn(id.toString(), t);\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } finally {\n         try {\n-          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n-          localFs.delete(localJobFile, true);              // delete local copy\n-          // Cleanup distributed cache\n-          localDistributedCacheManager.close();\n+          try {\n+            // Cleanup distributed cache\n+            localDistributedCacheManager.close();\n+          } finally {\n+            try {\n+              fs.delete(systemJobFile.getParent(), true); // delete submit dir\n+            } finally {\n+              localFs.delete(localJobFile, true);         // delete local copy\n+            }\n+          }\n         } catch (IOException e) {\n           LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      JobID jobId \u003d profile.getJobID();\n      JobContext jContext \u003d new JobContextImpl(job, jobId);\n      \n      org.apache.hadoop.mapreduce.OutputCommitter outputCommitter \u003d null;\n      try {\n        outputCommitter \u003d createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n      } catch (Exception e) {\n        LOG.info(\"Failed to createOutputCommitter\", e);\n        return;\n      }\n      \n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks \u003d job.getNumReduceTasks();\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n        \n        List\u003cRunnableWithThrowable\u003e mapRunnables \u003d getMapTaskRunnables(\n            taskSplitMetaInfos, jobId, mapOutputFiles);\n              \n        initCounters(mapRunnables.size(), numReduceTasks);\n        ExecutorService mapService \u003d createMapExecutor();\n        runTasks(mapRunnables, mapService, \"map\");\n\n        try {\n          if (numReduceTasks \u003e 0) {\n            List\u003cRunnableWithThrowable\u003e reduceRunnables \u003d getReduceTaskRunnables(\n                jobId, mapOutputFiles);\n            ExecutorService reduceService \u003d createReduceExecutor();\n            runTasks(reduceRunnables, reduceService, \"reduce\");\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id.toString(), t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          try {\n            // Cleanup distributed cache\n            localDistributedCacheManager.close();\n          } finally {\n            try {\n              fs.delete(systemJobFile.getParent(), true); // delete submit dir\n            } finally {\n              localFs.delete(localJobFile, true);         // delete local copy\n            }\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {}
    },
    "de09716fb8782596ccf44a3cfc90ac3c4f288667": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6977 Move logging APIs over to slf4j in hadoop-mapreduce-client-common.\nContributed by Jinjiang Ling.\n",
      "commitDate": "27/10/17 2:45 AM",
      "commitName": "de09716fb8782596ccf44a3cfc90ac3c4f288667",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "02/05/16 6:46 PM",
      "commitNameOld": "4ee4e5ca2b8488459d2231dd1de8ed44dd656d5c",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 542.33,
      "commitsBetweenForRepo": 3661,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,81 +1,81 @@\n     public void run() {\n       JobID jobId \u003d profile.getJobID();\n       JobContext jContext \u003d new JobContextImpl(job, jobId);\n       \n       org.apache.hadoop.mapreduce.OutputCommitter outputCommitter \u003d null;\n       try {\n         outputCommitter \u003d createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n       } catch (Exception e) {\n         LOG.info(\"Failed to createOutputCommitter\", e);\n         return;\n       }\n       \n       try {\n         TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n           SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n \n         int numReduceTasks \u003d job.getNumReduceTasks();\n         outputCommitter.setupJob(jContext);\n         status.setSetupProgress(1.0f);\n \n         Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n             Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n         \n         List\u003cRunnableWithThrowable\u003e mapRunnables \u003d getMapTaskRunnables(\n             taskSplitMetaInfos, jobId, mapOutputFiles);\n               \n         initCounters(mapRunnables.size(), numReduceTasks);\n         ExecutorService mapService \u003d createMapExecutor();\n         runTasks(mapRunnables, mapService, \"map\");\n \n         try {\n           if (numReduceTasks \u003e 0) {\n             List\u003cRunnableWithThrowable\u003e reduceRunnables \u003d getReduceTaskRunnables(\n                 jobId, mapOutputFiles);\n             ExecutorService reduceService \u003d createReduceExecutor();\n             runTasks(reduceRunnables, reduceService, \"reduce\");\n           }\n         } finally {\n           for (MapOutputFile output : mapOutputFiles.values()) {\n             output.removeAll();\n           }\n         }\n         // delete the temporary directory in output directory\n         outputCommitter.commitJob(jContext);\n         status.setCleanupProgress(1.0f);\n \n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.SUCCEEDED);\n         }\n \n         JobEndNotifier.localRunnerNotification(job, status);\n       } catch (Throwable t) {\n         try {\n           outputCommitter.abortJob(jContext, \n             org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n         } catch (IOException ioe) {\n           LOG.info(\"Error cleaning up job:\" + id);\n         }\n         status.setCleanupProgress(1.0f);\n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.FAILED);\n         }\n-        LOG.warn(id, t);\n+        LOG.warn(id.toString(), t);\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } finally {\n         try {\n           fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n           localFs.delete(localJobFile, true);              // delete local copy\n           // Cleanup distributed cache\n           localDistributedCacheManager.close();\n         } catch (IOException e) {\n           LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      JobID jobId \u003d profile.getJobID();\n      JobContext jContext \u003d new JobContextImpl(job, jobId);\n      \n      org.apache.hadoop.mapreduce.OutputCommitter outputCommitter \u003d null;\n      try {\n        outputCommitter \u003d createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n      } catch (Exception e) {\n        LOG.info(\"Failed to createOutputCommitter\", e);\n        return;\n      }\n      \n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks \u003d job.getNumReduceTasks();\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n        \n        List\u003cRunnableWithThrowable\u003e mapRunnables \u003d getMapTaskRunnables(\n            taskSplitMetaInfos, jobId, mapOutputFiles);\n              \n        initCounters(mapRunnables.size(), numReduceTasks);\n        ExecutorService mapService \u003d createMapExecutor();\n        runTasks(mapRunnables, mapService, \"map\");\n\n        try {\n          if (numReduceTasks \u003e 0) {\n            List\u003cRunnableWithThrowable\u003e reduceRunnables \u003d getReduceTaskRunnables(\n                jobId, mapOutputFiles);\n            ExecutorService reduceService \u003d createReduceExecutor();\n            runTasks(reduceRunnables, reduceService, \"reduce\");\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id.toString(), t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          localDistributedCacheManager.close();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {}
    },
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-434. LocalJobRunner limited to single reducer (Sandy Ryza and Aaron Kimball via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1510866 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/08/13 11:36 PM",
      "commitName": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
      "commitAuthor": "Sanford Ryza",
      "commitDateOld": "05/08/13 10:07 AM",
      "commitNameOld": "6ea797d0d0a65275aa6a194e97f8d016ac7803f3",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 0.56,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,151 +1,81 @@\n     public void run() {\n       JobID jobId \u003d profile.getJobID();\n       JobContext jContext \u003d new JobContextImpl(job, jobId);\n       \n       org.apache.hadoop.mapreduce.OutputCommitter outputCommitter \u003d null;\n       try {\n         outputCommitter \u003d createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n       } catch (Exception e) {\n         LOG.info(\"Failed to createOutputCommitter\", e);\n         return;\n       }\n       \n       try {\n         TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n           SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n \n         int numReduceTasks \u003d job.getNumReduceTasks();\n-        if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n-          // we only allow 0 or 1 reducer in local mode\n-          numReduceTasks \u003d 1;\n-          job.setNumReduceTasks(1);\n-        }\n         outputCommitter.setupJob(jContext);\n         status.setSetupProgress(1.0f);\n \n         Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n             Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n+        \n+        List\u003cRunnableWithThrowable\u003e mapRunnables \u003d getMapTaskRunnables(\n+            taskSplitMetaInfos, jobId, mapOutputFiles);\n+              \n+        initCounters(mapRunnables.size(), numReduceTasks);\n+        ExecutorService mapService \u003d createMapExecutor();\n+        runTasks(mapRunnables, mapService, \"map\");\n \n-        List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n-            jobId, mapOutputFiles);\n-        ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n-\n-        // Start populating the executor with work units.\n-        // They may begin running immediately (in other threads).\n-        for (Runnable r : taskRunnables) {\n-          mapService.submit(r);\n-        }\n-\n-        try {\n-          mapService.shutdown(); // Instructs queue to drain.\n-\n-          // Wait for tasks to finish; do not use a time-based timeout.\n-          // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n-          LOG.info(\"Waiting for map tasks\");\n-          mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n-        } catch (InterruptedException ie) {\n-          // Cancel all threads.\n-          mapService.shutdownNow();\n-          throw ie;\n-        }\n-\n-        LOG.info(\"Map task executor complete.\");\n-\n-        // After waiting for the map tasks to complete, if any of these\n-        // have thrown an exception, rethrow it now in the main thread context.\n-        for (MapTaskRunnable r : taskRunnables) {\n-          if (r.storedException !\u003d null) {\n-            throw new Exception(r.storedException);\n-          }\n-        }\n-\n-        TaskAttemptID reduceId \u003d\n-          new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n         try {\n           if (numReduceTasks \u003e 0) {\n-            ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n-                reduceId, 0, mapIds.size(), 1);\n-            reduce.setUser(UserGroupInformation.getCurrentUser().\n-                getShortUserName());\n-            JobConf localConf \u003d new JobConf(job);\n-            localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n-            setupChildMapredLocalDirs(localJobDir, reduce, localConf);\n-            // move map output to reduce input  \n-            for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n-              if (!this.isInterrupted()) {\n-                TaskAttemptID mapId \u003d mapIds.get(i);\n-                Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n-                MapOutputFile localOutputFile \u003d new MROutputFiles();\n-                localOutputFile.setConf(localConf);\n-                Path reduceIn \u003d\n-                  localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n-                        localFs.getFileStatus(mapOut).getLen());\n-                if (!localFs.mkdirs(reduceIn.getParent())) {\n-                  throw new IOException(\"Mkdirs failed to create \"\n-                      + reduceIn.getParent().toString());\n-                }\n-                if (!localFs.rename(mapOut, reduceIn))\n-                  throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n-              } else {\n-                throw new InterruptedException();\n-              }\n-            }\n-            if (!this.isInterrupted()) {\n-              reduce.setJobFile(localJobFile.toString());\n-              localConf.setUser(reduce.getUser());\n-              reduce.localizeConfiguration(localConf);\n-              reduce.setConf(localConf);\n-              reduce_tasks +\u003d 1;\n-              myMetrics.launchReduce(reduce.getTaskID());\n-              reduce.run(localConf, this);\n-              myMetrics.completeReduce(reduce.getTaskID());\n-              reduce_tasks -\u003d 1;\n-            } else {\n-              throw new InterruptedException();\n-            }\n+            List\u003cRunnableWithThrowable\u003e reduceRunnables \u003d getReduceTaskRunnables(\n+                jobId, mapOutputFiles);\n+            ExecutorService reduceService \u003d createReduceExecutor();\n+            runTasks(reduceRunnables, reduceService, \"reduce\");\n           }\n         } finally {\n           for (MapOutputFile output : mapOutputFiles.values()) {\n             output.removeAll();\n           }\n         }\n         // delete the temporary directory in output directory\n         outputCommitter.commitJob(jContext);\n         status.setCleanupProgress(1.0f);\n \n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.SUCCEEDED);\n         }\n \n         JobEndNotifier.localRunnerNotification(job, status);\n-\n       } catch (Throwable t) {\n         try {\n           outputCommitter.abortJob(jContext, \n             org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n         } catch (IOException ioe) {\n           LOG.info(\"Error cleaning up job:\" + id);\n         }\n         status.setCleanupProgress(1.0f);\n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.FAILED);\n         }\n         LOG.warn(id, t);\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } finally {\n         try {\n           fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n           localFs.delete(localJobFile, true);              // delete local copy\n           // Cleanup distributed cache\n           localDistributedCacheManager.close();\n         } catch (IOException e) {\n           LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      JobID jobId \u003d profile.getJobID();\n      JobContext jContext \u003d new JobContextImpl(job, jobId);\n      \n      org.apache.hadoop.mapreduce.OutputCommitter outputCommitter \u003d null;\n      try {\n        outputCommitter \u003d createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n      } catch (Exception e) {\n        LOG.info(\"Failed to createOutputCommitter\", e);\n        return;\n      }\n      \n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks \u003d job.getNumReduceTasks();\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n        \n        List\u003cRunnableWithThrowable\u003e mapRunnables \u003d getMapTaskRunnables(\n            taskSplitMetaInfos, jobId, mapOutputFiles);\n              \n        initCounters(mapRunnables.size(), numReduceTasks);\n        ExecutorService mapService \u003d createMapExecutor();\n        runTasks(mapRunnables, mapService, \"map\");\n\n        try {\n          if (numReduceTasks \u003e 0) {\n            List\u003cRunnableWithThrowable\u003e reduceRunnables \u003d getReduceTaskRunnables(\n                jobId, mapOutputFiles);\n            ExecutorService reduceService \u003d createReduceExecutor();\n            runTasks(reduceRunnables, reduceService, \"reduce\");\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id, t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          localDistributedCacheManager.close();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {}
    },
    "6ea797d0d0a65275aa6a194e97f8d016ac7803f3": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5367. Local jobs all use same local working directory (Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1510610 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/08/13 10:07 AM",
      "commitName": "6ea797d0d0a65275aa6a194e97f8d016ac7803f3",
      "commitAuthor": "Sanford Ryza",
      "commitDateOld": "24/04/13 5:54 PM",
      "commitNameOld": "daaa73b657fb28b2d437bfb03821faaa0d458f4e",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 102.68,
      "commitsBetweenForRepo": 635,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,151 +1,151 @@\n     public void run() {\n       JobID jobId \u003d profile.getJobID();\n       JobContext jContext \u003d new JobContextImpl(job, jobId);\n       \n       org.apache.hadoop.mapreduce.OutputCommitter outputCommitter \u003d null;\n       try {\n         outputCommitter \u003d createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n       } catch (Exception e) {\n         LOG.info(\"Failed to createOutputCommitter\", e);\n         return;\n       }\n       \n       try {\n         TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n           SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n \n         int numReduceTasks \u003d job.getNumReduceTasks();\n         if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n           // we only allow 0 or 1 reducer in local mode\n           numReduceTasks \u003d 1;\n           job.setNumReduceTasks(1);\n         }\n         outputCommitter.setupJob(jContext);\n         status.setSetupProgress(1.0f);\n \n         Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n             Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n \n         List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n             jobId, mapOutputFiles);\n         ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n \n         // Start populating the executor with work units.\n         // They may begin running immediately (in other threads).\n         for (Runnable r : taskRunnables) {\n           mapService.submit(r);\n         }\n \n         try {\n           mapService.shutdown(); // Instructs queue to drain.\n \n           // Wait for tasks to finish; do not use a time-based timeout.\n           // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n           LOG.info(\"Waiting for map tasks\");\n           mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n         } catch (InterruptedException ie) {\n           // Cancel all threads.\n           mapService.shutdownNow();\n           throw ie;\n         }\n \n         LOG.info(\"Map task executor complete.\");\n \n         // After waiting for the map tasks to complete, if any of these\n         // have thrown an exception, rethrow it now in the main thread context.\n         for (MapTaskRunnable r : taskRunnables) {\n           if (r.storedException !\u003d null) {\n             throw new Exception(r.storedException);\n           }\n         }\n \n         TaskAttemptID reduceId \u003d\n           new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n         try {\n           if (numReduceTasks \u003e 0) {\n             ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                 reduceId, 0, mapIds.size(), 1);\n             reduce.setUser(UserGroupInformation.getCurrentUser().\n                 getShortUserName());\n             JobConf localConf \u003d new JobConf(job);\n             localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n-            setupChildMapredLocalDirs(reduce, localConf);\n+            setupChildMapredLocalDirs(localJobDir, reduce, localConf);\n             // move map output to reduce input  \n             for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n               if (!this.isInterrupted()) {\n                 TaskAttemptID mapId \u003d mapIds.get(i);\n                 Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                 MapOutputFile localOutputFile \u003d new MROutputFiles();\n                 localOutputFile.setConf(localConf);\n                 Path reduceIn \u003d\n                   localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                         localFs.getFileStatus(mapOut).getLen());\n                 if (!localFs.mkdirs(reduceIn.getParent())) {\n                   throw new IOException(\"Mkdirs failed to create \"\n                       + reduceIn.getParent().toString());\n                 }\n                 if (!localFs.rename(mapOut, reduceIn))\n                   throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n               } else {\n                 throw new InterruptedException();\n               }\n             }\n             if (!this.isInterrupted()) {\n               reduce.setJobFile(localJobFile.toString());\n               localConf.setUser(reduce.getUser());\n               reduce.localizeConfiguration(localConf);\n               reduce.setConf(localConf);\n               reduce_tasks +\u003d 1;\n               myMetrics.launchReduce(reduce.getTaskID());\n               reduce.run(localConf, this);\n               myMetrics.completeReduce(reduce.getTaskID());\n               reduce_tasks -\u003d 1;\n             } else {\n               throw new InterruptedException();\n             }\n           }\n         } finally {\n           for (MapOutputFile output : mapOutputFiles.values()) {\n             output.removeAll();\n           }\n         }\n         // delete the temporary directory in output directory\n         outputCommitter.commitJob(jContext);\n         status.setCleanupProgress(1.0f);\n \n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.SUCCEEDED);\n         }\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } catch (Throwable t) {\n         try {\n           outputCommitter.abortJob(jContext, \n             org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n         } catch (IOException ioe) {\n           LOG.info(\"Error cleaning up job:\" + id);\n         }\n         status.setCleanupProgress(1.0f);\n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.FAILED);\n         }\n         LOG.warn(id, t);\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } finally {\n         try {\n           fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n           localFs.delete(localJobFile, true);              // delete local copy\n           // Cleanup distributed cache\n           localDistributedCacheManager.close();\n         } catch (IOException e) {\n           LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      JobID jobId \u003d profile.getJobID();\n      JobContext jContext \u003d new JobContextImpl(job, jobId);\n      \n      org.apache.hadoop.mapreduce.OutputCommitter outputCommitter \u003d null;\n      try {\n        outputCommitter \u003d createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n      } catch (Exception e) {\n        LOG.info(\"Failed to createOutputCommitter\", e);\n        return;\n      }\n      \n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks \u003d job.getNumReduceTasks();\n        if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n          // we only allow 0 or 1 reducer in local mode\n          numReduceTasks \u003d 1;\n          job.setNumReduceTasks(1);\n        }\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n\n        List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n            jobId, mapOutputFiles);\n        ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n\n        // Start populating the executor with work units.\n        // They may begin running immediately (in other threads).\n        for (Runnable r : taskRunnables) {\n          mapService.submit(r);\n        }\n\n        try {\n          mapService.shutdown(); // Instructs queue to drain.\n\n          // Wait for tasks to finish; do not use a time-based timeout.\n          // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n          LOG.info(\"Waiting for map tasks\");\n          mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n        } catch (InterruptedException ie) {\n          // Cancel all threads.\n          mapService.shutdownNow();\n          throw ie;\n        }\n\n        LOG.info(\"Map task executor complete.\");\n\n        // After waiting for the map tasks to complete, if any of these\n        // have thrown an exception, rethrow it now in the main thread context.\n        for (MapTaskRunnable r : taskRunnables) {\n          if (r.storedException !\u003d null) {\n            throw new Exception(r.storedException);\n          }\n        }\n\n        TaskAttemptID reduceId \u003d\n          new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n        try {\n          if (numReduceTasks \u003e 0) {\n            ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                reduceId, 0, mapIds.size(), 1);\n            reduce.setUser(UserGroupInformation.getCurrentUser().\n                getShortUserName());\n            JobConf localConf \u003d new JobConf(job);\n            localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n            setupChildMapredLocalDirs(localJobDir, reduce, localConf);\n            // move map output to reduce input  \n            for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n              if (!this.isInterrupted()) {\n                TaskAttemptID mapId \u003d mapIds.get(i);\n                Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                MapOutputFile localOutputFile \u003d new MROutputFiles();\n                localOutputFile.setConf(localConf);\n                Path reduceIn \u003d\n                  localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                        localFs.getFileStatus(mapOut).getLen());\n                if (!localFs.mkdirs(reduceIn.getParent())) {\n                  throw new IOException(\"Mkdirs failed to create \"\n                      + reduceIn.getParent().toString());\n                }\n                if (!localFs.rename(mapOut, reduceIn))\n                  throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n              } else {\n                throw new InterruptedException();\n              }\n            }\n            if (!this.isInterrupted()) {\n              reduce.setJobFile(localJobFile.toString());\n              localConf.setUser(reduce.getUser());\n              reduce.localizeConfiguration(localConf);\n              reduce.setConf(localConf);\n              reduce_tasks +\u003d 1;\n              myMetrics.launchReduce(reduce.getTaskID());\n              reduce.run(localConf, this);\n              myMetrics.completeReduce(reduce.getTaskID());\n              reduce_tasks -\u003d 1;\n            } else {\n              throw new InterruptedException();\n            }\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id, t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          localDistributedCacheManager.close();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {}
    },
    "a61a18cc098591eacd998e4a2f61babe27353a31": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3563. Fixed LocalJobRunner to work correctly with new mapreduce apis.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1220996 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/12/11 3:07 PM",
      "commitName": "a61a18cc098591eacd998e4a2f61babe27353a31",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "31/10/11 8:16 PM",
      "commitNameOld": "cfb6a9883d2bf02c99f258e9f19ffcd83805d077",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 48.83,
      "commitsBetweenForRepo": 279,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,144 +1,151 @@\n     public void run() {\n       JobID jobId \u003d profile.getJobID();\n       JobContext jContext \u003d new JobContextImpl(job, jobId);\n-      OutputCommitter outputCommitter \u003d job.getOutputCommitter();\n-\n+      \n+      org.apache.hadoop.mapreduce.OutputCommitter outputCommitter \u003d null;\n+      try {\n+        outputCommitter \u003d createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n+      } catch (Exception e) {\n+        LOG.info(\"Failed to createOutputCommitter\", e);\n+        return;\n+      }\n+      \n       try {\n         TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n           SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n \n         int numReduceTasks \u003d job.getNumReduceTasks();\n         if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n           // we only allow 0 or 1 reducer in local mode\n           numReduceTasks \u003d 1;\n           job.setNumReduceTasks(1);\n         }\n         outputCommitter.setupJob(jContext);\n         status.setSetupProgress(1.0f);\n \n         Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n             Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n \n         List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n             jobId, mapOutputFiles);\n         ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n \n         // Start populating the executor with work units.\n         // They may begin running immediately (in other threads).\n         for (Runnable r : taskRunnables) {\n           mapService.submit(r);\n         }\n \n         try {\n           mapService.shutdown(); // Instructs queue to drain.\n \n           // Wait for tasks to finish; do not use a time-based timeout.\n           // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n           LOG.info(\"Waiting for map tasks\");\n           mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n         } catch (InterruptedException ie) {\n           // Cancel all threads.\n           mapService.shutdownNow();\n           throw ie;\n         }\n \n         LOG.info(\"Map task executor complete.\");\n \n         // After waiting for the map tasks to complete, if any of these\n         // have thrown an exception, rethrow it now in the main thread context.\n         for (MapTaskRunnable r : taskRunnables) {\n           if (r.storedException !\u003d null) {\n             throw new Exception(r.storedException);\n           }\n         }\n \n         TaskAttemptID reduceId \u003d\n           new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n         try {\n           if (numReduceTasks \u003e 0) {\n             ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                 reduceId, 0, mapIds.size(), 1);\n             reduce.setUser(UserGroupInformation.getCurrentUser().\n                 getShortUserName());\n             JobConf localConf \u003d new JobConf(job);\n             localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n             setupChildMapredLocalDirs(reduce, localConf);\n             // move map output to reduce input  \n             for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n               if (!this.isInterrupted()) {\n                 TaskAttemptID mapId \u003d mapIds.get(i);\n                 Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                 MapOutputFile localOutputFile \u003d new MROutputFiles();\n                 localOutputFile.setConf(localConf);\n                 Path reduceIn \u003d\n                   localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                         localFs.getFileStatus(mapOut).getLen());\n                 if (!localFs.mkdirs(reduceIn.getParent())) {\n                   throw new IOException(\"Mkdirs failed to create \"\n                       + reduceIn.getParent().toString());\n                 }\n                 if (!localFs.rename(mapOut, reduceIn))\n                   throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n               } else {\n                 throw new InterruptedException();\n               }\n             }\n             if (!this.isInterrupted()) {\n               reduce.setJobFile(localJobFile.toString());\n               localConf.setUser(reduce.getUser());\n               reduce.localizeConfiguration(localConf);\n               reduce.setConf(localConf);\n               reduce_tasks +\u003d 1;\n               myMetrics.launchReduce(reduce.getTaskID());\n               reduce.run(localConf, this);\n               myMetrics.completeReduce(reduce.getTaskID());\n               reduce_tasks -\u003d 1;\n             } else {\n               throw new InterruptedException();\n             }\n           }\n         } finally {\n           for (MapOutputFile output : mapOutputFiles.values()) {\n             output.removeAll();\n           }\n         }\n         // delete the temporary directory in output directory\n         outputCommitter.commitJob(jContext);\n         status.setCleanupProgress(1.0f);\n \n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.SUCCEEDED);\n         }\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } catch (Throwable t) {\n         try {\n           outputCommitter.abortJob(jContext, \n             org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n         } catch (IOException ioe) {\n           LOG.info(\"Error cleaning up job:\" + id);\n         }\n         status.setCleanupProgress(1.0f);\n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.FAILED);\n         }\n         LOG.warn(id, t);\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } finally {\n         try {\n           fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n           localFs.delete(localJobFile, true);              // delete local copy\n           // Cleanup distributed cache\n           localDistributedCacheManager.close();\n         } catch (IOException e) {\n           LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      JobID jobId \u003d profile.getJobID();\n      JobContext jContext \u003d new JobContextImpl(job, jobId);\n      \n      org.apache.hadoop.mapreduce.OutputCommitter outputCommitter \u003d null;\n      try {\n        outputCommitter \u003d createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n      } catch (Exception e) {\n        LOG.info(\"Failed to createOutputCommitter\", e);\n        return;\n      }\n      \n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks \u003d job.getNumReduceTasks();\n        if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n          // we only allow 0 or 1 reducer in local mode\n          numReduceTasks \u003d 1;\n          job.setNumReduceTasks(1);\n        }\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n\n        List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n            jobId, mapOutputFiles);\n        ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n\n        // Start populating the executor with work units.\n        // They may begin running immediately (in other threads).\n        for (Runnable r : taskRunnables) {\n          mapService.submit(r);\n        }\n\n        try {\n          mapService.shutdown(); // Instructs queue to drain.\n\n          // Wait for tasks to finish; do not use a time-based timeout.\n          // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n          LOG.info(\"Waiting for map tasks\");\n          mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n        } catch (InterruptedException ie) {\n          // Cancel all threads.\n          mapService.shutdownNow();\n          throw ie;\n        }\n\n        LOG.info(\"Map task executor complete.\");\n\n        // After waiting for the map tasks to complete, if any of these\n        // have thrown an exception, rethrow it now in the main thread context.\n        for (MapTaskRunnable r : taskRunnables) {\n          if (r.storedException !\u003d null) {\n            throw new Exception(r.storedException);\n          }\n        }\n\n        TaskAttemptID reduceId \u003d\n          new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n        try {\n          if (numReduceTasks \u003e 0) {\n            ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                reduceId, 0, mapIds.size(), 1);\n            reduce.setUser(UserGroupInformation.getCurrentUser().\n                getShortUserName());\n            JobConf localConf \u003d new JobConf(job);\n            localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n            setupChildMapredLocalDirs(reduce, localConf);\n            // move map output to reduce input  \n            for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n              if (!this.isInterrupted()) {\n                TaskAttemptID mapId \u003d mapIds.get(i);\n                Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                MapOutputFile localOutputFile \u003d new MROutputFiles();\n                localOutputFile.setConf(localConf);\n                Path reduceIn \u003d\n                  localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                        localFs.getFileStatus(mapOut).getLen());\n                if (!localFs.mkdirs(reduceIn.getParent())) {\n                  throw new IOException(\"Mkdirs failed to create \"\n                      + reduceIn.getParent().toString());\n                }\n                if (!localFs.rename(mapOut, reduceIn))\n                  throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n              } else {\n                throw new InterruptedException();\n              }\n            }\n            if (!this.isInterrupted()) {\n              reduce.setJobFile(localJobFile.toString());\n              localConf.setUser(reduce.getUser());\n              reduce.localizeConfiguration(localConf);\n              reduce.setConf(localConf);\n              reduce_tasks +\u003d 1;\n              myMetrics.launchReduce(reduce.getTaskID());\n              reduce.run(localConf, this);\n              myMetrics.completeReduce(reduce.getTaskID());\n              reduce_tasks -\u003d 1;\n            } else {\n              throw new InterruptedException();\n            }\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id, t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          localDistributedCacheManager.close();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {}
    },
    "cfb6a9883d2bf02c99f258e9f19ffcd83805d077": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "MAPREDUCE-3237. Move LocalJobRunner to hadoop-mapreduce-client-core. Contributed by Tom White.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195792 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/10/11 8:16 PM",
      "commitName": "cfb6a9883d2bf02c99f258e9f19ffcd83805d077",
      "commitAuthor": "Arun Murthy",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "MAPREDUCE-3237. Move LocalJobRunner to hadoop-mapreduce-client-core. Contributed by Tom White.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195792 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "31/10/11 8:16 PM",
          "commitName": "cfb6a9883d2bf02c99f258e9f19ffcd83805d077",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "31/10/11 7:09 PM",
          "commitNameOld": "e5badc0c1a817ca8f7e4255ec4dcfdf858abb596",
          "commitAuthorOld": "Arun Murthy",
          "daysBetweenCommits": 0.05,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,144 @@\n     public void run() {\n       JobID jobId \u003d profile.getJobID();\n       JobContext jContext \u003d new JobContextImpl(job, jobId);\n       OutputCommitter outputCommitter \u003d job.getOutputCommitter();\n \n       try {\n         TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n           SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n \n         int numReduceTasks \u003d job.getNumReduceTasks();\n         if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n           // we only allow 0 or 1 reducer in local mode\n           numReduceTasks \u003d 1;\n           job.setNumReduceTasks(1);\n         }\n         outputCommitter.setupJob(jContext);\n         status.setSetupProgress(1.0f);\n \n         Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n             Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n \n         List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n             jobId, mapOutputFiles);\n         ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n \n         // Start populating the executor with work units.\n         // They may begin running immediately (in other threads).\n         for (Runnable r : taskRunnables) {\n           mapService.submit(r);\n         }\n \n         try {\n           mapService.shutdown(); // Instructs queue to drain.\n \n           // Wait for tasks to finish; do not use a time-based timeout.\n           // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n           LOG.info(\"Waiting for map tasks\");\n           mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n         } catch (InterruptedException ie) {\n           // Cancel all threads.\n           mapService.shutdownNow();\n           throw ie;\n         }\n \n         LOG.info(\"Map task executor complete.\");\n \n         // After waiting for the map tasks to complete, if any of these\n         // have thrown an exception, rethrow it now in the main thread context.\n         for (MapTaskRunnable r : taskRunnables) {\n           if (r.storedException !\u003d null) {\n             throw new Exception(r.storedException);\n           }\n         }\n \n         TaskAttemptID reduceId \u003d\n           new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n         try {\n           if (numReduceTasks \u003e 0) {\n             ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                 reduceId, 0, mapIds.size(), 1);\n             reduce.setUser(UserGroupInformation.getCurrentUser().\n                 getShortUserName());\n             JobConf localConf \u003d new JobConf(job);\n             localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n-            TaskRunner.setupChildMapredLocalDirs(reduce, localConf);\n+            setupChildMapredLocalDirs(reduce, localConf);\n             // move map output to reduce input  \n             for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n               if (!this.isInterrupted()) {\n                 TaskAttemptID mapId \u003d mapIds.get(i);\n                 Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                 MapOutputFile localOutputFile \u003d new MROutputFiles();\n                 localOutputFile.setConf(localConf);\n                 Path reduceIn \u003d\n                   localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                         localFs.getFileStatus(mapOut).getLen());\n                 if (!localFs.mkdirs(reduceIn.getParent())) {\n                   throw new IOException(\"Mkdirs failed to create \"\n                       + reduceIn.getParent().toString());\n                 }\n                 if (!localFs.rename(mapOut, reduceIn))\n                   throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n               } else {\n                 throw new InterruptedException();\n               }\n             }\n             if (!this.isInterrupted()) {\n               reduce.setJobFile(localJobFile.toString());\n               localConf.setUser(reduce.getUser());\n               reduce.localizeConfiguration(localConf);\n               reduce.setConf(localConf);\n               reduce_tasks +\u003d 1;\n               myMetrics.launchReduce(reduce.getTaskID());\n               reduce.run(localConf, this);\n               myMetrics.completeReduce(reduce.getTaskID());\n               reduce_tasks -\u003d 1;\n             } else {\n               throw new InterruptedException();\n             }\n           }\n         } finally {\n           for (MapOutputFile output : mapOutputFiles.values()) {\n             output.removeAll();\n           }\n         }\n         // delete the temporary directory in output directory\n         outputCommitter.commitJob(jContext);\n         status.setCleanupProgress(1.0f);\n \n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.SUCCEEDED);\n         }\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } catch (Throwable t) {\n         try {\n           outputCommitter.abortJob(jContext, \n             org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n         } catch (IOException ioe) {\n           LOG.info(\"Error cleaning up job:\" + id);\n         }\n         status.setCleanupProgress(1.0f);\n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.FAILED);\n         }\n         LOG.warn(id, t);\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } finally {\n         try {\n           fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n           localFs.delete(localJobFile, true);              // delete local copy\n           // Cleanup distributed cache\n-          taskDistributedCacheManager.release();\n-          trackerDistributerdCacheManager.purgeCache();\n+          localDistributedCacheManager.close();\n         } catch (IOException e) {\n           LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n         }\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void run() {\n      JobID jobId \u003d profile.getJobID();\n      JobContext jContext \u003d new JobContextImpl(job, jobId);\n      OutputCommitter outputCommitter \u003d job.getOutputCommitter();\n\n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks \u003d job.getNumReduceTasks();\n        if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n          // we only allow 0 or 1 reducer in local mode\n          numReduceTasks \u003d 1;\n          job.setNumReduceTasks(1);\n        }\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n\n        List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n            jobId, mapOutputFiles);\n        ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n\n        // Start populating the executor with work units.\n        // They may begin running immediately (in other threads).\n        for (Runnable r : taskRunnables) {\n          mapService.submit(r);\n        }\n\n        try {\n          mapService.shutdown(); // Instructs queue to drain.\n\n          // Wait for tasks to finish; do not use a time-based timeout.\n          // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n          LOG.info(\"Waiting for map tasks\");\n          mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n        } catch (InterruptedException ie) {\n          // Cancel all threads.\n          mapService.shutdownNow();\n          throw ie;\n        }\n\n        LOG.info(\"Map task executor complete.\");\n\n        // After waiting for the map tasks to complete, if any of these\n        // have thrown an exception, rethrow it now in the main thread context.\n        for (MapTaskRunnable r : taskRunnables) {\n          if (r.storedException !\u003d null) {\n            throw new Exception(r.storedException);\n          }\n        }\n\n        TaskAttemptID reduceId \u003d\n          new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n        try {\n          if (numReduceTasks \u003e 0) {\n            ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                reduceId, 0, mapIds.size(), 1);\n            reduce.setUser(UserGroupInformation.getCurrentUser().\n                getShortUserName());\n            JobConf localConf \u003d new JobConf(job);\n            localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n            setupChildMapredLocalDirs(reduce, localConf);\n            // move map output to reduce input  \n            for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n              if (!this.isInterrupted()) {\n                TaskAttemptID mapId \u003d mapIds.get(i);\n                Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                MapOutputFile localOutputFile \u003d new MROutputFiles();\n                localOutputFile.setConf(localConf);\n                Path reduceIn \u003d\n                  localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                        localFs.getFileStatus(mapOut).getLen());\n                if (!localFs.mkdirs(reduceIn.getParent())) {\n                  throw new IOException(\"Mkdirs failed to create \"\n                      + reduceIn.getParent().toString());\n                }\n                if (!localFs.rename(mapOut, reduceIn))\n                  throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n              } else {\n                throw new InterruptedException();\n              }\n            }\n            if (!this.isInterrupted()) {\n              reduce.setJobFile(localJobFile.toString());\n              localConf.setUser(reduce.getUser());\n              reduce.localizeConfiguration(localConf);\n              reduce.setConf(localConf);\n              reduce_tasks +\u003d 1;\n              myMetrics.launchReduce(reduce.getTaskID());\n              reduce.run(localConf, this);\n              myMetrics.completeReduce(reduce.getTaskID());\n              reduce_tasks -\u003d 1;\n            } else {\n              throw new InterruptedException();\n            }\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id, t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          localDistributedCacheManager.close();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
          "extendedDetails": {
            "oldPath": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
            "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-3237. Move LocalJobRunner to hadoop-mapreduce-client-core. Contributed by Tom White.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195792 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "31/10/11 8:16 PM",
          "commitName": "cfb6a9883d2bf02c99f258e9f19ffcd83805d077",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "31/10/11 7:09 PM",
          "commitNameOld": "e5badc0c1a817ca8f7e4255ec4dcfdf858abb596",
          "commitAuthorOld": "Arun Murthy",
          "daysBetweenCommits": 0.05,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,144 @@\n     public void run() {\n       JobID jobId \u003d profile.getJobID();\n       JobContext jContext \u003d new JobContextImpl(job, jobId);\n       OutputCommitter outputCommitter \u003d job.getOutputCommitter();\n \n       try {\n         TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n           SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n \n         int numReduceTasks \u003d job.getNumReduceTasks();\n         if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n           // we only allow 0 or 1 reducer in local mode\n           numReduceTasks \u003d 1;\n           job.setNumReduceTasks(1);\n         }\n         outputCommitter.setupJob(jContext);\n         status.setSetupProgress(1.0f);\n \n         Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n             Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n \n         List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n             jobId, mapOutputFiles);\n         ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n \n         // Start populating the executor with work units.\n         // They may begin running immediately (in other threads).\n         for (Runnable r : taskRunnables) {\n           mapService.submit(r);\n         }\n \n         try {\n           mapService.shutdown(); // Instructs queue to drain.\n \n           // Wait for tasks to finish; do not use a time-based timeout.\n           // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n           LOG.info(\"Waiting for map tasks\");\n           mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n         } catch (InterruptedException ie) {\n           // Cancel all threads.\n           mapService.shutdownNow();\n           throw ie;\n         }\n \n         LOG.info(\"Map task executor complete.\");\n \n         // After waiting for the map tasks to complete, if any of these\n         // have thrown an exception, rethrow it now in the main thread context.\n         for (MapTaskRunnable r : taskRunnables) {\n           if (r.storedException !\u003d null) {\n             throw new Exception(r.storedException);\n           }\n         }\n \n         TaskAttemptID reduceId \u003d\n           new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n         try {\n           if (numReduceTasks \u003e 0) {\n             ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                 reduceId, 0, mapIds.size(), 1);\n             reduce.setUser(UserGroupInformation.getCurrentUser().\n                 getShortUserName());\n             JobConf localConf \u003d new JobConf(job);\n             localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n-            TaskRunner.setupChildMapredLocalDirs(reduce, localConf);\n+            setupChildMapredLocalDirs(reduce, localConf);\n             // move map output to reduce input  \n             for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n               if (!this.isInterrupted()) {\n                 TaskAttemptID mapId \u003d mapIds.get(i);\n                 Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                 MapOutputFile localOutputFile \u003d new MROutputFiles();\n                 localOutputFile.setConf(localConf);\n                 Path reduceIn \u003d\n                   localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                         localFs.getFileStatus(mapOut).getLen());\n                 if (!localFs.mkdirs(reduceIn.getParent())) {\n                   throw new IOException(\"Mkdirs failed to create \"\n                       + reduceIn.getParent().toString());\n                 }\n                 if (!localFs.rename(mapOut, reduceIn))\n                   throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n               } else {\n                 throw new InterruptedException();\n               }\n             }\n             if (!this.isInterrupted()) {\n               reduce.setJobFile(localJobFile.toString());\n               localConf.setUser(reduce.getUser());\n               reduce.localizeConfiguration(localConf);\n               reduce.setConf(localConf);\n               reduce_tasks +\u003d 1;\n               myMetrics.launchReduce(reduce.getTaskID());\n               reduce.run(localConf, this);\n               myMetrics.completeReduce(reduce.getTaskID());\n               reduce_tasks -\u003d 1;\n             } else {\n               throw new InterruptedException();\n             }\n           }\n         } finally {\n           for (MapOutputFile output : mapOutputFiles.values()) {\n             output.removeAll();\n           }\n         }\n         // delete the temporary directory in output directory\n         outputCommitter.commitJob(jContext);\n         status.setCleanupProgress(1.0f);\n \n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.SUCCEEDED);\n         }\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } catch (Throwable t) {\n         try {\n           outputCommitter.abortJob(jContext, \n             org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n         } catch (IOException ioe) {\n           LOG.info(\"Error cleaning up job:\" + id);\n         }\n         status.setCleanupProgress(1.0f);\n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.FAILED);\n         }\n         LOG.warn(id, t);\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } finally {\n         try {\n           fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n           localFs.delete(localJobFile, true);              // delete local copy\n           // Cleanup distributed cache\n-          taskDistributedCacheManager.release();\n-          trackerDistributerdCacheManager.purgeCache();\n+          localDistributedCacheManager.close();\n         } catch (IOException e) {\n           LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n         }\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void run() {\n      JobID jobId \u003d profile.getJobID();\n      JobContext jContext \u003d new JobContextImpl(job, jobId);\n      OutputCommitter outputCommitter \u003d job.getOutputCommitter();\n\n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks \u003d job.getNumReduceTasks();\n        if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n          // we only allow 0 or 1 reducer in local mode\n          numReduceTasks \u003d 1;\n          job.setNumReduceTasks(1);\n        }\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n\n        List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n            jobId, mapOutputFiles);\n        ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n\n        // Start populating the executor with work units.\n        // They may begin running immediately (in other threads).\n        for (Runnable r : taskRunnables) {\n          mapService.submit(r);\n        }\n\n        try {\n          mapService.shutdown(); // Instructs queue to drain.\n\n          // Wait for tasks to finish; do not use a time-based timeout.\n          // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n          LOG.info(\"Waiting for map tasks\");\n          mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n        } catch (InterruptedException ie) {\n          // Cancel all threads.\n          mapService.shutdownNow();\n          throw ie;\n        }\n\n        LOG.info(\"Map task executor complete.\");\n\n        // After waiting for the map tasks to complete, if any of these\n        // have thrown an exception, rethrow it now in the main thread context.\n        for (MapTaskRunnable r : taskRunnables) {\n          if (r.storedException !\u003d null) {\n            throw new Exception(r.storedException);\n          }\n        }\n\n        TaskAttemptID reduceId \u003d\n          new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n        try {\n          if (numReduceTasks \u003e 0) {\n            ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                reduceId, 0, mapIds.size(), 1);\n            reduce.setUser(UserGroupInformation.getCurrentUser().\n                getShortUserName());\n            JobConf localConf \u003d new JobConf(job);\n            localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n            setupChildMapredLocalDirs(reduce, localConf);\n            // move map output to reduce input  \n            for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n              if (!this.isInterrupted()) {\n                TaskAttemptID mapId \u003d mapIds.get(i);\n                Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                MapOutputFile localOutputFile \u003d new MROutputFiles();\n                localOutputFile.setConf(localConf);\n                Path reduceIn \u003d\n                  localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                        localFs.getFileStatus(mapOut).getLen());\n                if (!localFs.mkdirs(reduceIn.getParent())) {\n                  throw new IOException(\"Mkdirs failed to create \"\n                      + reduceIn.getParent().toString());\n                }\n                if (!localFs.rename(mapOut, reduceIn))\n                  throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n              } else {\n                throw new InterruptedException();\n              }\n            }\n            if (!this.isInterrupted()) {\n              reduce.setJobFile(localJobFile.toString());\n              localConf.setUser(reduce.getUser());\n              reduce.localizeConfiguration(localConf);\n              reduce.setConf(localConf);\n              reduce_tasks +\u003d 1;\n              myMetrics.launchReduce(reduce.getTaskID());\n              reduce.run(localConf, this);\n              myMetrics.completeReduce(reduce.getTaskID());\n              reduce_tasks -\u003d 1;\n            } else {\n              throw new InterruptedException();\n            }\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id, t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          localDistributedCacheManager.close();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
          "extendedDetails": {}
        }
      ]
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void run() {\n      JobID jobId \u003d profile.getJobID();\n      JobContext jContext \u003d new JobContextImpl(job, jobId);\n      OutputCommitter outputCommitter \u003d job.getOutputCommitter();\n\n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks \u003d job.getNumReduceTasks();\n        if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n          // we only allow 0 or 1 reducer in local mode\n          numReduceTasks \u003d 1;\n          job.setNumReduceTasks(1);\n        }\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n\n        List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n            jobId, mapOutputFiles);\n        ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n\n        // Start populating the executor with work units.\n        // They may begin running immediately (in other threads).\n        for (Runnable r : taskRunnables) {\n          mapService.submit(r);\n        }\n\n        try {\n          mapService.shutdown(); // Instructs queue to drain.\n\n          // Wait for tasks to finish; do not use a time-based timeout.\n          // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n          LOG.info(\"Waiting for map tasks\");\n          mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n        } catch (InterruptedException ie) {\n          // Cancel all threads.\n          mapService.shutdownNow();\n          throw ie;\n        }\n\n        LOG.info(\"Map task executor complete.\");\n\n        // After waiting for the map tasks to complete, if any of these\n        // have thrown an exception, rethrow it now in the main thread context.\n        for (MapTaskRunnable r : taskRunnables) {\n          if (r.storedException !\u003d null) {\n            throw new Exception(r.storedException);\n          }\n        }\n\n        TaskAttemptID reduceId \u003d\n          new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n        try {\n          if (numReduceTasks \u003e 0) {\n            ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                reduceId, 0, mapIds.size(), 1);\n            reduce.setUser(UserGroupInformation.getCurrentUser().\n                getShortUserName());\n            JobConf localConf \u003d new JobConf(job);\n            localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n            TaskRunner.setupChildMapredLocalDirs(reduce, localConf);\n            // move map output to reduce input  \n            for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n              if (!this.isInterrupted()) {\n                TaskAttemptID mapId \u003d mapIds.get(i);\n                Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                MapOutputFile localOutputFile \u003d new MROutputFiles();\n                localOutputFile.setConf(localConf);\n                Path reduceIn \u003d\n                  localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                        localFs.getFileStatus(mapOut).getLen());\n                if (!localFs.mkdirs(reduceIn.getParent())) {\n                  throw new IOException(\"Mkdirs failed to create \"\n                      + reduceIn.getParent().toString());\n                }\n                if (!localFs.rename(mapOut, reduceIn))\n                  throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n              } else {\n                throw new InterruptedException();\n              }\n            }\n            if (!this.isInterrupted()) {\n              reduce.setJobFile(localJobFile.toString());\n              localConf.setUser(reduce.getUser());\n              reduce.localizeConfiguration(localConf);\n              reduce.setConf(localConf);\n              reduce_tasks +\u003d 1;\n              myMetrics.launchReduce(reduce.getTaskID());\n              reduce.run(localConf, this);\n              myMetrics.completeReduce(reduce.getTaskID());\n              reduce_tasks -\u003d 1;\n            } else {\n              throw new InterruptedException();\n            }\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id, t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          taskDistributedCacheManager.release();\n          trackerDistributerdCacheManager.purgeCache();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
        "newPath": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalJobRunner.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void run() {\n      JobID jobId \u003d profile.getJobID();\n      JobContext jContext \u003d new JobContextImpl(job, jobId);\n      OutputCommitter outputCommitter \u003d job.getOutputCommitter();\n\n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks \u003d job.getNumReduceTasks();\n        if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n          // we only allow 0 or 1 reducer in local mode\n          numReduceTasks \u003d 1;\n          job.setNumReduceTasks(1);\n        }\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n\n        List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n            jobId, mapOutputFiles);\n        ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n\n        // Start populating the executor with work units.\n        // They may begin running immediately (in other threads).\n        for (Runnable r : taskRunnables) {\n          mapService.submit(r);\n        }\n\n        try {\n          mapService.shutdown(); // Instructs queue to drain.\n\n          // Wait for tasks to finish; do not use a time-based timeout.\n          // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n          LOG.info(\"Waiting for map tasks\");\n          mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n        } catch (InterruptedException ie) {\n          // Cancel all threads.\n          mapService.shutdownNow();\n          throw ie;\n        }\n\n        LOG.info(\"Map task executor complete.\");\n\n        // After waiting for the map tasks to complete, if any of these\n        // have thrown an exception, rethrow it now in the main thread context.\n        for (MapTaskRunnable r : taskRunnables) {\n          if (r.storedException !\u003d null) {\n            throw new Exception(r.storedException);\n          }\n        }\n\n        TaskAttemptID reduceId \u003d\n          new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n        try {\n          if (numReduceTasks \u003e 0) {\n            ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                reduceId, 0, mapIds.size(), 1);\n            reduce.setUser(UserGroupInformation.getCurrentUser().\n                getShortUserName());\n            JobConf localConf \u003d new JobConf(job);\n            localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n            TaskRunner.setupChildMapredLocalDirs(reduce, localConf);\n            // move map output to reduce input  \n            for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n              if (!this.isInterrupted()) {\n                TaskAttemptID mapId \u003d mapIds.get(i);\n                Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                MapOutputFile localOutputFile \u003d new MROutputFiles();\n                localOutputFile.setConf(localConf);\n                Path reduceIn \u003d\n                  localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                        localFs.getFileStatus(mapOut).getLen());\n                if (!localFs.mkdirs(reduceIn.getParent())) {\n                  throw new IOException(\"Mkdirs failed to create \"\n                      + reduceIn.getParent().toString());\n                }\n                if (!localFs.rename(mapOut, reduceIn))\n                  throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n              } else {\n                throw new InterruptedException();\n              }\n            }\n            if (!this.isInterrupted()) {\n              reduce.setJobFile(localJobFile.toString());\n              localConf.setUser(reduce.getUser());\n              reduce.localizeConfiguration(localConf);\n              reduce.setConf(localConf);\n              reduce_tasks +\u003d 1;\n              myMetrics.launchReduce(reduce.getTaskID());\n              reduce.run(localConf, this);\n              myMetrics.completeReduce(reduce.getTaskID());\n              reduce_tasks -\u003d 1;\n            } else {\n              throw new InterruptedException();\n            }\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id, t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          taskDistributedCacheManager.release();\n          trackerDistributerdCacheManager.purgeCache();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
        "newPath": "hadoop-mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java"
      }
    },
    "ded6f225a55517deedc2bd502f2b68f1ca2ddee8": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2837. Ported bug fixes from y-merge to prepare for MAPREDUCE-279 merge. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1157249 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/08/11 2:00 PM",
      "commitName": "ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 60.96,
      "commitsBetweenForRepo": 224,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,145 +1,145 @@\n     public void run() {\n       JobID jobId \u003d profile.getJobID();\n       JobContext jContext \u003d new JobContextImpl(job, jobId);\n       OutputCommitter outputCommitter \u003d job.getOutputCommitter();\n \n       try {\n         TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n           SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n \n         int numReduceTasks \u003d job.getNumReduceTasks();\n         if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n           // we only allow 0 or 1 reducer in local mode\n           numReduceTasks \u003d 1;\n           job.setNumReduceTasks(1);\n         }\n         outputCommitter.setupJob(jContext);\n         status.setSetupProgress(1.0f);\n \n         Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n             Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n \n         List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n             jobId, mapOutputFiles);\n         ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n \n         // Start populating the executor with work units.\n         // They may begin running immediately (in other threads).\n         for (Runnable r : taskRunnables) {\n           mapService.submit(r);\n         }\n \n         try {\n           mapService.shutdown(); // Instructs queue to drain.\n \n           // Wait for tasks to finish; do not use a time-based timeout.\n           // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n           LOG.info(\"Waiting for map tasks\");\n           mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n         } catch (InterruptedException ie) {\n           // Cancel all threads.\n           mapService.shutdownNow();\n           throw ie;\n         }\n \n         LOG.info(\"Map task executor complete.\");\n \n         // After waiting for the map tasks to complete, if any of these\n         // have thrown an exception, rethrow it now in the main thread context.\n         for (MapTaskRunnable r : taskRunnables) {\n           if (r.storedException !\u003d null) {\n             throw new Exception(r.storedException);\n           }\n         }\n \n         TaskAttemptID reduceId \u003d\n           new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n         try {\n           if (numReduceTasks \u003e 0) {\n             ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                 reduceId, 0, mapIds.size(), 1);\n             reduce.setUser(UserGroupInformation.getCurrentUser().\n                 getShortUserName());\n             JobConf localConf \u003d new JobConf(job);\n             localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n             TaskRunner.setupChildMapredLocalDirs(reduce, localConf);\n             // move map output to reduce input  \n             for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n               if (!this.isInterrupted()) {\n                 TaskAttemptID mapId \u003d mapIds.get(i);\n                 Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n-                MapOutputFile localOutputFile \u003d new MapOutputFile();\n+                MapOutputFile localOutputFile \u003d new MROutputFiles();\n                 localOutputFile.setConf(localConf);\n                 Path reduceIn \u003d\n                   localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                         localFs.getFileStatus(mapOut).getLen());\n                 if (!localFs.mkdirs(reduceIn.getParent())) {\n                   throw new IOException(\"Mkdirs failed to create \"\n                       + reduceIn.getParent().toString());\n                 }\n                 if (!localFs.rename(mapOut, reduceIn))\n                   throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n               } else {\n                 throw new InterruptedException();\n               }\n             }\n             if (!this.isInterrupted()) {\n               reduce.setJobFile(localJobFile.toString());\n               localConf.setUser(reduce.getUser());\n               reduce.localizeConfiguration(localConf);\n               reduce.setConf(localConf);\n               reduce_tasks +\u003d 1;\n               myMetrics.launchReduce(reduce.getTaskID());\n               reduce.run(localConf, this);\n               myMetrics.completeReduce(reduce.getTaskID());\n               reduce_tasks -\u003d 1;\n             } else {\n               throw new InterruptedException();\n             }\n           }\n         } finally {\n           for (MapOutputFile output : mapOutputFiles.values()) {\n             output.removeAll();\n           }\n         }\n         // delete the temporary directory in output directory\n         outputCommitter.commitJob(jContext);\n         status.setCleanupProgress(1.0f);\n \n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.SUCCEEDED);\n         }\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } catch (Throwable t) {\n         try {\n           outputCommitter.abortJob(jContext, \n             org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n         } catch (IOException ioe) {\n           LOG.info(\"Error cleaning up job:\" + id);\n         }\n         status.setCleanupProgress(1.0f);\n         if (killed) {\n           this.status.setRunState(JobStatus.KILLED);\n         } else {\n           this.status.setRunState(JobStatus.FAILED);\n         }\n         LOG.warn(id, t);\n \n         JobEndNotifier.localRunnerNotification(job, status);\n \n       } finally {\n         try {\n           fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n           localFs.delete(localJobFile, true);              // delete local copy\n           // Cleanup distributed cache\n           taskDistributedCacheManager.release();\n           trackerDistributerdCacheManager.purgeCache();\n         } catch (IOException e) {\n           LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      JobID jobId \u003d profile.getJobID();\n      JobContext jContext \u003d new JobContextImpl(job, jobId);\n      OutputCommitter outputCommitter \u003d job.getOutputCommitter();\n\n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks \u003d job.getNumReduceTasks();\n        if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n          // we only allow 0 or 1 reducer in local mode\n          numReduceTasks \u003d 1;\n          job.setNumReduceTasks(1);\n        }\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n\n        List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n            jobId, mapOutputFiles);\n        ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n\n        // Start populating the executor with work units.\n        // They may begin running immediately (in other threads).\n        for (Runnable r : taskRunnables) {\n          mapService.submit(r);\n        }\n\n        try {\n          mapService.shutdown(); // Instructs queue to drain.\n\n          // Wait for tasks to finish; do not use a time-based timeout.\n          // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n          LOG.info(\"Waiting for map tasks\");\n          mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n        } catch (InterruptedException ie) {\n          // Cancel all threads.\n          mapService.shutdownNow();\n          throw ie;\n        }\n\n        LOG.info(\"Map task executor complete.\");\n\n        // After waiting for the map tasks to complete, if any of these\n        // have thrown an exception, rethrow it now in the main thread context.\n        for (MapTaskRunnable r : taskRunnables) {\n          if (r.storedException !\u003d null) {\n            throw new Exception(r.storedException);\n          }\n        }\n\n        TaskAttemptID reduceId \u003d\n          new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n        try {\n          if (numReduceTasks \u003e 0) {\n            ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                reduceId, 0, mapIds.size(), 1);\n            reduce.setUser(UserGroupInformation.getCurrentUser().\n                getShortUserName());\n            JobConf localConf \u003d new JobConf(job);\n            localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n            TaskRunner.setupChildMapredLocalDirs(reduce, localConf);\n            // move map output to reduce input  \n            for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n              if (!this.isInterrupted()) {\n                TaskAttemptID mapId \u003d mapIds.get(i);\n                Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                MapOutputFile localOutputFile \u003d new MROutputFiles();\n                localOutputFile.setConf(localConf);\n                Path reduceIn \u003d\n                  localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                        localFs.getFileStatus(mapOut).getLen());\n                if (!localFs.mkdirs(reduceIn.getParent())) {\n                  throw new IOException(\"Mkdirs failed to create \"\n                      + reduceIn.getParent().toString());\n                }\n                if (!localFs.rename(mapOut, reduceIn))\n                  throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n              } else {\n                throw new InterruptedException();\n              }\n            }\n            if (!this.isInterrupted()) {\n              reduce.setJobFile(localJobFile.toString());\n              localConf.setUser(reduce.getUser());\n              reduce.localizeConfiguration(localConf);\n              reduce.setConf(localConf);\n              reduce_tasks +\u003d 1;\n              myMetrics.launchReduce(reduce.getTaskID());\n              reduce.run(localConf, this);\n              myMetrics.completeReduce(reduce.getTaskID());\n              reduce_tasks -\u003d 1;\n            } else {\n              throw new InterruptedException();\n            }\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id, t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          taskDistributedCacheManager.release();\n          trackerDistributerdCacheManager.purgeCache();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,145 @@\n+    public void run() {\n+      JobID jobId \u003d profile.getJobID();\n+      JobContext jContext \u003d new JobContextImpl(job, jobId);\n+      OutputCommitter outputCommitter \u003d job.getOutputCommitter();\n+\n+      try {\n+        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n+          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n+\n+        int numReduceTasks \u003d job.getNumReduceTasks();\n+        if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n+          // we only allow 0 or 1 reducer in local mode\n+          numReduceTasks \u003d 1;\n+          job.setNumReduceTasks(1);\n+        }\n+        outputCommitter.setupJob(jContext);\n+        status.setSetupProgress(1.0f);\n+\n+        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n+            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n+\n+        List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n+            jobId, mapOutputFiles);\n+        ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n+\n+        // Start populating the executor with work units.\n+        // They may begin running immediately (in other threads).\n+        for (Runnable r : taskRunnables) {\n+          mapService.submit(r);\n+        }\n+\n+        try {\n+          mapService.shutdown(); // Instructs queue to drain.\n+\n+          // Wait for tasks to finish; do not use a time-based timeout.\n+          // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n+          LOG.info(\"Waiting for map tasks\");\n+          mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n+        } catch (InterruptedException ie) {\n+          // Cancel all threads.\n+          mapService.shutdownNow();\n+          throw ie;\n+        }\n+\n+        LOG.info(\"Map task executor complete.\");\n+\n+        // After waiting for the map tasks to complete, if any of these\n+        // have thrown an exception, rethrow it now in the main thread context.\n+        for (MapTaskRunnable r : taskRunnables) {\n+          if (r.storedException !\u003d null) {\n+            throw new Exception(r.storedException);\n+          }\n+        }\n+\n+        TaskAttemptID reduceId \u003d\n+          new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n+        try {\n+          if (numReduceTasks \u003e 0) {\n+            ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n+                reduceId, 0, mapIds.size(), 1);\n+            reduce.setUser(UserGroupInformation.getCurrentUser().\n+                getShortUserName());\n+            JobConf localConf \u003d new JobConf(job);\n+            localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n+            TaskRunner.setupChildMapredLocalDirs(reduce, localConf);\n+            // move map output to reduce input  \n+            for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n+              if (!this.isInterrupted()) {\n+                TaskAttemptID mapId \u003d mapIds.get(i);\n+                Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n+                MapOutputFile localOutputFile \u003d new MapOutputFile();\n+                localOutputFile.setConf(localConf);\n+                Path reduceIn \u003d\n+                  localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n+                        localFs.getFileStatus(mapOut).getLen());\n+                if (!localFs.mkdirs(reduceIn.getParent())) {\n+                  throw new IOException(\"Mkdirs failed to create \"\n+                      + reduceIn.getParent().toString());\n+                }\n+                if (!localFs.rename(mapOut, reduceIn))\n+                  throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n+              } else {\n+                throw new InterruptedException();\n+              }\n+            }\n+            if (!this.isInterrupted()) {\n+              reduce.setJobFile(localJobFile.toString());\n+              localConf.setUser(reduce.getUser());\n+              reduce.localizeConfiguration(localConf);\n+              reduce.setConf(localConf);\n+              reduce_tasks +\u003d 1;\n+              myMetrics.launchReduce(reduce.getTaskID());\n+              reduce.run(localConf, this);\n+              myMetrics.completeReduce(reduce.getTaskID());\n+              reduce_tasks -\u003d 1;\n+            } else {\n+              throw new InterruptedException();\n+            }\n+          }\n+        } finally {\n+          for (MapOutputFile output : mapOutputFiles.values()) {\n+            output.removeAll();\n+          }\n+        }\n+        // delete the temporary directory in output directory\n+        outputCommitter.commitJob(jContext);\n+        status.setCleanupProgress(1.0f);\n+\n+        if (killed) {\n+          this.status.setRunState(JobStatus.KILLED);\n+        } else {\n+          this.status.setRunState(JobStatus.SUCCEEDED);\n+        }\n+\n+        JobEndNotifier.localRunnerNotification(job, status);\n+\n+      } catch (Throwable t) {\n+        try {\n+          outputCommitter.abortJob(jContext, \n+            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n+        } catch (IOException ioe) {\n+          LOG.info(\"Error cleaning up job:\" + id);\n+        }\n+        status.setCleanupProgress(1.0f);\n+        if (killed) {\n+          this.status.setRunState(JobStatus.KILLED);\n+        } else {\n+          this.status.setRunState(JobStatus.FAILED);\n+        }\n+        LOG.warn(id, t);\n+\n+        JobEndNotifier.localRunnerNotification(job, status);\n+\n+      } finally {\n+        try {\n+          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n+          localFs.delete(localJobFile, true);              // delete local copy\n+          // Cleanup distributed cache\n+          taskDistributedCacheManager.release();\n+          trackerDistributerdCacheManager.purgeCache();\n+        } catch (IOException e) {\n+          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      JobID jobId \u003d profile.getJobID();\n      JobContext jContext \u003d new JobContextImpl(job, jobId);\n      OutputCommitter outputCommitter \u003d job.getOutputCommitter();\n\n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos \u003d \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks \u003d job.getNumReduceTasks();\n        if (numReduceTasks \u003e 1 || numReduceTasks \u003c 0) {\n          // we only allow 0 or 1 reducer in local mode\n          numReduceTasks \u003d 1;\n          job.setNumReduceTasks(1);\n        }\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map\u003cTaskAttemptID, MapOutputFile\u003e mapOutputFiles \u003d\n            Collections.synchronizedMap(new HashMap\u003cTaskAttemptID, MapOutputFile\u003e());\n\n        List\u003cMapTaskRunnable\u003e taskRunnables \u003d getMapTaskRunnables(taskSplitMetaInfos,\n            jobId, mapOutputFiles);\n        ExecutorService mapService \u003d createMapExecutor(taskRunnables.size());\n\n        // Start populating the executor with work units.\n        // They may begin running immediately (in other threads).\n        for (Runnable r : taskRunnables) {\n          mapService.submit(r);\n        }\n\n        try {\n          mapService.shutdown(); // Instructs queue to drain.\n\n          // Wait for tasks to finish; do not use a time-based timeout.\n          // (See http://bugs.sun.com/bugdatabase/view_bug.do?bug_id\u003d6179024)\n          LOG.info(\"Waiting for map tasks\");\n          mapService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);\n        } catch (InterruptedException ie) {\n          // Cancel all threads.\n          mapService.shutdownNow();\n          throw ie;\n        }\n\n        LOG.info(\"Map task executor complete.\");\n\n        // After waiting for the map tasks to complete, if any of these\n        // have thrown an exception, rethrow it now in the main thread context.\n        for (MapTaskRunnable r : taskRunnables) {\n          if (r.storedException !\u003d null) {\n            throw new Exception(r.storedException);\n          }\n        }\n\n        TaskAttemptID reduceId \u003d\n          new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\n        try {\n          if (numReduceTasks \u003e 0) {\n            ReduceTask reduce \u003d new ReduceTask(systemJobFile.toString(), \n                reduceId, 0, mapIds.size(), 1);\n            reduce.setUser(UserGroupInformation.getCurrentUser().\n                getShortUserName());\n            JobConf localConf \u003d new JobConf(job);\n            localConf.set(\"mapreduce.jobtracker.address\", \"local\");\n            TaskRunner.setupChildMapredLocalDirs(reduce, localConf);\n            // move map output to reduce input  \n            for (int i \u003d 0; i \u003c mapIds.size(); i++) {\n              if (!this.isInterrupted()) {\n                TaskAttemptID mapId \u003d mapIds.get(i);\n                Path mapOut \u003d mapOutputFiles.get(mapId).getOutputFile();\n                MapOutputFile localOutputFile \u003d new MapOutputFile();\n                localOutputFile.setConf(localConf);\n                Path reduceIn \u003d\n                  localOutputFile.getInputFileForWrite(mapId.getTaskID(),\n                        localFs.getFileStatus(mapOut).getLen());\n                if (!localFs.mkdirs(reduceIn.getParent())) {\n                  throw new IOException(\"Mkdirs failed to create \"\n                      + reduceIn.getParent().toString());\n                }\n                if (!localFs.rename(mapOut, reduceIn))\n                  throw new IOException(\"Couldn\u0027t rename \" + mapOut);\n              } else {\n                throw new InterruptedException();\n              }\n            }\n            if (!this.isInterrupted()) {\n              reduce.setJobFile(localJobFile.toString());\n              localConf.setUser(reduce.getUser());\n              reduce.localizeConfiguration(localConf);\n              reduce.setConf(localConf);\n              reduce_tasks +\u003d 1;\n              myMetrics.launchReduce(reduce.getTaskID());\n              reduce.run(localConf, this);\n              myMetrics.completeReduce(reduce.getTaskID());\n              reduce_tasks -\u003d 1;\n            } else {\n              throw new InterruptedException();\n            }\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id, t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          taskDistributedCacheManager.release();\n          trackerDistributerdCacheManager.purgeCache();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \"+id+\": \"+e);\n        }\n      }\n    }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java"
    }
  }
}