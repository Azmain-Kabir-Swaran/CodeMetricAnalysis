{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FlowActivityEntityReader.java",
  "functionName": "getResults",
  "functionId": "getResults___hbaseConf-Configuration__conn-Connection__filterList-FilterList",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java",
  "functionStartLine": 114,
  "functionEndLine": 154,
  "numCommitsSeen": 22,
  "timeTaken": 4334,
  "changeHistory": [
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2",
    "8bb26465956a37d7398818bc0919772e12953725",
    "b01514f65bc6090a50a583f67d1ecb5d74b6d276",
    "892b193bd77c15932b4c084c1d525b7017def0d4",
    "960af7d4717b8a8949d0b2e43949e7daab45aa88",
    "9cb1287e9b8425f91de925f411c3c2a8fa9fe2a3",
    "88f02941144824187b70fa2aaf0c6d90bcb77d8f",
    "1f710484e5b8ab4d5c67379c012004e8a4242d15",
    "e3e857866d9fdefb7e353b21ae24eab4401e60b3",
    "708fa8b1ae85b6efda318368bc0c0ba02d4958c8",
    "10fa6da7d8a6013698767c6136ae20f0e04415e9"
  ],
  "changeHistoryShort": {
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2": "Yfilerename",
    "8bb26465956a37d7398818bc0919772e12953725": "Ybodychange",
    "b01514f65bc6090a50a583f67d1ecb5d74b6d276": "Yfilerename",
    "892b193bd77c15932b4c084c1d525b7017def0d4": "Ybodychange",
    "960af7d4717b8a8949d0b2e43949e7daab45aa88": "Ybodychange",
    "9cb1287e9b8425f91de925f411c3c2a8fa9fe2a3": "Ybodychange",
    "88f02941144824187b70fa2aaf0c6d90bcb77d8f": "Yfilerename",
    "1f710484e5b8ab4d5c67379c012004e8a4242d15": "Yparameterchange",
    "e3e857866d9fdefb7e353b21ae24eab4401e60b3": "Ybodychange",
    "708fa8b1ae85b6efda318368bc0c0ba02d4958c8": "Yreturntypechange",
    "10fa6da7d8a6013698767c6136ae20f0e04415e9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2": {
      "type": "Yfilerename",
      "commitMessage": "YARN-7919. Refactor timelineservice-hbase module into submodules. Contributed by Haibo Chen.\n",
      "commitDate": "17/02/18 7:00 AM",
      "commitName": "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2",
      "commitAuthor": "Rohith Sharma K S",
      "commitDateOld": "17/02/18 3:24 AM",
      "commitNameOld": "a1e56a62863d8d494af309ec5f476c4b7e4d5ef9",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 0.15,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected ResultScanner getResults(Configuration hbaseConf,\n      Connection conn, FilterList filterList) throws IOException {\n    Scan scan \u003d new Scan();\n    String clusterId \u003d getContext().getClusterId();\n    if (getFilters().getFromId() \u003d\u003d null\n        \u0026\u0026 getFilters().getCreatedTimeBegin() \u003d\u003d 0L\n        \u0026\u0026 getFilters().getCreatedTimeEnd() \u003d\u003d Long.MAX_VALUE) {\n       // All records have to be chosen.\n      scan.setRowPrefixFilter(new FlowActivityRowKeyPrefix(clusterId)\n          .getRowKeyPrefix());\n    } else if (getFilters().getFromId() !\u003d null) {\n      FlowActivityRowKey key \u003d null;\n      try {\n        key \u003d\n            FlowActivityRowKey.parseRowKeyFromString(getFilters().getFromId());\n      } catch (IllegalArgumentException e) {\n        throw new BadRequestException(\"Invalid filter fromid is provided.\");\n      }\n      if (!clusterId.equals(key.getClusterId())) {\n        throw new BadRequestException(\n            \"fromid doesn\u0027t belong to clusterId\u003d\" + clusterId);\n      }\n      scan.setStartRow(key.getRowKey());\n      scan.setStopRow(\n          new FlowActivityRowKeyPrefix(clusterId,\n              (getFilters().getCreatedTimeBegin() \u003c\u003d 0 ? 0\n                  : (getFilters().getCreatedTimeBegin() - 1)))\n                      .getRowKeyPrefix());\n    } else {\n      scan.setStartRow(new FlowActivityRowKeyPrefix(clusterId, getFilters()\n          .getCreatedTimeEnd()).getRowKeyPrefix());\n      scan.setStopRow(new FlowActivityRowKeyPrefix(clusterId, (getFilters()\n          .getCreatedTimeBegin() \u003c\u003d 0 ? 0\n          : (getFilters().getCreatedTimeBegin() - 1))).getRowKeyPrefix());\n    }\n    // use the page filter to limit the result to the page size\n    // the scanner may still return more than the limit; therefore we need to\n    // read the right number as we iterate\n    scan.setFilter(new PageFilter(getFilters().getLimit()));\n    return getTable().getResultScanner(hbaseConf, conn, scan);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java"
      }
    },
    "8bb26465956a37d7398818bc0919772e12953725": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6027. Support fromid(offset) filter for /flows API (Rohith Sharma K S via Varun Saxena)\n",
      "commitDate": "29/08/17 10:59 PM",
      "commitName": "8bb26465956a37d7398818bc0919772e12953725",
      "commitAuthor": "Varun Saxena",
      "commitDateOld": "29/08/17 10:59 PM",
      "commitNameOld": "02a9710a099fc9572122d87dd3e90c78522f5836",
      "commitAuthorOld": "Varun Saxena",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,41 @@\n   protected ResultScanner getResults(Configuration hbaseConf,\n       Connection conn, FilterList filterList) throws IOException {\n     Scan scan \u003d new Scan();\n     String clusterId \u003d getContext().getClusterId();\n-    if (getFilters().getCreatedTimeBegin() \u003d\u003d 0L \u0026\u0026\n-        getFilters().getCreatedTimeEnd() \u003d\u003d Long.MAX_VALUE) {\n+    if (getFilters().getFromId() \u003d\u003d null\n+        \u0026\u0026 getFilters().getCreatedTimeBegin() \u003d\u003d 0L\n+        \u0026\u0026 getFilters().getCreatedTimeEnd() \u003d\u003d Long.MAX_VALUE) {\n        // All records have to be chosen.\n       scan.setRowPrefixFilter(new FlowActivityRowKeyPrefix(clusterId)\n           .getRowKeyPrefix());\n+    } else if (getFilters().getFromId() !\u003d null) {\n+      FlowActivityRowKey key \u003d null;\n+      try {\n+        key \u003d\n+            FlowActivityRowKey.parseRowKeyFromString(getFilters().getFromId());\n+      } catch (IllegalArgumentException e) {\n+        throw new BadRequestException(\"Invalid filter fromid is provided.\");\n+      }\n+      if (!clusterId.equals(key.getClusterId())) {\n+        throw new BadRequestException(\n+            \"fromid doesn\u0027t belong to clusterId\u003d\" + clusterId);\n+      }\n+      scan.setStartRow(key.getRowKey());\n+      scan.setStopRow(\n+          new FlowActivityRowKeyPrefix(clusterId,\n+              (getFilters().getCreatedTimeBegin() \u003c\u003d 0 ? 0\n+                  : (getFilters().getCreatedTimeBegin() - 1)))\n+                      .getRowKeyPrefix());\n     } else {\n       scan.setStartRow(new FlowActivityRowKeyPrefix(clusterId, getFilters()\n           .getCreatedTimeEnd()).getRowKeyPrefix());\n       scan.setStopRow(new FlowActivityRowKeyPrefix(clusterId, (getFilters()\n           .getCreatedTimeBegin() \u003c\u003d 0 ? 0\n           : (getFilters().getCreatedTimeBegin() - 1))).getRowKeyPrefix());\n     }\n     // use the page filter to limit the result to the page size\n     // the scanner may still return more than the limit; therefore we need to\n     // read the right number as we iterate\n     scan.setFilter(new PageFilter(getFilters().getLimit()));\n     return getTable().getResultScanner(hbaseConf, conn, scan);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected ResultScanner getResults(Configuration hbaseConf,\n      Connection conn, FilterList filterList) throws IOException {\n    Scan scan \u003d new Scan();\n    String clusterId \u003d getContext().getClusterId();\n    if (getFilters().getFromId() \u003d\u003d null\n        \u0026\u0026 getFilters().getCreatedTimeBegin() \u003d\u003d 0L\n        \u0026\u0026 getFilters().getCreatedTimeEnd() \u003d\u003d Long.MAX_VALUE) {\n       // All records have to be chosen.\n      scan.setRowPrefixFilter(new FlowActivityRowKeyPrefix(clusterId)\n          .getRowKeyPrefix());\n    } else if (getFilters().getFromId() !\u003d null) {\n      FlowActivityRowKey key \u003d null;\n      try {\n        key \u003d\n            FlowActivityRowKey.parseRowKeyFromString(getFilters().getFromId());\n      } catch (IllegalArgumentException e) {\n        throw new BadRequestException(\"Invalid filter fromid is provided.\");\n      }\n      if (!clusterId.equals(key.getClusterId())) {\n        throw new BadRequestException(\n            \"fromid doesn\u0027t belong to clusterId\u003d\" + clusterId);\n      }\n      scan.setStartRow(key.getRowKey());\n      scan.setStopRow(\n          new FlowActivityRowKeyPrefix(clusterId,\n              (getFilters().getCreatedTimeBegin() \u003c\u003d 0 ? 0\n                  : (getFilters().getCreatedTimeBegin() - 1)))\n                      .getRowKeyPrefix());\n    } else {\n      scan.setStartRow(new FlowActivityRowKeyPrefix(clusterId, getFilters()\n          .getCreatedTimeEnd()).getRowKeyPrefix());\n      scan.setStopRow(new FlowActivityRowKeyPrefix(clusterId, (getFilters()\n          .getCreatedTimeBegin() \u003c\u003d 0 ? 0\n          : (getFilters().getCreatedTimeBegin() - 1))).getRowKeyPrefix());\n    }\n    // use the page filter to limit the result to the page size\n    // the scanner may still return more than the limit; therefore we need to\n    // read the right number as we iterate\n    scan.setFilter(new PageFilter(getFilters().getLimit()));\n    return getTable().getResultScanner(hbaseConf, conn, scan);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java",
      "extendedDetails": {}
    },
    "b01514f65bc6090a50a583f67d1ecb5d74b6d276": {
      "type": "Yfilerename",
      "commitMessage": "YARN-5928. Move ATSv2 HBase backend code into a new module that is only dependent at runtime by yarn servers. Contributed by Haibo Chen.\n",
      "commitDate": "19/01/17 8:52 PM",
      "commitName": "b01514f65bc6090a50a583f67d1ecb5d74b6d276",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "19/01/17 5:32 PM",
      "commitNameOld": "60865c8ea08053f3d6ac23f81c3376a3de3ca996",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.14,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected ResultScanner getResults(Configuration hbaseConf,\n      Connection conn, FilterList filterList) throws IOException {\n    Scan scan \u003d new Scan();\n    String clusterId \u003d getContext().getClusterId();\n    if (getFilters().getCreatedTimeBegin() \u003d\u003d 0L \u0026\u0026\n        getFilters().getCreatedTimeEnd() \u003d\u003d Long.MAX_VALUE) {\n       // All records have to be chosen.\n      scan.setRowPrefixFilter(new FlowActivityRowKeyPrefix(clusterId)\n          .getRowKeyPrefix());\n    } else {\n      scan.setStartRow(new FlowActivityRowKeyPrefix(clusterId, getFilters()\n          .getCreatedTimeEnd()).getRowKeyPrefix());\n      scan.setStopRow(new FlowActivityRowKeyPrefix(clusterId, (getFilters()\n          .getCreatedTimeBegin() \u003c\u003d 0 ? 0\n          : (getFilters().getCreatedTimeBegin() - 1))).getRowKeyPrefix());\n    }\n    // use the page filter to limit the result to the page size\n    // the scanner may still return more than the limit; therefore we need to\n    // read the right number as we iterate\n    scan.setFilter(new PageFilter(getFilters().getLimit()));\n    return getTable().getResultScanner(hbaseConf, conn, scan);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java"
      }
    },
    "892b193bd77c15932b4c084c1d525b7017def0d4": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5170. Eliminate singleton converters and static method access. (Joep Rottinghuis via Varun Saxena)\n",
      "commitDate": "10/07/16 8:46 AM",
      "commitName": "892b193bd77c15932b4c084c1d525b7017def0d4",
      "commitAuthor": "Varun Saxena",
      "commitDateOld": "10/07/16 8:46 AM",
      "commitNameOld": "7b8cfa5c2ff62005c8b78867fedd64b48e50383d",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,22 @@\n   protected ResultScanner getResults(Configuration hbaseConf,\n       Connection conn, FilterList filterList) throws IOException {\n     Scan scan \u003d new Scan();\n     String clusterId \u003d getContext().getClusterId();\n     if (getFilters().getCreatedTimeBegin() \u003d\u003d 0L \u0026\u0026\n         getFilters().getCreatedTimeEnd() \u003d\u003d Long.MAX_VALUE) {\n        // All records have to be chosen.\n-      scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n+      scan.setRowPrefixFilter(new FlowActivityRowKeyPrefix(clusterId)\n+          .getRowKeyPrefix());\n     } else {\n-      scan.setStartRow(\n-          FlowActivityRowKey.getRowKeyPrefix(clusterId,\n-              getFilters().getCreatedTimeEnd()));\n-      scan.setStopRow(\n-          FlowActivityRowKey.getRowKeyPrefix(clusterId,\n-              (getFilters().getCreatedTimeBegin() \u003c\u003d 0 ? 0 :\n-              (getFilters().getCreatedTimeBegin() - 1))));\n+      scan.setStartRow(new FlowActivityRowKeyPrefix(clusterId, getFilters()\n+          .getCreatedTimeEnd()).getRowKeyPrefix());\n+      scan.setStopRow(new FlowActivityRowKeyPrefix(clusterId, (getFilters()\n+          .getCreatedTimeBegin() \u003c\u003d 0 ? 0\n+          : (getFilters().getCreatedTimeBegin() - 1))).getRowKeyPrefix());\n     }\n     // use the page filter to limit the result to the page size\n     // the scanner may still return more than the limit; therefore we need to\n     // read the right number as we iterate\n     scan.setFilter(new PageFilter(getFilters().getLimit()));\n     return getTable().getResultScanner(hbaseConf, conn, scan);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected ResultScanner getResults(Configuration hbaseConf,\n      Connection conn, FilterList filterList) throws IOException {\n    Scan scan \u003d new Scan();\n    String clusterId \u003d getContext().getClusterId();\n    if (getFilters().getCreatedTimeBegin() \u003d\u003d 0L \u0026\u0026\n        getFilters().getCreatedTimeEnd() \u003d\u003d Long.MAX_VALUE) {\n       // All records have to be chosen.\n      scan.setRowPrefixFilter(new FlowActivityRowKeyPrefix(clusterId)\n          .getRowKeyPrefix());\n    } else {\n      scan.setStartRow(new FlowActivityRowKeyPrefix(clusterId, getFilters()\n          .getCreatedTimeEnd()).getRowKeyPrefix());\n      scan.setStopRow(new FlowActivityRowKeyPrefix(clusterId, (getFilters()\n          .getCreatedTimeBegin() \u003c\u003d 0 ? 0\n          : (getFilters().getCreatedTimeBegin() - 1))).getRowKeyPrefix());\n    }\n    // use the page filter to limit the result to the page size\n    // the scanner may still return more than the limit; therefore we need to\n    // read the right number as we iterate\n    scan.setFilter(new PageFilter(getFilters().getLimit()));\n    return getTable().getResultScanner(hbaseConf, conn, scan);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java",
      "extendedDetails": {}
    },
    "960af7d4717b8a8949d0b2e43949e7daab45aa88": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4409. Fix javadoc and checkstyle issues in timelineservice code (Varun Saxena via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "960af7d4717b8a8949d0b2e43949e7daab45aa88",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "9cb1287e9b8425f91de925f411c3c2a8fa9fe2a3",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   protected ResultScanner getResults(Configuration hbaseConf,\n       Connection conn, FilterList filterList) throws IOException {\n     Scan scan \u003d new Scan();\n     String clusterId \u003d getContext().getClusterId();\n     if (getFilters().getCreatedTimeBegin() \u003d\u003d 0L \u0026\u0026\n         getFilters().getCreatedTimeEnd() \u003d\u003d Long.MAX_VALUE) {\n        // All records have to be chosen.\n       scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n     } else {\n       scan.setStartRow(\n           FlowActivityRowKey.getRowKeyPrefix(clusterId,\n               getFilters().getCreatedTimeEnd()));\n       scan.setStopRow(\n           FlowActivityRowKey.getRowKeyPrefix(clusterId,\n               (getFilters().getCreatedTimeBegin() \u003c\u003d 0 ? 0 :\n               (getFilters().getCreatedTimeBegin() - 1))));\n     }\n     // use the page filter to limit the result to the page size\n     // the scanner may still return more than the limit; therefore we need to\n     // read the right number as we iterate\n     scan.setFilter(new PageFilter(getFilters().getLimit()));\n-    return table.getResultScanner(hbaseConf, conn, scan);\n+    return getTable().getResultScanner(hbaseConf, conn, scan);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected ResultScanner getResults(Configuration hbaseConf,\n      Connection conn, FilterList filterList) throws IOException {\n    Scan scan \u003d new Scan();\n    String clusterId \u003d getContext().getClusterId();\n    if (getFilters().getCreatedTimeBegin() \u003d\u003d 0L \u0026\u0026\n        getFilters().getCreatedTimeEnd() \u003d\u003d Long.MAX_VALUE) {\n       // All records have to be chosen.\n      scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n    } else {\n      scan.setStartRow(\n          FlowActivityRowKey.getRowKeyPrefix(clusterId,\n              getFilters().getCreatedTimeEnd()));\n      scan.setStopRow(\n          FlowActivityRowKey.getRowKeyPrefix(clusterId,\n              (getFilters().getCreatedTimeBegin() \u003c\u003d 0 ? 0 :\n              (getFilters().getCreatedTimeBegin() - 1))));\n    }\n    // use the page filter to limit the result to the page size\n    // the scanner may still return more than the limit; therefore we need to\n    // read the right number as we iterate\n    scan.setFilter(new PageFilter(getFilters().getLimit()));\n    return getTable().getResultScanner(hbaseConf, conn, scan);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java",
      "extendedDetails": {}
    },
    "9cb1287e9b8425f91de925f411c3c2a8fa9fe2a3": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4446. Refactor reader API for better extensibility (Varun Saxena via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "9cb1287e9b8425f91de925f411c3c2a8fa9fe2a3",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "6934b05c7117a12286fb2ba7a47f75e227cacb22",
      "commitAuthorOld": "Naganarasimha",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,23 @@\n   protected ResultScanner getResults(Configuration hbaseConf,\n       Connection conn, FilterList filterList) throws IOException {\n     Scan scan \u003d new Scan();\n-    if (createdTimeBegin \u003d\u003d DEFAULT_BEGIN_TIME \u0026\u0026\n-        createdTimeEnd \u003d\u003d DEFAULT_END_TIME) {\n+    String clusterId \u003d getContext().getClusterId();\n+    if (getFilters().getCreatedTimeBegin() \u003d\u003d 0L \u0026\u0026\n+        getFilters().getCreatedTimeEnd() \u003d\u003d Long.MAX_VALUE) {\n+       // All records have to be chosen.\n       scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n     } else {\n       scan.setStartRow(\n-          FlowActivityRowKey.getRowKeyPrefix(clusterId, createdTimeEnd));\n+          FlowActivityRowKey.getRowKeyPrefix(clusterId,\n+              getFilters().getCreatedTimeEnd()));\n       scan.setStopRow(\n           FlowActivityRowKey.getRowKeyPrefix(clusterId,\n-              (createdTimeBegin \u003c\u003d 0 ? 0: (createdTimeBegin - 1))));\n+              (getFilters().getCreatedTimeBegin() \u003c\u003d 0 ? 0 :\n+              (getFilters().getCreatedTimeBegin() - 1))));\n     }\n     // use the page filter to limit the result to the page size\n     // the scanner may still return more than the limit; therefore we need to\n     // read the right number as we iterate\n-    scan.setFilter(new PageFilter(limit));\n+    scan.setFilter(new PageFilter(getFilters().getLimit()));\n     return table.getResultScanner(hbaseConf, conn, scan);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected ResultScanner getResults(Configuration hbaseConf,\n      Connection conn, FilterList filterList) throws IOException {\n    Scan scan \u003d new Scan();\n    String clusterId \u003d getContext().getClusterId();\n    if (getFilters().getCreatedTimeBegin() \u003d\u003d 0L \u0026\u0026\n        getFilters().getCreatedTimeEnd() \u003d\u003d Long.MAX_VALUE) {\n       // All records have to be chosen.\n      scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n    } else {\n      scan.setStartRow(\n          FlowActivityRowKey.getRowKeyPrefix(clusterId,\n              getFilters().getCreatedTimeEnd()));\n      scan.setStopRow(\n          FlowActivityRowKey.getRowKeyPrefix(clusterId,\n              (getFilters().getCreatedTimeBegin() \u003c\u003d 0 ? 0 :\n              (getFilters().getCreatedTimeBegin() - 1))));\n    }\n    // use the page filter to limit the result to the page size\n    // the scanner may still return more than the limit; therefore we need to\n    // read the right number as we iterate\n    scan.setFilter(new PageFilter(getFilters().getLimit()));\n    return table.getResultScanner(hbaseConf, conn, scan);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java",
      "extendedDetails": {}
    },
    "88f02941144824187b70fa2aaf0c6d90bcb77d8f": {
      "type": "Yfilerename",
      "commitMessage": "YARN-4200. Refactor reader classes in storage to nest under hbase\nspecific package name. Contributed by Li Lu.\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "88f02941144824187b70fa2aaf0c6d90bcb77d8f",
      "commitAuthor": "Li Lu",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "cc16683cefe2611cf4de7819496aa54854f5394c",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected ResultScanner getResults(Configuration hbaseConf,\n      Connection conn, FilterList filterList) throws IOException {\n    Scan scan \u003d new Scan();\n    if (createdTimeBegin \u003d\u003d DEFAULT_BEGIN_TIME \u0026\u0026\n        createdTimeEnd \u003d\u003d DEFAULT_END_TIME) {\n      scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n    } else {\n      scan.setStartRow(\n          FlowActivityRowKey.getRowKeyPrefix(clusterId, createdTimeEnd));\n      scan.setStopRow(\n          FlowActivityRowKey.getRowKeyPrefix(clusterId,\n              (createdTimeBegin \u003c\u003d 0 ? 0: (createdTimeBegin - 1))));\n    }\n    // use the page filter to limit the result to the page size\n    // the scanner may still return more than the limit; therefore we need to\n    // read the right number as we iterate\n    scan.setFilter(new PageFilter(limit));\n    return table.getResultScanner(hbaseConf, conn, scan);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowActivityEntityReader.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java"
      }
    },
    "1f710484e5b8ab4d5c67379c012004e8a4242d15": {
      "type": "Yparameterchange",
      "commitMessage": "YARN-3862. Support for fetching specific configs and metrics based on prefixes (Varun Saxena via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "1f710484e5b8ab4d5c67379c012004e8a4242d15",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "e3e857866d9fdefb7e353b21ae24eab4401e60b3",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   protected ResultScanner getResults(Configuration hbaseConf,\n-      Connection conn) throws IOException {\n+      Connection conn, FilterList filterList) throws IOException {\n     Scan scan \u003d new Scan();\n     if (createdTimeBegin \u003d\u003d DEFAULT_BEGIN_TIME \u0026\u0026\n         createdTimeEnd \u003d\u003d DEFAULT_END_TIME) {\n       scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n     } else {\n       scan.setStartRow(\n           FlowActivityRowKey.getRowKeyPrefix(clusterId, createdTimeEnd));\n       scan.setStopRow(\n           FlowActivityRowKey.getRowKeyPrefix(clusterId,\n               (createdTimeBegin \u003c\u003d 0 ? 0: (createdTimeBegin - 1))));\n     }\n     // use the page filter to limit the result to the page size\n     // the scanner may still return more than the limit; therefore we need to\n     // read the right number as we iterate\n     scan.setFilter(new PageFilter(limit));\n     return table.getResultScanner(hbaseConf, conn, scan);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected ResultScanner getResults(Configuration hbaseConf,\n      Connection conn, FilterList filterList) throws IOException {\n    Scan scan \u003d new Scan();\n    if (createdTimeBegin \u003d\u003d DEFAULT_BEGIN_TIME \u0026\u0026\n        createdTimeEnd \u003d\u003d DEFAULT_END_TIME) {\n      scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n    } else {\n      scan.setStartRow(\n          FlowActivityRowKey.getRowKeyPrefix(clusterId, createdTimeEnd));\n      scan.setStopRow(\n          FlowActivityRowKey.getRowKeyPrefix(clusterId,\n              (createdTimeBegin \u003c\u003d 0 ? 0: (createdTimeBegin - 1))));\n    }\n    // use the page filter to limit the result to the page size\n    // the scanner may still return more than the limit; therefore we need to\n    // read the right number as we iterate\n    scan.setFilter(new PageFilter(limit));\n    return table.getResultScanner(hbaseConf, conn, scan);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowActivityEntityReader.java",
      "extendedDetails": {
        "oldValue": "[hbaseConf-Configuration, conn-Connection]",
        "newValue": "[hbaseConf-Configuration, conn-Connection, filterList-FilterList]"
      }
    },
    "e3e857866d9fdefb7e353b21ae24eab4401e60b3": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4179. [reader implementation] support flow activity queries based on time (Varun Saxena via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "e3e857866d9fdefb7e353b21ae24eab4401e60b3",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "0f44b5508d2ffcae08f130b6535a9832d37e2b38",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,19 @@\n   protected ResultScanner getResults(Configuration hbaseConf,\n       Connection conn) throws IOException {\n     Scan scan \u003d new Scan();\n-    scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n+    if (createdTimeBegin \u003d\u003d DEFAULT_BEGIN_TIME \u0026\u0026\n+        createdTimeEnd \u003d\u003d DEFAULT_END_TIME) {\n+      scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n+    } else {\n+      scan.setStartRow(\n+          FlowActivityRowKey.getRowKeyPrefix(clusterId, createdTimeEnd));\n+      scan.setStopRow(\n+          FlowActivityRowKey.getRowKeyPrefix(clusterId,\n+              (createdTimeBegin \u003c\u003d 0 ? 0: (createdTimeBegin - 1))));\n+    }\n     // use the page filter to limit the result to the page size\n     // the scanner may still return more than the limit; therefore we need to\n     // read the right number as we iterate\n     scan.setFilter(new PageFilter(limit));\n     return table.getResultScanner(hbaseConf, conn, scan);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected ResultScanner getResults(Configuration hbaseConf,\n      Connection conn) throws IOException {\n    Scan scan \u003d new Scan();\n    if (createdTimeBegin \u003d\u003d DEFAULT_BEGIN_TIME \u0026\u0026\n        createdTimeEnd \u003d\u003d DEFAULT_END_TIME) {\n      scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n    } else {\n      scan.setStartRow(\n          FlowActivityRowKey.getRowKeyPrefix(clusterId, createdTimeEnd));\n      scan.setStopRow(\n          FlowActivityRowKey.getRowKeyPrefix(clusterId,\n              (createdTimeBegin \u003c\u003d 0 ? 0: (createdTimeBegin - 1))));\n    }\n    // use the page filter to limit the result to the page size\n    // the scanner may still return more than the limit; therefore we need to\n    // read the right number as we iterate\n    scan.setFilter(new PageFilter(limit));\n    return table.getResultScanner(hbaseConf, conn, scan);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowActivityEntityReader.java",
      "extendedDetails": {}
    },
    "708fa8b1ae85b6efda318368bc0c0ba02d4958c8": {
      "type": "Yreturntypechange",
      "commitMessage": "YARN-4210. HBase reader throws NPE if Get returns no rows (Varun Saxena via vrushali)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "708fa8b1ae85b6efda318368bc0c0ba02d4958c8",
      "commitAuthor": "Vrushali Channapattan",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "10fa6da7d8a6013698767c6136ae20f0e04415e9",
      "commitAuthorOld": "Vrushali",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,10 @@\n-  protected Iterable\u003cResult\u003e getResults(Configuration hbaseConf,\n+  protected ResultScanner getResults(Configuration hbaseConf,\n       Connection conn) throws IOException {\n     Scan scan \u003d new Scan();\n     scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n     // use the page filter to limit the result to the page size\n     // the scanner may still return more than the limit; therefore we need to\n     // read the right number as we iterate\n     scan.setFilter(new PageFilter(limit));\n     return table.getResultScanner(hbaseConf, conn, scan);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected ResultScanner getResults(Configuration hbaseConf,\n      Connection conn) throws IOException {\n    Scan scan \u003d new Scan();\n    scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n    // use the page filter to limit the result to the page size\n    // the scanner may still return more than the limit; therefore we need to\n    // read the right number as we iterate\n    scan.setFilter(new PageFilter(limit));\n    return table.getResultScanner(hbaseConf, conn, scan);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowActivityEntityReader.java",
      "extendedDetails": {
        "oldValue": "Iterable\u003cResult\u003e",
        "newValue": "ResultScanner"
      }
    },
    "10fa6da7d8a6013698767c6136ae20f0e04415e9": {
      "type": "Yintroduced",
      "commitMessage": "YARN-4074. [timeline reader] implement support for querying for flows and flow runs (sjlee via vrushali)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "10fa6da7d8a6013698767c6136ae20f0e04415e9",
      "commitAuthor": "Vrushali",
      "diff": "@@ -0,0 +1,10 @@\n+  protected Iterable\u003cResult\u003e getResults(Configuration hbaseConf,\n+      Connection conn) throws IOException {\n+    Scan scan \u003d new Scan();\n+    scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n+    // use the page filter to limit the result to the page size\n+    // the scanner may still return more than the limit; therefore we need to\n+    // read the right number as we iterate\n+    scan.setFilter(new PageFilter(limit));\n+    return table.getResultScanner(hbaseConf, conn, scan);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected Iterable\u003cResult\u003e getResults(Configuration hbaseConf,\n      Connection conn) throws IOException {\n    Scan scan \u003d new Scan();\n    scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));\n    // use the page filter to limit the result to the page size\n    // the scanner may still return more than the limit; therefore we need to\n    // read the right number as we iterate\n    scan.setFilter(new PageFilter(limit));\n    return table.getResultScanner(hbaseConf, conn, scan);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowActivityEntityReader.java"
    }
  }
}