{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockStorageMovementNeeded.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java",
  "functionStartLine": 232,
  "functionEndLine": 273,
  "numCommitsSeen": 18,
  "timeTaken": 8480,
  "changeHistory": [
    "39ed3a66dbb01383ed16b141183fc48bfd2e613d",
    "66e8f9b31529226309c924226a53dead3e6fcf11",
    "8467ec24fb74f30371d5a13e893fc56309ee9372",
    "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
    "99594b48b8e040ab5a0939d7c3dbcfb34400e6fc",
    "3159b39cf8ef704835325263154fb1a1cecc109d",
    "8d4f74e7339abc77dc0daa162d7bd2814bd79b3d",
    "05d4daf6ba3e5bd40f46e8003ee12fc7c613453d",
    "78420719eb1f138c6f10558befb7bc8ebcc28a54",
    "c561cb316e365ef674784cd6cf0b12c0fbc271a3",
    "68017e3349e3b71a9c49f2ccea2558231ff8485d",
    "bfd3f8bd8a9ae2186ec3e4addc71f912ec7b8923",
    "7ea24fc06c081e2ba6f5f66d212abb14b80c9064"
  ],
  "changeHistoryShort": {
    "39ed3a66dbb01383ed16b141183fc48bfd2e613d": "Ybodychange",
    "66e8f9b31529226309c924226a53dead3e6fcf11": "Ybodychange",
    "8467ec24fb74f30371d5a13e893fc56309ee9372": "Ybodychange",
    "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52": "Ybodychange",
    "99594b48b8e040ab5a0939d7c3dbcfb34400e6fc": "Ybodychange",
    "3159b39cf8ef704835325263154fb1a1cecc109d": "Ybodychange",
    "8d4f74e7339abc77dc0daa162d7bd2814bd79b3d": "Ybodychange",
    "05d4daf6ba3e5bd40f46e8003ee12fc7c613453d": "Ybodychange",
    "78420719eb1f138c6f10558befb7bc8ebcc28a54": "Yfilerename",
    "c561cb316e365ef674784cd6cf0b12c0fbc271a3": "Ybodychange",
    "68017e3349e3b71a9c49f2ccea2558231ff8485d": "Ybodychange",
    "bfd3f8bd8a9ae2186ec3e4addc71f912ec7b8923": "Ybodychange",
    "7ea24fc06c081e2ba6f5f66d212abb14b80c9064": "Yintroduced"
  },
  "changeHistoryDetails": {
    "39ed3a66dbb01383ed16b141183fc48bfd2e613d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13076: [SPS]: Cleanup work for HDFS-10285 merge. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "39ed3a66dbb01383ed16b141183fc48bfd2e613d",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "66e8f9b31529226309c924226a53dead3e6fcf11",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,42 @@\n     public void run() {\n       LOG.info(\"Starting SPSPathIdProcessor!.\");\n-      long lastStatusCleanTime \u003d 0;\n       Long startINode \u003d null;\n       while (ctxt.isRunning()) {\n         try {\n           if (!ctxt.isInSafeMode()) {\n             if (startINode \u003d\u003d null) {\n               startINode \u003d ctxt.getNextSPSPath();\n             } // else same id will be retried\n             if (startINode \u003d\u003d null) {\n               // Waiting for SPS path\n               Thread.sleep(3000);\n             } else {\n-              spsStatus.put(startINode,\n-                  new StoragePolicySatisfyPathStatusInfo(\n-                      StoragePolicySatisfyPathStatus.IN_PROGRESS));\n               ctxt.scanAndCollectFiles(startINode);\n               // check if directory was empty and no child added to queue\n               DirPendingWorkInfo dirPendingWorkInfo \u003d\n                   pendingWorkForDirectory.get(startINode);\n               if (dirPendingWorkInfo !\u003d null\n                   \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                 ctxt.removeSPSHint(startINode);\n                 pendingWorkForDirectory.remove(startINode);\n-                updateStatus(startINode, true);\n               }\n             }\n-            //Clear the SPS status if status is in SUCCESS more than 5 min.\n-            if (Time.monotonicNow()\n-                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n-              lastStatusCleanTime \u003d Time.monotonicNow();\n-              cleanSPSStatus();\n-            }\n             startINode \u003d null; // Current inode successfully scanned.\n           }\n         } catch (Throwable t) {\n           String reClass \u003d t.getClass().getName();\n           if (InterruptedException.class.getName().equals(reClass)) {\n             LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n             break;\n           }\n           LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n               t);\n           try {\n             Thread.sleep(3000);\n           } catch (InterruptedException e) {\n             LOG.info(\"Interrupted while waiting in SPSPathIdProcessor\", t);\n             break;\n           }\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting SPSPathIdProcessor!.\");\n      Long startINode \u003d null;\n      while (ctxt.isRunning()) {\n        try {\n          if (!ctxt.isInSafeMode()) {\n            if (startINode \u003d\u003d null) {\n              startINode \u003d ctxt.getNextSPSPath();\n            } // else same id will be retried\n            if (startINode \u003d\u003d null) {\n              // Waiting for SPS path\n              Thread.sleep(3000);\n            } else {\n              ctxt.scanAndCollectFiles(startINode);\n              // check if directory was empty and no child added to queue\n              DirPendingWorkInfo dirPendingWorkInfo \u003d\n                  pendingWorkForDirectory.get(startINode);\n              if (dirPendingWorkInfo !\u003d null\n                  \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                ctxt.removeSPSHint(startINode);\n                pendingWorkForDirectory.remove(startINode);\n              }\n            }\n            startINode \u003d null; // Current inode successfully scanned.\n          }\n        } catch (Throwable t) {\n          String reClass \u003d t.getClass().getName();\n          if (InterruptedException.class.getName().equals(reClass)) {\n            LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n            break;\n          }\n          LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n              t);\n          try {\n            Thread.sleep(3000);\n          } catch (InterruptedException e) {\n            LOG.info(\"Interrupted while waiting in SPSPathIdProcessor\", t);\n            break;\n          }\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java",
      "extendedDetails": {}
    },
    "66e8f9b31529226309c924226a53dead3e6fcf11": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13381 : [SPS]: Use DFSUtilClient#makePathFromFileId() to prepare satisfier file path. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "66e8f9b31529226309c924226a53dead3e6fcf11",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "8467ec24fb74f30371d5a13e893fc56309ee9372",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,53 @@\n     public void run() {\n       LOG.info(\"Starting SPSPathIdProcessor!.\");\n       long lastStatusCleanTime \u003d 0;\n-      T startINode \u003d null;\n+      Long startINode \u003d null;\n       while (ctxt.isRunning()) {\n         try {\n           if (!ctxt.isInSafeMode()) {\n             if (startINode \u003d\u003d null) {\n               startINode \u003d ctxt.getNextSPSPath();\n             } // else same id will be retried\n             if (startINode \u003d\u003d null) {\n               // Waiting for SPS path\n               Thread.sleep(3000);\n             } else {\n               spsStatus.put(startINode,\n                   new StoragePolicySatisfyPathStatusInfo(\n                       StoragePolicySatisfyPathStatus.IN_PROGRESS));\n-              fileCollector.scanAndCollectFiles(startINode);\n+              ctxt.scanAndCollectFiles(startINode);\n               // check if directory was empty and no child added to queue\n               DirPendingWorkInfo dirPendingWorkInfo \u003d\n                   pendingWorkForDirectory.get(startINode);\n               if (dirPendingWorkInfo !\u003d null\n                   \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                 ctxt.removeSPSHint(startINode);\n                 pendingWorkForDirectory.remove(startINode);\n                 updateStatus(startINode, true);\n               }\n             }\n             //Clear the SPS status if status is in SUCCESS more than 5 min.\n             if (Time.monotonicNow()\n                 - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n               lastStatusCleanTime \u003d Time.monotonicNow();\n               cleanSPSStatus();\n             }\n             startINode \u003d null; // Current inode successfully scanned.\n           }\n         } catch (Throwable t) {\n           String reClass \u003d t.getClass().getName();\n           if (InterruptedException.class.getName().equals(reClass)) {\n             LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n             break;\n           }\n           LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n               t);\n           try {\n             Thread.sleep(3000);\n           } catch (InterruptedException e) {\n             LOG.info(\"Interrupted while waiting in SPSPathIdProcessor\", t);\n             break;\n           }\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting SPSPathIdProcessor!.\");\n      long lastStatusCleanTime \u003d 0;\n      Long startINode \u003d null;\n      while (ctxt.isRunning()) {\n        try {\n          if (!ctxt.isInSafeMode()) {\n            if (startINode \u003d\u003d null) {\n              startINode \u003d ctxt.getNextSPSPath();\n            } // else same id will be retried\n            if (startINode \u003d\u003d null) {\n              // Waiting for SPS path\n              Thread.sleep(3000);\n            } else {\n              spsStatus.put(startINode,\n                  new StoragePolicySatisfyPathStatusInfo(\n                      StoragePolicySatisfyPathStatus.IN_PROGRESS));\n              ctxt.scanAndCollectFiles(startINode);\n              // check if directory was empty and no child added to queue\n              DirPendingWorkInfo dirPendingWorkInfo \u003d\n                  pendingWorkForDirectory.get(startINode);\n              if (dirPendingWorkInfo !\u003d null\n                  \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                ctxt.removeSPSHint(startINode);\n                pendingWorkForDirectory.remove(startINode);\n                updateStatus(startINode, true);\n              }\n            }\n            //Clear the SPS status if status is in SUCCESS more than 5 min.\n            if (Time.monotonicNow()\n                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n              lastStatusCleanTime \u003d Time.monotonicNow();\n              cleanSPSStatus();\n            }\n            startINode \u003d null; // Current inode successfully scanned.\n          }\n        } catch (Throwable t) {\n          String reClass \u003d t.getClass().getName();\n          if (InterruptedException.class.getName().equals(reClass)) {\n            LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n            break;\n          }\n          LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n              t);\n          try {\n            Thread.sleep(3000);\n          } catch (InterruptedException e) {\n            LOG.info(\"Interrupted while waiting in SPSPathIdProcessor\", t);\n            break;\n          }\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java",
      "extendedDetails": {}
    },
    "8467ec24fb74f30371d5a13e893fc56309ee9372": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13110: [SPS]: Reduce the number of APIs in NamenodeProtocol used by external satisfier. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "8467ec24fb74f30371d5a13e893fc56309ee9372",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,53 @@\n     public void run() {\n       LOG.info(\"Starting SPSPathIdProcessor!.\");\n       long lastStatusCleanTime \u003d 0;\n-      Long startINodeId \u003d null;\n+      T startINode \u003d null;\n       while (ctxt.isRunning()) {\n         try {\n           if (!ctxt.isInSafeMode()) {\n-            if (startINodeId \u003d\u003d null) {\n-              startINodeId \u003d ctxt.getNextSPSPathId();\n+            if (startINode \u003d\u003d null) {\n+              startINode \u003d ctxt.getNextSPSPath();\n             } // else same id will be retried\n-            if (startINodeId \u003d\u003d null) {\n+            if (startINode \u003d\u003d null) {\n               // Waiting for SPS path\n               Thread.sleep(3000);\n             } else {\n-              spsStatus.put(startINodeId,\n+              spsStatus.put(startINode,\n                   new StoragePolicySatisfyPathStatusInfo(\n                       StoragePolicySatisfyPathStatus.IN_PROGRESS));\n-              fileIDCollector.scanAndCollectFileIds(startINodeId);\n+              fileCollector.scanAndCollectFiles(startINode);\n               // check if directory was empty and no child added to queue\n               DirPendingWorkInfo dirPendingWorkInfo \u003d\n-                  pendingWorkForDirectory.get(startINodeId);\n+                  pendingWorkForDirectory.get(startINode);\n               if (dirPendingWorkInfo !\u003d null\n                   \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n-                ctxt.removeSPSHint(startINodeId);\n-                pendingWorkForDirectory.remove(startINodeId);\n-                updateStatus(startINodeId, true);\n+                ctxt.removeSPSHint(startINode);\n+                pendingWorkForDirectory.remove(startINode);\n+                updateStatus(startINode, true);\n               }\n             }\n             //Clear the SPS status if status is in SUCCESS more than 5 min.\n             if (Time.monotonicNow()\n                 - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n               lastStatusCleanTime \u003d Time.monotonicNow();\n               cleanSPSStatus();\n             }\n-            startINodeId \u003d null; // Current inode id successfully scanned.\n+            startINode \u003d null; // Current inode successfully scanned.\n           }\n         } catch (Throwable t) {\n           String reClass \u003d t.getClass().getName();\n           if (InterruptedException.class.getName().equals(reClass)) {\n             LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n             break;\n           }\n           LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n               t);\n           try {\n             Thread.sleep(3000);\n           } catch (InterruptedException e) {\n             LOG.info(\"Interrupted while waiting in SPSPathIdProcessor\", t);\n             break;\n           }\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting SPSPathIdProcessor!.\");\n      long lastStatusCleanTime \u003d 0;\n      T startINode \u003d null;\n      while (ctxt.isRunning()) {\n        try {\n          if (!ctxt.isInSafeMode()) {\n            if (startINode \u003d\u003d null) {\n              startINode \u003d ctxt.getNextSPSPath();\n            } // else same id will be retried\n            if (startINode \u003d\u003d null) {\n              // Waiting for SPS path\n              Thread.sleep(3000);\n            } else {\n              spsStatus.put(startINode,\n                  new StoragePolicySatisfyPathStatusInfo(\n                      StoragePolicySatisfyPathStatus.IN_PROGRESS));\n              fileCollector.scanAndCollectFiles(startINode);\n              // check if directory was empty and no child added to queue\n              DirPendingWorkInfo dirPendingWorkInfo \u003d\n                  pendingWorkForDirectory.get(startINode);\n              if (dirPendingWorkInfo !\u003d null\n                  \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                ctxt.removeSPSHint(startINode);\n                pendingWorkForDirectory.remove(startINode);\n                updateStatus(startINode, true);\n              }\n            }\n            //Clear the SPS status if status is in SUCCESS more than 5 min.\n            if (Time.monotonicNow()\n                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n              lastStatusCleanTime \u003d Time.monotonicNow();\n              cleanSPSStatus();\n            }\n            startINode \u003d null; // Current inode successfully scanned.\n          }\n        } catch (Throwable t) {\n          String reClass \u003d t.getClass().getName();\n          if (InterruptedException.class.getName().equals(reClass)) {\n            LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n            break;\n          }\n          LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n              t);\n          try {\n            Thread.sleep(3000);\n          } catch (InterruptedException e) {\n            LOG.info(\"Interrupted while waiting in SPSPathIdProcessor\", t);\n            break;\n          }\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java",
      "extendedDetails": {}
    },
    "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13097: [SPS]: Fix the branch review comments(Part1). Contributed by Surendra Singh.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "99594b48b8e040ab5a0939d7c3dbcfb34400e6fc",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,53 @@\n     public void run() {\n       LOG.info(\"Starting SPSPathIdProcessor!.\");\n       long lastStatusCleanTime \u003d 0;\n       Long startINodeId \u003d null;\n       while (ctxt.isRunning()) {\n         try {\n           if (!ctxt.isInSafeMode()) {\n             if (startINodeId \u003d\u003d null) {\n               startINodeId \u003d ctxt.getNextSPSPathId();\n             } // else same id will be retried\n             if (startINodeId \u003d\u003d null) {\n               // Waiting for SPS path\n               Thread.sleep(3000);\n             } else {\n               spsStatus.put(startINodeId,\n                   new StoragePolicySatisfyPathStatusInfo(\n                       StoragePolicySatisfyPathStatus.IN_PROGRESS));\n               fileIDCollector.scanAndCollectFileIds(startINodeId);\n               // check if directory was empty and no child added to queue\n               DirPendingWorkInfo dirPendingWorkInfo \u003d\n                   pendingWorkForDirectory.get(startINodeId);\n               if (dirPendingWorkInfo !\u003d null\n                   \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                 ctxt.removeSPSHint(startINodeId);\n                 pendingWorkForDirectory.remove(startINodeId);\n                 updateStatus(startINodeId, true);\n               }\n             }\n             //Clear the SPS status if status is in SUCCESS more than 5 min.\n             if (Time.monotonicNow()\n                 - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n               lastStatusCleanTime \u003d Time.monotonicNow();\n-              cleanSpsStatus();\n+              cleanSPSStatus();\n             }\n             startINodeId \u003d null; // Current inode id successfully scanned.\n           }\n         } catch (Throwable t) {\n           String reClass \u003d t.getClass().getName();\n           if (InterruptedException.class.getName().equals(reClass)) {\n             LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n             break;\n           }\n           LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n               t);\n           try {\n             Thread.sleep(3000);\n           } catch (InterruptedException e) {\n             LOG.info(\"Interrupted while waiting in SPSPathIdProcessor\", t);\n             break;\n           }\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting SPSPathIdProcessor!.\");\n      long lastStatusCleanTime \u003d 0;\n      Long startINodeId \u003d null;\n      while (ctxt.isRunning()) {\n        try {\n          if (!ctxt.isInSafeMode()) {\n            if (startINodeId \u003d\u003d null) {\n              startINodeId \u003d ctxt.getNextSPSPathId();\n            } // else same id will be retried\n            if (startINodeId \u003d\u003d null) {\n              // Waiting for SPS path\n              Thread.sleep(3000);\n            } else {\n              spsStatus.put(startINodeId,\n                  new StoragePolicySatisfyPathStatusInfo(\n                      StoragePolicySatisfyPathStatus.IN_PROGRESS));\n              fileIDCollector.scanAndCollectFileIds(startINodeId);\n              // check if directory was empty and no child added to queue\n              DirPendingWorkInfo dirPendingWorkInfo \u003d\n                  pendingWorkForDirectory.get(startINodeId);\n              if (dirPendingWorkInfo !\u003d null\n                  \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                ctxt.removeSPSHint(startINodeId);\n                pendingWorkForDirectory.remove(startINodeId);\n                updateStatus(startINodeId, true);\n              }\n            }\n            //Clear the SPS status if status is in SUCCESS more than 5 min.\n            if (Time.monotonicNow()\n                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n              lastStatusCleanTime \u003d Time.monotonicNow();\n              cleanSPSStatus();\n            }\n            startINodeId \u003d null; // Current inode id successfully scanned.\n          }\n        } catch (Throwable t) {\n          String reClass \u003d t.getClass().getName();\n          if (InterruptedException.class.getName().equals(reClass)) {\n            LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n            break;\n          }\n          LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n              t);\n          try {\n            Thread.sleep(3000);\n          } catch (InterruptedException e) {\n            LOG.info(\"Interrupted while waiting in SPSPathIdProcessor\", t);\n            break;\n          }\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java",
      "extendedDetails": {}
    },
    "99594b48b8e040ab5a0939d7c3dbcfb34400e6fc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13075. [SPS]: Provide External Context implementation. Contributed by Uma Maheswara Rao G.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "99594b48b8e040ab5a0939d7c3dbcfb34400e6fc",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "3b83110d5ed582b9f913ecf3f62ce410535f8fca",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,53 @@\n     public void run() {\n       LOG.info(\"Starting SPSPathIdProcessor!.\");\n       long lastStatusCleanTime \u003d 0;\n       Long startINodeId \u003d null;\n       while (ctxt.isRunning()) {\n         try {\n           if (!ctxt.isInSafeMode()) {\n             if (startINodeId \u003d\u003d null) {\n               startINodeId \u003d ctxt.getNextSPSPathId();\n             } // else same id will be retried\n             if (startINodeId \u003d\u003d null) {\n               // Waiting for SPS path\n               Thread.sleep(3000);\n             } else {\n               spsStatus.put(startINodeId,\n                   new StoragePolicySatisfyPathStatusInfo(\n                       StoragePolicySatisfyPathStatus.IN_PROGRESS));\n               fileIDCollector.scanAndCollectFileIds(startINodeId);\n               // check if directory was empty and no child added to queue\n               DirPendingWorkInfo dirPendingWorkInfo \u003d\n                   pendingWorkForDirectory.get(startINodeId);\n               if (dirPendingWorkInfo !\u003d null\n                   \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                 ctxt.removeSPSHint(startINodeId);\n                 pendingWorkForDirectory.remove(startINodeId);\n                 updateStatus(startINodeId, true);\n               }\n             }\n             //Clear the SPS status if status is in SUCCESS more than 5 min.\n             if (Time.monotonicNow()\n                 - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n               lastStatusCleanTime \u003d Time.monotonicNow();\n               cleanSpsStatus();\n             }\n             startINodeId \u003d null; // Current inode id successfully scanned.\n           }\n         } catch (Throwable t) {\n           String reClass \u003d t.getClass().getName();\n           if (InterruptedException.class.getName().equals(reClass)) {\n             LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n-            Thread.currentThread().interrupt();\n             break;\n           }\n           LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n               t);\n-          // TODO: may be we should retry the current inode id?\n+          try {\n+            Thread.sleep(3000);\n+          } catch (InterruptedException e) {\n+            LOG.info(\"Interrupted while waiting in SPSPathIdProcessor\", t);\n+            break;\n+          }\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting SPSPathIdProcessor!.\");\n      long lastStatusCleanTime \u003d 0;\n      Long startINodeId \u003d null;\n      while (ctxt.isRunning()) {\n        try {\n          if (!ctxt.isInSafeMode()) {\n            if (startINodeId \u003d\u003d null) {\n              startINodeId \u003d ctxt.getNextSPSPathId();\n            } // else same id will be retried\n            if (startINodeId \u003d\u003d null) {\n              // Waiting for SPS path\n              Thread.sleep(3000);\n            } else {\n              spsStatus.put(startINodeId,\n                  new StoragePolicySatisfyPathStatusInfo(\n                      StoragePolicySatisfyPathStatus.IN_PROGRESS));\n              fileIDCollector.scanAndCollectFileIds(startINodeId);\n              // check if directory was empty and no child added to queue\n              DirPendingWorkInfo dirPendingWorkInfo \u003d\n                  pendingWorkForDirectory.get(startINodeId);\n              if (dirPendingWorkInfo !\u003d null\n                  \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                ctxt.removeSPSHint(startINodeId);\n                pendingWorkForDirectory.remove(startINodeId);\n                updateStatus(startINodeId, true);\n              }\n            }\n            //Clear the SPS status if status is in SUCCESS more than 5 min.\n            if (Time.monotonicNow()\n                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n              lastStatusCleanTime \u003d Time.monotonicNow();\n              cleanSpsStatus();\n            }\n            startINodeId \u003d null; // Current inode id successfully scanned.\n          }\n        } catch (Throwable t) {\n          String reClass \u003d t.getClass().getName();\n          if (InterruptedException.class.getName().equals(reClass)) {\n            LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n            break;\n          }\n          LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n              t);\n          try {\n            Thread.sleep(3000);\n          } catch (InterruptedException e) {\n            LOG.info(\"Interrupted while waiting in SPSPathIdProcessor\", t);\n            break;\n          }\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java",
      "extendedDetails": {}
    },
    "3159b39cf8ef704835325263154fb1a1cecc109d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13025. [SPS]: Implement a mechanism to scan the files for external SPS. Contributed by Uma Maheswara Rao G.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "3159b39cf8ef704835325263154fb1a1cecc109d",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "8d4f74e7339abc77dc0daa162d7bd2814bd79b3d",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,49 @@\n     public void run() {\n-      LOG.info(\"Starting FileInodeIdCollector!.\");\n+      LOG.info(\"Starting SPSPathIdProcessor!.\");\n       long lastStatusCleanTime \u003d 0;\n+      Long startINodeId \u003d null;\n       while (ctxt.isRunning()) {\n-        LOG.info(\"Running FileInodeIdCollector!.\");\n         try {\n           if (!ctxt.isInSafeMode()) {\n-            Long startINodeId \u003d ctxt.getNextSPSPathId();\n+            if (startINodeId \u003d\u003d null) {\n+              startINodeId \u003d ctxt.getNextSPSPathId();\n+            } // else same id will be retried\n             if (startINodeId \u003d\u003d null) {\n               // Waiting for SPS path\n               Thread.sleep(3000);\n             } else {\n               spsStatus.put(startINodeId,\n                   new StoragePolicySatisfyPathStatusInfo(\n                       StoragePolicySatisfyPathStatus.IN_PROGRESS));\n               fileIDCollector.scanAndCollectFileIds(startINodeId);\n               // check if directory was empty and no child added to queue\n               DirPendingWorkInfo dirPendingWorkInfo \u003d\n                   pendingWorkForDirectory.get(startINodeId);\n               if (dirPendingWorkInfo !\u003d null\n                   \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                 ctxt.removeSPSHint(startINodeId);\n                 pendingWorkForDirectory.remove(startINodeId);\n                 updateStatus(startINodeId, true);\n               }\n             }\n             //Clear the SPS status if status is in SUCCESS more than 5 min.\n             if (Time.monotonicNow()\n                 - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n               lastStatusCleanTime \u003d Time.monotonicNow();\n               cleanSpsStatus();\n             }\n+            startINodeId \u003d null; // Current inode id successfully scanned.\n           }\n         } catch (Throwable t) {\n-          LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n+          String reClass \u003d t.getClass().getName();\n+          if (InterruptedException.class.getName().equals(reClass)) {\n+            LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n+            Thread.currentThread().interrupt();\n+            break;\n+          }\n+          LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n+              t);\n+          // TODO: may be we should retry the current inode id?\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting SPSPathIdProcessor!.\");\n      long lastStatusCleanTime \u003d 0;\n      Long startINodeId \u003d null;\n      while (ctxt.isRunning()) {\n        try {\n          if (!ctxt.isInSafeMode()) {\n            if (startINodeId \u003d\u003d null) {\n              startINodeId \u003d ctxt.getNextSPSPathId();\n            } // else same id will be retried\n            if (startINodeId \u003d\u003d null) {\n              // Waiting for SPS path\n              Thread.sleep(3000);\n            } else {\n              spsStatus.put(startINodeId,\n                  new StoragePolicySatisfyPathStatusInfo(\n                      StoragePolicySatisfyPathStatus.IN_PROGRESS));\n              fileIDCollector.scanAndCollectFileIds(startINodeId);\n              // check if directory was empty and no child added to queue\n              DirPendingWorkInfo dirPendingWorkInfo \u003d\n                  pendingWorkForDirectory.get(startINodeId);\n              if (dirPendingWorkInfo !\u003d null\n                  \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                ctxt.removeSPSHint(startINodeId);\n                pendingWorkForDirectory.remove(startINodeId);\n                updateStatus(startINodeId, true);\n              }\n            }\n            //Clear the SPS status if status is in SUCCESS more than 5 min.\n            if (Time.monotonicNow()\n                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n              lastStatusCleanTime \u003d Time.monotonicNow();\n              cleanSpsStatus();\n            }\n            startINodeId \u003d null; // Current inode id successfully scanned.\n          }\n        } catch (Throwable t) {\n          String reClass \u003d t.getClass().getName();\n          if (InterruptedException.class.getName().equals(reClass)) {\n            LOG.info(\"SPSPathIdProcessor thread is interrupted. Stopping..\");\n            Thread.currentThread().interrupt();\n            break;\n          }\n          LOG.warn(\"Exception while scanning file inodes to satisfy the policy\",\n              t);\n          // TODO: may be we should retry the current inode id?\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java",
      "extendedDetails": {}
    },
    "8d4f74e7339abc77dc0daa162d7bd2814bd79b3d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12911. [SPS]: Modularize the SPS code and expose necessary interfaces for external/internal implementations. Contributed by Uma Maheswara Rao G\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "8d4f74e7339abc77dc0daa162d7bd2814bd79b3d",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "05d4daf6ba3e5bd40f46e8003ee12fc7c613453d",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,38 @@\n     public void run() {\n       LOG.info(\"Starting FileInodeIdCollector!.\");\n       long lastStatusCleanTime \u003d 0;\n       while (ctxt.isRunning()) {\n+        LOG.info(\"Running FileInodeIdCollector!.\");\n         try {\n           if (!ctxt.isInSafeMode()) {\n-            Long startINodeId \u003d spsDirsToBeTraveresed.poll();\n+            Long startINodeId \u003d ctxt.getNextSPSPathId();\n             if (startINodeId \u003d\u003d null) {\n               // Waiting for SPS path\n-              synchronized (spsDirsToBeTraveresed) {\n-                spsDirsToBeTraveresed.wait(5000);\n-              }\n+              Thread.sleep(3000);\n             } else {\n-              INode startInode \u003d getFSDirectory().getInode(startINodeId);\n-              if (startInode !\u003d null) {\n-                try {\n-                  remainingCapacity \u003d remainingCapacity();\n-                  spsStatus.put(startINodeId,\n-                      new StoragePolicySatisfyPathStatusInfo(\n-                          StoragePolicySatisfyPathStatus.IN_PROGRESS));\n-                  readLock();\n-                  traverseDir(startInode.asDirectory(), startINodeId,\n-                      HdfsFileStatus.EMPTY_NAME,\n-                      new SPSTraverseInfo(startINodeId));\n-                } finally {\n-                  readUnlock();\n-                }\n-                // Mark startInode traverse is done\n-                addAll(startInode.getId(), currentBatch, true);\n-                currentBatch.clear();\n-\n-                // check if directory was empty and no child added to queue\n-                DirPendingWorkInfo dirPendingWorkInfo \u003d\n-                    pendingWorkForDirectory.get(startInode.getId());\n-                if (dirPendingWorkInfo.isDirWorkDone()) {\n-                  ctxt.removeSPSHint(startInode.getId());\n-                  pendingWorkForDirectory.remove(startInode.getId());\n-                  updateStatus(startInode.getId(), true);\n-                }\n+              spsStatus.put(startINodeId,\n+                  new StoragePolicySatisfyPathStatusInfo(\n+                      StoragePolicySatisfyPathStatus.IN_PROGRESS));\n+              fileIDCollector.scanAndCollectFileIds(startINodeId);\n+              // check if directory was empty and no child added to queue\n+              DirPendingWorkInfo dirPendingWorkInfo \u003d\n+                  pendingWorkForDirectory.get(startINodeId);\n+              if (dirPendingWorkInfo !\u003d null\n+                  \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n+                ctxt.removeSPSHint(startINodeId);\n+                pendingWorkForDirectory.remove(startINodeId);\n+                updateStatus(startINodeId, true);\n               }\n             }\n             //Clear the SPS status if status is in SUCCESS more than 5 min.\n             if (Time.monotonicNow()\n                 - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n               lastStatusCleanTime \u003d Time.monotonicNow();\n               cleanSpsStatus();\n             }\n           }\n         } catch (Throwable t) {\n           LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting FileInodeIdCollector!.\");\n      long lastStatusCleanTime \u003d 0;\n      while (ctxt.isRunning()) {\n        LOG.info(\"Running FileInodeIdCollector!.\");\n        try {\n          if (!ctxt.isInSafeMode()) {\n            Long startINodeId \u003d ctxt.getNextSPSPathId();\n            if (startINodeId \u003d\u003d null) {\n              // Waiting for SPS path\n              Thread.sleep(3000);\n            } else {\n              spsStatus.put(startINodeId,\n                  new StoragePolicySatisfyPathStatusInfo(\n                      StoragePolicySatisfyPathStatus.IN_PROGRESS));\n              fileIDCollector.scanAndCollectFileIds(startINodeId);\n              // check if directory was empty and no child added to queue\n              DirPendingWorkInfo dirPendingWorkInfo \u003d\n                  pendingWorkForDirectory.get(startINodeId);\n              if (dirPendingWorkInfo !\u003d null\n                  \u0026\u0026 dirPendingWorkInfo.isDirWorkDone()) {\n                ctxt.removeSPSHint(startINodeId);\n                pendingWorkForDirectory.remove(startINodeId);\n                updateStatus(startINodeId, true);\n              }\n            }\n            //Clear the SPS status if status is in SUCCESS more than 5 min.\n            if (Time.monotonicNow()\n                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n              lastStatusCleanTime \u003d Time.monotonicNow();\n              cleanSpsStatus();\n            }\n          }\n        } catch (Throwable t) {\n          LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java",
      "extendedDetails": {}
    },
    "05d4daf6ba3e5bd40f46e8003ee12fc7c613453d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12982 : [SPS]: Reduce the locking and cleanup the Namesystem access. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "05d4daf6ba3e5bd40f46e8003ee12fc7c613453d",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "78420719eb1f138c6f10558befb7bc8ebcc28a54",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,53 @@\n     public void run() {\n       LOG.info(\"Starting FileInodeIdCollector!.\");\n       long lastStatusCleanTime \u003d 0;\n-      while (namesystem.isRunning() \u0026\u0026 sps.isRunning()) {\n+      while (ctxt.isRunning()) {\n         try {\n-          if (!namesystem.isInSafeMode()) {\n-            FSDirectory fsd \u003d namesystem.getFSDirectory();\n+          if (!ctxt.isInSafeMode()) {\n             Long startINodeId \u003d spsDirsToBeTraveresed.poll();\n             if (startINodeId \u003d\u003d null) {\n               // Waiting for SPS path\n               synchronized (spsDirsToBeTraveresed) {\n                 spsDirsToBeTraveresed.wait(5000);\n               }\n             } else {\n-              INode startInode \u003d fsd.getInode(startINodeId);\n+              INode startInode \u003d getFSDirectory().getInode(startINodeId);\n               if (startInode !\u003d null) {\n                 try {\n                   remainingCapacity \u003d remainingCapacity();\n                   spsStatus.put(startINodeId,\n                       new StoragePolicySatisfyPathStatusInfo(\n                           StoragePolicySatisfyPathStatus.IN_PROGRESS));\n                   readLock();\n                   traverseDir(startInode.asDirectory(), startINodeId,\n                       HdfsFileStatus.EMPTY_NAME,\n                       new SPSTraverseInfo(startINodeId));\n                 } finally {\n                   readUnlock();\n                 }\n                 // Mark startInode traverse is done\n                 addAll(startInode.getId(), currentBatch, true);\n                 currentBatch.clear();\n \n                 // check if directory was empty and no child added to queue\n                 DirPendingWorkInfo dirPendingWorkInfo \u003d\n                     pendingWorkForDirectory.get(startInode.getId());\n                 if (dirPendingWorkInfo.isDirWorkDone()) {\n-                  namesystem.removeXattr(startInode.getId(),\n-                      XATTR_SATISFY_STORAGE_POLICY);\n+                  ctxt.removeSPSHint(startInode.getId());\n                   pendingWorkForDirectory.remove(startInode.getId());\n                   updateStatus(startInode.getId(), true);\n                 }\n               }\n             }\n             //Clear the SPS status if status is in SUCCESS more than 5 min.\n             if (Time.monotonicNow()\n                 - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n               lastStatusCleanTime \u003d Time.monotonicNow();\n               cleanSpsStatus();\n             }\n           }\n         } catch (Throwable t) {\n           LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting FileInodeIdCollector!.\");\n      long lastStatusCleanTime \u003d 0;\n      while (ctxt.isRunning()) {\n        try {\n          if (!ctxt.isInSafeMode()) {\n            Long startINodeId \u003d spsDirsToBeTraveresed.poll();\n            if (startINodeId \u003d\u003d null) {\n              // Waiting for SPS path\n              synchronized (spsDirsToBeTraveresed) {\n                spsDirsToBeTraveresed.wait(5000);\n              }\n            } else {\n              INode startInode \u003d getFSDirectory().getInode(startINodeId);\n              if (startInode !\u003d null) {\n                try {\n                  remainingCapacity \u003d remainingCapacity();\n                  spsStatus.put(startINodeId,\n                      new StoragePolicySatisfyPathStatusInfo(\n                          StoragePolicySatisfyPathStatus.IN_PROGRESS));\n                  readLock();\n                  traverseDir(startInode.asDirectory(), startINodeId,\n                      HdfsFileStatus.EMPTY_NAME,\n                      new SPSTraverseInfo(startINodeId));\n                } finally {\n                  readUnlock();\n                }\n                // Mark startInode traverse is done\n                addAll(startInode.getId(), currentBatch, true);\n                currentBatch.clear();\n\n                // check if directory was empty and no child added to queue\n                DirPendingWorkInfo dirPendingWorkInfo \u003d\n                    pendingWorkForDirectory.get(startInode.getId());\n                if (dirPendingWorkInfo.isDirWorkDone()) {\n                  ctxt.removeSPSHint(startInode.getId());\n                  pendingWorkForDirectory.remove(startInode.getId());\n                  updateStatus(startInode.getId(), true);\n                }\n              }\n            }\n            //Clear the SPS status if status is in SUCCESS more than 5 min.\n            if (Time.monotonicNow()\n                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n              lastStatusCleanTime \u003d Time.monotonicNow();\n              cleanSpsStatus();\n            }\n          }\n        } catch (Throwable t) {\n          LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java",
      "extendedDetails": {}
    },
    "78420719eb1f138c6f10558befb7bc8ebcc28a54": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-12955: [SPS]: Move SPS classes to a separate package. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "78420719eb1f138c6f10558befb7bc8ebcc28a54",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "c561cb316e365ef674784cd6cf0b12c0fbc271a3",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting FileInodeIdCollector!.\");\n      long lastStatusCleanTime \u003d 0;\n      while (namesystem.isRunning() \u0026\u0026 sps.isRunning()) {\n        try {\n          if (!namesystem.isInSafeMode()) {\n            FSDirectory fsd \u003d namesystem.getFSDirectory();\n            Long startINodeId \u003d spsDirsToBeTraveresed.poll();\n            if (startINodeId \u003d\u003d null) {\n              // Waiting for SPS path\n              synchronized (spsDirsToBeTraveresed) {\n                spsDirsToBeTraveresed.wait(5000);\n              }\n            } else {\n              INode startInode \u003d fsd.getInode(startINodeId);\n              if (startInode !\u003d null) {\n                try {\n                  remainingCapacity \u003d remainingCapacity();\n                  spsStatus.put(startINodeId,\n                      new StoragePolicySatisfyPathStatusInfo(\n                          StoragePolicySatisfyPathStatus.IN_PROGRESS));\n                  readLock();\n                  traverseDir(startInode.asDirectory(), startINodeId,\n                      HdfsFileStatus.EMPTY_NAME,\n                      new SPSTraverseInfo(startINodeId));\n                } finally {\n                  readUnlock();\n                }\n                // Mark startInode traverse is done\n                addAll(startInode.getId(), currentBatch, true);\n                currentBatch.clear();\n\n                // check if directory was empty and no child added to queue\n                DirPendingWorkInfo dirPendingWorkInfo \u003d\n                    pendingWorkForDirectory.get(startInode.getId());\n                if (dirPendingWorkInfo.isDirWorkDone()) {\n                  namesystem.removeXattr(startInode.getId(),\n                      XATTR_SATISFY_STORAGE_POLICY);\n                  pendingWorkForDirectory.remove(startInode.getId());\n                  updateStatus(startInode.getId(), true);\n                }\n              }\n            }\n            //Clear the SPS status if status is in SUCCESS more than 5 min.\n            if (Time.monotonicNow()\n                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n              lastStatusCleanTime \u003d Time.monotonicNow();\n              cleanSpsStatus();\n            }\n          }\n        } catch (Throwable t) {\n          LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementNeeded.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java"
      }
    },
    "c561cb316e365ef674784cd6cf0b12c0fbc271a3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12106: [SPS]: Improve storage policy satisfier configurations. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "c561cb316e365ef674784cd6cf0b12c0fbc271a3",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "68017e3349e3b71a9c49f2ccea2558231ff8485d",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n     public void run() {\n       LOG.info(\"Starting FileInodeIdCollector!.\");\n       long lastStatusCleanTime \u003d 0;\n       while (namesystem.isRunning() \u0026\u0026 sps.isRunning()) {\n         try {\n           if (!namesystem.isInSafeMode()) {\n             FSDirectory fsd \u003d namesystem.getFSDirectory();\n             Long startINodeId \u003d spsDirsToBeTraveresed.poll();\n             if (startINodeId \u003d\u003d null) {\n               // Waiting for SPS path\n               synchronized (spsDirsToBeTraveresed) {\n                 spsDirsToBeTraveresed.wait(5000);\n               }\n             } else {\n               INode startInode \u003d fsd.getInode(startINodeId);\n               if (startInode !\u003d null) {\n                 try {\n                   remainingCapacity \u003d remainingCapacity();\n                   spsStatus.put(startINodeId,\n                       new StoragePolicySatisfyPathStatusInfo(\n                           StoragePolicySatisfyPathStatus.IN_PROGRESS));\n                   readLock();\n                   traverseDir(startInode.asDirectory(), startINodeId,\n                       HdfsFileStatus.EMPTY_NAME,\n                       new SPSTraverseInfo(startINodeId));\n                 } finally {\n                   readUnlock();\n                 }\n                 // Mark startInode traverse is done\n                 addAll(startInode.getId(), currentBatch, true);\n                 currentBatch.clear();\n \n                 // check if directory was empty and no child added to queue\n                 DirPendingWorkInfo dirPendingWorkInfo \u003d\n                     pendingWorkForDirectory.get(startInode.getId());\n                 if (dirPendingWorkInfo.isDirWorkDone()) {\n                   namesystem.removeXattr(startInode.getId(),\n                       XATTR_SATISFY_STORAGE_POLICY);\n                   pendingWorkForDirectory.remove(startInode.getId());\n-                  markSuccess(startInode.getId());\n+                  updateStatus(startInode.getId(), true);\n                 }\n               }\n             }\n             //Clear the SPS status if status is in SUCCESS more than 5 min.\n             if (Time.monotonicNow()\n                 - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n               lastStatusCleanTime \u003d Time.monotonicNow();\n               cleanSpsStatus();\n             }\n           }\n         } catch (Throwable t) {\n           LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting FileInodeIdCollector!.\");\n      long lastStatusCleanTime \u003d 0;\n      while (namesystem.isRunning() \u0026\u0026 sps.isRunning()) {\n        try {\n          if (!namesystem.isInSafeMode()) {\n            FSDirectory fsd \u003d namesystem.getFSDirectory();\n            Long startINodeId \u003d spsDirsToBeTraveresed.poll();\n            if (startINodeId \u003d\u003d null) {\n              // Waiting for SPS path\n              synchronized (spsDirsToBeTraveresed) {\n                spsDirsToBeTraveresed.wait(5000);\n              }\n            } else {\n              INode startInode \u003d fsd.getInode(startINodeId);\n              if (startInode !\u003d null) {\n                try {\n                  remainingCapacity \u003d remainingCapacity();\n                  spsStatus.put(startINodeId,\n                      new StoragePolicySatisfyPathStatusInfo(\n                          StoragePolicySatisfyPathStatus.IN_PROGRESS));\n                  readLock();\n                  traverseDir(startInode.asDirectory(), startINodeId,\n                      HdfsFileStatus.EMPTY_NAME,\n                      new SPSTraverseInfo(startINodeId));\n                } finally {\n                  readUnlock();\n                }\n                // Mark startInode traverse is done\n                addAll(startInode.getId(), currentBatch, true);\n                currentBatch.clear();\n\n                // check if directory was empty and no child added to queue\n                DirPendingWorkInfo dirPendingWorkInfo \u003d\n                    pendingWorkForDirectory.get(startInode.getId());\n                if (dirPendingWorkInfo.isDirWorkDone()) {\n                  namesystem.removeXattr(startInode.getId(),\n                      XATTR_SATISFY_STORAGE_POLICY);\n                  pendingWorkForDirectory.remove(startInode.getId());\n                  updateStatus(startInode.getId(), true);\n                }\n              }\n            }\n            //Clear the SPS status if status is in SUCCESS more than 5 min.\n            if (Time.monotonicNow()\n                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n              lastStatusCleanTime \u003d Time.monotonicNow();\n              cleanSpsStatus();\n            }\n          }\n        } catch (Throwable t) {\n          LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementNeeded.java",
      "extendedDetails": {}
    },
    "68017e3349e3b71a9c49f2ccea2558231ff8485d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12310: [SPS]: Provide an option to track the status of in progress requests. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "68017e3349e3b71a9c49f2ccea2558231ff8485d",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "bfd3f8bd8a9ae2186ec3e4addc71f912ec7b8923",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,55 @@\n     public void run() {\n       LOG.info(\"Starting FileInodeIdCollector!.\");\n+      long lastStatusCleanTime \u003d 0;\n       while (namesystem.isRunning() \u0026\u0026 sps.isRunning()) {\n         try {\n           if (!namesystem.isInSafeMode()) {\n             FSDirectory fsd \u003d namesystem.getFSDirectory();\n             Long startINodeId \u003d spsDirsToBeTraveresed.poll();\n             if (startINodeId \u003d\u003d null) {\n               // Waiting for SPS path\n               synchronized (spsDirsToBeTraveresed) {\n                 spsDirsToBeTraveresed.wait(5000);\n               }\n             } else {\n               INode startInode \u003d fsd.getInode(startINodeId);\n               if (startInode !\u003d null) {\n                 try {\n                   remainingCapacity \u003d remainingCapacity();\n+                  spsStatus.put(startINodeId,\n+                      new StoragePolicySatisfyPathStatusInfo(\n+                          StoragePolicySatisfyPathStatus.IN_PROGRESS));\n                   readLock();\n                   traverseDir(startInode.asDirectory(), startINodeId,\n                       HdfsFileStatus.EMPTY_NAME,\n                       new SPSTraverseInfo(startINodeId));\n                 } finally {\n                   readUnlock();\n                 }\n                 // Mark startInode traverse is done\n                 addAll(startInode.getId(), currentBatch, true);\n                 currentBatch.clear();\n \n                 // check if directory was empty and no child added to queue\n                 DirPendingWorkInfo dirPendingWorkInfo \u003d\n                     pendingWorkForDirectory.get(startInode.getId());\n                 if (dirPendingWorkInfo.isDirWorkDone()) {\n                   namesystem.removeXattr(startInode.getId(),\n                       XATTR_SATISFY_STORAGE_POLICY);\n                   pendingWorkForDirectory.remove(startInode.getId());\n+                  markSuccess(startInode.getId());\n                 }\n               }\n             }\n+            //Clear the SPS status if status is in SUCCESS more than 5 min.\n+            if (Time.monotonicNow()\n+                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n+              lastStatusCleanTime \u003d Time.monotonicNow();\n+              cleanSpsStatus();\n+            }\n           }\n         } catch (Throwable t) {\n           LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting FileInodeIdCollector!.\");\n      long lastStatusCleanTime \u003d 0;\n      while (namesystem.isRunning() \u0026\u0026 sps.isRunning()) {\n        try {\n          if (!namesystem.isInSafeMode()) {\n            FSDirectory fsd \u003d namesystem.getFSDirectory();\n            Long startINodeId \u003d spsDirsToBeTraveresed.poll();\n            if (startINodeId \u003d\u003d null) {\n              // Waiting for SPS path\n              synchronized (spsDirsToBeTraveresed) {\n                spsDirsToBeTraveresed.wait(5000);\n              }\n            } else {\n              INode startInode \u003d fsd.getInode(startINodeId);\n              if (startInode !\u003d null) {\n                try {\n                  remainingCapacity \u003d remainingCapacity();\n                  spsStatus.put(startINodeId,\n                      new StoragePolicySatisfyPathStatusInfo(\n                          StoragePolicySatisfyPathStatus.IN_PROGRESS));\n                  readLock();\n                  traverseDir(startInode.asDirectory(), startINodeId,\n                      HdfsFileStatus.EMPTY_NAME,\n                      new SPSTraverseInfo(startINodeId));\n                } finally {\n                  readUnlock();\n                }\n                // Mark startInode traverse is done\n                addAll(startInode.getId(), currentBatch, true);\n                currentBatch.clear();\n\n                // check if directory was empty and no child added to queue\n                DirPendingWorkInfo dirPendingWorkInfo \u003d\n                    pendingWorkForDirectory.get(startInode.getId());\n                if (dirPendingWorkInfo.isDirWorkDone()) {\n                  namesystem.removeXattr(startInode.getId(),\n                      XATTR_SATISFY_STORAGE_POLICY);\n                  pendingWorkForDirectory.remove(startInode.getId());\n                  markSuccess(startInode.getId());\n                }\n              }\n            }\n            //Clear the SPS status if status is in SUCCESS more than 5 min.\n            if (Time.monotonicNow()\n                - lastStatusCleanTime \u003e statusClearanceElapsedTimeMs) {\n              lastStatusCleanTime \u003d Time.monotonicNow();\n              cleanSpsStatus();\n            }\n          }\n        } catch (Throwable t) {\n          LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementNeeded.java",
      "extendedDetails": {}
    },
    "bfd3f8bd8a9ae2186ec3e4addc71f912ec7b8923": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12291: [SPS]: Provide a mechanism to recursively iterate and satisfy storage policy of all the files under the given dir. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "bfd3f8bd8a9ae2186ec3e4addc71f912ec7b8923",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "7ea24fc06c081e2ba6f5f66d212abb14b80c9064",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,44 @@\n     public void run() {\n       LOG.info(\"Starting FileInodeIdCollector!.\");\n       while (namesystem.isRunning() \u0026\u0026 sps.isRunning()) {\n         try {\n           if (!namesystem.isInSafeMode()) {\n             FSDirectory fsd \u003d namesystem.getFSDirectory();\n-            Long rootINodeId \u003d spsDirsToBeTraveresed.poll();\n-            if (rootINodeId \u003d\u003d null) {\n+            Long startINodeId \u003d spsDirsToBeTraveresed.poll();\n+            if (startINodeId \u003d\u003d null) {\n               // Waiting for SPS path\n               synchronized (spsDirsToBeTraveresed) {\n                 spsDirsToBeTraveresed.wait(5000);\n               }\n             } else {\n-              INode rootInode \u003d fsd.getInode(rootINodeId);\n-              if (rootInode !\u003d null) {\n-                // TODO : HDFS-12291\n-                // 1. Implement an efficient recursive directory iteration\n-                // mechanism and satisfies storage policy for all the files\n-                // under the given directory.\n-                // 2. Process files in batches,so datanodes workload can be\n-                // handled.\n-                List\u003cItemInfo\u003e itemInfoList \u003d\n-                    new ArrayList\u003c\u003e();\n-                for (INode childInode : rootInode.asDirectory()\n-                    .getChildrenList(Snapshot.CURRENT_STATE_ID)) {\n-                  if (childInode.isFile()\n-                      \u0026\u0026 childInode.asFile().numBlocks() !\u003d 0) {\n-                    itemInfoList.add(\n-                        new ItemInfo(rootINodeId, childInode.getId()));\n-                  }\n+              INode startInode \u003d fsd.getInode(startINodeId);\n+              if (startInode !\u003d null) {\n+                try {\n+                  remainingCapacity \u003d remainingCapacity();\n+                  readLock();\n+                  traverseDir(startInode.asDirectory(), startINodeId,\n+                      HdfsFileStatus.EMPTY_NAME,\n+                      new SPSTraverseInfo(startINodeId));\n+                } finally {\n+                  readUnlock();\n                 }\n-                if (itemInfoList.isEmpty()) {\n-                  // satisfy track info is empty, so remove the xAttr from the\n-                  // directory\n-                  namesystem.removeXattr(rootINodeId,\n+                // Mark startInode traverse is done\n+                addAll(startInode.getId(), currentBatch, true);\n+                currentBatch.clear();\n+\n+                // check if directory was empty and no child added to queue\n+                DirPendingWorkInfo dirPendingWorkInfo \u003d\n+                    pendingWorkForDirectory.get(startInode.getId());\n+                if (dirPendingWorkInfo.isDirWorkDone()) {\n+                  namesystem.removeXattr(startInode.getId(),\n                       XATTR_SATISFY_STORAGE_POLICY);\n+                  pendingWorkForDirectory.remove(startInode.getId());\n                 }\n-                addAll(rootINodeId, itemInfoList);\n               }\n             }\n           }\n         } catch (Throwable t) {\n           LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting FileInodeIdCollector!.\");\n      while (namesystem.isRunning() \u0026\u0026 sps.isRunning()) {\n        try {\n          if (!namesystem.isInSafeMode()) {\n            FSDirectory fsd \u003d namesystem.getFSDirectory();\n            Long startINodeId \u003d spsDirsToBeTraveresed.poll();\n            if (startINodeId \u003d\u003d null) {\n              // Waiting for SPS path\n              synchronized (spsDirsToBeTraveresed) {\n                spsDirsToBeTraveresed.wait(5000);\n              }\n            } else {\n              INode startInode \u003d fsd.getInode(startINodeId);\n              if (startInode !\u003d null) {\n                try {\n                  remainingCapacity \u003d remainingCapacity();\n                  readLock();\n                  traverseDir(startInode.asDirectory(), startINodeId,\n                      HdfsFileStatus.EMPTY_NAME,\n                      new SPSTraverseInfo(startINodeId));\n                } finally {\n                  readUnlock();\n                }\n                // Mark startInode traverse is done\n                addAll(startInode.getId(), currentBatch, true);\n                currentBatch.clear();\n\n                // check if directory was empty and no child added to queue\n                DirPendingWorkInfo dirPendingWorkInfo \u003d\n                    pendingWorkForDirectory.get(startInode.getId());\n                if (dirPendingWorkInfo.isDirWorkDone()) {\n                  namesystem.removeXattr(startInode.getId(),\n                      XATTR_SATISFY_STORAGE_POLICY);\n                  pendingWorkForDirectory.remove(startInode.getId());\n                }\n              }\n            }\n          }\n        } catch (Throwable t) {\n          LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementNeeded.java",
      "extendedDetails": {}
    },
    "7ea24fc06c081e2ba6f5f66d212abb14b80c9064": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-12225: [SPS]: Optimize extended attributes for tracking SPS movements. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "7ea24fc06c081e2ba6f5f66d212abb14b80c9064",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,46 @@\n+    public void run() {\n+      LOG.info(\"Starting FileInodeIdCollector!.\");\n+      while (namesystem.isRunning() \u0026\u0026 sps.isRunning()) {\n+        try {\n+          if (!namesystem.isInSafeMode()) {\n+            FSDirectory fsd \u003d namesystem.getFSDirectory();\n+            Long rootINodeId \u003d spsDirsToBeTraveresed.poll();\n+            if (rootINodeId \u003d\u003d null) {\n+              // Waiting for SPS path\n+              synchronized (spsDirsToBeTraveresed) {\n+                spsDirsToBeTraveresed.wait(5000);\n+              }\n+            } else {\n+              INode rootInode \u003d fsd.getInode(rootINodeId);\n+              if (rootInode !\u003d null) {\n+                // TODO : HDFS-12291\n+                // 1. Implement an efficient recursive directory iteration\n+                // mechanism and satisfies storage policy for all the files\n+                // under the given directory.\n+                // 2. Process files in batches,so datanodes workload can be\n+                // handled.\n+                List\u003cItemInfo\u003e itemInfoList \u003d\n+                    new ArrayList\u003c\u003e();\n+                for (INode childInode : rootInode.asDirectory()\n+                    .getChildrenList(Snapshot.CURRENT_STATE_ID)) {\n+                  if (childInode.isFile()\n+                      \u0026\u0026 childInode.asFile().numBlocks() !\u003d 0) {\n+                    itemInfoList.add(\n+                        new ItemInfo(rootINodeId, childInode.getId()));\n+                  }\n+                }\n+                if (itemInfoList.isEmpty()) {\n+                  // satisfy track info is empty, so remove the xAttr from the\n+                  // directory\n+                  namesystem.removeXattr(rootINodeId,\n+                      XATTR_SATISFY_STORAGE_POLICY);\n+                }\n+                addAll(rootINodeId, itemInfoList);\n+              }\n+            }\n+          }\n+        } catch (Throwable t) {\n+          LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      LOG.info(\"Starting FileInodeIdCollector!.\");\n      while (namesystem.isRunning() \u0026\u0026 sps.isRunning()) {\n        try {\n          if (!namesystem.isInSafeMode()) {\n            FSDirectory fsd \u003d namesystem.getFSDirectory();\n            Long rootINodeId \u003d spsDirsToBeTraveresed.poll();\n            if (rootINodeId \u003d\u003d null) {\n              // Waiting for SPS path\n              synchronized (spsDirsToBeTraveresed) {\n                spsDirsToBeTraveresed.wait(5000);\n              }\n            } else {\n              INode rootInode \u003d fsd.getInode(rootINodeId);\n              if (rootInode !\u003d null) {\n                // TODO : HDFS-12291\n                // 1. Implement an efficient recursive directory iteration\n                // mechanism and satisfies storage policy for all the files\n                // under the given directory.\n                // 2. Process files in batches,so datanodes workload can be\n                // handled.\n                List\u003cItemInfo\u003e itemInfoList \u003d\n                    new ArrayList\u003c\u003e();\n                for (INode childInode : rootInode.asDirectory()\n                    .getChildrenList(Snapshot.CURRENT_STATE_ID)) {\n                  if (childInode.isFile()\n                      \u0026\u0026 childInode.asFile().numBlocks() !\u003d 0) {\n                    itemInfoList.add(\n                        new ItemInfo(rootINodeId, childInode.getId()));\n                  }\n                }\n                if (itemInfoList.isEmpty()) {\n                  // satisfy track info is empty, so remove the xAttr from the\n                  // directory\n                  namesystem.removeXattr(rootINodeId,\n                      XATTR_SATISFY_STORAGE_POLICY);\n                }\n                addAll(rootINodeId, itemInfoList);\n              }\n            }\n          }\n        } catch (Throwable t) {\n          LOG.warn(\"Exception while loading inodes to satisfy the policy\", t);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockStorageMovementNeeded.java"
    }
  }
}