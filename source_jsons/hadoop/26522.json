{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "TaskAttemptImpl.java",
  "functionName": "createCommonContainerLaunchContext",
  "functionId": "createCommonContainerLaunchContext___applicationACLs-Map__ApplicationAccessType,String____conf-Configuration__jobToken-Token__JobTokenIdentifier____oldJobId-org.apache.hadoop.mapred.JobID(modifiers-final)__credentials-Credentials",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
  "functionStartLine": 897,
  "functionEndLine": 942,
  "numCommitsSeen": 183,
  "timeTaken": 10438,
  "changeHistory": [
    "845529b3ab338e759665a687eb525fb2cccde7bf",
    "10f9f5101c44be7c675a44ded4aad212627ecdee",
    "96e0ca2d272dc7ecd7f7f0e65a0c596fcc063bcb",
    "f2745babf34f348aef1727f5ad04fe9ad1683007",
    "a83fb61ac07c0468cbc7a38526e92683883dd932",
    "643155cbee54809e1a7febd96cbb7d8111689b38",
    "259edf8dca44de54033e96f7eb65a83aaa6096f2",
    "e4c55e17fea55e2fcbef182bb2b0c4b22686f38c",
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865",
    "0ba7078ef4ee127a47c5042c82db0b113a967b23",
    "49b20c2ed1be55c90a057acea71b55a28a3f69fb",
    "050fd3a11744cde3d54c1fff23d8fdeb3803bf92",
    "dc33a0765cd27255021911c4abb435b5850387aa",
    "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05",
    "f73daf6af1c87c65dd97e5ec4608ba2742dc83ea",
    "0870734787d7005d85697549eab5b6479d97d453",
    "df2991c0cbc3f35c2640b93680667507c4f810dd",
    "c1d90772b6e38bb4e4be7ed75cb5d34f3048ad7b",
    "d00b3c49f6fb3f6a617add6203c6b55f6c345940",
    "88b82a0f6687ce103817fbb460fd30d870f717a0",
    "6165875dc6bf67d72fc3ce1d96dfc80ba312d4a1",
    "ade0f0560f729e50382c6992f713f29e2dd5b270",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517"
  ],
  "changeHistoryShort": {
    "845529b3ab338e759665a687eb525fb2cccde7bf": "Ybodychange",
    "10f9f5101c44be7c675a44ded4aad212627ecdee": "Ybodychange",
    "96e0ca2d272dc7ecd7f7f0e65a0c596fcc063bcb": "Ybodychange",
    "f2745babf34f348aef1727f5ad04fe9ad1683007": "Ybodychange",
    "a83fb61ac07c0468cbc7a38526e92683883dd932": "Ybodychange",
    "643155cbee54809e1a7febd96cbb7d8111689b38": "Ybodychange",
    "259edf8dca44de54033e96f7eb65a83aaa6096f2": "Ybodychange",
    "e4c55e17fea55e2fcbef182bb2b0c4b22686f38c": "Ybodychange",
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865": "Ybodychange",
    "0ba7078ef4ee127a47c5042c82db0b113a967b23": "Ybodychange",
    "49b20c2ed1be55c90a057acea71b55a28a3f69fb": "Ybodychange",
    "050fd3a11744cde3d54c1fff23d8fdeb3803bf92": "Ybodychange",
    "dc33a0765cd27255021911c4abb435b5850387aa": "Ybodychange",
    "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05": "Ymultichange(Yparameterchange,Ybodychange)",
    "f73daf6af1c87c65dd97e5ec4608ba2742dc83ea": "Ybodychange",
    "0870734787d7005d85697549eab5b6479d97d453": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
    "df2991c0cbc3f35c2640b93680667507c4f810dd": "Ymultichange(Yparameterchange,Ybodychange)",
    "c1d90772b6e38bb4e4be7ed75cb5d34f3048ad7b": "Ybodychange",
    "d00b3c49f6fb3f6a617add6203c6b55f6c345940": "Ybodychange",
    "88b82a0f6687ce103817fbb460fd30d870f717a0": "Ybodychange",
    "6165875dc6bf67d72fc3ce1d96dfc80ba312d4a1": "Ybodychange",
    "ade0f0560f729e50382c6992f713f29e2dd5b270": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yintroduced"
  },
  "changeHistoryDetails": {
    "845529b3ab338e759665a687eb525fb2cccde7bf": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6824. TaskAttemptImpl#createCommonContainerLaunchContext is longer than 150 lines. Contributed by Chris Trezzo.\n",
      "commitDate": "02/04/17 9:06 PM",
      "commitName": "845529b3ab338e759665a687eb525fb2cccde7bf",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "11/07/16 10:36 PM",
      "commitNameOld": "819224dcf9c683aa52f58633ac8e13663f1916d8",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 264.94,
      "commitsBetweenForRepo": 1679,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,168 +1,46 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n-    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n+    Map\u003cString, String\u003e environment;\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n-      FileSystem remoteFS \u003d FileSystem.get(conf);\n \n-      // //////////// Set up JobJar to be localized properly on the remote NM.\n-      String jobJar \u003d conf.get(MRJobConfig.JAR);\n-      if (jobJar !\u003d null) {\n-        final Path jobJarPath \u003d new Path(jobJar);\n-        final FileSystem jobJarFs \u003d FileSystem.get(jobJarPath.toUri(), conf);\n-        Path remoteJobJar \u003d jobJarPath.makeQualified(jobJarFs.getUri(),\n-            jobJarFs.getWorkingDirectory());\n-        LocalResource rc \u003d createLocalResource(jobJarFs, remoteJobJar,\n-            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n-        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n-            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n-        rc.setPattern(pattern);\n-        localResources.put(MRJobConfig.JOB_JAR, rc);\n-        LOG.info(\"The job-jar file on the remote FS is \"\n-            + remoteJobJar.toUri().toASCIIString());\n-      } else {\n-        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n-        // mapreduce jar itself which is already on the classpath.\n-        LOG.info(\"Job jar is not present. \"\n-            + \"Not adding any jar to the list of resources.\");\n-      }\n-      // //////////// End of JobJar setup\n+      configureJobJar(conf, localResources);\n \n-      // //////////// Set up JobConf to be localized properly on the remote NM.\n-      Path path \u003d\n-          MRApps.getStagingAreaDir(conf, UserGroupInformation\n-              .getCurrentUser().getShortUserName());\n-      Path remoteJobSubmitDir \u003d\n-          new Path(path, oldJobId.toString());\n-      Path remoteJobConfPath \u003d \n-          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n-      localResources.put(\n-          MRJobConfig.JOB_CONF_FILE,\n-          createLocalResource(remoteFS, remoteJobConfPath,\n-              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n-      LOG.info(\"The job-conf file on the remote FS is \"\n-          + remoteJobConfPath.toUri().toASCIIString());\n-      // //////////// End of JobConf setup\n+      configureJobConf(conf, localResources, oldJobId);\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n-      // Setup up task credentials buffer\n-      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n-          + \" tokens and #\" + credentials.numberOfSecretKeys()\n-          + \" secret keys for NM use for launching container\");\n-      Credentials taskCredentials \u003d new Credentials(credentials);\n-\n-      // LocalStorageToken is needed irrespective of whether security is enabled\n-      // or not.\n-      TokenCache.setJobToken(jobToken, taskCredentials);\n-\n-      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n-      LOG.info(\"Size of containertokens_dob is \"\n-          + taskCredentials.numberOfTokens());\n-      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n-          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n-              containerTokens_dob.getLength());\n+          configureTokens(jobToken, credentials, serviceData);\n \n-      // Add shuffle secret key\n-      // The secret key is converted to a JobToken to preserve backwards\n-      // compatibility with an older ShuffleHandler running on an NM.\n-      LOG.info(\"Putting shuffle token in serviceData\");\n-      byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n-      if (shuffleSecret \u003d\u003d null) {\n-        LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n-            + \" Using job token as shuffle secret.\");\n-        shuffleSecret \u003d jobToken.getPassword();\n-      }\n-      Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n-          jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n-          jobToken.getService());\n-      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n-          ShuffleHandler.serializeServiceData(shuffleToken));\n+      addExternalShuffleProviders(conf, serviceData);\n \n-      // add external shuffle-providers - if any\n-      Collection\u003cString\u003e shuffleProviders \u003d conf.getStringCollection(\n-          MRJobConfig.MAPREDUCE_JOB_SHUFFLE_PROVIDER_SERVICES);\n-      if (! shuffleProviders.isEmpty()) {\n-        Collection\u003cString\u003e auxNames \u003d conf.getStringCollection(\n-            YarnConfiguration.NM_AUX_SERVICES);\n+      environment \u003d configureEnv(conf);\n \n-        for (final String shuffleProvider : shuffleProviders) {\n-          if (shuffleProvider.equals(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID)) {\n-            continue; // skip built-in shuffle-provider that was already inserted with shuffle secret key\n-          }\n-          if (auxNames.contains(shuffleProvider)) {\n-                LOG.info(\"Adding ShuffleProvider Service: \" + shuffleProvider + \" to serviceData\");\n-                // This only serves for INIT_APP notifications\n-                // The shuffle service needs to be able to work with the host:port information provided by the AM\n-                // (i.e. shuffle services which require custom location / other configuration are not supported)\n-                serviceData.put(shuffleProvider, ByteBuffer.allocate(0));\n-          }\n-          else {\n-            throw new YarnRuntimeException(\"ShuffleProvider Service: \" + shuffleProvider +\n-            \" was NOT found in the list of aux-services that are available in this NM.\" +\n-            \" You may need to specify this ShuffleProvider as an aux-service in your yarn-site.xml\");\n-          }\n-        }\n-      }\n-\n-      MRApps.addToEnvironment(\n-          environment,  \n-          Environment.CLASSPATH.name(), \n-          getInitialClasspath(conf), conf);\n-\n-      if (initialAppClasspath !\u003d null) {\n-        MRApps.addToEnvironment(\n-            environment,  \n-            Environment.APP_CLASSPATH.name(), \n-            initialAppClasspath, conf);\n-      }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(e);\n     }\n \n-    // Shell\n-    environment.put(\n-        Environment.SHELL.name(), \n-        conf.get(\n-            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n-            MRJobConfig.DEFAULT_SHELL)\n-            );\n-\n-    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n-    MRApps.addToEnvironment(\n-        environment, \n-        Environment.LD_LIBRARY_PATH.name(), \n-        MRApps.crossPlatformifyMREnv(conf, Environment.PWD), conf);\n-\n-    // Add the env variables passed by the admin\n-    MRApps.setEnvFromInputString(\n-        environment, \n-        conf.get(\n-            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n-            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV), conf\n-        );\n-\n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d\n         ContainerLaunchContext.newInstance(localResources, environment, null,\n-          serviceData, taskCredentialsBuffer, applicationACLs);\n+            serviceData, taskCredentialsBuffer, applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment;\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n\n      configureJobJar(conf, localResources);\n\n      configureJobConf(conf, localResources, oldJobId);\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      taskCredentialsBuffer \u003d\n          configureTokens(jobToken, credentials, serviceData);\n\n      addExternalShuffleProviders(conf, serviceData);\n\n      environment \u003d configureEnv(conf);\n\n    } catch (IOException e) {\n      throw new YarnRuntimeException(e);\n    }\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d\n        ContainerLaunchContext.newInstance(localResources, environment, null,\n            serviceData, taskCredentialsBuffer, applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "10f9f5101c44be7c675a44ded4aad212627ecdee": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5960. JobSubmitter\u0027s check whether job.jar is local is incorrect with no authority in job jar path. Contributed by Gera Shegalov\n",
      "commitDate": "06/11/14 7:10 AM",
      "commitName": "10f9f5101c44be7c675a44ded4aad212627ecdee",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "15/10/14 3:22 PM",
      "commitNameOld": "0af1a2b5bc1469ba22edb63cd58f9b436b1dc4d3",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 21.7,
      "commitsBetweenForRepo": 220,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,166 +1,168 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n-        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n-            .getUri(), remoteFS.getWorkingDirectory());\n-        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n+        final Path jobJarPath \u003d new Path(jobJar);\n+        final FileSystem jobJarFs \u003d FileSystem.get(jobJarPath.toUri(), conf);\n+        Path remoteJobJar \u003d jobJarPath.makeQualified(jobJarFs.getUri(),\n+            jobJarFs.getWorkingDirectory());\n+        LocalResource rc \u003d createLocalResource(jobJarFs, remoteJobJar,\n             LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n         String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n             JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n         rc.setPattern(pattern);\n         localResources.put(MRJobConfig.JOB_JAR, rc);\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n       LOG.info(\"Adding #\" + credentials.numberOfTokens()\n           + \" tokens and #\" + credentials.numberOfSecretKeys()\n           + \" secret keys for NM use for launching container\");\n       Credentials taskCredentials \u003d new Credentials(credentials);\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle secret key\n       // The secret key is converted to a JobToken to preserve backwards\n       // compatibility with an older ShuffleHandler running on an NM.\n       LOG.info(\"Putting shuffle token in serviceData\");\n       byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n       if (shuffleSecret \u003d\u003d null) {\n         LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n             + \" Using job token as shuffle secret.\");\n         shuffleSecret \u003d jobToken.getPassword();\n       }\n       Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n           jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n           jobToken.getService());\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(shuffleToken));\n \n       // add external shuffle-providers - if any\n       Collection\u003cString\u003e shuffleProviders \u003d conf.getStringCollection(\n           MRJobConfig.MAPREDUCE_JOB_SHUFFLE_PROVIDER_SERVICES);\n       if (! shuffleProviders.isEmpty()) {\n         Collection\u003cString\u003e auxNames \u003d conf.getStringCollection(\n             YarnConfiguration.NM_AUX_SERVICES);\n \n         for (final String shuffleProvider : shuffleProviders) {\n           if (shuffleProvider.equals(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID)) {\n             continue; // skip built-in shuffle-provider that was already inserted with shuffle secret key\n           }\n           if (auxNames.contains(shuffleProvider)) {\n                 LOG.info(\"Adding ShuffleProvider Service: \" + shuffleProvider + \" to serviceData\");\n                 // This only serves for INIT_APP notifications\n                 // The shuffle service needs to be able to work with the host:port information provided by the AM\n                 // (i.e. shuffle services which require custom location / other configuration are not supported)\n                 serviceData.put(shuffleProvider, ByteBuffer.allocate(0));\n           }\n           else {\n             throw new YarnRuntimeException(\"ShuffleProvider Service: \" + shuffleProvider +\n             \" was NOT found in the list of aux-services that are available in this NM.\" +\n             \" You may need to specify this ShuffleProvider as an aux-service in your yarn-site.xml\");\n           }\n         }\n       }\n \n       MRApps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf), conf);\n \n       if (initialAppClasspath !\u003d null) {\n         MRApps.addToEnvironment(\n             environment,  \n             Environment.APP_CLASSPATH.name(), \n             initialAppClasspath, conf);\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     MRApps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         MRApps.crossPlatformifyMREnv(conf, Environment.PWD), conf);\n \n     // Add the env variables passed by the admin\n     MRApps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV), conf\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d\n         ContainerLaunchContext.newInstance(localResources, environment, null,\n           serviceData, taskCredentialsBuffer, applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        final Path jobJarPath \u003d new Path(jobJar);\n        final FileSystem jobJarFs \u003d FileSystem.get(jobJarPath.toUri(), conf);\n        Path remoteJobJar \u003d jobJarPath.makeQualified(jobJarFs.getUri(),\n            jobJarFs.getWorkingDirectory());\n        LocalResource rc \u003d createLocalResource(jobJarFs, remoteJobJar,\n            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n        rc.setPattern(pattern);\n        localResources.put(MRJobConfig.JOB_JAR, rc);\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n          + \" tokens and #\" + credentials.numberOfSecretKeys()\n          + \" secret keys for NM use for launching container\");\n      Credentials taskCredentials \u003d new Credentials(credentials);\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle secret key\n      // The secret key is converted to a JobToken to preserve backwards\n      // compatibility with an older ShuffleHandler running on an NM.\n      LOG.info(\"Putting shuffle token in serviceData\");\n      byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n      if (shuffleSecret \u003d\u003d null) {\n        LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n            + \" Using job token as shuffle secret.\");\n        shuffleSecret \u003d jobToken.getPassword();\n      }\n      Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n          jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n          jobToken.getService());\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(shuffleToken));\n\n      // add external shuffle-providers - if any\n      Collection\u003cString\u003e shuffleProviders \u003d conf.getStringCollection(\n          MRJobConfig.MAPREDUCE_JOB_SHUFFLE_PROVIDER_SERVICES);\n      if (! shuffleProviders.isEmpty()) {\n        Collection\u003cString\u003e auxNames \u003d conf.getStringCollection(\n            YarnConfiguration.NM_AUX_SERVICES);\n\n        for (final String shuffleProvider : shuffleProviders) {\n          if (shuffleProvider.equals(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID)) {\n            continue; // skip built-in shuffle-provider that was already inserted with shuffle secret key\n          }\n          if (auxNames.contains(shuffleProvider)) {\n                LOG.info(\"Adding ShuffleProvider Service: \" + shuffleProvider + \" to serviceData\");\n                // This only serves for INIT_APP notifications\n                // The shuffle service needs to be able to work with the host:port information provided by the AM\n                // (i.e. shuffle services which require custom location / other configuration are not supported)\n                serviceData.put(shuffleProvider, ByteBuffer.allocate(0));\n          }\n          else {\n            throw new YarnRuntimeException(\"ShuffleProvider Service: \" + shuffleProvider +\n            \" was NOT found in the list of aux-services that are available in this NM.\" +\n            \" You may need to specify this ShuffleProvider as an aux-service in your yarn-site.xml\");\n          }\n        }\n      }\n\n      MRApps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf), conf);\n\n      if (initialAppClasspath !\u003d null) {\n        MRApps.addToEnvironment(\n            environment,  \n            Environment.APP_CLASSPATH.name(), \n            initialAppClasspath, conf);\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    MRApps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        MRApps.crossPlatformifyMREnv(conf, Environment.PWD), conf);\n\n    // Add the env variables passed by the admin\n    MRApps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV), conf\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d\n        ContainerLaunchContext.newInstance(localResources, environment, null,\n          serviceData, taskCredentialsBuffer, applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "96e0ca2d272dc7ecd7f7f0e65a0c596fcc063bcb": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1824. Improved NodeManager and clients to be able to handle cross platform application submissions. Contributed by Jian He.\nMAPREDUCE-4052. Improved MapReduce clients to use NodeManagers\u0027 ability to handle cross platform application submissions. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1578135 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/03/14 11:32 AM",
      "commitName": "96e0ca2d272dc7ecd7f7f0e65a0c596fcc063bcb",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "14/03/14 1:33 PM",
      "commitNameOld": "8a2a74159540f2e8d8f767639200e5e92bb6efbc",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 1.92,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,167 +1,166 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n-\n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n             LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n         String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n             JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n         rc.setPattern(pattern);\n         localResources.put(MRJobConfig.JOB_JAR, rc);\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n       LOG.info(\"Adding #\" + credentials.numberOfTokens()\n           + \" tokens and #\" + credentials.numberOfSecretKeys()\n           + \" secret keys for NM use for launching container\");\n       Credentials taskCredentials \u003d new Credentials(credentials);\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle secret key\n       // The secret key is converted to a JobToken to preserve backwards\n       // compatibility with an older ShuffleHandler running on an NM.\n       LOG.info(\"Putting shuffle token in serviceData\");\n       byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n       if (shuffleSecret \u003d\u003d null) {\n         LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n             + \" Using job token as shuffle secret.\");\n         shuffleSecret \u003d jobToken.getPassword();\n       }\n       Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n           jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n           jobToken.getService());\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(shuffleToken));\n \n       // add external shuffle-providers - if any\n       Collection\u003cString\u003e shuffleProviders \u003d conf.getStringCollection(\n           MRJobConfig.MAPREDUCE_JOB_SHUFFLE_PROVIDER_SERVICES);\n       if (! shuffleProviders.isEmpty()) {\n         Collection\u003cString\u003e auxNames \u003d conf.getStringCollection(\n             YarnConfiguration.NM_AUX_SERVICES);\n \n         for (final String shuffleProvider : shuffleProviders) {\n           if (shuffleProvider.equals(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID)) {\n             continue; // skip built-in shuffle-provider that was already inserted with shuffle secret key\n           }\n           if (auxNames.contains(shuffleProvider)) {\n                 LOG.info(\"Adding ShuffleProvider Service: \" + shuffleProvider + \" to serviceData\");\n                 // This only serves for INIT_APP notifications\n                 // The shuffle service needs to be able to work with the host:port information provided by the AM\n                 // (i.e. shuffle services which require custom location / other configuration are not supported)\n                 serviceData.put(shuffleProvider, ByteBuffer.allocate(0));\n           }\n           else {\n             throw new YarnRuntimeException(\"ShuffleProvider Service: \" + shuffleProvider +\n             \" was NOT found in the list of aux-services that are available in this NM.\" +\n             \" You may need to specify this ShuffleProvider as an aux-service in your yarn-site.xml\");\n           }\n         }\n       }\n \n-      Apps.addToEnvironment(\n+      MRApps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n-          getInitialClasspath(conf));\n+          getInitialClasspath(conf), conf);\n \n       if (initialAppClasspath !\u003d null) {\n-        Apps.addToEnvironment(\n+        MRApps.addToEnvironment(\n             environment,  \n             Environment.APP_CLASSPATH.name(), \n-            initialAppClasspath);\n+            initialAppClasspath, conf);\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n-    Apps.addToEnvironment(\n+    MRApps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n-        Environment.PWD.$());\n+        MRApps.crossPlatformifyMREnv(conf, Environment.PWD), conf);\n \n     // Add the env variables passed by the admin\n-    Apps.setEnvFromInputString(\n+    MRApps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n-            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n+            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV), conf\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d\n         ContainerLaunchContext.newInstance(localResources, environment, null,\n           serviceData, taskCredentialsBuffer, applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n        rc.setPattern(pattern);\n        localResources.put(MRJobConfig.JOB_JAR, rc);\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n          + \" tokens and #\" + credentials.numberOfSecretKeys()\n          + \" secret keys for NM use for launching container\");\n      Credentials taskCredentials \u003d new Credentials(credentials);\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle secret key\n      // The secret key is converted to a JobToken to preserve backwards\n      // compatibility with an older ShuffleHandler running on an NM.\n      LOG.info(\"Putting shuffle token in serviceData\");\n      byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n      if (shuffleSecret \u003d\u003d null) {\n        LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n            + \" Using job token as shuffle secret.\");\n        shuffleSecret \u003d jobToken.getPassword();\n      }\n      Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n          jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n          jobToken.getService());\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(shuffleToken));\n\n      // add external shuffle-providers - if any\n      Collection\u003cString\u003e shuffleProviders \u003d conf.getStringCollection(\n          MRJobConfig.MAPREDUCE_JOB_SHUFFLE_PROVIDER_SERVICES);\n      if (! shuffleProviders.isEmpty()) {\n        Collection\u003cString\u003e auxNames \u003d conf.getStringCollection(\n            YarnConfiguration.NM_AUX_SERVICES);\n\n        for (final String shuffleProvider : shuffleProviders) {\n          if (shuffleProvider.equals(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID)) {\n            continue; // skip built-in shuffle-provider that was already inserted with shuffle secret key\n          }\n          if (auxNames.contains(shuffleProvider)) {\n                LOG.info(\"Adding ShuffleProvider Service: \" + shuffleProvider + \" to serviceData\");\n                // This only serves for INIT_APP notifications\n                // The shuffle service needs to be able to work with the host:port information provided by the AM\n                // (i.e. shuffle services which require custom location / other configuration are not supported)\n                serviceData.put(shuffleProvider, ByteBuffer.allocate(0));\n          }\n          else {\n            throw new YarnRuntimeException(\"ShuffleProvider Service: \" + shuffleProvider +\n            \" was NOT found in the list of aux-services that are available in this NM.\" +\n            \" You may need to specify this ShuffleProvider as an aux-service in your yarn-site.xml\");\n          }\n        }\n      }\n\n      MRApps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf), conf);\n\n      if (initialAppClasspath !\u003d null) {\n        MRApps.addToEnvironment(\n            environment,  \n            Environment.APP_CLASSPATH.name(), \n            initialAppClasspath, conf);\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    MRApps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        MRApps.crossPlatformifyMREnv(conf, Environment.PWD), conf);\n\n    // Add the env variables passed by the admin\n    MRApps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV), conf\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d\n        ContainerLaunchContext.newInstance(localResources, environment, null,\n          serviceData, taskCredentialsBuffer, applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "f2745babf34f348aef1727f5ad04fe9ad1683007": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5329. Allow MR applications to use additional AuxServices, which are compatible with the default MapReduce shuffle. Contributed by Avner BenHanoch.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1531741 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/10/13 1:17 PM",
      "commitName": "f2745babf34f348aef1727f5ad04fe9ad1683007",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "16/06/13 9:49 PM",
      "commitNameOld": "d4a811edb25e2d1569ee6b8972a887a0180dfa2e",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 118.64,
      "commitsBetweenForRepo": 707,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,141 +1,167 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n             LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n         String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n             JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n         rc.setPattern(pattern);\n         localResources.put(MRJobConfig.JOB_JAR, rc);\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n       LOG.info(\"Adding #\" + credentials.numberOfTokens()\n           + \" tokens and #\" + credentials.numberOfSecretKeys()\n           + \" secret keys for NM use for launching container\");\n       Credentials taskCredentials \u003d new Credentials(credentials);\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle secret key\n       // The secret key is converted to a JobToken to preserve backwards\n       // compatibility with an older ShuffleHandler running on an NM.\n       LOG.info(\"Putting shuffle token in serviceData\");\n       byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n       if (shuffleSecret \u003d\u003d null) {\n         LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n             + \" Using job token as shuffle secret.\");\n         shuffleSecret \u003d jobToken.getPassword();\n       }\n       Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n           jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n           jobToken.getService());\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(shuffleToken));\n \n+      // add external shuffle-providers - if any\n+      Collection\u003cString\u003e shuffleProviders \u003d conf.getStringCollection(\n+          MRJobConfig.MAPREDUCE_JOB_SHUFFLE_PROVIDER_SERVICES);\n+      if (! shuffleProviders.isEmpty()) {\n+        Collection\u003cString\u003e auxNames \u003d conf.getStringCollection(\n+            YarnConfiguration.NM_AUX_SERVICES);\n+\n+        for (final String shuffleProvider : shuffleProviders) {\n+          if (shuffleProvider.equals(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID)) {\n+            continue; // skip built-in shuffle-provider that was already inserted with shuffle secret key\n+          }\n+          if (auxNames.contains(shuffleProvider)) {\n+                LOG.info(\"Adding ShuffleProvider Service: \" + shuffleProvider + \" to serviceData\");\n+                // This only serves for INIT_APP notifications\n+                // The shuffle service needs to be able to work with the host:port information provided by the AM\n+                // (i.e. shuffle services which require custom location / other configuration are not supported)\n+                serviceData.put(shuffleProvider, ByteBuffer.allocate(0));\n+          }\n+          else {\n+            throw new YarnRuntimeException(\"ShuffleProvider Service: \" + shuffleProvider +\n+            \" was NOT found in the list of aux-services that are available in this NM.\" +\n+            \" You may need to specify this ShuffleProvider as an aux-service in your yarn-site.xml\");\n+          }\n+        }\n+      }\n+\n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n \n       if (initialAppClasspath !\u003d null) {\n         Apps.addToEnvironment(\n             environment,  \n             Environment.APP_CLASSPATH.name(), \n             initialAppClasspath);\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d\n         ContainerLaunchContext.newInstance(localResources, environment, null,\n           serviceData, taskCredentialsBuffer, applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n        rc.setPattern(pattern);\n        localResources.put(MRJobConfig.JOB_JAR, rc);\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n          + \" tokens and #\" + credentials.numberOfSecretKeys()\n          + \" secret keys for NM use for launching container\");\n      Credentials taskCredentials \u003d new Credentials(credentials);\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle secret key\n      // The secret key is converted to a JobToken to preserve backwards\n      // compatibility with an older ShuffleHandler running on an NM.\n      LOG.info(\"Putting shuffle token in serviceData\");\n      byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n      if (shuffleSecret \u003d\u003d null) {\n        LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n            + \" Using job token as shuffle secret.\");\n        shuffleSecret \u003d jobToken.getPassword();\n      }\n      Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n          jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n          jobToken.getService());\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(shuffleToken));\n\n      // add external shuffle-providers - if any\n      Collection\u003cString\u003e shuffleProviders \u003d conf.getStringCollection(\n          MRJobConfig.MAPREDUCE_JOB_SHUFFLE_PROVIDER_SERVICES);\n      if (! shuffleProviders.isEmpty()) {\n        Collection\u003cString\u003e auxNames \u003d conf.getStringCollection(\n            YarnConfiguration.NM_AUX_SERVICES);\n\n        for (final String shuffleProvider : shuffleProviders) {\n          if (shuffleProvider.equals(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID)) {\n            continue; // skip built-in shuffle-provider that was already inserted with shuffle secret key\n          }\n          if (auxNames.contains(shuffleProvider)) {\n                LOG.info(\"Adding ShuffleProvider Service: \" + shuffleProvider + \" to serviceData\");\n                // This only serves for INIT_APP notifications\n                // The shuffle service needs to be able to work with the host:port information provided by the AM\n                // (i.e. shuffle services which require custom location / other configuration are not supported)\n                serviceData.put(shuffleProvider, ByteBuffer.allocate(0));\n          }\n          else {\n            throw new YarnRuntimeException(\"ShuffleProvider Service: \" + shuffleProvider +\n            \" was NOT found in the list of aux-services that are available in this NM.\" +\n            \" You may need to specify this ShuffleProvider as an aux-service in your yarn-site.xml\");\n          }\n        }\n      }\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n\n      if (initialAppClasspath !\u003d null) {\n        Apps.addToEnvironment(\n            environment,  \n            Environment.APP_CLASSPATH.name(), \n            initialAppClasspath);\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d\n        ContainerLaunchContext.newInstance(localResources, environment, null,\n          serviceData, taskCredentialsBuffer, applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "a83fb61ac07c0468cbc7a38526e92683883dd932": {
      "type": "Ybodychange",
      "commitMessage": "YARN-635. Renamed YarnRemoteException to YarnException. Contributed by Siddharth Seth.\nMAPREDUCE-5301. Updated MR code to work with YARN-635 changes of renaming YarnRemoteException to YarnException. Contributed by Siddharth Seth\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489283 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/06/13 9:05 PM",
      "commitName": "a83fb61ac07c0468cbc7a38526e92683883dd932",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "29/05/13 9:59 PM",
      "commitNameOld": "b16c5638b5190c56f9d854d873589cb5c11c8b32",
      "commitAuthorOld": "Siddharth Seth",
      "daysBetweenCommits": 4.96,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,141 +1,141 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n             LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n         String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n             JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n         rc.setPattern(pattern);\n         localResources.put(MRJobConfig.JOB_JAR, rc);\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n       LOG.info(\"Adding #\" + credentials.numberOfTokens()\n           + \" tokens and #\" + credentials.numberOfSecretKeys()\n           + \" secret keys for NM use for launching container\");\n       Credentials taskCredentials \u003d new Credentials(credentials);\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle secret key\n       // The secret key is converted to a JobToken to preserve backwards\n       // compatibility with an older ShuffleHandler running on an NM.\n       LOG.info(\"Putting shuffle token in serviceData\");\n       byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n       if (shuffleSecret \u003d\u003d null) {\n         LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n             + \" Using job token as shuffle secret.\");\n         shuffleSecret \u003d jobToken.getPassword();\n       }\n       Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n           jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n           jobToken.getService());\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(shuffleToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n \n       if (initialAppClasspath !\u003d null) {\n         Apps.addToEnvironment(\n             environment,  \n             Environment.APP_CLASSPATH.name(), \n             initialAppClasspath);\n       }\n     } catch (IOException e) {\n-      throw new YarnException(e);\n+      throw new YarnRuntimeException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d\n         ContainerLaunchContext.newInstance(localResources, environment, null,\n           serviceData, taskCredentialsBuffer, applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n        rc.setPattern(pattern);\n        localResources.put(MRJobConfig.JOB_JAR, rc);\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n          + \" tokens and #\" + credentials.numberOfSecretKeys()\n          + \" secret keys for NM use for launching container\");\n      Credentials taskCredentials \u003d new Credentials(credentials);\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle secret key\n      // The secret key is converted to a JobToken to preserve backwards\n      // compatibility with an older ShuffleHandler running on an NM.\n      LOG.info(\"Putting shuffle token in serviceData\");\n      byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n      if (shuffleSecret \u003d\u003d null) {\n        LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n            + \" Using job token as shuffle secret.\");\n        shuffleSecret \u003d jobToken.getPassword();\n      }\n      Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n          jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n          jobToken.getService());\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(shuffleToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n\n      if (initialAppClasspath !\u003d null) {\n        Apps.addToEnvironment(\n            environment,  \n            Environment.APP_CLASSPATH.name(), \n            initialAppClasspath);\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d\n        ContainerLaunchContext.newInstance(localResources, environment, null,\n          serviceData, taskCredentialsBuffer, applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "643155cbee54809e1a7febd96cbb7d8111689b38": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5270. Migrated MR app from using BuilderUtil factory methods to individual record factory methods. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1486271 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/05/13 6:46 PM",
      "commitName": "643155cbee54809e1a7febd96cbb7d8111689b38",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "23/05/13 8:22 PM",
      "commitNameOld": "259edf8dca44de54033e96f7eb65a83aaa6096f2",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 0.93,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,142 +1,141 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n             LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n         String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n             JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n         rc.setPattern(pattern);\n         localResources.put(MRJobConfig.JOB_JAR, rc);\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n       LOG.info(\"Adding #\" + credentials.numberOfTokens()\n           + \" tokens and #\" + credentials.numberOfSecretKeys()\n           + \" secret keys for NM use for launching container\");\n       Credentials taskCredentials \u003d new Credentials(credentials);\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle secret key\n       // The secret key is converted to a JobToken to preserve backwards\n       // compatibility with an older ShuffleHandler running on an NM.\n       LOG.info(\"Putting shuffle token in serviceData\");\n       byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n       if (shuffleSecret \u003d\u003d null) {\n         LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n             + \" Using job token as shuffle secret.\");\n         shuffleSecret \u003d jobToken.getPassword();\n       }\n       Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n           jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n           jobToken.getService());\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(shuffleToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n \n       if (initialAppClasspath !\u003d null) {\n         Apps.addToEnvironment(\n             environment,  \n             Environment.APP_CLASSPATH.name(), \n             initialAppClasspath);\n       }\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n-    ContainerLaunchContext container \u003d BuilderUtils\n-        .newContainerLaunchContext(localResources,\n-            environment, null, serviceData, taskCredentialsBuffer,\n-            applicationACLs);\n+    ContainerLaunchContext container \u003d\n+        ContainerLaunchContext.newInstance(localResources, environment, null,\n+          serviceData, taskCredentialsBuffer, applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n        rc.setPattern(pattern);\n        localResources.put(MRJobConfig.JOB_JAR, rc);\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n          + \" tokens and #\" + credentials.numberOfSecretKeys()\n          + \" secret keys for NM use for launching container\");\n      Credentials taskCredentials \u003d new Credentials(credentials);\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle secret key\n      // The secret key is converted to a JobToken to preserve backwards\n      // compatibility with an older ShuffleHandler running on an NM.\n      LOG.info(\"Putting shuffle token in serviceData\");\n      byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n      if (shuffleSecret \u003d\u003d null) {\n        LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n            + \" Using job token as shuffle secret.\");\n        shuffleSecret \u003d jobToken.getPassword();\n      }\n      Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n          jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n          jobToken.getService());\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(shuffleToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n\n      if (initialAppClasspath !\u003d null) {\n        Apps.addToEnvironment(\n            environment,  \n            Environment.APP_CLASSPATH.name(), \n            initialAppClasspath);\n      }\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d\n        ContainerLaunchContext.newInstance(localResources, environment, null,\n          serviceData, taskCredentialsBuffer, applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "259edf8dca44de54033e96f7eb65a83aaa6096f2": {
      "type": "Ybodychange",
      "commitMessage": "YARN-571. Remove user from ContainerLaunchContext. Contributed by Omkar Vinit Joshi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1485928 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/05/13 8:22 PM",
      "commitName": "259edf8dca44de54033e96f7eb65a83aaa6096f2",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "13/05/13 9:11 PM",
      "commitNameOld": "1a119f87b4f0a78d56e1bb998b1cbc081484fbd1",
      "commitAuthorOld": "Siddharth Seth",
      "daysBetweenCommits": 9.97,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,143 +1,142 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n             LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n         String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n             JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n         rc.setPattern(pattern);\n         localResources.put(MRJobConfig.JOB_JAR, rc);\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n       LOG.info(\"Adding #\" + credentials.numberOfTokens()\n           + \" tokens and #\" + credentials.numberOfSecretKeys()\n           + \" secret keys for NM use for launching container\");\n       Credentials taskCredentials \u003d new Credentials(credentials);\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle secret key\n       // The secret key is converted to a JobToken to preserve backwards\n       // compatibility with an older ShuffleHandler running on an NM.\n       LOG.info(\"Putting shuffle token in serviceData\");\n       byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n       if (shuffleSecret \u003d\u003d null) {\n         LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n             + \" Using job token as shuffle secret.\");\n         shuffleSecret \u003d jobToken.getPassword();\n       }\n       Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n           jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n           jobToken.getService());\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(shuffleToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n \n       if (initialAppClasspath !\u003d null) {\n         Apps.addToEnvironment(\n             environment,  \n             Environment.APP_CLASSPATH.name(), \n             initialAppClasspath);\n       }\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n-        .newContainerLaunchContext(conf\n-            .get(MRJobConfig.USER_NAME), localResources,\n+        .newContainerLaunchContext(localResources,\n             environment, null, serviceData, taskCredentialsBuffer,\n             applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n        rc.setPattern(pattern);\n        localResources.put(MRJobConfig.JOB_JAR, rc);\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n          + \" tokens and #\" + credentials.numberOfSecretKeys()\n          + \" secret keys for NM use for launching container\");\n      Credentials taskCredentials \u003d new Credentials(credentials);\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle secret key\n      // The secret key is converted to a JobToken to preserve backwards\n      // compatibility with an older ShuffleHandler running on an NM.\n      LOG.info(\"Putting shuffle token in serviceData\");\n      byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n      if (shuffleSecret \u003d\u003d null) {\n        LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n            + \" Using job token as shuffle secret.\");\n        shuffleSecret \u003d jobToken.getPassword();\n      }\n      Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n          jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n          jobToken.getService());\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(shuffleToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n\n      if (initialAppClasspath !\u003d null) {\n        Apps.addToEnvironment(\n            environment,  \n            Environment.APP_CLASSPATH.name(), \n            initialAppClasspath);\n      }\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(localResources,\n            environment, null, serviceData, taskCredentialsBuffer,\n            applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "e4c55e17fea55e2fcbef182bb2b0c4b22686f38c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-486. Changed NM\u0027s startContainer API to accept Container record given by RM as a direct parameter instead of as part of the ContainerLaunchContext record. Contributed by Xuan Gong.\nMAPREDUCE-5139. Update MR AM to use the modified startContainer API after YARN-486. Contributed by Xuan Gong.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1467063 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/04/13 12:28 PM",
      "commitName": "e4c55e17fea55e2fcbef182bb2b0c4b22686f38c",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "10/04/13 9:52 PM",
      "commitNameOld": "6a1c41111edcdc58c846fc50e53554fbba230171",
      "commitAuthorOld": "Siddharth Seth",
      "daysBetweenCommits": 0.61,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,143 +1,143 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n             LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n         String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n             JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n         rc.setPattern(pattern);\n         localResources.put(MRJobConfig.JOB_JAR, rc);\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n       LOG.info(\"Adding #\" + credentials.numberOfTokens()\n           + \" tokens and #\" + credentials.numberOfSecretKeys()\n           + \" secret keys for NM use for launching container\");\n       Credentials taskCredentials \u003d new Credentials(credentials);\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle secret key\n       // The secret key is converted to a JobToken to preserve backwards\n       // compatibility with an older ShuffleHandler running on an NM.\n       LOG.info(\"Putting shuffle token in serviceData\");\n       byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n       if (shuffleSecret \u003d\u003d null) {\n         LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n             + \" Using job token as shuffle secret.\");\n         shuffleSecret \u003d jobToken.getPassword();\n       }\n       Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n           jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n           jobToken.getService());\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(shuffleToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n \n       if (initialAppClasspath !\u003d null) {\n         Apps.addToEnvironment(\n             environment,  \n             Environment.APP_CLASSPATH.name(), \n             initialAppClasspath);\n       }\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n-        .newContainerLaunchContext(null, conf\n-            .get(MRJobConfig.USER_NAME), null, localResources,\n+        .newContainerLaunchContext(conf\n+            .get(MRJobConfig.USER_NAME), localResources,\n             environment, null, serviceData, taskCredentialsBuffer,\n             applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n        rc.setPattern(pattern);\n        localResources.put(MRJobConfig.JOB_JAR, rc);\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n          + \" tokens and #\" + credentials.numberOfSecretKeys()\n          + \" secret keys for NM use for launching container\");\n      Credentials taskCredentials \u003d new Credentials(credentials);\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle secret key\n      // The secret key is converted to a JobToken to preserve backwards\n      // compatibility with an older ShuffleHandler running on an NM.\n      LOG.info(\"Putting shuffle token in serviceData\");\n      byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n      if (shuffleSecret \u003d\u003d null) {\n        LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n            + \" Using job token as shuffle secret.\");\n        shuffleSecret \u003d jobToken.getPassword();\n      }\n      Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n          jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n          jobToken.getService());\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(shuffleToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n\n      if (initialAppClasspath !\u003d null) {\n        Apps.addToEnvironment(\n            environment,  \n            Environment.APP_CLASSPATH.name(), \n            initialAppClasspath);\n      }\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(conf\n            .get(MRJobConfig.USER_NAME), localResources,\n            environment, null, serviceData, taskCredentialsBuffer,\n            applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5042. Reducer unable to fetch for a map task that was recovered (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1457119 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/03/13 2:09 PM",
      "commitName": "7d7553c4eb7d9a282410a3213d26a89fea9b7865",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "04/03/13 8:32 AM",
      "commitNameOld": "ec13f1eb3afce78f1e86b039936f0c38535d05b3",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 11.19,
      "commitsBetweenForRepo": 82,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,132 +1,143 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n             LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n         String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n             JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n         rc.setPattern(pattern);\n         localResources.put(MRJobConfig.JOB_JAR, rc);\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n       LOG.info(\"Adding #\" + credentials.numberOfTokens()\n           + \" tokens and #\" + credentials.numberOfSecretKeys()\n           + \" secret keys for NM use for launching container\");\n       Credentials taskCredentials \u003d new Credentials(credentials);\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n-      // Add shuffle token\n+      // Add shuffle secret key\n+      // The secret key is converted to a JobToken to preserve backwards\n+      // compatibility with an older ShuffleHandler running on an NM.\n       LOG.info(\"Putting shuffle token in serviceData\");\n+      byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n+      if (shuffleSecret \u003d\u003d null) {\n+        LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n+            + \" Using job token as shuffle secret.\");\n+        shuffleSecret \u003d jobToken.getPassword();\n+      }\n+      Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n+          jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n+          jobToken.getService());\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n-          ShuffleHandler.serializeServiceData(jobToken));\n+          ShuffleHandler.serializeServiceData(shuffleToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n \n       if (initialAppClasspath !\u003d null) {\n         Apps.addToEnvironment(\n             environment,  \n             Environment.APP_CLASSPATH.name(), \n             initialAppClasspath);\n       }\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n         .newContainerLaunchContext(null, conf\n             .get(MRJobConfig.USER_NAME), null, localResources,\n             environment, null, serviceData, taskCredentialsBuffer,\n             applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n        rc.setPattern(pattern);\n        localResources.put(MRJobConfig.JOB_JAR, rc);\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n          + \" tokens and #\" + credentials.numberOfSecretKeys()\n          + \" secret keys for NM use for launching container\");\n      Credentials taskCredentials \u003d new Credentials(credentials);\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle secret key\n      // The secret key is converted to a JobToken to preserve backwards\n      // compatibility with an older ShuffleHandler running on an NM.\n      LOG.info(\"Putting shuffle token in serviceData\");\n      byte[] shuffleSecret \u003d TokenCache.getShuffleSecretKey(credentials);\n      if (shuffleSecret \u003d\u003d null) {\n        LOG.warn(\"Cannot locate shuffle secret in credentials.\"\n            + \" Using job token as shuffle secret.\");\n        shuffleSecret \u003d jobToken.getPassword();\n      }\n      Token\u003cJobTokenIdentifier\u003e shuffleToken \u003d new Token\u003cJobTokenIdentifier\u003e(\n          jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(),\n          jobToken.getService());\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(shuffleToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n\n      if (initialAppClasspath !\u003d null) {\n        Apps.addToEnvironment(\n            environment,  \n            Environment.APP_CLASSPATH.name(), \n            initialAppClasspath);\n      }\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, taskCredentialsBuffer,\n            applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "0ba7078ef4ee127a47c5042c82db0b113a967b23": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-1700. User supplied dependencies may conflict with MapReduce system JARs.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430929 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 8:12 AM",
      "commitName": "0ba7078ef4ee127a47c5042c82db0b113a967b23",
      "commitAuthor": "Thomas White",
      "commitDateOld": "08/01/13 9:28 PM",
      "commitNameOld": "2c5c8fdb80546467274607b26a1295b352c58fc8",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.45,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,125 +1,132 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n             LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n         String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n             JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n         rc.setPattern(pattern);\n         localResources.put(MRJobConfig.JOB_JAR, rc);\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n       LOG.info(\"Adding #\" + credentials.numberOfTokens()\n           + \" tokens and #\" + credentials.numberOfSecretKeys()\n           + \" secret keys for NM use for launching container\");\n       Credentials taskCredentials \u003d new Credentials(credentials);\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n+\n+      if (initialAppClasspath !\u003d null) {\n+        Apps.addToEnvironment(\n+            environment,  \n+            Environment.APP_CLASSPATH.name(), \n+            initialAppClasspath);\n+      }\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n         .newContainerLaunchContext(null, conf\n             .get(MRJobConfig.USER_NAME), null, localResources,\n             environment, null, serviceData, taskCredentialsBuffer,\n             applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n        rc.setPattern(pattern);\n        localResources.put(MRJobConfig.JOB_JAR, rc);\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n          + \" tokens and #\" + credentials.numberOfSecretKeys()\n          + \" secret keys for NM use for launching container\");\n      Credentials taskCredentials \u003d new Credentials(credentials);\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n\n      if (initialAppClasspath !\u003d null) {\n        Apps.addToEnvironment(\n            environment,  \n            Environment.APP_CLASSPATH.name(), \n            initialAppClasspath);\n      }\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, taskCredentialsBuffer,\n            applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "49b20c2ed1be55c90a057acea71b55a28a3f69fb": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4554. Job Credentials are not transmitted if security is turned off (Benoy Antony via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1395769 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/10/12 1:50 PM",
      "commitName": "49b20c2ed1be55c90a057acea71b55a28a3f69fb",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "26/09/12 8:22 AM",
      "commitNameOld": "050fd3a11744cde3d54c1fff23d8fdeb3803bf92",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 12.23,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,129 +1,125 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n             LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n         String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n             JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n         rc.setPattern(pattern);\n         localResources.put(MRJobConfig.JOB_JAR, rc);\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n-      Credentials taskCredentials \u003d new Credentials();\n-\n-      if (UserGroupInformation.isSecurityEnabled()) {\n-        LOG.info(\"Adding #\" + credentials.numberOfTokens()\n-            + \" tokens and #\" + credentials.numberOfSecretKeys()\n-            + \" secret keys for NM use for launching container\");\n-        taskCredentials.addAll(credentials);\n-      }\n+      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n+          + \" tokens and #\" + credentials.numberOfSecretKeys()\n+          + \" secret keys for NM use for launching container\");\n+      Credentials taskCredentials \u003d new Credentials(credentials);\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n         .newContainerLaunchContext(null, conf\n             .get(MRJobConfig.USER_NAME), null, localResources,\n             environment, null, serviceData, taskCredentialsBuffer,\n             applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n        rc.setPattern(pattern);\n        localResources.put(MRJobConfig.JOB_JAR, rc);\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      LOG.info(\"Adding #\" + credentials.numberOfTokens()\n          + \" tokens and #\" + credentials.numberOfSecretKeys()\n          + \" secret keys for NM use for launching container\");\n      Credentials taskCredentials \u003d new Credentials(credentials);\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, taskCredentialsBuffer,\n            applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "050fd3a11744cde3d54c1fff23d8fdeb3803bf92": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4647. We should only unjar jobjar if there is a lib directory in it. (Robert Evans via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1390557 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/09/12 8:22 AM",
      "commitName": "050fd3a11744cde3d54c1fff23d8fdeb3803bf92",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "11/09/12 7:04 AM",
      "commitNameOld": "3b46295c283cb73d9487d82a4102b77b3b362f03",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 15.05,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,127 +1,129 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n-        localResources.put(\n-            MRJobConfig.JOB_JAR,\n-            createLocalResource(remoteFS, remoteJobJar,\n-                LocalResourceType.ARCHIVE, LocalResourceVisibility.APPLICATION));\n+        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n+            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n+        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n+            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n+        rc.setPattern(pattern);\n+        localResources.put(MRJobConfig.JOB_JAR, rc);\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         LOG.info(\"Adding #\" + credentials.numberOfTokens()\n             + \" tokens and #\" + credentials.numberOfSecretKeys()\n             + \" secret keys for NM use for launching container\");\n         taskCredentials.addAll(credentials);\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n         .newContainerLaunchContext(null, conf\n             .get(MRJobConfig.USER_NAME), null, localResources,\n             environment, null, serviceData, taskCredentialsBuffer,\n             applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        LocalResource rc \u003d createLocalResource(remoteFS, remoteJobJar,\n            LocalResourceType.PATTERN, LocalResourceVisibility.APPLICATION);\n        String pattern \u003d conf.getPattern(JobContext.JAR_UNPACK_PATTERN, \n            JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\n        rc.setPattern(pattern);\n        localResources.put(MRJobConfig.JOB_JAR, rc);\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        LOG.info(\"Adding #\" + credentials.numberOfTokens()\n            + \" tokens and #\" + credentials.numberOfSecretKeys()\n            + \" secret keys for NM use for launching container\");\n        taskCredentials.addAll(credentials);\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, taskCredentialsBuffer,\n            applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "dc33a0765cd27255021911c4abb435b5850387aa": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4068. Jars in lib subdirectory of the submittable JAR are not added to the classpath (rkanter via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1376253 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/08/12 2:18 PM",
      "commitName": "dc33a0765cd27255021911c4abb435b5850387aa",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "31/07/12 1:52 PM",
      "commitNameOld": "9d42fb2e8e57df1cfd08c21e6213aaaaf38a2850",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 22.02,
      "commitsBetweenForRepo": 116,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,127 +1,127 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, remoteJobJar,\n-                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n+                LocalResourceType.ARCHIVE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up task credentials buffer\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         LOG.info(\"Adding #\" + credentials.numberOfTokens()\n             + \" tokens and #\" + credentials.numberOfSecretKeys()\n             + \" secret keys for NM use for launching container\");\n         taskCredentials.addAll(credentials);\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n         .newContainerLaunchContext(null, conf\n             .get(MRJobConfig.USER_NAME), null, localResources,\n             environment, null, serviceData, taskCredentialsBuffer,\n             applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, remoteJobJar,\n                LocalResourceType.ARCHIVE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        LOG.info(\"Adding #\" + credentials.numberOfTokens()\n            + \" tokens and #\" + credentials.numberOfSecretKeys()\n            + \" secret keys for NM use for launching container\");\n        taskCredentials.addAll(credentials);\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, taskCredentialsBuffer,\n            applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-4043. Secret keys set in Credentials are not seen by tasks (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1304587 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/03/12 1:46 PM",
      "commitName": "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05",
      "commitAuthor": "Robert Joseph Evans",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-4043. Secret keys set in Credentials are not seen by tasks (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1304587 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/03/12 1:46 PM",
          "commitName": "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05",
          "commitAuthor": "Robert Joseph Evans",
          "commitDateOld": "19/03/12 1:31 PM",
          "commitNameOld": "9d8d02b68b5ffc18e8f9f00db1750a80d3fc9061",
          "commitAuthorOld": "Robert Joseph Evans",
          "daysBetweenCommits": 4.01,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,128 +1,127 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n-      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n+      Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n-    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n+    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n-      // Setup up tokens\n+      // Setup up task credentials buffer\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n-        // Add file-system tokens\n-        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n-          LOG.info(\"Putting fs-token for NM use for launching container : \"\n-              + token.toString());\n-          taskCredentials.addToken(token.getService(), token);\n-        }\n+        LOG.info(\"Adding #\" + credentials.numberOfTokens()\n+            + \" tokens and #\" + credentials.numberOfSecretKeys()\n+            + \" secret keys for NM use for launching container\");\n+        taskCredentials.addAll(credentials);\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n-      tokens \u003d \n+      taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n         .newContainerLaunchContext(null, conf\n             .get(MRJobConfig.USER_NAME), null, localResources,\n-            environment, null, serviceData, tokens, applicationACLs);\n+            environment, null, serviceData, taskCredentialsBuffer,\n+            applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        LOG.info(\"Adding #\" + credentials.numberOfTokens()\n            + \" tokens and #\" + credentials.numberOfSecretKeys()\n            + \" secret keys for NM use for launching container\");\n        taskCredentials.addAll(credentials);\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, taskCredentialsBuffer,\n            applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {
            "oldValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e, conf-Configuration, jobToken-Token\u003cJobTokenIdentifier\u003e, oldJobId-org.apache.hadoop.mapred.JobID(modifiers-final), fsTokens-Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e]",
            "newValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e, conf-Configuration, jobToken-Token\u003cJobTokenIdentifier\u003e, oldJobId-org.apache.hadoop.mapred.JobID(modifiers-final), credentials-Credentials]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-4043. Secret keys set in Credentials are not seen by tasks (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1304587 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/03/12 1:46 PM",
          "commitName": "f67c2d1bd0c8abc3d4ea76deffe45fdd92ef5e05",
          "commitAuthor": "Robert Joseph Evans",
          "commitDateOld": "19/03/12 1:31 PM",
          "commitNameOld": "9d8d02b68b5ffc18e8f9f00db1750a80d3fc9061",
          "commitAuthorOld": "Robert Joseph Evans",
          "daysBetweenCommits": 4.01,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,128 +1,127 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n-      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n+      Credentials credentials) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n-    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n+    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n-      // Setup up tokens\n+      // Setup up task credentials buffer\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n-        // Add file-system tokens\n-        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n-          LOG.info(\"Putting fs-token for NM use for launching container : \"\n-              + token.toString());\n-          taskCredentials.addToken(token.getService(), token);\n-        }\n+        LOG.info(\"Adding #\" + credentials.numberOfTokens()\n+            + \" tokens and #\" + credentials.numberOfSecretKeys()\n+            + \" secret keys for NM use for launching container\");\n+        taskCredentials.addAll(credentials);\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n-      tokens \u003d \n+      taskCredentialsBuffer \u003d\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath(conf));\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n         .newContainerLaunchContext(null, conf\n             .get(MRJobConfig.USER_NAME), null, localResources,\n-            environment, null, serviceData, tokens, applicationACLs);\n+            environment, null, serviceData, taskCredentialsBuffer,\n+            applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Credentials credentials) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer taskCredentialsBuffer \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up task credentials buffer\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        LOG.info(\"Adding #\" + credentials.numberOfTokens()\n            + \" tokens and #\" + credentials.numberOfSecretKeys()\n            + \" secret keys for NM use for launching container\");\n        taskCredentials.addAll(credentials);\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      taskCredentialsBuffer \u003d\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, taskCredentialsBuffer,\n            applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "f73daf6af1c87c65dd97e5ec4608ba2742dc83ea": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3505. yarn APPLICATION_CLASSPATH needs to be overridable. (ahmed via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1235391 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/01/12 10:21 AM",
      "commitName": "f73daf6af1c87c65dd97e5ec4608ba2742dc83ea",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "13/01/12 1:31 PM",
      "commitNameOld": "0c278b0f636a01c81aba9e46fe7658fcdfb0f33c",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 10.87,
      "commitsBetweenForRepo": 41,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,128 +1,128 @@\n   private static ContainerLaunchContext createCommonContainerLaunchContext(\n       Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n       Token\u003cJobTokenIdentifier\u003e jobToken,\n       final org.apache.hadoop.mapred.JobID oldJobId,\n       Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       String jobJar \u003d conf.get(MRJobConfig.JAR);\n       if (jobJar !\u003d null) {\n         Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n             .getUri(), remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n-          getInitialClasspath());\n+          getInitialClasspath(conf));\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Shell\n     environment.put(\n         Environment.SHELL.name(), \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n             MRJobConfig.DEFAULT_SHELL)\n             );\n \n     // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n     Apps.addToEnvironment(\n         environment, \n         Environment.LD_LIBRARY_PATH.name(), \n         Environment.PWD.$());\n \n     // Add the env variables passed by the admin\n     Apps.setEnvFromInputString(\n         environment, \n         conf.get(\n             MRJobConfig.MAPRED_ADMIN_USER_ENV, \n             MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n         );\n \n     // Construct the actual Container\n     // The null fields are per-container and will be constructed for each\n     // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n         .newContainerLaunchContext(null, conf\n             .get(MRJobConfig.USER_NAME), null, localResources,\n             environment, null, serviceData, tokens, applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath(conf));\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, tokens, applicationACLs);\n\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "0870734787d7005d85697549eab5b6479d97d453": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-3566. Fixed MR AM to construct CLC only once across all tasks. Contributed by Vinod K V.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227422 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/01/12 5:29 PM",
      "commitName": "0870734787d7005d85697549eab5b6479d97d453",
      "commitAuthor": "Arun Murthy",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "MAPREDUCE-3566. Fixed MR AM to construct CLC only once across all tasks. Contributed by Vinod K V.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227422 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:29 PM",
          "commitName": "0870734787d7005d85697549eab5b6479d97d453",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "22/12/11 2:34 PM",
          "commitNameOld": "8fa0a3c737f27ff9d12fb657a7b22865754a5fd8",
          "commitAuthorOld": "Siddharth Seth",
          "daysBetweenCommits": 13.12,
          "commitsBetweenForRepo": 31,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,109 +1,128 @@\n-  private ContainerLaunchContext createContainerLaunchContext(\n-      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n+  private static ContainerLaunchContext createCommonContainerLaunchContext(\n+      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n+      Token\u003cJobTokenIdentifier\u003e jobToken,\n+      final org.apache.hadoop.mapred.JobID oldJobId,\n+      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n-      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n-        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n-              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n-                                               remoteFS.getWorkingDirectory());\n+      String jobJar \u003d conf.get(MRJobConfig.JAR);\n+      if (jobJar !\u003d null) {\n+        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n+            .getUri(), remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n-            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n+            createLocalResource(remoteFS, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n-          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n+          createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n-    // Setup environment\n-    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n+    // Shell\n+    environment.put(\n+        Environment.SHELL.name(), \n+        conf.get(\n+            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n+            MRJobConfig.DEFAULT_SHELL)\n+            );\n \n-    // Set up the launch command\n-    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n-        taskAttemptListener.getAddress(), remoteTask,\n-        jvmID);\n-    \n+    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n+    Apps.addToEnvironment(\n+        environment, \n+        Environment.LD_LIBRARY_PATH.name(), \n+        Environment.PWD.$());\n+\n+    // Add the env variables passed by the admin\n+    Apps.setEnvFromInputString(\n+        environment, \n+        conf.get(\n+            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n+            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n+        );\n+\n     // Construct the actual Container\n+    // The null fields are per-container and will be constructed for each\n+    // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n-        .newContainerLaunchContext(containerID, conf\n-            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n-            environment, commands, serviceData, tokens, applicationACLs);\n+        .newContainerLaunchContext(null, conf\n+            .get(MRJobConfig.USER_NAME), null, localResources,\n+            environment, null, serviceData, tokens, applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, tokens, applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {
            "oldValue": "createContainerLaunchContext",
            "newValue": "createCommonContainerLaunchContext"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-3566. Fixed MR AM to construct CLC only once across all tasks. Contributed by Vinod K V.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227422 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:29 PM",
          "commitName": "0870734787d7005d85697549eab5b6479d97d453",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "22/12/11 2:34 PM",
          "commitNameOld": "8fa0a3c737f27ff9d12fb657a7b22865754a5fd8",
          "commitAuthorOld": "Siddharth Seth",
          "daysBetweenCommits": 13.12,
          "commitsBetweenForRepo": 31,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,109 +1,128 @@\n-  private ContainerLaunchContext createContainerLaunchContext(\n-      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n+  private static ContainerLaunchContext createCommonContainerLaunchContext(\n+      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n+      Token\u003cJobTokenIdentifier\u003e jobToken,\n+      final org.apache.hadoop.mapred.JobID oldJobId,\n+      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n-      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n-        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n-              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n-                                               remoteFS.getWorkingDirectory());\n+      String jobJar \u003d conf.get(MRJobConfig.JAR);\n+      if (jobJar !\u003d null) {\n+        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n+            .getUri(), remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n-            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n+            createLocalResource(remoteFS, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n-          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n+          createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n-    // Setup environment\n-    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n+    // Shell\n+    environment.put(\n+        Environment.SHELL.name(), \n+        conf.get(\n+            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n+            MRJobConfig.DEFAULT_SHELL)\n+            );\n \n-    // Set up the launch command\n-    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n-        taskAttemptListener.getAddress(), remoteTask,\n-        jvmID);\n-    \n+    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n+    Apps.addToEnvironment(\n+        environment, \n+        Environment.LD_LIBRARY_PATH.name(), \n+        Environment.PWD.$());\n+\n+    // Add the env variables passed by the admin\n+    Apps.setEnvFromInputString(\n+        environment, \n+        conf.get(\n+            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n+            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n+        );\n+\n     // Construct the actual Container\n+    // The null fields are per-container and will be constructed for each\n+    // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n-        .newContainerLaunchContext(containerID, conf\n-            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n-            environment, commands, serviceData, tokens, applicationACLs);\n+        .newContainerLaunchContext(null, conf\n+            .get(MRJobConfig.USER_NAME), null, localResources,\n+            environment, null, serviceData, tokens, applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, tokens, applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {
            "oldValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e]",
            "newValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e, conf-Configuration, jobToken-Token\u003cJobTokenIdentifier\u003e, oldJobId-org.apache.hadoop.mapred.JobID(modifiers-final), fsTokens-Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "MAPREDUCE-3566. Fixed MR AM to construct CLC only once across all tasks. Contributed by Vinod K V.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227422 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:29 PM",
          "commitName": "0870734787d7005d85697549eab5b6479d97d453",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "22/12/11 2:34 PM",
          "commitNameOld": "8fa0a3c737f27ff9d12fb657a7b22865754a5fd8",
          "commitAuthorOld": "Siddharth Seth",
          "daysBetweenCommits": 13.12,
          "commitsBetweenForRepo": 31,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,109 +1,128 @@\n-  private ContainerLaunchContext createContainerLaunchContext(\n-      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n+  private static ContainerLaunchContext createCommonContainerLaunchContext(\n+      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n+      Token\u003cJobTokenIdentifier\u003e jobToken,\n+      final org.apache.hadoop.mapred.JobID oldJobId,\n+      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n-      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n-        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n-              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n-                                               remoteFS.getWorkingDirectory());\n+      String jobJar \u003d conf.get(MRJobConfig.JAR);\n+      if (jobJar !\u003d null) {\n+        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n+            .getUri(), remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n-            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n+            createLocalResource(remoteFS, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n-          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n+          createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n-    // Setup environment\n-    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n+    // Shell\n+    environment.put(\n+        Environment.SHELL.name(), \n+        conf.get(\n+            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n+            MRJobConfig.DEFAULT_SHELL)\n+            );\n \n-    // Set up the launch command\n-    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n-        taskAttemptListener.getAddress(), remoteTask,\n-        jvmID);\n-    \n+    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n+    Apps.addToEnvironment(\n+        environment, \n+        Environment.LD_LIBRARY_PATH.name(), \n+        Environment.PWD.$());\n+\n+    // Add the env variables passed by the admin\n+    Apps.setEnvFromInputString(\n+        environment, \n+        conf.get(\n+            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n+            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n+        );\n+\n     // Construct the actual Container\n+    // The null fields are per-container and will be constructed for each\n+    // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n-        .newContainerLaunchContext(containerID, conf\n-            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n-            environment, commands, serviceData, tokens, applicationACLs);\n+        .newContainerLaunchContext(null, conf\n+            .get(MRJobConfig.USER_NAME), null, localResources,\n+            environment, null, serviceData, tokens, applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, tokens, applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[private, static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-3566. Fixed MR AM to construct CLC only once across all tasks. Contributed by Vinod K V.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227422 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:29 PM",
          "commitName": "0870734787d7005d85697549eab5b6479d97d453",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "22/12/11 2:34 PM",
          "commitNameOld": "8fa0a3c737f27ff9d12fb657a7b22865754a5fd8",
          "commitAuthorOld": "Siddharth Seth",
          "daysBetweenCommits": 13.12,
          "commitsBetweenForRepo": 31,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,109 +1,128 @@\n-  private ContainerLaunchContext createContainerLaunchContext(\n-      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n+  private static ContainerLaunchContext createCommonContainerLaunchContext(\n+      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n+      Token\u003cJobTokenIdentifier\u003e jobToken,\n+      final org.apache.hadoop.mapred.JobID oldJobId,\n+      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n-      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n-        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n-              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n-                                               remoteFS.getWorkingDirectory());\n+      String jobJar \u003d conf.get(MRJobConfig.JAR);\n+      if (jobJar !\u003d null) {\n+        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n+            .getUri(), remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n-            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n+            createLocalResource(remoteFS, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n-          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n+          createLocalResource(remoteFS, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n-    // Setup environment\n-    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n+    // Shell\n+    environment.put(\n+        Environment.SHELL.name(), \n+        conf.get(\n+            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n+            MRJobConfig.DEFAULT_SHELL)\n+            );\n \n-    // Set up the launch command\n-    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n-        taskAttemptListener.getAddress(), remoteTask,\n-        jvmID);\n-    \n+    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n+    Apps.addToEnvironment(\n+        environment, \n+        Environment.LD_LIBRARY_PATH.name(), \n+        Environment.PWD.$());\n+\n+    // Add the env variables passed by the admin\n+    Apps.setEnvFromInputString(\n+        environment, \n+        conf.get(\n+            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n+            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n+        );\n+\n     // Construct the actual Container\n+    // The null fields are per-container and will be constructed for each\n+    // container separately.\n     ContainerLaunchContext container \u003d BuilderUtils\n-        .newContainerLaunchContext(containerID, conf\n-            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n-            environment, commands, serviceData, tokens, applicationACLs);\n+        .newContainerLaunchContext(null, conf\n+            .get(MRJobConfig.USER_NAME), null, localResources,\n+            environment, null, serviceData, tokens, applicationACLs);\n \n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static ContainerLaunchContext createCommonContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs, Configuration conf,\n      Token\u003cJobTokenIdentifier\u003e jobToken,\n      final org.apache.hadoop.mapred.JobID oldJobId,\n      Collection\u003cToken\u003c? extends TokenIdentifier\u003e\u003e fsTokens) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      String jobJar \u003d conf.get(MRJobConfig.JAR);\n      if (jobJar !\u003d null) {\n        Path remoteJobJar \u003d (new Path(jobJar)).makeQualified(remoteFS\n            .getUri(), remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Shell\n    environment.put(\n        Environment.SHELL.name(), \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_SHELL, \n            MRJobConfig.DEFAULT_SHELL)\n            );\n\n    // Add pwd to LD_LIBRARY_PATH, add this before adding anything else\n    Apps.addToEnvironment(\n        environment, \n        Environment.LD_LIBRARY_PATH.name(), \n        Environment.PWD.$());\n\n    // Add the env variables passed by the admin\n    Apps.setEnvFromInputString(\n        environment, \n        conf.get(\n            MRJobConfig.MAPRED_ADMIN_USER_ENV, \n            MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV)\n        );\n\n    // Construct the actual Container\n    // The null fields are per-container and will be constructed for each\n    // container separately.\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(null, conf\n            .get(MRJobConfig.USER_NAME), null, localResources,\n            environment, null, serviceData, tokens, applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "df2991c0cbc3f35c2640b93680667507c4f810dd": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-3104. Implemented Application-acls. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186748 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/10/11 4:45 AM",
      "commitName": "df2991c0cbc3f35c2640b93680667507c4f810dd",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-3104. Implemented Application-acls. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186748 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "20/10/11 4:45 AM",
          "commitName": "df2991c0cbc3f35c2640b93680667507c4f810dd",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "18/10/11 10:21 PM",
          "commitNameOld": "13e4562924a6cb3d16c262e0f595b2ffbf9e0546",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,114 +1,109 @@\n-  private ContainerLaunchContext createContainerLaunchContext() {\n+  private ContainerLaunchContext createContainerLaunchContext(\n+      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Setup environment\n     MapReduceChildJVM.setVMEnv(environment, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask,\n         jvmID);\n     \n     // Construct the actual Container\n-    ContainerLaunchContext container \u003d\n-        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n-    container.setContainerId(containerID);\n-    container.setUser(conf.get(MRJobConfig.USER_NAME));\n-    container.setResource(assignedCapability);\n-    container.setLocalResources(localResources);\n-    container.setEnvironment(environment);\n-    container.setCommands(commands);\n-    container.setServiceData(serviceData);\n-    container.setContainerTokens(tokens);\n-    \n+    ContainerLaunchContext container \u003d BuilderUtils\n+        .newContainerLaunchContext(containerID, conf\n+            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n+            environment, commands, serviceData, tokens, applicationACLs);\n+\n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Setup environment\n    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask,\n        jvmID);\n    \n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(containerID, conf\n            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n            environment, commands, serviceData, tokens, applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[applicationACLs-Map\u003cApplicationAccessType,String\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-3104. Implemented Application-acls. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186748 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "20/10/11 4:45 AM",
          "commitName": "df2991c0cbc3f35c2640b93680667507c4f810dd",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "18/10/11 10:21 PM",
          "commitNameOld": "13e4562924a6cb3d16c262e0f595b2ffbf9e0546",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,114 +1,109 @@\n-  private ContainerLaunchContext createContainerLaunchContext() {\n+  private ContainerLaunchContext createContainerLaunchContext(\n+      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n       Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Setup environment\n     MapReduceChildJVM.setVMEnv(environment, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask,\n         jvmID);\n     \n     // Construct the actual Container\n-    ContainerLaunchContext container \u003d\n-        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n-    container.setContainerId(containerID);\n-    container.setUser(conf.get(MRJobConfig.USER_NAME));\n-    container.setResource(assignedCapability);\n-    container.setLocalResources(localResources);\n-    container.setEnvironment(environment);\n-    container.setCommands(commands);\n-    container.setServiceData(serviceData);\n-    container.setContainerTokens(tokens);\n-    \n+    ContainerLaunchContext container \u003d BuilderUtils\n+        .newContainerLaunchContext(containerID, conf\n+            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n+            environment, commands, serviceData, tokens, applicationACLs);\n+\n     return container;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerLaunchContext createContainerLaunchContext(\n      Map\u003cApplicationAccessType, String\u003e applicationACLs) {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Setup environment\n    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask,\n        jvmID);\n    \n    // Construct the actual Container\n    ContainerLaunchContext container \u003d BuilderUtils\n        .newContainerLaunchContext(containerID, conf\n            .get(MRJobConfig.USER_NAME), assignedCapability, localResources,\n            environment, commands, serviceData, tokens, applicationACLs);\n\n    return container;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "c1d90772b6e38bb4e4be7ed75cb5d34f3048ad7b": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3068. Added a whitelist of environment variables for containers from the NodeManager and set MALLOC_ARENA_MAX for all daemons and containers. Contributed by Chris Riccomini. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1185447 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/10/11 6:22 PM",
      "commitName": "c1d90772b6e38bb4e4be7ed75cb5d34f3048ad7b",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "17/10/11 8:17 AM",
      "commitNameOld": "a26b1672a85e97bea973cfcc7eab22b4cca01448",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 0.42,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,114 @@\n   private ContainerLaunchContext createContainerLaunchContext() {\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n \n     // Service data\n     Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n \n     // Tokens\n     ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n           MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       MRApps.setupDistributedCache(conf, localResources);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n \n-      MRApps.addToEnvironment(\n+      Apps.addToEnvironment(\n           environment,  \n           Environment.CLASSPATH.name(), \n           getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     // Setup environment\n     MapReduceChildJVM.setVMEnv(environment, remoteTask);\n \n     // Set up the launch command\n     List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask,\n         jvmID);\n     \n     // Construct the actual Container\n     ContainerLaunchContext container \u003d\n         recordFactory.newRecordInstance(ContainerLaunchContext.class);\n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME));\n     container.setResource(assignedCapability);\n     container.setLocalResources(localResources);\n     container.setEnvironment(environment);\n     container.setCommands(commands);\n     container.setServiceData(serviceData);\n     container.setContainerTokens(tokens);\n     \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      Apps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Setup environment\n    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask,\n        jvmID);\n    \n    // Construct the actual Container\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    container.setLocalResources(localResources);\n    container.setEnvironment(environment);\n    container.setCommands(commands);\n    container.setServiceData(serviceData);\n    container.setContainerTokens(tokens);\n    \n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "d00b3c49f6fb3f6a617add6203c6b55f6c345940": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2880. Improved classpath-construction for mapreduce AM and containers. Contributed by Arun C Murthy.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1173783 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/09/11 11:28 AM",
      "commitName": "d00b3c49f6fb3f6a617add6203c6b55f6c345940",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "20/09/11 6:26 PM",
      "commitNameOld": "1d067c6e2b14e08943a46129f4ed521890d3ca22",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.71,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,125 +1,114 @@\n   private ContainerLaunchContext createContainerLaunchContext() {\n \n-    ContainerLaunchContext container \u003d\n-        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n-\n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n-    \n+\n+    // Service data\n+    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n+\n+    // Tokens\n+    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         localResources.put(\n-            MRConstants.JOB_JAR,\n+            MRJobConfig.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n-          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n+          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n       localResources.put(\n-          MRConstants.JOB_CONF_FILE,\n+          MRJobConfig.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n-      MRApps.setupDistributedCache(conf, localResources, environment);\n+      MRApps.setupDistributedCache(conf, localResources);\n \n-      // Set local-resources and environment\n-      container.setLocalResources(localResources);\n-      container.setEnvironment(environment);\n-      \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n-      container.setContainerTokens(\n+      tokens \u003d \n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n-              containerTokens_dob.getLength()));\n+              containerTokens_dob.getLength());\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n-      Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n-      container.setServiceData(serviceData);\n \n-      MRApps.addToClassPath(container.getEnvironment(), getInitialClasspath());\n+      MRApps.addToEnvironment(\n+          environment,  \n+          Environment.CLASSPATH.name(), \n+          getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n+\n+    // Setup environment\n+    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n+\n+    // Set up the launch command\n+    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n+        taskAttemptListener.getAddress(), remoteTask,\n+        jvmID);\n     \n-    container.setContainerId(containerID);\n-    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n-\n-    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n-    String containerLogDir \u003d\n-        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n-    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n-    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n-    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n-    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n-\n-    String localizedApplicationTokensFile \u003d\n-        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n-    classPaths.add(MRConstants.JOB_JAR);\n-    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n-    classPaths.add(workDir.toString()); // TODO\n-\n     // Construct the actual Container\n-    container.setCommands(MapReduceChildJVM.getVMCommand(\n-        taskAttemptListener.getAddress(), remoteTask, javaHome,\n-        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n-\n-    MapReduceChildJVM.setVMEnv(container.getEnvironment(), classPaths,\n-        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n-        localizedApplicationTokensFile);\n-\n-    // Construct the actual Container\n+    ContainerLaunchContext container \u003d\n+        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME));\n     container.setResource(assignedCapability);\n+    container.setLocalResources(localResources);\n+    container.setEnvironment(environment);\n+    container.setCommands(commands);\n+    container.setServiceData(serviceData);\n+    container.setContainerTokens(tokens);\n+    \n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n\n    // Service data\n    Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n\n    // Tokens\n    ByteBuffer tokens \u003d ByteBuffer.wrap(new byte[]{});\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRJobConfig.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n      localResources.put(\n          MRJobConfig.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      tokens \u003d \n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength());\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n\n      MRApps.addToEnvironment(\n          environment,  \n          Environment.CLASSPATH.name(), \n          getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    // Setup environment\n    MapReduceChildJVM.setVMEnv(environment, remoteTask);\n\n    // Set up the launch command\n    List\u003cString\u003e commands \u003d MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask,\n        jvmID);\n    \n    // Construct the actual Container\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    container.setLocalResources(localResources);\n    container.setEnvironment(environment);\n    container.setCommands(commands);\n    container.setServiceData(serviceData);\n    container.setContainerTokens(tokens);\n    \n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "88b82a0f6687ce103817fbb460fd30d870f717a0": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2899. Replace major parts of ApplicationSubmissionContext with a ContainerLaunchContext (Arun Murthy via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1170459 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/09/11 12:26 AM",
      "commitName": "88b82a0f6687ce103817fbb460fd30d870f717a0",
      "commitAuthor": "Mahadev Konar",
      "commitDateOld": "13/09/11 11:12 AM",
      "commitNameOld": "53f921418d25cb232c7a0e1fa24c17bda729ac35",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.55,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,126 +1,125 @@\n   private ContainerLaunchContext createContainerLaunchContext() {\n \n     ContainerLaunchContext container \u003d\n         recordFactory.newRecordInstance(ContainerLaunchContext.class);\n \n     // Application resources\n     Map\u003cString, LocalResource\u003e localResources \u003d \n         new HashMap\u003cString, LocalResource\u003e();\n     \n     // Application environment\n     Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n     \n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         localResources.put(\n             MRConstants.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n       localResources.put(\n           MRConstants.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n-      \n       // Setup DistributedCache\n-      setupDistributedCache(remoteFS, conf, localResources, environment);\n+      MRApps.setupDistributedCache(conf, localResources, environment);\n \n       // Set local-resources and environment\n       container.setLocalResources(localResources);\n-      container.setEnv(environment);\n+      container.setEnvironment(environment);\n       \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       container.setContainerTokens(\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength()));\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n       Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n       serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n           ShuffleHandler.serializeServiceData(jobToken));\n       container.setServiceData(serviceData);\n \n-      MRApps.addToClassPath(container.getEnv(), getInitialClasspath());\n+      MRApps.addToClassPath(container.getEnvironment(), getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n     \n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n \n     File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n     String containerLogDir \u003d\n         new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n     String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n     String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n     String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n     List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n \n     String localizedApplicationTokensFile \u003d\n         new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n     classPaths.add(MRConstants.JOB_JAR);\n     classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n     classPaths.add(workDir.toString()); // TODO\n \n     // Construct the actual Container\n     container.setCommands(MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, javaHome,\n         workDir.toString(), containerLogDir, childTmpDir, jvmID));\n \n-    MapReduceChildJVM.setVMEnv(container.getEnv(), classPaths,\n+    MapReduceChildJVM.setVMEnv(container.getEnvironment(), classPaths,\n         workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n         localizedApplicationTokensFile);\n \n     // Construct the actual Container\n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME));\n     container.setResource(assignedCapability);\n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n    \n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRConstants.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n      localResources.put(\n          MRConstants.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      MRApps.setupDistributedCache(conf, localResources, environment);\n\n      // Set local-resources and environment\n      container.setLocalResources(localResources);\n      container.setEnvironment(environment);\n      \n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      container.setContainerTokens(\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength()));\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n      container.setServiceData(serviceData);\n\n      MRApps.addToClassPath(container.getEnvironment(), getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n    \n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n\n    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n    String containerLogDir \u003d\n        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n\n    String localizedApplicationTokensFile \u003d\n        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n    classPaths.add(MRConstants.JOB_JAR);\n    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n    classPaths.add(workDir.toString()); // TODO\n\n    // Construct the actual Container\n    container.setCommands(MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, javaHome,\n        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n\n    MapReduceChildJVM.setVMEnv(container.getEnvironment(), classPaths,\n        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n        localizedApplicationTokensFile);\n\n    // Construct the actual Container\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "6165875dc6bf67d72fc3ce1d96dfc80ba312d4a1": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2896. Simplify all apis to in org.apache.hadoop.yarn.api.records.* to be get/set only. Added javadocs to all public records.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1169980 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/09/11 5:05 PM",
      "commitName": "6165875dc6bf67d72fc3ce1d96dfc80ba312d4a1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "10/09/11 11:21 PM",
      "commitNameOld": "8fb67650b146573c20ae010e28b1eca6e16433b3",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 1.74,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,126 @@\n   private ContainerLaunchContext createContainerLaunchContext() {\n \n     ContainerLaunchContext container \u003d\n         recordFactory.newRecordInstance(ContainerLaunchContext.class);\n \n+    // Application resources\n+    Map\u003cString, LocalResource\u003e localResources \u003d \n+        new HashMap\u003cString, LocalResource\u003e();\n+    \n+    // Application environment\n+    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n+    \n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n-        container.setLocalResource(\n+        localResources.put(\n             MRConstants.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n-      container.setLocalResource(\n+      localResources.put(\n           MRConstants.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n+      \n       // Setup DistributedCache\n-      setupDistributedCache(remoteFS, conf, container);\n+      setupDistributedCache(remoteFS, conf, localResources, environment);\n \n+      // Set local-resources and environment\n+      container.setLocalResources(localResources);\n+      container.setEnv(environment);\n+      \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       container.setContainerTokens(\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength()));\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n-      container\n-          .setServiceData(\n-              ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n-              ShuffleHandler.serializeServiceData(jobToken));\n+      Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n+      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n+          ShuffleHandler.serializeServiceData(jobToken));\n+      container.setServiceData(serviceData);\n \n-      MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n+      MRApps.addToClassPath(container.getEnv(), getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n     \n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n \n     File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n     String containerLogDir \u003d\n         new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n     String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n     String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n     String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n     List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n \n     String localizedApplicationTokensFile \u003d\n         new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n     classPaths.add(MRConstants.JOB_JAR);\n     classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n     classPaths.add(workDir.toString()); // TODO\n \n     // Construct the actual Container\n-    container.addAllCommands(MapReduceChildJVM.getVMCommand(\n+    container.setCommands(MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, javaHome,\n         workDir.toString(), containerLogDir, childTmpDir, jvmID));\n \n-    MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n+    MapReduceChildJVM.setVMEnv(container.getEnv(), classPaths,\n         workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n         localizedApplicationTokensFile);\n \n     // Construct the actual Container\n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME));\n     container.setResource(assignedCapability);\n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n\n    // Application resources\n    Map\u003cString, LocalResource\u003e localResources \u003d \n        new HashMap\u003cString, LocalResource\u003e();\n    \n    // Application environment\n    Map\u003cString, String\u003e environment \u003d new HashMap\u003cString, String\u003e();\n    \n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        localResources.put(\n            MRConstants.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n      localResources.put(\n          MRConstants.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      \n      // Setup DistributedCache\n      setupDistributedCache(remoteFS, conf, localResources, environment);\n\n      // Set local-resources and environment\n      container.setLocalResources(localResources);\n      container.setEnv(environment);\n      \n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      container.setContainerTokens(\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength()));\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      Map\u003cString, ByteBuffer\u003e serviceData \u003d new HashMap\u003cString, ByteBuffer\u003e();\n      serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n          ShuffleHandler.serializeServiceData(jobToken));\n      container.setServiceData(serviceData);\n\n      MRApps.addToClassPath(container.getEnv(), getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n    \n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n\n    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n    String containerLogDir \u003d\n        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n\n    String localizedApplicationTokensFile \u003d\n        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n    classPaths.add(MRConstants.JOB_JAR);\n    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n    classPaths.add(workDir.toString()); // TODO\n\n    // Construct the actual Container\n    container.setCommands(MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, javaHome,\n        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n\n    MapReduceChildJVM.setVMEnv(container.getEnv(), classPaths,\n        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n        localizedApplicationTokensFile);\n\n    // Construct the actual Container\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "ade0f0560f729e50382c6992f713f29e2dd5b270": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2652. Enabled multiple NMs to be runnable on a single node by making shuffle service port to be truely configurable. Contributed by Robert Joseph Evans.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1163585 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/08/11 4:38 AM",
      "commitName": "ade0f0560f729e50382c6992f713f29e2dd5b270",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 6.48,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,117 +1,114 @@\n   private ContainerLaunchContext createContainerLaunchContext() {\n \n     ContainerLaunchContext container \u003d\n         recordFactory.newRecordInstance(ContainerLaunchContext.class);\n \n     try {\n       FileSystem remoteFS \u003d FileSystem.get(conf);\n \n       // //////////// Set up JobJar to be localized properly on the remote NM.\n       if (conf.get(MRJobConfig.JAR) !\u003d null) {\n         Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n               MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                                remoteFS.getWorkingDirectory());\n         container.setLocalResource(\n             MRConstants.JOB_JAR,\n             createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                 LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n         LOG.info(\"The job-jar file on the remote FS is \"\n             + remoteJobJar.toUri().toASCIIString());\n       } else {\n         // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n         // mapreduce jar itself which is already on the classpath.\n         LOG.info(\"Job jar is not present. \"\n             + \"Not adding any jar to the list of resources.\");\n       }\n       // //////////// End of JobJar setup\n \n       // //////////// Set up JobConf to be localized properly on the remote NM.\n       Path path \u003d\n           MRApps.getStagingAreaDir(conf, UserGroupInformation\n               .getCurrentUser().getShortUserName());\n       Path remoteJobSubmitDir \u003d\n           new Path(path, oldJobId.toString());\n       Path remoteJobConfPath \u003d \n           new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n       container.setLocalResource(\n           MRConstants.JOB_CONF_FILE,\n           createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n               LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n       LOG.info(\"The job-conf file on the remote FS is \"\n           + remoteJobConfPath.toUri().toASCIIString());\n       // //////////// End of JobConf setup\n \n       // Setup DistributedCache\n       setupDistributedCache(remoteFS, conf, container);\n \n       // Setup up tokens\n       Credentials taskCredentials \u003d new Credentials();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Add file-system tokens\n         for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n           LOG.info(\"Putting fs-token for NM use for launching container : \"\n               + token.toString());\n           taskCredentials.addToken(token.getService(), token);\n         }\n       }\n \n       // LocalStorageToken is needed irrespective of whether security is enabled\n       // or not.\n       TokenCache.setJobToken(jobToken, taskCredentials);\n \n       DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n       LOG.info(\"Size of containertokens_dob is \"\n           + taskCredentials.numberOfTokens());\n       taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n       container.setContainerTokens(\n           ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n               containerTokens_dob.getLength()));\n \n       // Add shuffle token\n       LOG.info(\"Putting shuffle token in serviceData\");\n-      DataOutputBuffer jobToken_dob \u003d new DataOutputBuffer();\n-      jobToken.write(jobToken_dob);\n       container\n           .setServiceData(\n               ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n-              ByteBuffer.wrap(jobToken_dob.getData(), 0,\n-                  jobToken_dob.getLength()));\n+              ShuffleHandler.serializeServiceData(jobToken));\n \n       MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n     \n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n \n     File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n     String containerLogDir \u003d\n         new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n     String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n     String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n     String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n     List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n \n     String localizedApplicationTokensFile \u003d\n         new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n     classPaths.add(MRConstants.JOB_JAR);\n     classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n     classPaths.add(workDir.toString()); // TODO\n \n     // Construct the actual Container\n     container.addAllCommands(MapReduceChildJVM.getVMCommand(\n         taskAttemptListener.getAddress(), remoteTask, javaHome,\n         workDir.toString(), containerLogDir, childTmpDir, jvmID));\n \n     MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n         workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n         localizedApplicationTokensFile);\n \n     // Construct the actual Container\n     container.setContainerId(containerID);\n     container.setUser(conf.get(MRJobConfig.USER_NAME));\n     container.setResource(assignedCapability);\n     return container;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        container.setLocalResource(\n            MRConstants.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n      container.setLocalResource(\n          MRConstants.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      setupDistributedCache(remoteFS, conf, container);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      container.setContainerTokens(\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength()));\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      container\n          .setServiceData(\n              ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n              ShuffleHandler.serializeServiceData(jobToken));\n\n      MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n    \n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n\n    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n    String containerLogDir \u003d\n        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n\n    String localizedApplicationTokensFile \u003d\n        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n    classPaths.add(MRConstants.JOB_JAR);\n    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n    classPaths.add(workDir.toString()); // TODO\n\n    // Construct the actual Container\n    container.addAllCommands(MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, javaHome,\n        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n\n    MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n        localizedApplicationTokensFile);\n\n    // Construct the actual Container\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        container.setLocalResource(\n            MRConstants.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n      container.setLocalResource(\n          MRConstants.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      setupDistributedCache(remoteFS, conf, container);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      container.setContainerTokens(\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength()));\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      DataOutputBuffer jobToken_dob \u003d new DataOutputBuffer();\n      jobToken.write(jobToken_dob);\n      container\n          .setServiceData(\n              ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n              ByteBuffer.wrap(jobToken_dob.getData(), 0,\n                  jobToken_dob.getLength()));\n\n      MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n    \n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n\n    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n    String containerLogDir \u003d\n        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n\n    String localizedApplicationTokensFile \u003d\n        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n    classPaths.add(MRConstants.JOB_JAR);\n    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n    classPaths.add(workDir.toString()); // TODO\n\n    // Construct the actual Container\n    container.addAllCommands(MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, javaHome,\n        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n\n    MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n        localizedApplicationTokensFile);\n\n    // Construct the actual Container\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    return container;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,117 @@\n+  private ContainerLaunchContext createContainerLaunchContext() {\n+\n+    ContainerLaunchContext container \u003d\n+        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n+\n+    try {\n+      FileSystem remoteFS \u003d FileSystem.get(conf);\n+\n+      // //////////// Set up JobJar to be localized properly on the remote NM.\n+      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n+        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n+              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n+                                               remoteFS.getWorkingDirectory());\n+        container.setLocalResource(\n+            MRConstants.JOB_JAR,\n+            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n+                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n+        LOG.info(\"The job-jar file on the remote FS is \"\n+            + remoteJobJar.toUri().toASCIIString());\n+      } else {\n+        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n+        // mapreduce jar itself which is already on the classpath.\n+        LOG.info(\"Job jar is not present. \"\n+            + \"Not adding any jar to the list of resources.\");\n+      }\n+      // //////////// End of JobJar setup\n+\n+      // //////////// Set up JobConf to be localized properly on the remote NM.\n+      Path path \u003d\n+          MRApps.getStagingAreaDir(conf, UserGroupInformation\n+              .getCurrentUser().getShortUserName());\n+      Path remoteJobSubmitDir \u003d\n+          new Path(path, oldJobId.toString());\n+      Path remoteJobConfPath \u003d \n+          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n+      container.setLocalResource(\n+          MRConstants.JOB_CONF_FILE,\n+          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n+              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n+      LOG.info(\"The job-conf file on the remote FS is \"\n+          + remoteJobConfPath.toUri().toASCIIString());\n+      // //////////// End of JobConf setup\n+\n+      // Setup DistributedCache\n+      setupDistributedCache(remoteFS, conf, container);\n+\n+      // Setup up tokens\n+      Credentials taskCredentials \u003d new Credentials();\n+\n+      if (UserGroupInformation.isSecurityEnabled()) {\n+        // Add file-system tokens\n+        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n+          LOG.info(\"Putting fs-token for NM use for launching container : \"\n+              + token.toString());\n+          taskCredentials.addToken(token.getService(), token);\n+        }\n+      }\n+\n+      // LocalStorageToken is needed irrespective of whether security is enabled\n+      // or not.\n+      TokenCache.setJobToken(jobToken, taskCredentials);\n+\n+      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n+      LOG.info(\"Size of containertokens_dob is \"\n+          + taskCredentials.numberOfTokens());\n+      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n+      container.setContainerTokens(\n+          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n+              containerTokens_dob.getLength()));\n+\n+      // Add shuffle token\n+      LOG.info(\"Putting shuffle token in serviceData\");\n+      DataOutputBuffer jobToken_dob \u003d new DataOutputBuffer();\n+      jobToken.write(jobToken_dob);\n+      container\n+          .setServiceData(\n+              ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n+              ByteBuffer.wrap(jobToken_dob.getData(), 0,\n+                  jobToken_dob.getLength()));\n+\n+      MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n+    } catch (IOException e) {\n+      throw new YarnException(e);\n+    }\n+    \n+    container.setContainerId(containerID);\n+    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n+\n+    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n+    String containerLogDir \u003d\n+        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n+    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n+    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n+    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n+    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n+\n+    String localizedApplicationTokensFile \u003d\n+        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n+    classPaths.add(MRConstants.JOB_JAR);\n+    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n+    classPaths.add(workDir.toString()); // TODO\n+\n+    // Construct the actual Container\n+    container.addAllCommands(MapReduceChildJVM.getVMCommand(\n+        taskAttemptListener.getAddress(), remoteTask, javaHome,\n+        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n+\n+    MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n+        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n+        localizedApplicationTokensFile);\n+\n+    // Construct the actual Container\n+    container.setContainerId(containerID);\n+    container.setUser(conf.get(MRJobConfig.USER_NAME));\n+    container.setResource(assignedCapability);\n+    return container;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerLaunchContext createContainerLaunchContext() {\n\n    ContainerLaunchContext container \u003d\n        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n\n    try {\n      FileSystem remoteFS \u003d FileSystem.get(conf);\n\n      // //////////// Set up JobJar to be localized properly on the remote NM.\n      if (conf.get(MRJobConfig.JAR) !\u003d null) {\n        Path remoteJobJar \u003d (new Path(remoteTask.getConf().get(\n              MRJobConfig.JAR))).makeQualified(remoteFS.getUri(), \n                                               remoteFS.getWorkingDirectory());\n        container.setLocalResource(\n            MRConstants.JOB_JAR,\n            createLocalResource(remoteFS, recordFactory, remoteJobJar,\n                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n        LOG.info(\"The job-jar file on the remote FS is \"\n            + remoteJobJar.toUri().toASCIIString());\n      } else {\n        // Job jar may be null. For e.g, for pipes, the job jar is the hadoop\n        // mapreduce jar itself which is already on the classpath.\n        LOG.info(\"Job jar is not present. \"\n            + \"Not adding any jar to the list of resources.\");\n      }\n      // //////////// End of JobJar setup\n\n      // //////////// Set up JobConf to be localized properly on the remote NM.\n      Path path \u003d\n          MRApps.getStagingAreaDir(conf, UserGroupInformation\n              .getCurrentUser().getShortUserName());\n      Path remoteJobSubmitDir \u003d\n          new Path(path, oldJobId.toString());\n      Path remoteJobConfPath \u003d \n          new Path(remoteJobSubmitDir, MRConstants.JOB_CONF_FILE);\n      container.setLocalResource(\n          MRConstants.JOB_CONF_FILE,\n          createLocalResource(remoteFS, recordFactory, remoteJobConfPath,\n              LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\n      LOG.info(\"The job-conf file on the remote FS is \"\n          + remoteJobConfPath.toUri().toASCIIString());\n      // //////////// End of JobConf setup\n\n      // Setup DistributedCache\n      setupDistributedCache(remoteFS, conf, container);\n\n      // Setup up tokens\n      Credentials taskCredentials \u003d new Credentials();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Add file-system tokens\n        for (Token\u003c? extends TokenIdentifier\u003e token : fsTokens) {\n          LOG.info(\"Putting fs-token for NM use for launching container : \"\n              + token.toString());\n          taskCredentials.addToken(token.getService(), token);\n        }\n      }\n\n      // LocalStorageToken is needed irrespective of whether security is enabled\n      // or not.\n      TokenCache.setJobToken(jobToken, taskCredentials);\n\n      DataOutputBuffer containerTokens_dob \u003d new DataOutputBuffer();\n      LOG.info(\"Size of containertokens_dob is \"\n          + taskCredentials.numberOfTokens());\n      taskCredentials.writeTokenStorageToStream(containerTokens_dob);\n      container.setContainerTokens(\n          ByteBuffer.wrap(containerTokens_dob.getData(), 0,\n              containerTokens_dob.getLength()));\n\n      // Add shuffle token\n      LOG.info(\"Putting shuffle token in serviceData\");\n      DataOutputBuffer jobToken_dob \u003d new DataOutputBuffer();\n      jobToken.write(jobToken_dob);\n      container\n          .setServiceData(\n              ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID,\n              ByteBuffer.wrap(jobToken_dob.getData(), 0,\n                  jobToken_dob.getLength()));\n\n      MRApps.addToClassPath(container.getAllEnv(), getInitialClasspath());\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n    \n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME)); // TODO: Fix\n\n    File workDir \u003d new File(\"$PWD\"); // Will be expanded by the shell.\n    String containerLogDir \u003d\n        new File(ApplicationConstants.LOG_DIR_EXPANSION_VAR).toString();\n    String childTmpDir \u003d new File(workDir, \"tmp\").toString();\n    String javaHome \u003d \"${JAVA_HOME}\"; // Will be expanded by the shell.\n    String nmLdLibraryPath \u003d \"{LD_LIBRARY_PATH}\"; // Expanded by the shell?\n    List\u003cString\u003e classPaths \u003d new ArrayList\u003cString\u003e();\n\n    String localizedApplicationTokensFile \u003d\n        new File(workDir, MRConstants.APPLICATION_TOKENS_FILE).toString();\n    classPaths.add(MRConstants.JOB_JAR);\n    classPaths.add(MRConstants.YARN_MAPREDUCE_APP_JAR_PATH);\n    classPaths.add(workDir.toString()); // TODO\n\n    // Construct the actual Container\n    container.addAllCommands(MapReduceChildJVM.getVMCommand(\n        taskAttemptListener.getAddress(), remoteTask, javaHome,\n        workDir.toString(), containerLogDir, childTmpDir, jvmID));\n\n    MapReduceChildJVM.setVMEnv(container.getAllEnv(), classPaths,\n        workDir.toString(), containerLogDir, nmLdLibraryPath, remoteTask,\n        localizedApplicationTokensFile);\n\n    // Construct the actual Container\n    container.setContainerId(containerID);\n    container.setUser(conf.get(MRJobConfig.USER_NAME));\n    container.setResource(assignedCapability);\n    return container;\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java"
    }
  }
}