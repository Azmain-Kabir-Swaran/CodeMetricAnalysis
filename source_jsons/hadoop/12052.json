{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 3319,
  "functionEndLine": 3340,
  "numCommitsSeen": 197,
  "timeTaken": 5651,
  "changeHistory": [
    "c7d022b66f0c5baafbb7000a435c1d6e39906efe",
    "b2d5ed36bcb80e2581191dcdc3976e825c959142",
    "fe38d2e9b5ac7e13f97cd2d3d2a984ab6bbaaf77"
  ],
  "changeHistoryShort": {
    "c7d022b66f0c5baafbb7000a435c1d6e39906efe": "Ybodychange",
    "b2d5ed36bcb80e2581191dcdc3976e825c959142": "Ybodychange",
    "fe38d2e9b5ac7e13f97cd2d3d2a984ab6bbaaf77": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c7d022b66f0c5baafbb7000a435c1d6e39906efe": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8192. Eviction should key off used locked memory instead of ram disk free space. (Contributed by Arpit Agarwal)\n",
      "commitDate": "20/06/15 1:27 PM",
      "commitName": "c7d022b66f0c5baafbb7000a435c1d6e39906efe",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "16/05/15 9:05 AM",
      "commitNameOld": "e453989a5722e653bd97e3e54f9bbdffc9454fba",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 35.18,
      "commitsBetweenForRepo": 253,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,22 @@\n     public void run() {\n       int numSuccessiveFailures \u003d 0;\n \n       while (fsRunning \u0026\u0026 shouldRun) {\n         try {\n           numSuccessiveFailures \u003d saveNextReplica() ? 0 : (numSuccessiveFailures + 1);\n-          evictBlocks();\n \n           // Sleep if we have no more work to do or if it looks like we are not\n           // making any forward progress. This is to ensure that if all persist\n           // operations are failing we don\u0027t keep retrying them in a tight loop.\n           if (numSuccessiveFailures \u003e\u003d ramDiskReplicaTracker.numReplicasNotPersisted()) {\n             Thread.sleep(checkpointerInterval * 1000);\n             numSuccessiveFailures \u003d 0;\n           }\n         } catch (InterruptedException e) {\n           LOG.info(\"LazyWriter was interrupted, exiting\");\n           break;\n         } catch (Exception e) {\n           LOG.warn(\"Ignoring exception in LazyWriter:\", e);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      int numSuccessiveFailures \u003d 0;\n\n      while (fsRunning \u0026\u0026 shouldRun) {\n        try {\n          numSuccessiveFailures \u003d saveNextReplica() ? 0 : (numSuccessiveFailures + 1);\n\n          // Sleep if we have no more work to do or if it looks like we are not\n          // making any forward progress. This is to ensure that if all persist\n          // operations are failing we don\u0027t keep retrying them in a tight loop.\n          if (numSuccessiveFailures \u003e\u003d ramDiskReplicaTracker.numReplicasNotPersisted()) {\n            Thread.sleep(checkpointerInterval * 1000);\n            numSuccessiveFailures \u003d 0;\n          }\n        } catch (InterruptedException e) {\n          LOG.info(\"LazyWriter was interrupted, exiting\");\n          break;\n        } catch (Exception e) {\n          LOG.warn(\"Ignoring exception in LazyWriter:\", e);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "b2d5ed36bcb80e2581191dcdc3976e825c959142": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7100. Make eviction scheme pluggable. (Arpit Agarwal)\n",
      "commitDate": "20/09/14 1:25 PM",
      "commitName": "b2d5ed36bcb80e2581191dcdc3976e825c959142",
      "commitAuthor": "arp",
      "commitDateOld": "19/09/14 10:02 AM",
      "commitNameOld": "222bf0fe6706ee43964fd39b8315c1a339fbc84a",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.14,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n     public void run() {\n       int numSuccessiveFailures \u003d 0;\n \n       while (fsRunning \u0026\u0026 shouldRun) {\n         try {\n           numSuccessiveFailures \u003d saveNextReplica() ? 0 : (numSuccessiveFailures + 1);\n           evictBlocks();\n \n           // Sleep if we have no more work to do or if it looks like we are not\n           // making any forward progress. This is to ensure that if all persist\n           // operations are failing we don\u0027t keep retrying them in a tight loop.\n-          if (numSuccessiveFailures \u003e\u003d lazyWriteReplicaTracker.numReplicasNotPersisted()) {\n+          if (numSuccessiveFailures \u003e\u003d ramDiskReplicaTracker.numReplicasNotPersisted()) {\n             Thread.sleep(checkpointerInterval * 1000);\n             numSuccessiveFailures \u003d 0;\n           }\n         } catch (InterruptedException e) {\n           LOG.info(\"LazyWriter was interrupted, exiting\");\n           break;\n         } catch (Exception e) {\n           LOG.warn(\"Ignoring exception in LazyWriter:\", e);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      int numSuccessiveFailures \u003d 0;\n\n      while (fsRunning \u0026\u0026 shouldRun) {\n        try {\n          numSuccessiveFailures \u003d saveNextReplica() ? 0 : (numSuccessiveFailures + 1);\n          evictBlocks();\n\n          // Sleep if we have no more work to do or if it looks like we are not\n          // making any forward progress. This is to ensure that if all persist\n          // operations are failing we don\u0027t keep retrying them in a tight loop.\n          if (numSuccessiveFailures \u003e\u003d ramDiskReplicaTracker.numReplicasNotPersisted()) {\n            Thread.sleep(checkpointerInterval * 1000);\n            numSuccessiveFailures \u003d 0;\n          }\n        } catch (InterruptedException e) {\n          LOG.info(\"LazyWriter was interrupted, exiting\");\n          break;\n        } catch (Exception e) {\n          LOG.warn(\"Ignoring exception in LazyWriter:\", e);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "fe38d2e9b5ac7e13f97cd2d3d2a984ab6bbaaf77": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-6727. Refresh data volumes on DataNode based on configuration changes (Lei Xu via Colin Patrick McCabe)\n",
      "commitDate": "18/09/14 4:52 PM",
      "commitName": "fe38d2e9b5ac7e13f97cd2d3d2a984ab6bbaaf77",
      "commitAuthor": "Colin Patrick Mccabe",
      "diff": "@@ -0,0 +1,15 @@\n+        public void run() {\n+          StorageLocation vol \u003d volumes.get(idx);\n+          try {\n+            String key \u003d vol.getFile().getCanonicalPath();\n+            if (!allStorageDirs.containsKey(key)) {\n+              LOG.warn(\"Attempt to add an invalid volume: \" + vol.getFile());\n+            } else {\n+              addVolumeAndBlockPool(dataLocations, allStorageDirs.get(key),\n+                  bpids);\n+              successFlags[idx] \u003d true;\n+            }\n+          } catch (IOException e) {\n+            LOG.warn(\"Caught exception when adding volume \" + vol, e);\n+          }\n+        }\n\\ No newline at end of file\n",
      "actualSource": "        public void run() {\n          StorageLocation vol \u003d volumes.get(idx);\n          try {\n            String key \u003d vol.getFile().getCanonicalPath();\n            if (!allStorageDirs.containsKey(key)) {\n              LOG.warn(\"Attempt to add an invalid volume: \" + vol.getFile());\n            } else {\n              addVolumeAndBlockPool(dataLocations, allStorageDirs.get(key),\n                  bpids);\n              successFlags[idx] \u003d true;\n            }\n          } catch (IOException e) {\n            LOG.warn(\"Caught exception when adding volume \" + vol, e);\n          }\n        }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java"
    }
  }
}