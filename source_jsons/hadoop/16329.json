{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RouterClientProtocol.java",
  "functionName": "getFileInfo",
  "functionId": "getFileInfo___src-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
  "functionStartLine": 891,
  "functionEndLine": 926,
  "numCommitsSeen": 64,
  "timeTaken": 5193,
  "changeHistory": [
    "3deb5d345f439cbebcad5296c69689e8334f59ce",
    "e2a3c4494ba27a7b82117dac275b9d115aee7f95",
    "b3fee1d2bfe5d289b8f279071589f21ace99e04c",
    "6425ed27ea638da75f656204d6df4adad1d91fe1",
    "fb5b3dce6192265bce9b9d93ab663bdc5be8048e"
  ],
  "changeHistoryShort": {
    "3deb5d345f439cbebcad5296c69689e8334f59ce": "Ybodychange",
    "e2a3c4494ba27a7b82117dac275b9d115aee7f95": "Ybodychange",
    "b3fee1d2bfe5d289b8f279071589f21ace99e04c": "Ybodychange",
    "6425ed27ea638da75f656204d6df4adad1d91fe1": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yrename,Yparameterchange)",
    "fb5b3dce6192265bce9b9d93ab663bdc5be8048e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3deb5d345f439cbebcad5296c69689e8334f59ce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14490. RBF: Remove unnecessary quota checks. Contributed by Ayush Saxena.\n",
      "commitDate": "24/06/19 9:33 AM",
      "commitName": "3deb5d345f439cbebcad5296c69689e8334f59ce",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "24/06/19 9:33 AM",
      "commitNameOld": "62fa53a01dc7165d7965cdd4fddb444082f0602c",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   public HdfsFileStatus getFileInfo(String src) throws IOException {\n     rpcServer.checkOperation(NameNode.OperationCategory.READ);\n \n     final List\u003cRemoteLocation\u003e locations \u003d\n-        rpcServer.getLocationsForPath(src, false);\n+        rpcServer.getLocationsForPath(src, false, false);\n     RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n         new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n \n     HdfsFileStatus ret \u003d null;\n     // If it\u0027s a directory, we check in all locations\n     if (rpcServer.isPathAll(src)) {\n       ret \u003d getFileInfoAll(locations, method);\n     } else {\n       // Check for file information sequentially\n       ret \u003d rpcClient.invokeSequential(\n           locations, method, HdfsFileStatus.class, null);\n     }\n \n     // If there is no real path, check mount points\n     if (ret \u003d\u003d null) {\n       List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n       if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n         Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n         long date \u003d 0;\n         if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n           date \u003d dates.get(src);\n         }\n         ret \u003d getMountPointStatus(src, children.size(), date);\n       } else if (children !\u003d null) {\n         // The src is a mount point, but there are no files or directories\n         ret \u003d getMountPointStatus(src, 0, 0);\n       }\n     }\n \n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public HdfsFileStatus getFileInfo(String src) throws IOException {\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n\n    final List\u003cRemoteLocation\u003e locations \u003d\n        rpcServer.getLocationsForPath(src, false, false);\n    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n\n    HdfsFileStatus ret \u003d null;\n    // If it\u0027s a directory, we check in all locations\n    if (rpcServer.isPathAll(src)) {\n      ret \u003d getFileInfoAll(locations, method);\n    } else {\n      // Check for file information sequentially\n      ret \u003d rpcClient.invokeSequential(\n          locations, method, HdfsFileStatus.class, null);\n    }\n\n    // If there is no real path, check mount points\n    if (ret \u003d\u003d null) {\n      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n        long date \u003d 0;\n        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n          date \u003d dates.get(src);\n        }\n        ret \u003d getMountPointStatus(src, children.size(), date);\n      } else if (children !\u003d null) {\n        // The src is a mount point, but there are no files or directories\n        ret \u003d getMountPointStatus(src, 0, 0);\n      }\n    }\n\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
      "extendedDetails": {}
    },
    "e2a3c4494ba27a7b82117dac275b9d115aee7f95": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14226. RBF: Setting attributes should set on all subclusters\u0027 directories. Contributed by Ayush Saxena.\n",
      "commitDate": "24/06/19 9:33 AM",
      "commitName": "e2a3c4494ba27a7b82117dac275b9d115aee7f95",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "24/06/19 9:33 AM",
      "commitNameOld": "75f8b6ccfa6160e695ce8f7ad13c6e3624e9e7aa",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   public HdfsFileStatus getFileInfo(String src) throws IOException {\n     rpcServer.checkOperation(NameNode.OperationCategory.READ);\n \n     final List\u003cRemoteLocation\u003e locations \u003d\n         rpcServer.getLocationsForPath(src, false);\n     RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n         new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n \n     HdfsFileStatus ret \u003d null;\n     // If it\u0027s a directory, we check in all locations\n-    if (isPathAll(src)) {\n+    if (rpcServer.isPathAll(src)) {\n       ret \u003d getFileInfoAll(locations, method);\n     } else {\n       // Check for file information sequentially\n       ret \u003d rpcClient.invokeSequential(\n           locations, method, HdfsFileStatus.class, null);\n     }\n \n     // If there is no real path, check mount points\n     if (ret \u003d\u003d null) {\n       List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n       if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n         Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n         long date \u003d 0;\n         if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n           date \u003d dates.get(src);\n         }\n         ret \u003d getMountPointStatus(src, children.size(), date);\n       } else if (children !\u003d null) {\n         // The src is a mount point, but there are no files or directories\n         ret \u003d getMountPointStatus(src, 0, 0);\n       }\n     }\n \n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public HdfsFileStatus getFileInfo(String src) throws IOException {\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n\n    final List\u003cRemoteLocation\u003e locations \u003d\n        rpcServer.getLocationsForPath(src, false);\n    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n\n    HdfsFileStatus ret \u003d null;\n    // If it\u0027s a directory, we check in all locations\n    if (rpcServer.isPathAll(src)) {\n      ret \u003d getFileInfoAll(locations, method);\n    } else {\n      // Check for file information sequentially\n      ret \u003d rpcClient.invokeSequential(\n          locations, method, HdfsFileStatus.class, null);\n    }\n\n    // If there is no real path, check mount points\n    if (ret \u003d\u003d null) {\n      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n        long date \u003d 0;\n        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n          date \u003d dates.get(src);\n        }\n        ret \u003d getMountPointStatus(src, children.size(), date);\n      } else if (children !\u003d null) {\n        // The src is a mount point, but there are no files or directories\n        ret \u003d getMountPointStatus(src, 0, 0);\n      }\n    }\n\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
      "extendedDetails": {}
    },
    "b3fee1d2bfe5d289b8f279071589f21ace99e04c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14011. RBF: Add more information to HdfsFileStatus for a mount point. Contributed by Akira Ajisaka.\n",
      "commitDate": "24/06/19 9:33 AM",
      "commitName": "b3fee1d2bfe5d289b8f279071589f21ace99e04c",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "20/03/19 11:20 AM",
      "commitNameOld": "399563fec607a8c2ddc5d1a46b94a60389bef68c",
      "commitAuthorOld": "Ajay Kumar",
      "daysBetweenCommits": 95.93,
      "commitsBetweenForRepo": 613,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,36 @@\n   public HdfsFileStatus getFileInfo(String src) throws IOException {\n     rpcServer.checkOperation(NameNode.OperationCategory.READ);\n \n     final List\u003cRemoteLocation\u003e locations \u003d\n         rpcServer.getLocationsForPath(src, false);\n     RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n         new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n \n     HdfsFileStatus ret \u003d null;\n     // If it\u0027s a directory, we check in all locations\n     if (isPathAll(src)) {\n       ret \u003d getFileInfoAll(locations, method);\n     } else {\n       // Check for file information sequentially\n       ret \u003d rpcClient.invokeSequential(\n           locations, method, HdfsFileStatus.class, null);\n     }\n \n     // If there is no real path, check mount points\n     if (ret \u003d\u003d null) {\n       List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n       if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n         Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n         long date \u003d 0;\n         if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n           date \u003d dates.get(src);\n         }\n         ret \u003d getMountPointStatus(src, children.size(), date);\n+      } else if (children !\u003d null) {\n+        // The src is a mount point, but there are no files or directories\n+        ret \u003d getMountPointStatus(src, 0, 0);\n       }\n     }\n \n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public HdfsFileStatus getFileInfo(String src) throws IOException {\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n\n    final List\u003cRemoteLocation\u003e locations \u003d\n        rpcServer.getLocationsForPath(src, false);\n    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n\n    HdfsFileStatus ret \u003d null;\n    // If it\u0027s a directory, we check in all locations\n    if (isPathAll(src)) {\n      ret \u003d getFileInfoAll(locations, method);\n    } else {\n      // Check for file information sequentially\n      ret \u003d rpcClient.invokeSequential(\n          locations, method, HdfsFileStatus.class, null);\n    }\n\n    // If there is no real path, check mount points\n    if (ret \u003d\u003d null) {\n      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n        long date \u003d 0;\n        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n          date \u003d dates.get(src);\n        }\n        ret \u003d getMountPointStatus(src, children.size(), date);\n      } else if (children !\u003d null) {\n        // The src is a mount point, but there are no files or directories\n        ret \u003d getMountPointStatus(src, 0, 0);\n      }\n    }\n\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
      "extendedDetails": {}
    },
    "6425ed27ea638da75f656204d6df4adad1d91fe1": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.\n",
      "commitDate": "19/08/18 11:50 PM",
      "commitName": "6425ed27ea638da75f656204d6df4adad1d91fe1",
      "commitAuthor": "Brahma Reddy Battula",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.\n",
          "commitDate": "19/08/18 11:50 PM",
          "commitName": "6425ed27ea638da75f656204d6df4adad1d91fe1",
          "commitAuthor": "Brahma Reddy Battula",
          "commitDateOld": "17/08/18 9:56 PM",
          "commitNameOld": "4aacbfff605262aaf3dbd926258afcadc86c72c0",
          "commitAuthorOld": "Rohith Sharma K S",
          "daysBetweenCommits": 2.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,33 @@\n-  private HdfsFileStatus getFileInfoAll(final List\u003cRemoteLocation\u003e locations,\n-      final RemoteMethod method) throws IOException {\n+  public HdfsFileStatus getFileInfo(String src) throws IOException {\n+    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n \n-    // Get the file info from everybody\n-    Map\u003cRemoteLocation, HdfsFileStatus\u003e results \u003d\n-        rpcClient.invokeConcurrent(locations, method, HdfsFileStatus.class);\n+    final List\u003cRemoteLocation\u003e locations \u003d\n+        rpcServer.getLocationsForPath(src, false);\n+    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n+        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n \n-    // We return the first file\n-    HdfsFileStatus dirStatus \u003d null;\n-    for (RemoteLocation loc : locations) {\n-      HdfsFileStatus fileStatus \u003d results.get(loc);\n-      if (fileStatus !\u003d null) {\n-        if (!fileStatus.isDirectory()) {\n-          return fileStatus;\n-        } else if (dirStatus \u003d\u003d null) {\n-          dirStatus \u003d fileStatus;\n+    HdfsFileStatus ret \u003d null;\n+    // If it\u0027s a directory, we check in all locations\n+    if (isPathAll(src)) {\n+      ret \u003d getFileInfoAll(locations, method);\n+    } else {\n+      // Check for file information sequentially\n+      ret \u003d rpcClient.invokeSequential(\n+          locations, method, HdfsFileStatus.class, null);\n+    }\n+\n+    // If there is no real path, check mount points\n+    if (ret \u003d\u003d null) {\n+      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n+      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n+        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n+        long date \u003d 0;\n+        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n+          date \u003d dates.get(src);\n         }\n+        ret \u003d getMountPointStatus(src, children.size(), date);\n       }\n     }\n-    return dirStatus;\n+\n+    return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public HdfsFileStatus getFileInfo(String src) throws IOException {\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n\n    final List\u003cRemoteLocation\u003e locations \u003d\n        rpcServer.getLocationsForPath(src, false);\n    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n\n    HdfsFileStatus ret \u003d null;\n    // If it\u0027s a directory, we check in all locations\n    if (isPathAll(src)) {\n      ret \u003d getFileInfoAll(locations, method);\n    } else {\n      // Check for file information sequentially\n      ret \u003d rpcClient.invokeSequential(\n          locations, method, HdfsFileStatus.class, null);\n    }\n\n    // If there is no real path, check mount points\n    if (ret \u003d\u003d null) {\n      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n        long date \u003d 0;\n        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n          date \u003d dates.get(src);\n        }\n        ret \u003d getMountPointStatus(src, children.size(), date);\n      }\n    }\n\n    return ret;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
            "oldMethodName": "getFileInfoAll",
            "newMethodName": "getFileInfo"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.\n",
          "commitDate": "19/08/18 11:50 PM",
          "commitName": "6425ed27ea638da75f656204d6df4adad1d91fe1",
          "commitAuthor": "Brahma Reddy Battula",
          "commitDateOld": "17/08/18 9:56 PM",
          "commitNameOld": "4aacbfff605262aaf3dbd926258afcadc86c72c0",
          "commitAuthorOld": "Rohith Sharma K S",
          "daysBetweenCommits": 2.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,33 @@\n-  private HdfsFileStatus getFileInfoAll(final List\u003cRemoteLocation\u003e locations,\n-      final RemoteMethod method) throws IOException {\n+  public HdfsFileStatus getFileInfo(String src) throws IOException {\n+    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n \n-    // Get the file info from everybody\n-    Map\u003cRemoteLocation, HdfsFileStatus\u003e results \u003d\n-        rpcClient.invokeConcurrent(locations, method, HdfsFileStatus.class);\n+    final List\u003cRemoteLocation\u003e locations \u003d\n+        rpcServer.getLocationsForPath(src, false);\n+    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n+        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n \n-    // We return the first file\n-    HdfsFileStatus dirStatus \u003d null;\n-    for (RemoteLocation loc : locations) {\n-      HdfsFileStatus fileStatus \u003d results.get(loc);\n-      if (fileStatus !\u003d null) {\n-        if (!fileStatus.isDirectory()) {\n-          return fileStatus;\n-        } else if (dirStatus \u003d\u003d null) {\n-          dirStatus \u003d fileStatus;\n+    HdfsFileStatus ret \u003d null;\n+    // If it\u0027s a directory, we check in all locations\n+    if (isPathAll(src)) {\n+      ret \u003d getFileInfoAll(locations, method);\n+    } else {\n+      // Check for file information sequentially\n+      ret \u003d rpcClient.invokeSequential(\n+          locations, method, HdfsFileStatus.class, null);\n+    }\n+\n+    // If there is no real path, check mount points\n+    if (ret \u003d\u003d null) {\n+      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n+      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n+        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n+        long date \u003d 0;\n+        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n+          date \u003d dates.get(src);\n         }\n+        ret \u003d getMountPointStatus(src, children.size(), date);\n       }\n     }\n-    return dirStatus;\n+\n+    return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public HdfsFileStatus getFileInfo(String src) throws IOException {\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n\n    final List\u003cRemoteLocation\u003e locations \u003d\n        rpcServer.getLocationsForPath(src, false);\n    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n\n    HdfsFileStatus ret \u003d null;\n    // If it\u0027s a directory, we check in all locations\n    if (isPathAll(src)) {\n      ret \u003d getFileInfoAll(locations, method);\n    } else {\n      // Check for file information sequentially\n      ret \u003d rpcClient.invokeSequential(\n          locations, method, HdfsFileStatus.class, null);\n    }\n\n    // If there is no real path, check mount points\n    if (ret \u003d\u003d null) {\n      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n        long date \u003d 0;\n        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n          date \u003d dates.get(src);\n        }\n        ret \u003d getMountPointStatus(src, children.size(), date);\n      }\n    }\n\n    return ret;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.\n",
          "commitDate": "19/08/18 11:50 PM",
          "commitName": "6425ed27ea638da75f656204d6df4adad1d91fe1",
          "commitAuthor": "Brahma Reddy Battula",
          "commitDateOld": "17/08/18 9:56 PM",
          "commitNameOld": "4aacbfff605262aaf3dbd926258afcadc86c72c0",
          "commitAuthorOld": "Rohith Sharma K S",
          "daysBetweenCommits": 2.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,33 @@\n-  private HdfsFileStatus getFileInfoAll(final List\u003cRemoteLocation\u003e locations,\n-      final RemoteMethod method) throws IOException {\n+  public HdfsFileStatus getFileInfo(String src) throws IOException {\n+    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n \n-    // Get the file info from everybody\n-    Map\u003cRemoteLocation, HdfsFileStatus\u003e results \u003d\n-        rpcClient.invokeConcurrent(locations, method, HdfsFileStatus.class);\n+    final List\u003cRemoteLocation\u003e locations \u003d\n+        rpcServer.getLocationsForPath(src, false);\n+    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n+        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n \n-    // We return the first file\n-    HdfsFileStatus dirStatus \u003d null;\n-    for (RemoteLocation loc : locations) {\n-      HdfsFileStatus fileStatus \u003d results.get(loc);\n-      if (fileStatus !\u003d null) {\n-        if (!fileStatus.isDirectory()) {\n-          return fileStatus;\n-        } else if (dirStatus \u003d\u003d null) {\n-          dirStatus \u003d fileStatus;\n+    HdfsFileStatus ret \u003d null;\n+    // If it\u0027s a directory, we check in all locations\n+    if (isPathAll(src)) {\n+      ret \u003d getFileInfoAll(locations, method);\n+    } else {\n+      // Check for file information sequentially\n+      ret \u003d rpcClient.invokeSequential(\n+          locations, method, HdfsFileStatus.class, null);\n+    }\n+\n+    // If there is no real path, check mount points\n+    if (ret \u003d\u003d null) {\n+      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n+      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n+        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n+        long date \u003d 0;\n+        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n+          date \u003d dates.get(src);\n         }\n+        ret \u003d getMountPointStatus(src, children.size(), date);\n       }\n     }\n-    return dirStatus;\n+\n+    return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public HdfsFileStatus getFileInfo(String src) throws IOException {\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n\n    final List\u003cRemoteLocation\u003e locations \u003d\n        rpcServer.getLocationsForPath(src, false);\n    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n\n    HdfsFileStatus ret \u003d null;\n    // If it\u0027s a directory, we check in all locations\n    if (isPathAll(src)) {\n      ret \u003d getFileInfoAll(locations, method);\n    } else {\n      // Check for file information sequentially\n      ret \u003d rpcClient.invokeSequential(\n          locations, method, HdfsFileStatus.class, null);\n    }\n\n    // If there is no real path, check mount points\n    if (ret \u003d\u003d null) {\n      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n        long date \u003d 0;\n        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n          date \u003d dates.get(src);\n        }\n        ret \u003d getMountPointStatus(src, children.size(), date);\n      }\n    }\n\n    return ret;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.\n",
          "commitDate": "19/08/18 11:50 PM",
          "commitName": "6425ed27ea638da75f656204d6df4adad1d91fe1",
          "commitAuthor": "Brahma Reddy Battula",
          "commitDateOld": "17/08/18 9:56 PM",
          "commitNameOld": "4aacbfff605262aaf3dbd926258afcadc86c72c0",
          "commitAuthorOld": "Rohith Sharma K S",
          "daysBetweenCommits": 2.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,33 @@\n-  private HdfsFileStatus getFileInfoAll(final List\u003cRemoteLocation\u003e locations,\n-      final RemoteMethod method) throws IOException {\n+  public HdfsFileStatus getFileInfo(String src) throws IOException {\n+    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n \n-    // Get the file info from everybody\n-    Map\u003cRemoteLocation, HdfsFileStatus\u003e results \u003d\n-        rpcClient.invokeConcurrent(locations, method, HdfsFileStatus.class);\n+    final List\u003cRemoteLocation\u003e locations \u003d\n+        rpcServer.getLocationsForPath(src, false);\n+    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n+        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n \n-    // We return the first file\n-    HdfsFileStatus dirStatus \u003d null;\n-    for (RemoteLocation loc : locations) {\n-      HdfsFileStatus fileStatus \u003d results.get(loc);\n-      if (fileStatus !\u003d null) {\n-        if (!fileStatus.isDirectory()) {\n-          return fileStatus;\n-        } else if (dirStatus \u003d\u003d null) {\n-          dirStatus \u003d fileStatus;\n+    HdfsFileStatus ret \u003d null;\n+    // If it\u0027s a directory, we check in all locations\n+    if (isPathAll(src)) {\n+      ret \u003d getFileInfoAll(locations, method);\n+    } else {\n+      // Check for file information sequentially\n+      ret \u003d rpcClient.invokeSequential(\n+          locations, method, HdfsFileStatus.class, null);\n+    }\n+\n+    // If there is no real path, check mount points\n+    if (ret \u003d\u003d null) {\n+      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n+      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n+        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n+        long date \u003d 0;\n+        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n+          date \u003d dates.get(src);\n         }\n+        ret \u003d getMountPointStatus(src, children.size(), date);\n       }\n     }\n-    return dirStatus;\n+\n+    return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public HdfsFileStatus getFileInfo(String src) throws IOException {\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n\n    final List\u003cRemoteLocation\u003e locations \u003d\n        rpcServer.getLocationsForPath(src, false);\n    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n\n    HdfsFileStatus ret \u003d null;\n    // If it\u0027s a directory, we check in all locations\n    if (isPathAll(src)) {\n      ret \u003d getFileInfoAll(locations, method);\n    } else {\n      // Check for file information sequentially\n      ret \u003d rpcClient.invokeSequential(\n          locations, method, HdfsFileStatus.class, null);\n    }\n\n    // If there is no real path, check mount points\n    if (ret \u003d\u003d null) {\n      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n        long date \u003d 0;\n        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n          date \u003d dates.get(src);\n        }\n        ret \u003d getMountPointStatus(src, children.size(), date);\n      }\n    }\n\n    return ret;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
          "extendedDetails": {
            "oldValue": "getFileInfoAll",
            "newValue": "getFileInfo"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.\n",
          "commitDate": "19/08/18 11:50 PM",
          "commitName": "6425ed27ea638da75f656204d6df4adad1d91fe1",
          "commitAuthor": "Brahma Reddy Battula",
          "commitDateOld": "17/08/18 9:56 PM",
          "commitNameOld": "4aacbfff605262aaf3dbd926258afcadc86c72c0",
          "commitAuthorOld": "Rohith Sharma K S",
          "daysBetweenCommits": 2.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,33 @@\n-  private HdfsFileStatus getFileInfoAll(final List\u003cRemoteLocation\u003e locations,\n-      final RemoteMethod method) throws IOException {\n+  public HdfsFileStatus getFileInfo(String src) throws IOException {\n+    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n \n-    // Get the file info from everybody\n-    Map\u003cRemoteLocation, HdfsFileStatus\u003e results \u003d\n-        rpcClient.invokeConcurrent(locations, method, HdfsFileStatus.class);\n+    final List\u003cRemoteLocation\u003e locations \u003d\n+        rpcServer.getLocationsForPath(src, false);\n+    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n+        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n \n-    // We return the first file\n-    HdfsFileStatus dirStatus \u003d null;\n-    for (RemoteLocation loc : locations) {\n-      HdfsFileStatus fileStatus \u003d results.get(loc);\n-      if (fileStatus !\u003d null) {\n-        if (!fileStatus.isDirectory()) {\n-          return fileStatus;\n-        } else if (dirStatus \u003d\u003d null) {\n-          dirStatus \u003d fileStatus;\n+    HdfsFileStatus ret \u003d null;\n+    // If it\u0027s a directory, we check in all locations\n+    if (isPathAll(src)) {\n+      ret \u003d getFileInfoAll(locations, method);\n+    } else {\n+      // Check for file information sequentially\n+      ret \u003d rpcClient.invokeSequential(\n+          locations, method, HdfsFileStatus.class, null);\n+    }\n+\n+    // If there is no real path, check mount points\n+    if (ret \u003d\u003d null) {\n+      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n+      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n+        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n+        long date \u003d 0;\n+        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n+          date \u003d dates.get(src);\n         }\n+        ret \u003d getMountPointStatus(src, children.size(), date);\n       }\n     }\n-    return dirStatus;\n+\n+    return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public HdfsFileStatus getFileInfo(String src) throws IOException {\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n\n    final List\u003cRemoteLocation\u003e locations \u003d\n        rpcServer.getLocationsForPath(src, false);\n    RemoteMethod method \u003d new RemoteMethod(\"getFileInfo\",\n        new Class\u003c?\u003e[] {String.class}, new RemoteParam());\n\n    HdfsFileStatus ret \u003d null;\n    // If it\u0027s a directory, we check in all locations\n    if (isPathAll(src)) {\n      ret \u003d getFileInfoAll(locations, method);\n    } else {\n      // Check for file information sequentially\n      ret \u003d rpcClient.invokeSequential(\n          locations, method, HdfsFileStatus.class, null);\n    }\n\n    // If there is no real path, check mount points\n    if (ret \u003d\u003d null) {\n      List\u003cString\u003e children \u003d subclusterResolver.getMountPoints(src);\n      if (children !\u003d null \u0026\u0026 !children.isEmpty()) {\n        Map\u003cString, Long\u003e dates \u003d getMountPointDates(src);\n        long date \u003d 0;\n        if (dates !\u003d null \u0026\u0026 dates.containsKey(src)) {\n          date \u003d dates.get(src);\n        }\n        ret \u003d getMountPointStatus(src, children.size(), date);\n      }\n    }\n\n    return ret;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
          "extendedDetails": {
            "oldValue": "[locations-List\u003cRemoteLocation\u003e(modifiers-final), method-RemoteMethod(modifiers-final)]",
            "newValue": "[src-String]"
          }
        }
      ]
    },
    "fb5b3dce6192265bce9b9d93ab663bdc5be8048e": {
      "type": "Yintroduced",
      "commitMessage": "Revert \"HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.\"\n\nThis reverts commit fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a.\n",
      "commitDate": "17/08/18 8:01 AM",
      "commitName": "fb5b3dce6192265bce9b9d93ab663bdc5be8048e",
      "commitAuthor": "Xiaoyu Yao",
      "diff": "@@ -0,0 +1,21 @@\n+  private HdfsFileStatus getFileInfoAll(final List\u003cRemoteLocation\u003e locations,\n+      final RemoteMethod method) throws IOException {\n+\n+    // Get the file info from everybody\n+    Map\u003cRemoteLocation, HdfsFileStatus\u003e results \u003d\n+        rpcClient.invokeConcurrent(locations, method, HdfsFileStatus.class);\n+\n+    // We return the first file\n+    HdfsFileStatus dirStatus \u003d null;\n+    for (RemoteLocation loc : locations) {\n+      HdfsFileStatus fileStatus \u003d results.get(loc);\n+      if (fileStatus !\u003d null) {\n+        if (!fileStatus.isDirectory()) {\n+          return fileStatus;\n+        } else if (dirStatus \u003d\u003d null) {\n+          dirStatus \u003d fileStatus;\n+        }\n+      }\n+    }\n+    return dirStatus;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus getFileInfoAll(final List\u003cRemoteLocation\u003e locations,\n      final RemoteMethod method) throws IOException {\n\n    // Get the file info from everybody\n    Map\u003cRemoteLocation, HdfsFileStatus\u003e results \u003d\n        rpcClient.invokeConcurrent(locations, method, HdfsFileStatus.class);\n\n    // We return the first file\n    HdfsFileStatus dirStatus \u003d null;\n    for (RemoteLocation loc : locations) {\n      HdfsFileStatus fileStatus \u003d results.get(loc);\n      if (fileStatus !\u003d null) {\n        if (!fileStatus.isDirectory()) {\n          return fileStatus;\n        } else if (dirStatus \u003d\u003d null) {\n          dirStatus \u003d fileStatus;\n        }\n      }\n    }\n    return dirStatus;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java"
    }
  }
}