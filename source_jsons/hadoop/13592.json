{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LowRedundancyBlocks.java",
  "functionName": "getPriorityStriped",
  "functionId": "getPriorityStriped___curReplicas-int__outOfServiceReplicas-int__dataBlkNum-short__parityBlkNum-short",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java",
  "functionStartLine": 257,
  "functionEndLine": 276,
  "numCommitsSeen": 28,
  "timeTaken": 3027,
  "changeHistory": [
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5"
  ],
  "changeHistoryShort": {
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ymultichange(Yparameterchange,Ybodychange)",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ymultichange(Yfilerename,Ybodychange)"
  },
  "changeHistoryDetails": {
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
          "commitDate": "17/10/16 5:45 PM",
          "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
          "commitAuthor": "Ming Ma",
          "commitDateOld": "16/03/16 4:53 PM",
          "commitNameOld": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 215.04,
          "commitsBetweenForRepo": 1525,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  private int getPriorityStriped(int curReplicas, int decommissionedReplicas,\n+  private int getPriorityStriped(int curReplicas, int outOfServiceReplicas,\n       short dataBlkNum, short parityBlkNum) {\n     if (curReplicas \u003c dataBlkNum) {\n       // There are some replicas on decommissioned nodes so it\u0027s not corrupted\n-      if (curReplicas + decommissionedReplicas \u003e\u003d dataBlkNum) {\n+      if (curReplicas + outOfServiceReplicas \u003e\u003d dataBlkNum) {\n         return QUEUE_HIGHEST_PRIORITY;\n       }\n       return QUEUE_WITH_CORRUPT_BLOCKS;\n     } else if (curReplicas \u003d\u003d dataBlkNum) {\n       // highest risk of loss, highest priority\n       return QUEUE_HIGHEST_PRIORITY;\n     } else if ((curReplicas - dataBlkNum) * 3 \u003c parityBlkNum + 1) {\n       // can only afford one replica loss\n       // this is considered very insufficiently redundant blocks.\n       return QUEUE_VERY_LOW_REDUNDANCY;\n     } else {\n       // add to the normal queue for insufficiently redundant blocks.\n       return QUEUE_LOW_REDUNDANCY;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int getPriorityStriped(int curReplicas, int outOfServiceReplicas,\n      short dataBlkNum, short parityBlkNum) {\n    if (curReplicas \u003c dataBlkNum) {\n      // There are some replicas on decommissioned nodes so it\u0027s not corrupted\n      if (curReplicas + outOfServiceReplicas \u003e\u003d dataBlkNum) {\n        return QUEUE_HIGHEST_PRIORITY;\n      }\n      return QUEUE_WITH_CORRUPT_BLOCKS;\n    } else if (curReplicas \u003d\u003d dataBlkNum) {\n      // highest risk of loss, highest priority\n      return QUEUE_HIGHEST_PRIORITY;\n    } else if ((curReplicas - dataBlkNum) * 3 \u003c parityBlkNum + 1) {\n      // can only afford one replica loss\n      // this is considered very insufficiently redundant blocks.\n      return QUEUE_VERY_LOW_REDUNDANCY;\n    } else {\n      // add to the normal queue for insufficiently redundant blocks.\n      return QUEUE_LOW_REDUNDANCY;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java",
          "extendedDetails": {
            "oldValue": "[curReplicas-int, decommissionedReplicas-int, dataBlkNum-short, parityBlkNum-short]",
            "newValue": "[curReplicas-int, outOfServiceReplicas-int, dataBlkNum-short, parityBlkNum-short]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
          "commitDate": "17/10/16 5:45 PM",
          "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
          "commitAuthor": "Ming Ma",
          "commitDateOld": "16/03/16 4:53 PM",
          "commitNameOld": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 215.04,
          "commitsBetweenForRepo": 1525,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  private int getPriorityStriped(int curReplicas, int decommissionedReplicas,\n+  private int getPriorityStriped(int curReplicas, int outOfServiceReplicas,\n       short dataBlkNum, short parityBlkNum) {\n     if (curReplicas \u003c dataBlkNum) {\n       // There are some replicas on decommissioned nodes so it\u0027s not corrupted\n-      if (curReplicas + decommissionedReplicas \u003e\u003d dataBlkNum) {\n+      if (curReplicas + outOfServiceReplicas \u003e\u003d dataBlkNum) {\n         return QUEUE_HIGHEST_PRIORITY;\n       }\n       return QUEUE_WITH_CORRUPT_BLOCKS;\n     } else if (curReplicas \u003d\u003d dataBlkNum) {\n       // highest risk of loss, highest priority\n       return QUEUE_HIGHEST_PRIORITY;\n     } else if ((curReplicas - dataBlkNum) * 3 \u003c parityBlkNum + 1) {\n       // can only afford one replica loss\n       // this is considered very insufficiently redundant blocks.\n       return QUEUE_VERY_LOW_REDUNDANCY;\n     } else {\n       // add to the normal queue for insufficiently redundant blocks.\n       return QUEUE_LOW_REDUNDANCY;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int getPriorityStriped(int curReplicas, int outOfServiceReplicas,\n      short dataBlkNum, short parityBlkNum) {\n    if (curReplicas \u003c dataBlkNum) {\n      // There are some replicas on decommissioned nodes so it\u0027s not corrupted\n      if (curReplicas + outOfServiceReplicas \u003e\u003d dataBlkNum) {\n        return QUEUE_HIGHEST_PRIORITY;\n      }\n      return QUEUE_WITH_CORRUPT_BLOCKS;\n    } else if (curReplicas \u003d\u003d dataBlkNum) {\n      // highest risk of loss, highest priority\n      return QUEUE_HIGHEST_PRIORITY;\n    } else if ((curReplicas - dataBlkNum) * 3 \u003c parityBlkNum + 1) {\n      // can only afford one replica loss\n      // this is considered very insufficiently redundant blocks.\n      return QUEUE_VERY_LOW_REDUNDANCY;\n    } else {\n      // add to the normal queue for insufficiently redundant blocks.\n      return QUEUE_LOW_REDUNDANCY;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java",
          "extendedDetails": {}
        }
      ]
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
          "commitDate": "16/03/16 4:53 PM",
          "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "16/03/16 7:35 AM",
          "commitNameOld": "605fdcbb81687c73ba91a3bd0d607cabd3dc5a67",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.39,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n   private int getPriorityStriped(int curReplicas, int decommissionedReplicas,\n       short dataBlkNum, short parityBlkNum) {\n     if (curReplicas \u003c dataBlkNum) {\n       // There are some replicas on decommissioned nodes so it\u0027s not corrupted\n       if (curReplicas + decommissionedReplicas \u003e\u003d dataBlkNum) {\n         return QUEUE_HIGHEST_PRIORITY;\n       }\n       return QUEUE_WITH_CORRUPT_BLOCKS;\n     } else if (curReplicas \u003d\u003d dataBlkNum) {\n       // highest risk of loss, highest priority\n       return QUEUE_HIGHEST_PRIORITY;\n     } else if ((curReplicas - dataBlkNum) * 3 \u003c parityBlkNum + 1) {\n-      // there is less than a third as many blocks as requested;\n-      // this is considered very under-replicated\n-      return QUEUE_VERY_UNDER_REPLICATED;\n+      // can only afford one replica loss\n+      // this is considered very insufficiently redundant blocks.\n+      return QUEUE_VERY_LOW_REDUNDANCY;\n     } else {\n-      // add to the normal queue for under replicated blocks\n-      return QUEUE_UNDER_REPLICATED;\n+      // add to the normal queue for insufficiently redundant blocks.\n+      return QUEUE_LOW_REDUNDANCY;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int getPriorityStriped(int curReplicas, int decommissionedReplicas,\n      short dataBlkNum, short parityBlkNum) {\n    if (curReplicas \u003c dataBlkNum) {\n      // There are some replicas on decommissioned nodes so it\u0027s not corrupted\n      if (curReplicas + decommissionedReplicas \u003e\u003d dataBlkNum) {\n        return QUEUE_HIGHEST_PRIORITY;\n      }\n      return QUEUE_WITH_CORRUPT_BLOCKS;\n    } else if (curReplicas \u003d\u003d dataBlkNum) {\n      // highest risk of loss, highest priority\n      return QUEUE_HIGHEST_PRIORITY;\n    } else if ((curReplicas - dataBlkNum) * 3 \u003c parityBlkNum + 1) {\n      // can only afford one replica loss\n      // this is considered very insufficiently redundant blocks.\n      return QUEUE_VERY_LOW_REDUNDANCY;\n    } else {\n      // add to the normal queue for insufficiently redundant blocks.\n      return QUEUE_LOW_REDUNDANCY;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/UnderReplicatedBlocks.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
          "commitDate": "16/03/16 4:53 PM",
          "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "16/03/16 7:35 AM",
          "commitNameOld": "605fdcbb81687c73ba91a3bd0d607cabd3dc5a67",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.39,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n   private int getPriorityStriped(int curReplicas, int decommissionedReplicas,\n       short dataBlkNum, short parityBlkNum) {\n     if (curReplicas \u003c dataBlkNum) {\n       // There are some replicas on decommissioned nodes so it\u0027s not corrupted\n       if (curReplicas + decommissionedReplicas \u003e\u003d dataBlkNum) {\n         return QUEUE_HIGHEST_PRIORITY;\n       }\n       return QUEUE_WITH_CORRUPT_BLOCKS;\n     } else if (curReplicas \u003d\u003d dataBlkNum) {\n       // highest risk of loss, highest priority\n       return QUEUE_HIGHEST_PRIORITY;\n     } else if ((curReplicas - dataBlkNum) * 3 \u003c parityBlkNum + 1) {\n-      // there is less than a third as many blocks as requested;\n-      // this is considered very under-replicated\n-      return QUEUE_VERY_UNDER_REPLICATED;\n+      // can only afford one replica loss\n+      // this is considered very insufficiently redundant blocks.\n+      return QUEUE_VERY_LOW_REDUNDANCY;\n     } else {\n-      // add to the normal queue for under replicated blocks\n-      return QUEUE_UNDER_REPLICATED;\n+      // add to the normal queue for insufficiently redundant blocks.\n+      return QUEUE_LOW_REDUNDANCY;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int getPriorityStriped(int curReplicas, int decommissionedReplicas,\n      short dataBlkNum, short parityBlkNum) {\n    if (curReplicas \u003c dataBlkNum) {\n      // There are some replicas on decommissioned nodes so it\u0027s not corrupted\n      if (curReplicas + decommissionedReplicas \u003e\u003d dataBlkNum) {\n        return QUEUE_HIGHEST_PRIORITY;\n      }\n      return QUEUE_WITH_CORRUPT_BLOCKS;\n    } else if (curReplicas \u003d\u003d dataBlkNum) {\n      // highest risk of loss, highest priority\n      return QUEUE_HIGHEST_PRIORITY;\n    } else if ((curReplicas - dataBlkNum) * 3 \u003c parityBlkNum + 1) {\n      // can only afford one replica loss\n      // this is considered very insufficiently redundant blocks.\n      return QUEUE_VERY_LOW_REDUNDANCY;\n    } else {\n      // add to the normal queue for insufficiently redundant blocks.\n      return QUEUE_LOW_REDUNDANCY;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}