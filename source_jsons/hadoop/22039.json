{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Fetcher.java",
  "functionName": "copyFromHost",
  "functionId": "copyFromHost___host-MapHost",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
  "functionStartLine": 305,
  "functionEndLine": 381,
  "numCommitsSeen": 37,
  "timeTaken": 8023,
  "changeHistory": [
    "178751ed8c9d47038acf8616c226f1f52e884feb",
    "4d98936eec1b5d196053426c70d455cf8f83f84f",
    "72d08a0e41efda635baa985d55d67cb059a7c07c",
    "eccb7d46efbf07abcc6a01bd5e7d682f6815b824",
    "2c3da25fd718b3a9c1ed67f05b577975ae613f4e",
    "f8e871d01b851cd5d8c57dd7e364b3e787521765",
    "11bcd2ed12f7f0e02fdaefaefea56929b32d5ee6",
    "16616f9e5b1e58bde880ca72e5290e826b5f8bf7",
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865",
    "57803245ecc806249fcd0cd5fb3ca593098ac877",
    "a7d444d002c664208669ec6ddf3bcb1db71e3741",
    "6ee6eb843013324788f30384d9d967ff8743a970",
    "d87b545165f9442f614665521ce04424af1490e8",
    "9d16c9354b0c05edb30d23003dcdec4cc44ed925",
    "d45922de2c5645e11339b94e4c31935ead66fefc",
    "8e69f883a0ac407781fa09328d9fb87faf5a8d0a",
    "04a47dea7489e3f860b0ab7b8f3ab1fc5a395426",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "178751ed8c9d47038acf8616c226f1f52e884feb": "Ybodychange",
    "4d98936eec1b5d196053426c70d455cf8f83f84f": "Ybodychange",
    "72d08a0e41efda635baa985d55d67cb059a7c07c": "Ybodychange",
    "eccb7d46efbf07abcc6a01bd5e7d682f6815b824": "Ybodychange",
    "2c3da25fd718b3a9c1ed67f05b577975ae613f4e": "Ybodychange",
    "f8e871d01b851cd5d8c57dd7e364b3e787521765": "Ybodychange",
    "11bcd2ed12f7f0e02fdaefaefea56929b32d5ee6": "Ybodychange",
    "16616f9e5b1e58bde880ca72e5290e826b5f8bf7": "Ybodychange",
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865": "Ybodychange",
    "57803245ecc806249fcd0cd5fb3ca593098ac877": "Ybodychange",
    "a7d444d002c664208669ec6ddf3bcb1db71e3741": "Ybodychange",
    "6ee6eb843013324788f30384d9d967ff8743a970": "Ybodychange",
    "d87b545165f9442f614665521ce04424af1490e8": "Ymultichange(Ymodifierchange,Ybodychange)",
    "9d16c9354b0c05edb30d23003dcdec4cc44ed925": "Ybodychange",
    "d45922de2c5645e11339b94e4c31935ead66fefc": "Ymultichange(Ymodifierchange,Ybodychange)",
    "8e69f883a0ac407781fa09328d9fb87faf5a8d0a": "Ymultichange(Ymodifierchange,Ybodychange)",
    "04a47dea7489e3f860b0ab7b8f3ab1fc5a395426": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "178751ed8c9d47038acf8616c226f1f52e884feb": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6983. Moving logging APIs over to slf4j in hadoop-mapreduce-client-core. Contributed by Jinjiang Ling.\n",
      "commitDate": "02/11/17 1:43 AM",
      "commitName": "178751ed8c9d47038acf8616c226f1f52e884feb",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "13/09/17 3:21 PM",
      "commitNameOld": "4d98936eec1b5d196053426c70d455cf8f83f84f",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 49.43,
      "commitsBetweenForRepo": 372,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,77 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n     // reset retryStartTime for a new host\n     retryStartTime \u003d 0;\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     URL url \u003d getMapOutputURL(host, maps);\n     DataInputStream input \u003d null;\n     \n     try {\n       input \u003d openShuffleUrl(host, remaining, url);\n       if (input \u003d\u003d null) {\n         return;\n       }\n \n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n         try {\n           failedTasks \u003d copyMapOutput(host, input, remaining, fetchRetryEnabled);\n         } catch (IOException e) {\n-          IOUtils.cleanup(LOG, input);\n+          IOUtils.cleanupWithLogger(LOG, input);\n           //\n           // Setup connection again if disconnected by NM\n           connection.disconnect();\n           // Get map output from remaining tasks only.\n           url \u003d getMapOutputURL(host, remaining);\n           input \u003d openShuffleUrl(host, remaining, url);\n           if (input \u003d\u003d null) {\n             return;\n           }\n         }\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n         scheduler.hostFailed(host.getHostName());\n         for(TaskAttemptID left: failedTasks) {\n           scheduler.copyFailed(left, host, true, false);\n         }\n       }\n \n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n       input.close();\n       input \u003d null;\n     } finally {\n       if (input !\u003d null) {\n-        IOUtils.cleanup(LOG, input);\n+        IOUtils.cleanupWithLogger(LOG, input);\n         input \u003d null;\n       }\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // reset retryStartTime for a new host\n    retryStartTime \u003d 0;\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    URL url \u003d getMapOutputURL(host, maps);\n    DataInputStream input \u003d null;\n    \n    try {\n      input \u003d openShuffleUrl(host, remaining, url);\n      if (input \u003d\u003d null) {\n        return;\n      }\n\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        try {\n          failedTasks \u003d copyMapOutput(host, input, remaining, fetchRetryEnabled);\n        } catch (IOException e) {\n          IOUtils.cleanupWithLogger(LOG, input);\n          //\n          // Setup connection again if disconnected by NM\n          connection.disconnect();\n          // Get map output from remaining tasks only.\n          url \u003d getMapOutputURL(host, remaining);\n          input \u003d openShuffleUrl(host, remaining, url);\n          if (input \u003d\u003d null) {\n            return;\n          }\n        }\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        scheduler.hostFailed(host.getHostName());\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n\n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n      input.close();\n      input \u003d null;\n    } finally {\n      if (input !\u003d null) {\n        IOUtils.cleanupWithLogger(LOG, input);\n        input \u003d null;\n      }\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "4d98936eec1b5d196053426c70d455cf8f83f84f": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6957. shuffle hangs after a node manager connection timeout. Contributed by Jooseong Kim\n",
      "commitDate": "13/09/17 3:21 PM",
      "commitName": "4d98936eec1b5d196053426c70d455cf8f83f84f",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "29/05/17 10:48 PM",
      "commitNameOld": "d4015f8628dd973c7433639451a9acc3e741d2a2",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 106.69,
      "commitsBetweenForRepo": 747,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,77 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n     // reset retryStartTime for a new host\n     retryStartTime \u003d 0;\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     URL url \u003d getMapOutputURL(host, maps);\n-    DataInputStream input \u003d openShuffleUrl(host, remaining, url);\n-    if (input \u003d\u003d null) {\n-      return;\n-    }\n+    DataInputStream input \u003d null;\n     \n     try {\n+      input \u003d openShuffleUrl(host, remaining, url);\n+      if (input \u003d\u003d null) {\n+        return;\n+      }\n+\n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n         try {\n           failedTasks \u003d copyMapOutput(host, input, remaining, fetchRetryEnabled);\n         } catch (IOException e) {\n           IOUtils.cleanup(LOG, input);\n           //\n           // Setup connection again if disconnected by NM\n           connection.disconnect();\n           // Get map output from remaining tasks only.\n           url \u003d getMapOutputURL(host, remaining);\n           input \u003d openShuffleUrl(host, remaining, url);\n           if (input \u003d\u003d null) {\n             return;\n           }\n         }\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n         scheduler.hostFailed(host.getHostName());\n         for(TaskAttemptID left: failedTasks) {\n           scheduler.copyFailed(left, host, true, false);\n         }\n       }\n \n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n       input.close();\n       input \u003d null;\n     } finally {\n       if (input !\u003d null) {\n         IOUtils.cleanup(LOG, input);\n         input \u003d null;\n       }\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // reset retryStartTime for a new host\n    retryStartTime \u003d 0;\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    URL url \u003d getMapOutputURL(host, maps);\n    DataInputStream input \u003d null;\n    \n    try {\n      input \u003d openShuffleUrl(host, remaining, url);\n      if (input \u003d\u003d null) {\n        return;\n      }\n\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        try {\n          failedTasks \u003d copyMapOutput(host, input, remaining, fetchRetryEnabled);\n        } catch (IOException e) {\n          IOUtils.cleanup(LOG, input);\n          //\n          // Setup connection again if disconnected by NM\n          connection.disconnect();\n          // Get map output from remaining tasks only.\n          url \u003d getMapOutputURL(host, remaining);\n          input \u003d openShuffleUrl(host, remaining, url);\n          if (input \u003d\u003d null) {\n            return;\n          }\n        }\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        scheduler.hostFailed(host.getHostName());\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n\n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n      input.close();\n      input \u003d null;\n    } finally {\n      if (input !\u003d null) {\n        IOUtils.cleanup(LOG, input);\n        input \u003d null;\n      }\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "72d08a0e41efda635baa985d55d67cb059a7c07c": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6400. Multiple shuffle transfer fails because input is closed too early. Contributed by Brahma Reddy Battula, Akira AJISAKA, and Gera Shegalov.\n",
      "commitDate": "24/06/15 8:29 AM",
      "commitName": "72d08a0e41efda635baa985d55d67cb059a7c07c",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "08/05/15 8:57 AM",
      "commitNameOld": "dc2b2ae31f2eb6dae324c2e14ed7660ce605a89b",
      "commitAuthorOld": "Devaraj K",
      "daysBetweenCommits": 46.98,
      "commitsBetweenForRepo": 367,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,75 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n     // reset retryStartTime for a new host\n     retryStartTime \u003d 0;\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     URL url \u003d getMapOutputURL(host, maps);\n     DataInputStream input \u003d openShuffleUrl(host, remaining, url);\n     if (input \u003d\u003d null) {\n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n         try {\n           failedTasks \u003d copyMapOutput(host, input, remaining, fetchRetryEnabled);\n         } catch (IOException e) {\n+          IOUtils.cleanup(LOG, input);\n           //\n           // Setup connection again if disconnected by NM\n           connection.disconnect();\n           // Get map output from remaining tasks only.\n           url \u003d getMapOutputURL(host, remaining);\n           input \u003d openShuffleUrl(host, remaining, url);\n           if (input \u003d\u003d null) {\n             return;\n           }\n         }\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n         scheduler.hostFailed(host.getHostName());\n         for(TaskAttemptID left: failedTasks) {\n           scheduler.copyFailed(left, host, true, false);\n         }\n       }\n \n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n       input.close();\n       input \u003d null;\n     } finally {\n       if (input !\u003d null) {\n         IOUtils.cleanup(LOG, input);\n         input \u003d null;\n       }\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // reset retryStartTime for a new host\n    retryStartTime \u003d 0;\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    URL url \u003d getMapOutputURL(host, maps);\n    DataInputStream input \u003d openShuffleUrl(host, remaining, url);\n    if (input \u003d\u003d null) {\n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        try {\n          failedTasks \u003d copyMapOutput(host, input, remaining, fetchRetryEnabled);\n        } catch (IOException e) {\n          IOUtils.cleanup(LOG, input);\n          //\n          // Setup connection again if disconnected by NM\n          connection.disconnect();\n          // Get map output from remaining tasks only.\n          url \u003d getMapOutputURL(host, remaining);\n          input \u003d openShuffleUrl(host, remaining, url);\n          if (input \u003d\u003d null) {\n            return;\n          }\n        }\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        scheduler.hostFailed(host.getHostName());\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n\n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n      input.close();\n      input \u003d null;\n    } finally {\n      if (input !\u003d null) {\n        IOUtils.cleanup(LOG, input);\n        input \u003d null;\n      }\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "eccb7d46efbf07abcc6a01bd5e7d682f6815b824": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6303. Read timeout when retrying a fetch error can be fatal to a reducer. Contributed by Jason Lowe.\n",
      "commitDate": "02/04/15 12:13 PM",
      "commitName": "eccb7d46efbf07abcc6a01bd5e7d682f6815b824",
      "commitAuthor": "Junping Du",
      "commitDateOld": "13/11/14 7:42 AM",
      "commitNameOld": "177e8090f5809beb3ebcb656cd0affbb3f487de8",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 140.15,
      "commitsBetweenForRepo": 1085,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,74 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n     // reset retryStartTime for a new host\n     retryStartTime \u003d 0;\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n-    DataInputStream input \u003d null;\n     URL url \u003d getMapOutputURL(host, maps);\n-    try {\n-      setupConnectionsWithRetry(host, remaining, url);\n-      \n-      if (stopped) {\n-        abortConnect(host, remaining);\n-        return;\n-      }\n-    } catch (IOException ie) {\n-      boolean connectExcpt \u003d ie instanceof ConnectException;\n-      ioErrs.increment(1);\n-      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n-               \" map outputs\", ie);\n-\n-      // If connect did not succeed, just mark all the maps as failed,\n-      // indirectly penalizing the host\n-      scheduler.hostFailed(host.getHostName());\n-      for(TaskAttemptID left: remaining) {\n-        scheduler.copyFailed(left, host, false, connectExcpt);\n-      }\n-     \n-      // Add back all the remaining maps, WITHOUT marking them as failed\n-      for(TaskAttemptID left: remaining) {\n-        scheduler.putBackKnownMapOutput(host, left);\n-      }\n-      \n+    DataInputStream input \u003d openShuffleUrl(host, remaining, url);\n+    if (input \u003d\u003d null) {\n       return;\n     }\n     \n-    input \u003d new DataInputStream(connection.getInputStream());\n-    \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n         try {\n           failedTasks \u003d copyMapOutput(host, input, remaining, fetchRetryEnabled);\n         } catch (IOException e) {\n           //\n           // Setup connection again if disconnected by NM\n           connection.disconnect();\n           // Get map output from remaining tasks only.\n           url \u003d getMapOutputURL(host, remaining);\n-          \n-          // Connect with retry as expecting host\u0027s recovery take sometime.\n-          setupConnectionsWithRetry(host, remaining, url);\n-          if (stopped) {\n-            abortConnect(host, remaining);\n+          input \u003d openShuffleUrl(host, remaining, url);\n+          if (input \u003d\u003d null) {\n             return;\n           }\n-          input \u003d new DataInputStream(connection.getInputStream());\n         }\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n         scheduler.hostFailed(host.getHostName());\n         for(TaskAttemptID left: failedTasks) {\n           scheduler.copyFailed(left, host, true, false);\n         }\n       }\n \n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n       input.close();\n       input \u003d null;\n     } finally {\n       if (input !\u003d null) {\n         IOUtils.cleanup(LOG, input);\n         input \u003d null;\n       }\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // reset retryStartTime for a new host\n    retryStartTime \u003d 0;\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    URL url \u003d getMapOutputURL(host, maps);\n    DataInputStream input \u003d openShuffleUrl(host, remaining, url);\n    if (input \u003d\u003d null) {\n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        try {\n          failedTasks \u003d copyMapOutput(host, input, remaining, fetchRetryEnabled);\n        } catch (IOException e) {\n          //\n          // Setup connection again if disconnected by NM\n          connection.disconnect();\n          // Get map output from remaining tasks only.\n          url \u003d getMapOutputURL(host, remaining);\n          input \u003d openShuffleUrl(host, remaining, url);\n          if (input \u003d\u003d null) {\n            return;\n          }\n        }\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        scheduler.hostFailed(host.getHostName());\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n\n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n      input.close();\n      input \u003d null;\n    } finally {\n      if (input !\u003d null) {\n        IOUtils.cleanup(LOG, input);\n        input \u003d null;\n      }\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "2c3da25fd718b3a9c1ed67f05b577975ae613f4e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5891. Improved shuffle error handling across NM restarts. Contributed by Junping Du\n",
      "commitDate": "18/09/14 3:00 PM",
      "commitName": "2c3da25fd718b3a9c1ed67f05b577975ae613f4e",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "18/08/14 11:41 AM",
      "commitNameOld": "0cc08f6da4d299141b19b07147e39caef050faf6",
      "commitAuthorOld": "",
      "daysBetweenCommits": 31.14,
      "commitsBetweenForRepo": 258,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,130 +1,104 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n+    // reset retryStartTime for a new host\n+    retryStartTime \u003d 0;\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input \u003d null;\n+    URL url \u003d getMapOutputURL(host, maps);\n     try {\n-      URL url \u003d getMapOutputURL(host, maps);\n-      openConnection(url);\n+      setupConnectionsWithRetry(host, remaining, url);\n+      \n       if (stopped) {\n         abortConnect(host, remaining);\n         return;\n       }\n-      \n-      // generate hash of the url\n-      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n-      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode,\n-          shuffleSecretKey);\n-      \n-      // put url hash into http header\n-      connection.addRequestProperty(\n-          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n-      // set the read timeout\n-      connection.setReadTimeout(readTimeout);\n-      // put shuffle version into http header\n-      connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_NAME,\n-          ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\n-      connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_VERSION,\n-          ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\n-      connect(connection, connectionTimeout);\n-      // verify that the thread wasn\u0027t stopped during calls to connect\n-      if (stopped) {\n-        abortConnect(host, remaining);\n-        return;\n-      }\n-      input \u003d new DataInputStream(connection.getInputStream());\n-\n-      // Validate response code\n-      int rc \u003d connection.getResponseCode();\n-      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n-        throw new IOException(\n-            \"Got invalid response code \" + rc + \" from \" + url +\n-            \": \" + connection.getResponseMessage());\n-      }\n-      // get the shuffle version\n-      if (!ShuffleHeader.DEFAULT_HTTP_HEADER_NAME.equals(\n-          connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME))\n-          || !ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION.equals(\n-              connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION))) {\n-        throw new IOException(\"Incompatible shuffle response version\");\n-      }\n-      // get the replyHash which is HMac of the encHash we sent to the server\n-      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n-      if(replyHash\u003d\u003dnull) {\n-        throw new IOException(\"security validation of TT Map output failed\");\n-      }\n-      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n-      // verify that replyHash is HMac of encHash\n-      SecureShuffleUtils.verifyReply(replyHash, encHash, shuffleSecretKey);\n-      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n     } catch (IOException ie) {\n       boolean connectExcpt \u003d ie instanceof ConnectException;\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       scheduler.hostFailed(host.getHostName());\n       for(TaskAttemptID left: remaining) {\n         scheduler.copyFailed(left, host, false, connectExcpt);\n       }\n      \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n+    input \u003d new DataInputStream(connection.getInputStream());\n+    \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n-        failedTasks \u003d copyMapOutput(host, input, remaining);\n+        try {\n+          failedTasks \u003d copyMapOutput(host, input, remaining, fetchRetryEnabled);\n+        } catch (IOException e) {\n+          //\n+          // Setup connection again if disconnected by NM\n+          connection.disconnect();\n+          // Get map output from remaining tasks only.\n+          url \u003d getMapOutputURL(host, remaining);\n+          \n+          // Connect with retry as expecting host\u0027s recovery take sometime.\n+          setupConnectionsWithRetry(host, remaining, url);\n+          if (stopped) {\n+            abortConnect(host, remaining);\n+            return;\n+          }\n+          input \u003d new DataInputStream(connection.getInputStream());\n+        }\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n         scheduler.hostFailed(host.getHostName());\n         for(TaskAttemptID left: failedTasks) {\n           scheduler.copyFailed(left, host, true, false);\n         }\n       }\n \n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n       input.close();\n       input \u003d null;\n     } finally {\n       if (input !\u003d null) {\n         IOUtils.cleanup(LOG, input);\n         input \u003d null;\n       }\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // reset retryStartTime for a new host\n    retryStartTime \u003d 0;\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input \u003d null;\n    URL url \u003d getMapOutputURL(host, maps);\n    try {\n      setupConnectionsWithRetry(host, remaining, url);\n      \n      if (stopped) {\n        abortConnect(host, remaining);\n        return;\n      }\n    } catch (IOException ie) {\n      boolean connectExcpt \u003d ie instanceof ConnectException;\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      scheduler.hostFailed(host.getHostName());\n      for(TaskAttemptID left: remaining) {\n        scheduler.copyFailed(left, host, false, connectExcpt);\n      }\n     \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    input \u003d new DataInputStream(connection.getInputStream());\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        try {\n          failedTasks \u003d copyMapOutput(host, input, remaining, fetchRetryEnabled);\n        } catch (IOException e) {\n          //\n          // Setup connection again if disconnected by NM\n          connection.disconnect();\n          // Get map output from remaining tasks only.\n          url \u003d getMapOutputURL(host, remaining);\n          \n          // Connect with retry as expecting host\u0027s recovery take sometime.\n          setupConnectionsWithRetry(host, remaining, url);\n          if (stopped) {\n            abortConnect(host, remaining);\n            return;\n          }\n          input \u003d new DataInputStream(connection.getInputStream());\n        }\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        scheduler.hostFailed(host.getHostName());\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n\n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n      input.close();\n      input \u003d null;\n    } finally {\n      if (input !\u003d null) {\n        IOUtils.cleanup(LOG, input);\n        input \u003d null;\n      }\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "f8e871d01b851cd5d8c57dd7e364b3e787521765": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6024. Shortened the time when Fetcher is stuck in retrying before concluding the failure by configuration. Contributed by Yunjiong Zhao.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1618677 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/14 10:57 AM",
      "commitName": "f8e871d01b851cd5d8c57dd7e364b3e787521765",
      "commitAuthor": "Zhijie Shen",
      "commitDateOld": "05/08/13 11:36 PM",
      "commitNameOld": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 377.47,
      "commitsBetweenForRepo": 2549,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,128 +1,130 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input \u003d null;\n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n       openConnection(url);\n       if (stopped) {\n         abortConnect(host, remaining);\n         return;\n       }\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode,\n           shuffleSecretKey);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       // put shuffle version into http header\n       connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_NAME,\n           ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\n       connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_VERSION,\n           ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\n       connect(connection, connectionTimeout);\n       // verify that the thread wasn\u0027t stopped during calls to connect\n       if (stopped) {\n         abortConnect(host, remaining);\n         return;\n       }\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       // get the shuffle version\n       if (!ShuffleHeader.DEFAULT_HTTP_HEADER_NAME.equals(\n           connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME))\n           || !ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION.equals(\n               connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION))) {\n         throw new IOException(\"Incompatible shuffle response version\");\n       }\n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, shuffleSecretKey);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n     } catch (IOException ie) {\n       boolean connectExcpt \u003d ie instanceof ConnectException;\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n+      scheduler.hostFailed(host.getHostName());\n       for(TaskAttemptID left: remaining) {\n         scheduler.copyFailed(left, host, false, connectExcpt);\n       }\n      \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n         failedTasks \u003d copyMapOutput(host, input, remaining);\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n+        scheduler.hostFailed(host.getHostName());\n         for(TaskAttemptID left: failedTasks) {\n           scheduler.copyFailed(left, host, true, false);\n         }\n       }\n \n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n       input.close();\n       input \u003d null;\n     } finally {\n       if (input !\u003d null) {\n         IOUtils.cleanup(LOG, input);\n         input \u003d null;\n       }\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input \u003d null;\n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      openConnection(url);\n      if (stopped) {\n        abortConnect(host, remaining);\n        return;\n      }\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode,\n          shuffleSecretKey);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      // put shuffle version into http header\n      connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_NAME,\n          ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\n      connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_VERSION,\n          ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\n      connect(connection, connectionTimeout);\n      // verify that the thread wasn\u0027t stopped during calls to connect\n      if (stopped) {\n        abortConnect(host, remaining);\n        return;\n      }\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      // get the shuffle version\n      if (!ShuffleHeader.DEFAULT_HTTP_HEADER_NAME.equals(\n          connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME))\n          || !ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION.equals(\n              connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION))) {\n        throw new IOException(\"Incompatible shuffle response version\");\n      }\n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, shuffleSecretKey);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n    } catch (IOException ie) {\n      boolean connectExcpt \u003d ie instanceof ConnectException;\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      scheduler.hostFailed(host.getHostName());\n      for(TaskAttemptID left: remaining) {\n        scheduler.copyFailed(left, host, false, connectExcpt);\n      }\n     \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        failedTasks \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        scheduler.hostFailed(host.getHostName());\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n\n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n      input.close();\n      input \u003d null;\n    } finally {\n      if (input !\u003d null) {\n        IOUtils.cleanup(LOG, input);\n        input \u003d null;\n      }\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "11bcd2ed12f7f0e02fdaefaefea56929b32d5ee6": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5326. Added version to shuffle header. Contributed by Zhijie Shen.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1496741 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/06/13 10:49 PM",
      "commitName": "11bcd2ed12f7f0e02fdaefaefea56929b32d5ee6",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "18/06/13 7:12 PM",
      "commitNameOld": "16616f9e5b1e58bde880ca72e5290e826b5f8bf7",
      "commitAuthorOld": "Christopher Douglas",
      "daysBetweenCommits": 7.15,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,117 +1,128 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input \u003d null;\n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n       openConnection(url);\n       if (stopped) {\n         abortConnect(host, remaining);\n         return;\n       }\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode,\n           shuffleSecretKey);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n+      // put shuffle version into http header\n+      connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_NAME,\n+          ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\n+      connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_VERSION,\n+          ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\n       connect(connection, connectionTimeout);\n       // verify that the thread wasn\u0027t stopped during calls to connect\n       if (stopped) {\n         abortConnect(host, remaining);\n         return;\n       }\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n-      \n+      // get the shuffle version\n+      if (!ShuffleHeader.DEFAULT_HTTP_HEADER_NAME.equals(\n+          connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME))\n+          || !ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION.equals(\n+              connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION))) {\n+        throw new IOException(\"Incompatible shuffle response version\");\n+      }\n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, shuffleSecretKey);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n     } catch (IOException ie) {\n       boolean connectExcpt \u003d ie instanceof ConnectException;\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       for(TaskAttemptID left: remaining) {\n         scheduler.copyFailed(left, host, false, connectExcpt);\n       }\n      \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n         failedTasks \u003d copyMapOutput(host, input, remaining);\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n         for(TaskAttemptID left: failedTasks) {\n           scheduler.copyFailed(left, host, true, false);\n         }\n       }\n \n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n       input.close();\n       input \u003d null;\n     } finally {\n       if (input !\u003d null) {\n         IOUtils.cleanup(LOG, input);\n         input \u003d null;\n       }\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input \u003d null;\n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      openConnection(url);\n      if (stopped) {\n        abortConnect(host, remaining);\n        return;\n      }\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode,\n          shuffleSecretKey);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      // put shuffle version into http header\n      connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_NAME,\n          ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\n      connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_VERSION,\n          ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\n      connect(connection, connectionTimeout);\n      // verify that the thread wasn\u0027t stopped during calls to connect\n      if (stopped) {\n        abortConnect(host, remaining);\n        return;\n      }\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      // get the shuffle version\n      if (!ShuffleHeader.DEFAULT_HTTP_HEADER_NAME.equals(\n          connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME))\n          || !ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION.equals(\n              connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION))) {\n        throw new IOException(\"Incompatible shuffle response version\");\n      }\n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, shuffleSecretKey);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n    } catch (IOException ie) {\n      boolean connectExcpt \u003d ie instanceof ConnectException;\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      for(TaskAttemptID left: remaining) {\n        scheduler.copyFailed(left, host, false, connectExcpt);\n      }\n     \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        failedTasks \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n\n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n      input.close();\n      input \u003d null;\n    } finally {\n      if (input !\u003d null) {\n        IOUtils.cleanup(LOG, input);\n        input \u003d null;\n      }\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "16616f9e5b1e58bde880ca72e5290e826b5f8bf7": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5194. Heed interrupts during Fetcher shutdown.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1494416 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/06/13 7:12 PM",
      "commitName": "16616f9e5b1e58bde880ca72e5290e826b5f8bf7",
      "commitAuthor": "Christopher Douglas",
      "commitDateOld": "15/06/13 8:07 PM",
      "commitNameOld": "1a389305b27ac1efec4d7923b87de3e703bf70e1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 2.96,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,105 +1,117 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n-    DataInputStream input;\n-    \n+    DataInputStream input \u003d null;\n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n-      HttpURLConnection connection \u003d openConnection(url);\n+      openConnection(url);\n+      if (stopped) {\n+        abortConnect(host, remaining);\n+        return;\n+      }\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode,\n           shuffleSecretKey);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n+      // verify that the thread wasn\u0027t stopped during calls to connect\n+      if (stopped) {\n+        abortConnect(host, remaining);\n+        return;\n+      }\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, shuffleSecretKey);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n     } catch (IOException ie) {\n       boolean connectExcpt \u003d ie instanceof ConnectException;\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       for(TaskAttemptID left: remaining) {\n         scheduler.copyFailed(left, host, false, connectExcpt);\n       }\n      \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n         failedTasks \u003d copyMapOutput(host, input, remaining);\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n         for(TaskAttemptID left: failedTasks) {\n           scheduler.copyFailed(left, host, true, false);\n         }\n       }\n-      \n-      IOUtils.cleanup(LOG, input);\n-      \n+\n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n+      input.close();\n+      input \u003d null;\n     } finally {\n+      if (input !\u003d null) {\n+        IOUtils.cleanup(LOG, input);\n+        input \u003d null;\n+      }\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input \u003d null;\n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      openConnection(url);\n      if (stopped) {\n        abortConnect(host, remaining);\n        return;\n      }\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode,\n          shuffleSecretKey);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      // verify that the thread wasn\u0027t stopped during calls to connect\n      if (stopped) {\n        abortConnect(host, remaining);\n        return;\n      }\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, shuffleSecretKey);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n    } catch (IOException ie) {\n      boolean connectExcpt \u003d ie instanceof ConnectException;\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      for(TaskAttemptID left: remaining) {\n        scheduler.copyFailed(left, host, false, connectExcpt);\n      }\n     \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        failedTasks \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n\n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n      input.close();\n      input \u003d null;\n    } finally {\n      if (input !\u003d null) {\n        IOUtils.cleanup(LOG, input);\n        input \u003d null;\n      }\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5042. Reducer unable to fetch for a map task that was recovered (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1457119 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/03/13 2:09 PM",
      "commitName": "7d7553c4eb7d9a282410a3213d26a89fea9b7865",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "12/03/13 3:51 PM",
      "commitNameOld": "57803245ecc806249fcd0cd5fb3ca593098ac877",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 2.93,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,105 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n       HttpURLConnection connection \u003d openConnection(url);\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n-      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n+      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode,\n+          shuffleSecretKey);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n-      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n+      SecureShuffleUtils.verifyReply(replyHash, encHash, shuffleSecretKey);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n     } catch (IOException ie) {\n       boolean connectExcpt \u003d ie instanceof ConnectException;\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       for(TaskAttemptID left: remaining) {\n         scheduler.copyFailed(left, host, false, connectExcpt);\n       }\n      \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n         failedTasks \u003d copyMapOutput(host, input, remaining);\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n         for(TaskAttemptID left: failedTasks) {\n           scheduler.copyFailed(left, host, true, false);\n         }\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d openConnection(url);\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode,\n          shuffleSecretKey);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, shuffleSecretKey);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n    } catch (IOException ie) {\n      boolean connectExcpt \u003d ie instanceof ConnectException;\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      for(TaskAttemptID left: remaining) {\n        scheduler.copyFailed(left, host, false, connectExcpt);\n      }\n     \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        failedTasks \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "57803245ecc806249fcd0cd5fb3ca593098ac877": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5060. Fetch failures that time out only count against the first map task. Contributed by Robert Joseph Evans\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1455740 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/03/13 3:51 PM",
      "commitName": "57803245ecc806249fcd0cd5fb3ca593098ac877",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "22/01/13 6:10 AM",
      "commitNameOld": "73fd247c7649919350ecfd16806af57ffe554649",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 49.36,
      "commitsBetweenForRepo": 204,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,104 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n-    boolean connectSucceeded \u003d false;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n       HttpURLConnection connection \u003d openConnection(url);\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n-      connectSucceeded \u003d true;\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n     } catch (IOException ie) {\n       boolean connectExcpt \u003d ie instanceof ConnectException;\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n-      if (!connectSucceeded) {\n-        for(TaskAttemptID left: remaining) {\n-          scheduler.copyFailed(left, host, connectSucceeded, connectExcpt);\n-        }\n-      } else {\n-        // If we got a read error at this stage, it implies there was a problem\n-        // with the first map, typically lost map. So, penalize only that map\n-        // and add the rest\n-        TaskAttemptID firstMap \u003d maps.get(0);\n-        scheduler.copyFailed(firstMap, host, connectSucceeded, connectExcpt);\n+      for(TaskAttemptID left: remaining) {\n+        scheduler.copyFailed(left, host, false, connectExcpt);\n       }\n-      \n+     \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n         failedTasks \u003d copyMapOutput(host, input, remaining);\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n         for(TaskAttemptID left: failedTasks) {\n           scheduler.copyFailed(left, host, true, false);\n         }\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d openConnection(url);\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n    } catch (IOException ie) {\n      boolean connectExcpt \u003d ie instanceof ConnectException;\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      for(TaskAttemptID left: remaining) {\n        scheduler.copyFailed(left, host, false, connectExcpt);\n      }\n     \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        failedTasks \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "a7d444d002c664208669ec6ddf3bcb1db71e3741": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4902. Fix typo \"receievd\" should be \"received\" in log output. Contributed by Albert Chu\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1426018 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/12/12 12:47 PM",
      "commitName": "a7d444d002c664208669ec6ddf3bcb1db71e3741",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "08/11/12 7:17 AM",
      "commitNameOld": "6ee6eb843013324788f30384d9d967ff8743a970",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 48.23,
      "commitsBetweenForRepo": 203,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,114 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n     boolean connectSucceeded \u003d false;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n       HttpURLConnection connection \u003d openConnection(url);\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n       connectSucceeded \u003d true;\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n-      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n+      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n     } catch (IOException ie) {\n       boolean connectExcpt \u003d ie instanceof ConnectException;\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       if (!connectSucceeded) {\n         for(TaskAttemptID left: remaining) {\n           scheduler.copyFailed(left, host, connectSucceeded, connectExcpt);\n         }\n       } else {\n         // If we got a read error at this stage, it implies there was a problem\n         // with the first map, typically lost map. So, penalize only that map\n         // and add the rest\n         TaskAttemptID firstMap \u003d maps.get(0);\n         scheduler.copyFailed(firstMap, host, connectSucceeded, connectExcpt);\n       }\n       \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n         failedTasks \u003d copyMapOutput(host, input, remaining);\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n         for(TaskAttemptID left: failedTasks) {\n           scheduler.copyFailed(left, host, true, false);\n         }\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d openConnection(url);\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and received reply\");\n    } catch (IOException ie) {\n      boolean connectExcpt \u003d ie instanceof ConnectException;\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded, connectExcpt);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded, connectExcpt);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        failedTasks \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "6ee6eb843013324788f30384d9d967ff8743a970": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4772. Fetch failures can take way too long for a map to be restarted (bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1407118 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/11/12 7:17 AM",
      "commitName": "6ee6eb843013324788f30384d9d967ff8743a970",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "15/08/12 4:08 PM",
      "commitNameOld": "8fcad7e8e9fd8c80207d9593115901d53b3b7d42",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 84.67,
      "commitsBetweenForRepo": 511,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,113 +1,114 @@\n   protected void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n         + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n     boolean connectSucceeded \u003d false;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n       HttpURLConnection connection \u003d openConnection(url);\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n       connectSucceeded \u003d true;\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n     } catch (IOException ie) {\n+      boolean connectExcpt \u003d ie instanceof ConnectException;\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       if (!connectSucceeded) {\n         for(TaskAttemptID left: remaining) {\n-          scheduler.copyFailed(left, host, connectSucceeded);\n+          scheduler.copyFailed(left, host, connectSucceeded, connectExcpt);\n         }\n       } else {\n         // If we got a read error at this stage, it implies there was a problem\n         // with the first map, typically lost map. So, penalize only that map\n         // and add the rest\n         TaskAttemptID firstMap \u003d maps.get(0);\n-        scheduler.copyFailed(firstMap, host, connectSucceeded);\n+        scheduler.copyFailed(firstMap, host, connectSucceeded, connectExcpt);\n       }\n       \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, faildTasks is not null and we exit\n       // after putting back the remaining maps to the \n       // yet_to_be_fetched list and marking the failed tasks.\n       TaskAttemptID[] failedTasks \u003d null;\n       while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n         failedTasks \u003d copyMapOutput(host, input, remaining);\n       }\n       \n       if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n         LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n         for(TaskAttemptID left: failedTasks) {\n-          scheduler.copyFailed(left, host, true);\n+          scheduler.copyFailed(left, host, true, false);\n         }\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n       if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d openConnection(url);\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      boolean connectExcpt \u003d ie instanceof ConnectException;\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded, connectExcpt);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded, connectExcpt);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        failedTasks \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true, false);\n        }\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "d87b545165f9442f614665521ce04424af1490e8": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": " MAPREDUCE-4423. Potential infinite fetching of map output (Robert Evans via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1366258 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/12 6:48 PM",
      "commitName": "d87b545165f9442f614665521ce04424af1490e8",
      "commitAuthor": "Thomas Graves",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": " MAPREDUCE-4423. Potential infinite fetching of map output (Robert Evans via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1366258 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "26/07/12 6:48 PM",
          "commitName": "d87b545165f9442f614665521ce04424af1490e8",
          "commitAuthor": "Thomas Graves",
          "commitDateOld": "26/07/12 6:23 AM",
          "commitNameOld": "9d16c9354b0c05edb30d23003dcdec4cc44ed925",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.52,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,106 +1,113 @@\n-  private void copyFromHost(MapHost host) throws IOException {\n+  protected void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n-    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n-    for (TaskAttemptID tmp: maps) {\n-      LOG.debug(tmp);\n+    if(LOG.isDebugEnabled()) {\n+      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n+        + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n     boolean connectSucceeded \u003d false;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n       HttpURLConnection connection \u003d openConnection(url);\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n       connectSucceeded \u003d true;\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n     } catch (IOException ie) {\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       if (!connectSucceeded) {\n         for(TaskAttemptID left: remaining) {\n           scheduler.copyFailed(left, host, connectSucceeded);\n         }\n       } else {\n         // If we got a read error at this stage, it implies there was a problem\n         // with the first map, typically lost map. So, penalize only that map\n         // and add the rest\n         TaskAttemptID firstMap \u003d maps.get(0);\n         scheduler.copyFailed(firstMap, host, connectSucceeded);\n       }\n       \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n-      // On any error, good becomes false and we exit after putting back\n-      // the remaining maps to the yet_to_be_fetched list\n-      boolean good \u003d true;\n-      while (!remaining.isEmpty() \u0026\u0026 good) {\n-        good \u003d copyMapOutput(host, input, remaining);\n+      // On any error, faildTasks is not null and we exit\n+      // after putting back the remaining maps to the \n+      // yet_to_be_fetched list and marking the failed tasks.\n+      TaskAttemptID[] failedTasks \u003d null;\n+      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n+        failedTasks \u003d copyMapOutput(host, input, remaining);\n+      }\n+      \n+      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n+        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n+        for(TaskAttemptID left: failedTasks) {\n+          scheduler.copyFailed(left, host, true);\n+        }\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n-      if (good \u0026\u0026 !remaining.isEmpty()) {\n+      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n-      \n-   }\n\\ No newline at end of file\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d openConnection(url);\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        failedTasks \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true);\n        }\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[protected]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": " MAPREDUCE-4423. Potential infinite fetching of map output (Robert Evans via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1366258 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "26/07/12 6:48 PM",
          "commitName": "d87b545165f9442f614665521ce04424af1490e8",
          "commitAuthor": "Thomas Graves",
          "commitDateOld": "26/07/12 6:23 AM",
          "commitNameOld": "9d16c9354b0c05edb30d23003dcdec4cc44ed925",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.52,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,106 +1,113 @@\n-  private void copyFromHost(MapHost host) throws IOException {\n+  protected void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n-    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n-    for (TaskAttemptID tmp: maps) {\n-      LOG.debug(tmp);\n+    if(LOG.isDebugEnabled()) {\n+      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n+        + maps);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n     boolean connectSucceeded \u003d false;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n       HttpURLConnection connection \u003d openConnection(url);\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n       connectSucceeded \u003d true;\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n     } catch (IOException ie) {\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       if (!connectSucceeded) {\n         for(TaskAttemptID left: remaining) {\n           scheduler.copyFailed(left, host, connectSucceeded);\n         }\n       } else {\n         // If we got a read error at this stage, it implies there was a problem\n         // with the first map, typically lost map. So, penalize only that map\n         // and add the rest\n         TaskAttemptID firstMap \u003d maps.get(0);\n         scheduler.copyFailed(firstMap, host, connectSucceeded);\n       }\n       \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n-      // On any error, good becomes false and we exit after putting back\n-      // the remaining maps to the yet_to_be_fetched list\n-      boolean good \u003d true;\n-      while (!remaining.isEmpty() \u0026\u0026 good) {\n-        good \u003d copyMapOutput(host, input, remaining);\n+      // On any error, faildTasks is not null and we exit\n+      // after putting back the remaining maps to the \n+      // yet_to_be_fetched list and marking the failed tasks.\n+      TaskAttemptID[] failedTasks \u003d null;\n+      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n+        failedTasks \u003d copyMapOutput(host, input, remaining);\n+      }\n+      \n+      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n+        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n+        for(TaskAttemptID left: failedTasks) {\n+          scheduler.copyFailed(left, host, true);\n+        }\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n-      if (good \u0026\u0026 !remaining.isEmpty()) {\n+      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n-      \n-   }\n\\ No newline at end of file\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \"\n        + maps);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d openConnection(url);\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        failedTasks \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      if(failedTasks !\u003d null \u0026\u0026 failedTasks.length \u003e 0) {\n        LOG.warn(\"copyMapOutput failed for tasks \"+Arrays.toString(failedTasks));\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true);\n        }\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
          "extendedDetails": {}
        }
      ]
    },
    "9d16c9354b0c05edb30d23003dcdec4cc44ed925": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4417. add support for encrypted shuffle (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1365979 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/12 6:23 AM",
      "commitName": "9d16c9354b0c05edb30d23003dcdec4cc44ed925",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "20/07/12 1:18 PM",
      "commitNameOld": "d45922de2c5645e11339b94e4c31935ead66fefc",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 5.71,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,106 +1,106 @@\n   private void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n     for (TaskAttemptID tmp: maps) {\n       LOG.debug(tmp);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n     boolean connectSucceeded \u003d false;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n-      HttpURLConnection connection \u003d (HttpURLConnection)url.openConnection();\n+      HttpURLConnection connection \u003d openConnection(url);\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n       connectSucceeded \u003d true;\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n     } catch (IOException ie) {\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       if (!connectSucceeded) {\n         for(TaskAttemptID left: remaining) {\n           scheduler.copyFailed(left, host, connectSucceeded);\n         }\n       } else {\n         // If we got a read error at this stage, it implies there was a problem\n         // with the first map, typically lost map. So, penalize only that map\n         // and add the rest\n         TaskAttemptID firstMap \u003d maps.get(0);\n         scheduler.copyFailed(firstMap, host, connectSucceeded);\n       }\n       \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, good becomes false and we exit after putting back\n       // the remaining maps to the yet_to_be_fetched list\n       boolean good \u003d true;\n       while (!remaining.isEmpty() \u0026\u0026 good) {\n         good \u003d copyMapOutput(host, input, remaining);\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n       if (good \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n       \n    }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n    for (TaskAttemptID tmp: maps) {\n      LOG.debug(tmp);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d openConnection(url);\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, good becomes false and we exit after putting back\n      // the remaining maps to the yet_to_be_fetched list\n      boolean good \u003d true;\n      while (!remaining.isEmpty() \u0026\u0026 good) {\n        good \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (good \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n      \n   }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "d45922de2c5645e11339b94e4c31935ead66fefc": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "svn merge --change -1363454 for reverting MAPREDUCE-4423\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1363935 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/07/12 1:18 PM",
      "commitName": "d45922de2c5645e11339b94e4c31935ead66fefc",
      "commitAuthor": "Thomas Graves",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "svn merge --change -1363454 for reverting MAPREDUCE-4423\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1363935 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "20/07/12 1:18 PM",
          "commitName": "d45922de2c5645e11339b94e4c31935ead66fefc",
          "commitAuthor": "Thomas Graves",
          "commitDateOld": "19/07/12 11:19 AM",
          "commitNameOld": "8e69f883a0ac407781fa09328d9fb87faf5a8d0a",
          "commitAuthorOld": "Thomas Graves",
          "daysBetweenCommits": 1.08,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,114 +1,106 @@\n-  protected void copyFromHost(MapHost host) throws IOException {\n+  private void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n-      for (TaskAttemptID tmp: maps) {\n-        LOG.debug(tmp);\n-      }\n+    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n+    for (TaskAttemptID tmp: maps) {\n+      LOG.debug(tmp);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n     boolean connectSucceeded \u003d false;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n-      HttpURLConnection connection \u003d openConnection(url);\n+      HttpURLConnection connection \u003d (HttpURLConnection)url.openConnection();\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n       connectSucceeded \u003d true;\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n     } catch (IOException ie) {\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       if (!connectSucceeded) {\n         for(TaskAttemptID left: remaining) {\n           scheduler.copyFailed(left, host, connectSucceeded);\n         }\n       } else {\n         // If we got a read error at this stage, it implies there was a problem\n         // with the first map, typically lost map. So, penalize only that map\n         // and add the rest\n         TaskAttemptID firstMap \u003d maps.get(0);\n         scheduler.copyFailed(firstMap, host, connectSucceeded);\n       }\n       \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n-      // On any error, faildTasks is not null and we exit\n-      // after putting back the remaining maps to the \n-      // yet_to_be_fetched list and marking the failed tasks.\n-      TaskAttemptID[] failedTasks \u003d null;\n-      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n-        failedTasks \u003d copyMapOutput(host, input, remaining);\n-      }\n-      \n-      if(failedTasks !\u003d null) {\n-        for(TaskAttemptID left: failedTasks) {\n-          scheduler.copyFailed(left, host, true);\n-        }\n+      // On any error, good becomes false and we exit after putting back\n+      // the remaining maps to the yet_to_be_fetched list\n+      boolean good \u003d true;\n+      while (!remaining.isEmpty() \u0026\u0026 good) {\n+        good \u003d copyMapOutput(host, input, remaining);\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n-      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n+      if (good \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n-  }\n\\ No newline at end of file\n+      \n+   }\n\\ No newline at end of file\n",
          "actualSource": "  private void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n    for (TaskAttemptID tmp: maps) {\n      LOG.debug(tmp);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d (HttpURLConnection)url.openConnection();\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, good becomes false and we exit after putting back\n      // the remaining maps to the yet_to_be_fetched list\n      boolean good \u003d true;\n      while (!remaining.isEmpty() \u0026\u0026 good) {\n        good \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (good \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n      \n   }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
          "extendedDetails": {
            "oldValue": "[protected]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "svn merge --change -1363454 for reverting MAPREDUCE-4423\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1363935 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "20/07/12 1:18 PM",
          "commitName": "d45922de2c5645e11339b94e4c31935ead66fefc",
          "commitAuthor": "Thomas Graves",
          "commitDateOld": "19/07/12 11:19 AM",
          "commitNameOld": "8e69f883a0ac407781fa09328d9fb87faf5a8d0a",
          "commitAuthorOld": "Thomas Graves",
          "daysBetweenCommits": 1.08,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,114 +1,106 @@\n-  protected void copyFromHost(MapHost host) throws IOException {\n+  private void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n-      for (TaskAttemptID tmp: maps) {\n-        LOG.debug(tmp);\n-      }\n+    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n+    for (TaskAttemptID tmp: maps) {\n+      LOG.debug(tmp);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n     boolean connectSucceeded \u003d false;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n-      HttpURLConnection connection \u003d openConnection(url);\n+      HttpURLConnection connection \u003d (HttpURLConnection)url.openConnection();\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n       connectSucceeded \u003d true;\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n     } catch (IOException ie) {\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       if (!connectSucceeded) {\n         for(TaskAttemptID left: remaining) {\n           scheduler.copyFailed(left, host, connectSucceeded);\n         }\n       } else {\n         // If we got a read error at this stage, it implies there was a problem\n         // with the first map, typically lost map. So, penalize only that map\n         // and add the rest\n         TaskAttemptID firstMap \u003d maps.get(0);\n         scheduler.copyFailed(firstMap, host, connectSucceeded);\n       }\n       \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n-      // On any error, faildTasks is not null and we exit\n-      // after putting back the remaining maps to the \n-      // yet_to_be_fetched list and marking the failed tasks.\n-      TaskAttemptID[] failedTasks \u003d null;\n-      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n-        failedTasks \u003d copyMapOutput(host, input, remaining);\n-      }\n-      \n-      if(failedTasks !\u003d null) {\n-        for(TaskAttemptID left: failedTasks) {\n-          scheduler.copyFailed(left, host, true);\n-        }\n+      // On any error, good becomes false and we exit after putting back\n+      // the remaining maps to the yet_to_be_fetched list\n+      boolean good \u003d true;\n+      while (!remaining.isEmpty() \u0026\u0026 good) {\n+        good \u003d copyMapOutput(host, input, remaining);\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n-      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n+      if (good \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n-  }\n\\ No newline at end of file\n+      \n+   }\n\\ No newline at end of file\n",
          "actualSource": "  private void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n    for (TaskAttemptID tmp: maps) {\n      LOG.debug(tmp);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d (HttpURLConnection)url.openConnection();\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, good becomes false and we exit after putting back\n      // the remaining maps to the yet_to_be_fetched list\n      boolean good \u003d true;\n      while (!remaining.isEmpty() \u0026\u0026 good) {\n        good \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (good \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n      \n   }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
          "extendedDetails": {}
        }
      ]
    },
    "8e69f883a0ac407781fa09328d9fb87faf5a8d0a": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-4423. Potential infinite fetching of map output (Robert Evans via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1363454 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/07/12 11:19 AM",
      "commitName": "8e69f883a0ac407781fa09328d9fb87faf5a8d0a",
      "commitAuthor": "Thomas Graves",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "MAPREDUCE-4423. Potential infinite fetching of map output (Robert Evans via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1363454 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "19/07/12 11:19 AM",
          "commitName": "8e69f883a0ac407781fa09328d9fb87faf5a8d0a",
          "commitAuthor": "Thomas Graves",
          "commitDateOld": "19/03/12 7:28 PM",
          "commitNameOld": "04a47dea7489e3f860b0ab7b8f3ab1fc5a395426",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 121.66,
          "commitsBetweenForRepo": 769,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,106 +1,114 @@\n-  private void copyFromHost(MapHost host) throws IOException {\n+  protected void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n-    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n-    for (TaskAttemptID tmp: maps) {\n-      LOG.debug(tmp);\n+    if(LOG.isDebugEnabled()) {\n+      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n+      for (TaskAttemptID tmp: maps) {\n+        LOG.debug(tmp);\n+      }\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n     boolean connectSucceeded \u003d false;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n-      HttpURLConnection connection \u003d (HttpURLConnection)url.openConnection();\n+      HttpURLConnection connection \u003d openConnection(url);\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n       connectSucceeded \u003d true;\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n     } catch (IOException ie) {\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       if (!connectSucceeded) {\n         for(TaskAttemptID left: remaining) {\n           scheduler.copyFailed(left, host, connectSucceeded);\n         }\n       } else {\n         // If we got a read error at this stage, it implies there was a problem\n         // with the first map, typically lost map. So, penalize only that map\n         // and add the rest\n         TaskAttemptID firstMap \u003d maps.get(0);\n         scheduler.copyFailed(firstMap, host, connectSucceeded);\n       }\n       \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n-      // On any error, good becomes false and we exit after putting back\n-      // the remaining maps to the yet_to_be_fetched list\n-      boolean good \u003d true;\n-      while (!remaining.isEmpty() \u0026\u0026 good) {\n-        good \u003d copyMapOutput(host, input, remaining);\n+      // On any error, faildTasks is not null and we exit\n+      // after putting back the remaining maps to the \n+      // yet_to_be_fetched list and marking the failed tasks.\n+      TaskAttemptID[] failedTasks \u003d null;\n+      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n+        failedTasks \u003d copyMapOutput(host, input, remaining);\n+      }\n+      \n+      if(failedTasks !\u003d null) {\n+        for(TaskAttemptID left: failedTasks) {\n+          scheduler.copyFailed(left, host, true);\n+        }\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n-      if (good \u0026\u0026 !remaining.isEmpty()) {\n+      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n-      \n-   }\n\\ No newline at end of file\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n      for (TaskAttemptID tmp: maps) {\n        LOG.debug(tmp);\n      }\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d openConnection(url);\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        failedTasks \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      if(failedTasks !\u003d null) {\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true);\n        }\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[protected]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-4423. Potential infinite fetching of map output (Robert Evans via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1363454 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "19/07/12 11:19 AM",
          "commitName": "8e69f883a0ac407781fa09328d9fb87faf5a8d0a",
          "commitAuthor": "Thomas Graves",
          "commitDateOld": "19/03/12 7:28 PM",
          "commitNameOld": "04a47dea7489e3f860b0ab7b8f3ab1fc5a395426",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 121.66,
          "commitsBetweenForRepo": 769,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,106 +1,114 @@\n-  private void copyFromHost(MapHost host) throws IOException {\n+  protected void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n-    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n-    for (TaskAttemptID tmp: maps) {\n-      LOG.debug(tmp);\n+    if(LOG.isDebugEnabled()) {\n+      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n+      for (TaskAttemptID tmp: maps) {\n+        LOG.debug(tmp);\n+      }\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n     boolean connectSucceeded \u003d false;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n-      HttpURLConnection connection \u003d (HttpURLConnection)url.openConnection();\n+      HttpURLConnection connection \u003d openConnection(url);\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n       connectSucceeded \u003d true;\n       input \u003d new DataInputStream(connection.getInputStream());\n \n       // Validate response code\n       int rc \u003d connection.getResponseCode();\n       if (rc !\u003d HttpURLConnection.HTTP_OK) {\n         throw new IOException(\n             \"Got invalid response code \" + rc + \" from \" + url +\n             \": \" + connection.getResponseMessage());\n       }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n     } catch (IOException ie) {\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       if (!connectSucceeded) {\n         for(TaskAttemptID left: remaining) {\n           scheduler.copyFailed(left, host, connectSucceeded);\n         }\n       } else {\n         // If we got a read error at this stage, it implies there was a problem\n         // with the first map, typically lost map. So, penalize only that map\n         // and add the rest\n         TaskAttemptID firstMap \u003d maps.get(0);\n         scheduler.copyFailed(firstMap, host, connectSucceeded);\n       }\n       \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n-      // On any error, good becomes false and we exit after putting back\n-      // the remaining maps to the yet_to_be_fetched list\n-      boolean good \u003d true;\n-      while (!remaining.isEmpty() \u0026\u0026 good) {\n-        good \u003d copyMapOutput(host, input, remaining);\n+      // On any error, faildTasks is not null and we exit\n+      // after putting back the remaining maps to the \n+      // yet_to_be_fetched list and marking the failed tasks.\n+      TaskAttemptID[] failedTasks \u003d null;\n+      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n+        failedTasks \u003d copyMapOutput(host, input, remaining);\n+      }\n+      \n+      if(failedTasks !\u003d null) {\n+        for(TaskAttemptID left: failedTasks) {\n+          scheduler.copyFailed(left, host, true);\n+        }\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n-      if (good \u0026\u0026 !remaining.isEmpty()) {\n+      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n-      \n-   }\n\\ No newline at end of file\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  protected void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n      for (TaskAttemptID tmp: maps) {\n        LOG.debug(tmp);\n      }\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d openConnection(url);\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, faildTasks is not null and we exit\n      // after putting back the remaining maps to the \n      // yet_to_be_fetched list and marking the failed tasks.\n      TaskAttemptID[] failedTasks \u003d null;\n      while (!remaining.isEmpty() \u0026\u0026 failedTasks \u003d\u003d null) {\n        failedTasks \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      if(failedTasks !\u003d null) {\n        for(TaskAttemptID left: failedTasks) {\n          scheduler.copyFailed(left, host, true);\n        }\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (failedTasks \u003d\u003d null \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
          "extendedDetails": {}
        }
      ]
    },
    "04a47dea7489e3f860b0ab7b8f3ab1fc5a395426": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3992. Reduce fetcher doesn\u0027t verify HTTP status code of response. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1302754 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/03/12 7:28 PM",
      "commitName": "04a47dea7489e3f860b0ab7b8f3ab1fc5a395426",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "24/01/12 3:18 PM",
      "commitNameOld": "078ae89a4793eb6a153a88b106d330fd059a4933",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 55.13,
      "commitsBetweenForRepo": 451,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,106 @@\n   private void copyFromHost(MapHost host) throws IOException {\n     // Get completed maps on \u0027host\u0027\n     List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n     \n     // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n     // especially at the tail of large jobs\n     if (maps.size() \u003d\u003d 0) {\n       return;\n     }\n     \n     LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n     for (TaskAttemptID tmp: maps) {\n       LOG.debug(tmp);\n     }\n     \n     // List of maps to be fetched yet\n     Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n     \n     // Construct the url and connect\n     DataInputStream input;\n     boolean connectSucceeded \u003d false;\n     \n     try {\n       URL url \u003d getMapOutputURL(host, maps);\n-      URLConnection connection \u003d url.openConnection();\n+      HttpURLConnection connection \u003d (HttpURLConnection)url.openConnection();\n       \n       // generate hash of the url\n       String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n       String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n       \n       // put url hash into http header\n       connection.addRequestProperty(\n           SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n       // set the read timeout\n       connection.setReadTimeout(readTimeout);\n       connect(connection, connectionTimeout);\n       connectSucceeded \u003d true;\n       input \u003d new DataInputStream(connection.getInputStream());\n+\n+      // Validate response code\n+      int rc \u003d connection.getResponseCode();\n+      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n+        throw new IOException(\n+            \"Got invalid response code \" + rc + \" from \" + url +\n+            \": \" + connection.getResponseMessage());\n+      }\n       \n       // get the replyHash which is HMac of the encHash we sent to the server\n       String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n       if(replyHash\u003d\u003dnull) {\n         throw new IOException(\"security validation of TT Map output failed\");\n       }\n       LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n       LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n     } catch (IOException ie) {\n       ioErrs.increment(1);\n       LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n                \" map outputs\", ie);\n \n       // If connect did not succeed, just mark all the maps as failed,\n       // indirectly penalizing the host\n       if (!connectSucceeded) {\n         for(TaskAttemptID left: remaining) {\n           scheduler.copyFailed(left, host, connectSucceeded);\n         }\n       } else {\n         // If we got a read error at this stage, it implies there was a problem\n         // with the first map, typically lost map. So, penalize only that map\n         // and add the rest\n         TaskAttemptID firstMap \u003d maps.get(0);\n         scheduler.copyFailed(firstMap, host, connectSucceeded);\n       }\n       \n       // Add back all the remaining maps, WITHOUT marking them as failed\n       for(TaskAttemptID left: remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n       \n       return;\n     }\n     \n     try {\n       // Loop through available map-outputs and fetch them\n       // On any error, good becomes false and we exit after putting back\n       // the remaining maps to the yet_to_be_fetched list\n       boolean good \u003d true;\n       while (!remaining.isEmpty() \u0026\u0026 good) {\n         good \u003d copyMapOutput(host, input, remaining);\n       }\n       \n       IOUtils.cleanup(LOG, input);\n       \n       // Sanity check\n       if (good \u0026\u0026 !remaining.isEmpty()) {\n         throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n             + remaining.size() + \" left.\");\n       }\n     } finally {\n       for (TaskAttemptID left : remaining) {\n         scheduler.putBackKnownMapOutput(host, left);\n       }\n     }\n       \n    }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n    for (TaskAttemptID tmp: maps) {\n      LOG.debug(tmp);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      HttpURLConnection connection \u003d (HttpURLConnection)url.openConnection();\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n\n      // Validate response code\n      int rc \u003d connection.getResponseCode();\n      if (rc !\u003d HttpURLConnection.HTTP_OK) {\n        throw new IOException(\n            \"Got invalid response code \" + rc + \" from \" + url +\n            \": \" + connection.getResponseMessage());\n      }\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, good becomes false and we exit after putting back\n      // the remaining maps to the yet_to_be_fetched list\n      boolean good \u003d true;\n      while (!remaining.isEmpty() \u0026\u0026 good) {\n        good \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (good \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n      \n   }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n    for (TaskAttemptID tmp: maps) {\n      LOG.debug(tmp);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      URLConnection connection \u003d url.openConnection();\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, good becomes false and we exit after putting back\n      // the remaining maps to the yet_to_be_fetched list\n      boolean good \u003d true;\n      while (!remaining.isEmpty() \u0026\u0026 good) {\n        good \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (good \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n      \n   }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n    for (TaskAttemptID tmp: maps) {\n      LOG.debug(tmp);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      URLConnection connection \u003d url.openConnection();\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, good becomes false and we exit after putting back\n      // the remaining maps to the yet_to_be_fetched list\n      boolean good \u003d true;\n      while (!remaining.isEmpty() \u0026\u0026 good) {\n        good \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (good \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n      \n   }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,98 @@\n+  private void copyFromHost(MapHost host) throws IOException {\n+    // Get completed maps on \u0027host\u0027\n+    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n+    \n+    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n+    // especially at the tail of large jobs\n+    if (maps.size() \u003d\u003d 0) {\n+      return;\n+    }\n+    \n+    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n+    for (TaskAttemptID tmp: maps) {\n+      LOG.debug(tmp);\n+    }\n+    \n+    // List of maps to be fetched yet\n+    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n+    \n+    // Construct the url and connect\n+    DataInputStream input;\n+    boolean connectSucceeded \u003d false;\n+    \n+    try {\n+      URL url \u003d getMapOutputURL(host, maps);\n+      URLConnection connection \u003d url.openConnection();\n+      \n+      // generate hash of the url\n+      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n+      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n+      \n+      // put url hash into http header\n+      connection.addRequestProperty(\n+          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n+      // set the read timeout\n+      connection.setReadTimeout(readTimeout);\n+      connect(connection, connectionTimeout);\n+      connectSucceeded \u003d true;\n+      input \u003d new DataInputStream(connection.getInputStream());\n+      \n+      // get the replyHash which is HMac of the encHash we sent to the server\n+      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n+      if(replyHash\u003d\u003dnull) {\n+        throw new IOException(\"security validation of TT Map output failed\");\n+      }\n+      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n+      // verify that replyHash is HMac of encHash\n+      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n+      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n+    } catch (IOException ie) {\n+      ioErrs.increment(1);\n+      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n+               \" map outputs\", ie);\n+\n+      // If connect did not succeed, just mark all the maps as failed,\n+      // indirectly penalizing the host\n+      if (!connectSucceeded) {\n+        for(TaskAttemptID left: remaining) {\n+          scheduler.copyFailed(left, host, connectSucceeded);\n+        }\n+      } else {\n+        // If we got a read error at this stage, it implies there was a problem\n+        // with the first map, typically lost map. So, penalize only that map\n+        // and add the rest\n+        TaskAttemptID firstMap \u003d maps.get(0);\n+        scheduler.copyFailed(firstMap, host, connectSucceeded);\n+      }\n+      \n+      // Add back all the remaining maps, WITHOUT marking them as failed\n+      for(TaskAttemptID left: remaining) {\n+        scheduler.putBackKnownMapOutput(host, left);\n+      }\n+      \n+      return;\n+    }\n+    \n+    try {\n+      // Loop through available map-outputs and fetch them\n+      // On any error, good becomes false and we exit after putting back\n+      // the remaining maps to the yet_to_be_fetched list\n+      boolean good \u003d true;\n+      while (!remaining.isEmpty() \u0026\u0026 good) {\n+        good \u003d copyMapOutput(host, input, remaining);\n+      }\n+      \n+      IOUtils.cleanup(LOG, input);\n+      \n+      // Sanity check\n+      if (good \u0026\u0026 !remaining.isEmpty()) {\n+        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n+            + remaining.size() + \" left.\");\n+      }\n+    } finally {\n+      for (TaskAttemptID left : remaining) {\n+        scheduler.putBackKnownMapOutput(host, left);\n+      }\n+    }\n+      \n+   }\n\\ No newline at end of file\n",
      "actualSource": "  private void copyFromHost(MapHost host) throws IOException {\n    // Get completed maps on \u0027host\u0027\n    List\u003cTaskAttemptID\u003e maps \u003d scheduler.getMapsForHost(host);\n    \n    // Sanity check to catch hosts with only \u0027OBSOLETE\u0027 maps, \n    // especially at the tail of large jobs\n    if (maps.size() \u003d\u003d 0) {\n      return;\n    }\n    \n    LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host);\n    for (TaskAttemptID tmp: maps) {\n      LOG.debug(tmp);\n    }\n    \n    // List of maps to be fetched yet\n    Set\u003cTaskAttemptID\u003e remaining \u003d new HashSet\u003cTaskAttemptID\u003e(maps);\n    \n    // Construct the url and connect\n    DataInputStream input;\n    boolean connectSucceeded \u003d false;\n    \n    try {\n      URL url \u003d getMapOutputURL(host, maps);\n      URLConnection connection \u003d url.openConnection();\n      \n      // generate hash of the url\n      String msgToEncode \u003d SecureShuffleUtils.buildMsgFrom(url);\n      String encHash \u003d SecureShuffleUtils.hashFromString(msgToEncode, jobTokenSecret);\n      \n      // put url hash into http header\n      connection.addRequestProperty(\n          SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\n      // set the read timeout\n      connection.setReadTimeout(readTimeout);\n      connect(connection, connectionTimeout);\n      connectSucceeded \u003d true;\n      input \u003d new DataInputStream(connection.getInputStream());\n      \n      // get the replyHash which is HMac of the encHash we sent to the server\n      String replyHash \u003d connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\n      if(replyHash\u003d\u003dnull) {\n        throw new IOException(\"security validation of TT Map output failed\");\n      }\n      LOG.debug(\"url\u003d\"+msgToEncode+\";encHash\u003d\"+encHash+\";replyHash\u003d\"+replyHash);\n      // verify that replyHash is HMac of encHash\n      SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n      LOG.info(\"for url\u003d\"+msgToEncode+\" sent hash and receievd reply\");\n    } catch (IOException ie) {\n      ioErrs.increment(1);\n      LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \n               \" map outputs\", ie);\n\n      // If connect did not succeed, just mark all the maps as failed,\n      // indirectly penalizing the host\n      if (!connectSucceeded) {\n        for(TaskAttemptID left: remaining) {\n          scheduler.copyFailed(left, host, connectSucceeded);\n        }\n      } else {\n        // If we got a read error at this stage, it implies there was a problem\n        // with the first map, typically lost map. So, penalize only that map\n        // and add the rest\n        TaskAttemptID firstMap \u003d maps.get(0);\n        scheduler.copyFailed(firstMap, host, connectSucceeded);\n      }\n      \n      // Add back all the remaining maps, WITHOUT marking them as failed\n      for(TaskAttemptID left: remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n      \n      return;\n    }\n    \n    try {\n      // Loop through available map-outputs and fetch them\n      // On any error, good becomes false and we exit after putting back\n      // the remaining maps to the yet_to_be_fetched list\n      boolean good \u003d true;\n      while (!remaining.isEmpty() \u0026\u0026 good) {\n        good \u003d copyMapOutput(host, input, remaining);\n      }\n      \n      IOUtils.cleanup(LOG, input);\n      \n      // Sanity check\n      if (good \u0026\u0026 !remaining.isEmpty()) {\n        throw new IOException(\"server didn\u0027t return all expected map outputs: \"\n            + remaining.size() + \" left.\");\n      }\n    } finally {\n      for (TaskAttemptID left : remaining) {\n        scheduler.putBackKnownMapOutput(host, left);\n      }\n    }\n      \n   }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java"
    }
  }
}