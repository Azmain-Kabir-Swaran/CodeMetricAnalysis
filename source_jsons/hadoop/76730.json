{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ByteBufferDecodingState.java",
  "functionName": "convertToByteArrayState",
  "functionId": "convertToByteArrayState",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java",
  "functionStartLine": 66,
  "functionEndLine": 91,
  "numCommitsSeen": 2,
  "timeTaken": 969,
  "changeHistory": [
    "77202fa1035a54496d11d07472fbc399148ff630"
  ],
  "changeHistoryShort": {
    "77202fa1035a54496d11d07472fbc399148ff630": "Yintroduced"
  },
  "changeHistoryDetails": {
    "77202fa1035a54496d11d07472fbc399148ff630": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
      "commitDate": "26/05/16 10:23 PM",
      "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
      "commitAuthor": "Kai Zheng",
      "diff": "@@ -0,0 +1,26 @@\n+  ByteArrayDecodingState convertToByteArrayState() {\n+    int[] inputOffsets \u003d new int[inputs.length];\n+    int[] outputOffsets \u003d new int[outputs.length];\n+    byte[][] newInputs \u003d new byte[inputs.length][];\n+    byte[][] newOutputs \u003d new byte[outputs.length][];\n+\n+    ByteBuffer buffer;\n+    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n+      buffer \u003d inputs[i];\n+      if (buffer !\u003d null) {\n+        inputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n+        newInputs[i] \u003d buffer.array();\n+      }\n+    }\n+\n+    for (int i \u003d 0; i \u003c outputs.length; ++i) {\n+      buffer \u003d outputs[i];\n+      outputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n+      newOutputs[i] \u003d buffer.array();\n+    }\n+\n+    ByteArrayDecodingState baeState \u003d new ByteArrayDecodingState(decoder,\n+        decodeLength, erasedIndexes, newInputs,\n+        inputOffsets, newOutputs, outputOffsets);\n+    return baeState;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  ByteArrayDecodingState convertToByteArrayState() {\n    int[] inputOffsets \u003d new int[inputs.length];\n    int[] outputOffsets \u003d new int[outputs.length];\n    byte[][] newInputs \u003d new byte[inputs.length][];\n    byte[][] newOutputs \u003d new byte[outputs.length][];\n\n    ByteBuffer buffer;\n    for (int i \u003d 0; i \u003c inputs.length; ++i) {\n      buffer \u003d inputs[i];\n      if (buffer !\u003d null) {\n        inputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n        newInputs[i] \u003d buffer.array();\n      }\n    }\n\n    for (int i \u003d 0; i \u003c outputs.length; ++i) {\n      buffer \u003d outputs[i];\n      outputOffsets[i] \u003d buffer.arrayOffset() + buffer.position();\n      newOutputs[i] \u003d buffer.array();\n    }\n\n    ByteArrayDecodingState baeState \u003d new ByteArrayDecodingState(decoder,\n        decodeLength, erasedIndexes, newInputs,\n        inputOffsets, newOutputs, outputOffsets);\n    return baeState;\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java"
    }
  }
}