{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WriteOperationHelper.java",
  "functionName": "newUploadPartRequest",
  "functionId": "newUploadPartRequest___destKey-String__uploadId-String__partNumber-int__size-int__uploadStream-InputStream__sourceFile-File__offset-Long",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java",
  "functionStartLine": 403,
  "functionEndLine": 456,
  "numCommitsSeen": 143,
  "timeTaken": 5771,
  "changeHistory": [
    "29b19cd59245c8809b697b3d7d7445813a685aad",
    "f365957c6326f88734bc0a5d01cfb7eac713db20",
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
    "dab00da19f25619ccc71c7f803a235b21766bf1e",
    "6c348c56918973fd988b110e79231324a8befe12"
  ],
  "changeHistoryShort": {
    "29b19cd59245c8809b697b3d7d7445813a685aad": "Ymultichange(Yexceptionschange,Ybodychange)",
    "f365957c6326f88734bc0a5d01cfb7eac713db20": "Ybodychange",
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "dab00da19f25619ccc71c7f803a235b21766bf1e": "Ymultichange(Yparameterchange,Ybodychange)",
    "6c348c56918973fd988b110e79231324a8befe12": "Yintroduced"
  },
  "changeHistoryDetails": {
    "29b19cd59245c8809b697b3d7d7445813a685aad": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HADOOP-16900. Very large files can be truncated when written through the S3A FileSystem.\n\nContributed by Mukund Thakur and Steve Loughran.\n\nThis patch ensures that writes to S3A fail when more than 10,000 blocks are\nwritten. That upper bound still exists. To write massive files, make sure\nthat the value of fs.s3a.multipart.size is set to a size which is large\nenough to upload the files in fewer than 10,000 blocks.\n\nChange-Id: Icec604e2a357ffd38d7ae7bc3f887ff55f2d721a\n",
      "commitDate": "20/05/20 5:42 AM",
      "commitName": "29b19cd59245c8809b697b3d7d7445813a685aad",
      "commitAuthor": "Mukund Thakur",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HADOOP-16900. Very large files can be truncated when written through the S3A FileSystem.\n\nContributed by Mukund Thakur and Steve Loughran.\n\nThis patch ensures that writes to S3A fail when more than 10,000 blocks are\nwritten. That upper bound still exists. To write massive files, make sure\nthat the value of fs.s3a.multipart.size is set to a size which is large\nenough to upload the files in fewer than 10,000 blocks.\n\nChange-Id: Icec604e2a357ffd38d7ae7bc3f887ff55f2d721a\n",
          "commitDate": "20/05/20 5:42 AM",
          "commitName": "29b19cd59245c8809b697b3d7d7445813a685aad",
          "commitAuthor": "Mukund Thakur",
          "commitDateOld": "12/11/19 7:31 AM",
          "commitNameOld": "f6697aa82b69af54dca6ac6b0eddacd0a74ede49",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 189.88,
          "commitsBetweenForRepo": 660,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,54 @@\n   public UploadPartRequest newUploadPartRequest(\n       String destKey,\n       String uploadId,\n       int partNumber,\n       int size,\n       InputStream uploadStream,\n       File sourceFile,\n-      Long offset) {\n+      Long offset) throws PathIOException {\n     checkNotNull(uploadId);\n     // exactly one source must be set; xor verifies this\n     checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n         \"Data source\");\n     checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n-    checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n-        \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n-        partNumber);\n+    checkArgument(partNumber \u003e 0,\n+        \"partNumber must be between 1 and %s inclusive, but is %s\",\n+            DEFAULT_UPLOAD_PART_COUNT_LIMIT, partNumber);\n \n     LOG.debug(\"Creating part upload request for {} #{} size {}\",\n         uploadId, partNumber, size);\n+    long partCountLimit \u003d longOption(conf,\n+        UPLOAD_PART_COUNT_LIMIT,\n+        DEFAULT_UPLOAD_PART_COUNT_LIMIT,\n+        1);\n+    if (partCountLimit !\u003d DEFAULT_UPLOAD_PART_COUNT_LIMIT) {\n+      LOG.warn(\"Configuration property {} shouldn\u0027t be overridden by client\",\n+              UPLOAD_PART_COUNT_LIMIT);\n+    }\n+    final String pathErrorMsg \u003d \"Number of parts in multipart upload exceeded.\"\n+        + \" Current part count \u003d %s, Part count limit \u003d %s \";\n+    if (partNumber \u003e partCountLimit) {\n+      throw new PathIOException(destKey,\n+          String.format(pathErrorMsg, partNumber, partCountLimit));\n+    }\n     UploadPartRequest request \u003d new UploadPartRequest()\n         .withBucketName(bucket)\n         .withKey(destKey)\n         .withUploadId(uploadId)\n         .withPartNumber(partNumber)\n         .withPartSize(size);\n     if (uploadStream !\u003d null) {\n       // there\u0027s an upload stream. Bind to it.\n       request.setInputStream(uploadStream);\n     } else {\n       checkArgument(sourceFile.exists(),\n           \"Source file does not exist: %s\", sourceFile);\n       checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n       long length \u003d sourceFile.length();\n       checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n           \"Offset %s beyond length of file %s\", offset, length);\n       request.setFile(sourceFile);\n       request.setFileOffset(offset);\n     }\n     return request;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public UploadPartRequest newUploadPartRequest(\n      String destKey,\n      String uploadId,\n      int partNumber,\n      int size,\n      InputStream uploadStream,\n      File sourceFile,\n      Long offset) throws PathIOException {\n    checkNotNull(uploadId);\n    // exactly one source must be set; xor verifies this\n    checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n        \"Data source\");\n    checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n    checkArgument(partNumber \u003e 0,\n        \"partNumber must be between 1 and %s inclusive, but is %s\",\n            DEFAULT_UPLOAD_PART_COUNT_LIMIT, partNumber);\n\n    LOG.debug(\"Creating part upload request for {} #{} size {}\",\n        uploadId, partNumber, size);\n    long partCountLimit \u003d longOption(conf,\n        UPLOAD_PART_COUNT_LIMIT,\n        DEFAULT_UPLOAD_PART_COUNT_LIMIT,\n        1);\n    if (partCountLimit !\u003d DEFAULT_UPLOAD_PART_COUNT_LIMIT) {\n      LOG.warn(\"Configuration property {} shouldn\u0027t be overridden by client\",\n              UPLOAD_PART_COUNT_LIMIT);\n    }\n    final String pathErrorMsg \u003d \"Number of parts in multipart upload exceeded.\"\n        + \" Current part count \u003d %s, Part count limit \u003d %s \";\n    if (partNumber \u003e partCountLimit) {\n      throw new PathIOException(destKey,\n          String.format(pathErrorMsg, partNumber, partCountLimit));\n    }\n    UploadPartRequest request \u003d new UploadPartRequest()\n        .withBucketName(bucket)\n        .withKey(destKey)\n        .withUploadId(uploadId)\n        .withPartNumber(partNumber)\n        .withPartSize(size);\n    if (uploadStream !\u003d null) {\n      // there\u0027s an upload stream. Bind to it.\n      request.setInputStream(uploadStream);\n    } else {\n      checkArgument(sourceFile.exists(),\n          \"Source file does not exist: %s\", sourceFile);\n      checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n      long length \u003d sourceFile.length();\n      checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n          \"Offset %s beyond length of file %s\", offset, length);\n      request.setFile(sourceFile);\n      request.setFileOffset(offset);\n    }\n    return request;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[PathIOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-16900. Very large files can be truncated when written through the S3A FileSystem.\n\nContributed by Mukund Thakur and Steve Loughran.\n\nThis patch ensures that writes to S3A fail when more than 10,000 blocks are\nwritten. That upper bound still exists. To write massive files, make sure\nthat the value of fs.s3a.multipart.size is set to a size which is large\nenough to upload the files in fewer than 10,000 blocks.\n\nChange-Id: Icec604e2a357ffd38d7ae7bc3f887ff55f2d721a\n",
          "commitDate": "20/05/20 5:42 AM",
          "commitName": "29b19cd59245c8809b697b3d7d7445813a685aad",
          "commitAuthor": "Mukund Thakur",
          "commitDateOld": "12/11/19 7:31 AM",
          "commitNameOld": "f6697aa82b69af54dca6ac6b0eddacd0a74ede49",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 189.88,
          "commitsBetweenForRepo": 660,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,54 @@\n   public UploadPartRequest newUploadPartRequest(\n       String destKey,\n       String uploadId,\n       int partNumber,\n       int size,\n       InputStream uploadStream,\n       File sourceFile,\n-      Long offset) {\n+      Long offset) throws PathIOException {\n     checkNotNull(uploadId);\n     // exactly one source must be set; xor verifies this\n     checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n         \"Data source\");\n     checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n-    checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n-        \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n-        partNumber);\n+    checkArgument(partNumber \u003e 0,\n+        \"partNumber must be between 1 and %s inclusive, but is %s\",\n+            DEFAULT_UPLOAD_PART_COUNT_LIMIT, partNumber);\n \n     LOG.debug(\"Creating part upload request for {} #{} size {}\",\n         uploadId, partNumber, size);\n+    long partCountLimit \u003d longOption(conf,\n+        UPLOAD_PART_COUNT_LIMIT,\n+        DEFAULT_UPLOAD_PART_COUNT_LIMIT,\n+        1);\n+    if (partCountLimit !\u003d DEFAULT_UPLOAD_PART_COUNT_LIMIT) {\n+      LOG.warn(\"Configuration property {} shouldn\u0027t be overridden by client\",\n+              UPLOAD_PART_COUNT_LIMIT);\n+    }\n+    final String pathErrorMsg \u003d \"Number of parts in multipart upload exceeded.\"\n+        + \" Current part count \u003d %s, Part count limit \u003d %s \";\n+    if (partNumber \u003e partCountLimit) {\n+      throw new PathIOException(destKey,\n+          String.format(pathErrorMsg, partNumber, partCountLimit));\n+    }\n     UploadPartRequest request \u003d new UploadPartRequest()\n         .withBucketName(bucket)\n         .withKey(destKey)\n         .withUploadId(uploadId)\n         .withPartNumber(partNumber)\n         .withPartSize(size);\n     if (uploadStream !\u003d null) {\n       // there\u0027s an upload stream. Bind to it.\n       request.setInputStream(uploadStream);\n     } else {\n       checkArgument(sourceFile.exists(),\n           \"Source file does not exist: %s\", sourceFile);\n       checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n       long length \u003d sourceFile.length();\n       checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n           \"Offset %s beyond length of file %s\", offset, length);\n       request.setFile(sourceFile);\n       request.setFileOffset(offset);\n     }\n     return request;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public UploadPartRequest newUploadPartRequest(\n      String destKey,\n      String uploadId,\n      int partNumber,\n      int size,\n      InputStream uploadStream,\n      File sourceFile,\n      Long offset) throws PathIOException {\n    checkNotNull(uploadId);\n    // exactly one source must be set; xor verifies this\n    checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n        \"Data source\");\n    checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n    checkArgument(partNumber \u003e 0,\n        \"partNumber must be between 1 and %s inclusive, but is %s\",\n            DEFAULT_UPLOAD_PART_COUNT_LIMIT, partNumber);\n\n    LOG.debug(\"Creating part upload request for {} #{} size {}\",\n        uploadId, partNumber, size);\n    long partCountLimit \u003d longOption(conf,\n        UPLOAD_PART_COUNT_LIMIT,\n        DEFAULT_UPLOAD_PART_COUNT_LIMIT,\n        1);\n    if (partCountLimit !\u003d DEFAULT_UPLOAD_PART_COUNT_LIMIT) {\n      LOG.warn(\"Configuration property {} shouldn\u0027t be overridden by client\",\n              UPLOAD_PART_COUNT_LIMIT);\n    }\n    final String pathErrorMsg \u003d \"Number of parts in multipart upload exceeded.\"\n        + \" Current part count \u003d %s, Part count limit \u003d %s \";\n    if (partNumber \u003e partCountLimit) {\n      throw new PathIOException(destKey,\n          String.format(pathErrorMsg, partNumber, partCountLimit));\n    }\n    UploadPartRequest request \u003d new UploadPartRequest()\n        .withBucketName(bucket)\n        .withKey(destKey)\n        .withUploadId(uploadId)\n        .withPartNumber(partNumber)\n        .withPartSize(size);\n    if (uploadStream !\u003d null) {\n      // there\u0027s an upload stream. Bind to it.\n      request.setInputStream(uploadStream);\n    } else {\n      checkArgument(sourceFile.exists(),\n          \"Source file does not exist: %s\", sourceFile);\n      checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n      long length \u003d sourceFile.length();\n      checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n          \"Offset %s beyond length of file %s\", offset, length);\n      request.setFile(sourceFile);\n      request.setFileOffset(offset);\n    }\n    return request;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "f365957c6326f88734bc0a5d01cfb7eac713db20": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15229. Add FileSystem builder-based openFile() API to match createFile();\nS3A to implement S3 Select through this API.\n\nThe new openFile() API is asynchronous, and implemented across FileSystem and FileContext.\n\nThe MapReduce V2 inputs are moved to this API, and you can actually set must/may\noptions to pass in.\n\nThis is more useful for setting things like s3a seek policy than for S3 select,\nas the existing input format/record readers can\u0027t handle S3 select output where\nthe stream is shorter than the file length, and splitting plain text is suboptimal.\nFuture work is needed there.\n\nIn the meantime, any/all filesystem connectors are now free to add their own filesystem-specific\nconfiguration parameters which can be set in jobs and used to set filesystem input stream\noptions (seek policy, retry, encryption secrets, etc).\n\nContributed by Steve Loughran\n",
      "commitDate": "05/02/19 3:51 AM",
      "commitName": "f365957c6326f88734bc0a5d01cfb7eac713db20",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "16/10/18 12:02 PM",
      "commitNameOld": "d59ca43bff8a457ce7ab62a61acd89aacbe71b93",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 111.7,
      "commitsBetweenForRepo": 781,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   public UploadPartRequest newUploadPartRequest(\n       String destKey,\n       String uploadId,\n       int partNumber,\n       int size,\n       InputStream uploadStream,\n       File sourceFile,\n       Long offset) {\n     checkNotNull(uploadId);\n     // exactly one source must be set; xor verifies this\n     checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n         \"Data source\");\n     checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n     checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n         \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n         partNumber);\n \n     LOG.debug(\"Creating part upload request for {} #{} size {}\",\n         uploadId, partNumber, size);\n     UploadPartRequest request \u003d new UploadPartRequest()\n-        .withBucketName(owner.getBucket())\n+        .withBucketName(bucket)\n         .withKey(destKey)\n         .withUploadId(uploadId)\n         .withPartNumber(partNumber)\n         .withPartSize(size);\n     if (uploadStream !\u003d null) {\n       // there\u0027s an upload stream. Bind to it.\n       request.setInputStream(uploadStream);\n     } else {\n       checkArgument(sourceFile.exists(),\n           \"Source file does not exist: %s\", sourceFile);\n       checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n       long length \u003d sourceFile.length();\n       checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n           \"Offset %s beyond length of file %s\", offset, length);\n       request.setFile(sourceFile);\n       request.setFileOffset(offset);\n     }\n     return request;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public UploadPartRequest newUploadPartRequest(\n      String destKey,\n      String uploadId,\n      int partNumber,\n      int size,\n      InputStream uploadStream,\n      File sourceFile,\n      Long offset) {\n    checkNotNull(uploadId);\n    // exactly one source must be set; xor verifies this\n    checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n        \"Data source\");\n    checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n    checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n        \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n        partNumber);\n\n    LOG.debug(\"Creating part upload request for {} #{} size {}\",\n        uploadId, partNumber, size);\n    UploadPartRequest request \u003d new UploadPartRequest()\n        .withBucketName(bucket)\n        .withKey(destKey)\n        .withUploadId(uploadId)\n        .withPartNumber(partNumber)\n        .withPartSize(size);\n    if (uploadStream !\u003d null) {\n      // there\u0027s an upload stream. Bind to it.\n      request.setInputStream(uploadStream);\n    } else {\n      checkArgument(sourceFile.exists(),\n          \"Source file does not exist: %s\", sourceFile);\n      checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n      long length \u003d sourceFile.length();\n      checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n          \"Offset %s beyond length of file %s\", offset, length);\n      request.setFile(sourceFile);\n      request.setFileOffset(offset);\n    }\n    return request;\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java",
      "extendedDetails": {}
    },
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HADOOP-13786 Add S3A committer for zero-rename commits to S3 endpoints.\nContributed by Steve Loughran and Ryan Blue.\n",
      "commitDate": "22/11/17 7:28 AM",
      "commitName": "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HADOOP-13786 Add S3A committer for zero-rename commits to S3 endpoints.\nContributed by Steve Loughran and Ryan Blue.\n",
          "commitDate": "22/11/17 7:28 AM",
          "commitName": "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "21/11/17 8:42 PM",
          "commitNameOld": "782ba3bf9da52699b27405a3f147464975d1df99",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,40 @@\n-    UploadPartRequest newUploadPartRequest(String uploadId,\n-        int partNumber, int size, InputStream uploadStream, File sourceFile) {\n-      Preconditions.checkNotNull(uploadId);\n-      // exactly one source must be set; xor verifies this\n-      Preconditions.checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n-          \"Data source\");\n-      Preconditions.checkArgument(size \u003e 0, \"Invalid partition size %s\", size);\n-      Preconditions.checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n-          \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n-          partNumber);\n+  public UploadPartRequest newUploadPartRequest(\n+      String destKey,\n+      String uploadId,\n+      int partNumber,\n+      int size,\n+      InputStream uploadStream,\n+      File sourceFile,\n+      Long offset) {\n+    checkNotNull(uploadId);\n+    // exactly one source must be set; xor verifies this\n+    checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n+        \"Data source\");\n+    checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n+    checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n+        \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n+        partNumber);\n \n-      LOG.debug(\"Creating part upload request for {} #{} size {}\",\n-          uploadId, partNumber, size);\n-      UploadPartRequest request \u003d new UploadPartRequest()\n-          .withBucketName(bucket)\n-          .withKey(key)\n-          .withUploadId(uploadId)\n-          .withPartNumber(partNumber)\n-          .withPartSize(size);\n-      if (uploadStream !\u003d null) {\n-        // there\u0027s an upload stream. Bind to it.\n-        request.setInputStream(uploadStream);\n-      } else {\n-        request.setFile(sourceFile);\n-      }\n-      return request;\n-    }\n\\ No newline at end of file\n+    LOG.debug(\"Creating part upload request for {} #{} size {}\",\n+        uploadId, partNumber, size);\n+    UploadPartRequest request \u003d new UploadPartRequest()\n+        .withBucketName(owner.getBucket())\n+        .withKey(destKey)\n+        .withUploadId(uploadId)\n+        .withPartNumber(partNumber)\n+        .withPartSize(size);\n+    if (uploadStream !\u003d null) {\n+      // there\u0027s an upload stream. Bind to it.\n+      request.setInputStream(uploadStream);\n+    } else {\n+      checkArgument(sourceFile.exists(),\n+          \"Source file does not exist: %s\", sourceFile);\n+      checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n+      long length \u003d sourceFile.length();\n+      checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n+          \"Offset %s beyond length of file %s\", offset, length);\n+      request.setFile(sourceFile);\n+      request.setFileOffset(offset);\n+    }\n+    return request;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public UploadPartRequest newUploadPartRequest(\n      String destKey,\n      String uploadId,\n      int partNumber,\n      int size,\n      InputStream uploadStream,\n      File sourceFile,\n      Long offset) {\n    checkNotNull(uploadId);\n    // exactly one source must be set; xor verifies this\n    checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n        \"Data source\");\n    checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n    checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n        \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n        partNumber);\n\n    LOG.debug(\"Creating part upload request for {} #{} size {}\",\n        uploadId, partNumber, size);\n    UploadPartRequest request \u003d new UploadPartRequest()\n        .withBucketName(owner.getBucket())\n        .withKey(destKey)\n        .withUploadId(uploadId)\n        .withPartNumber(partNumber)\n        .withPartSize(size);\n    if (uploadStream !\u003d null) {\n      // there\u0027s an upload stream. Bind to it.\n      request.setInputStream(uploadStream);\n    } else {\n      checkArgument(sourceFile.exists(),\n          \"Source file does not exist: %s\", sourceFile);\n      checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n      long length \u003d sourceFile.length();\n      checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n          \"Offset %s beyond length of file %s\", offset, length);\n      request.setFile(sourceFile);\n      request.setFileOffset(offset);\n    }\n    return request;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java",
          "extendedDetails": {
            "oldPath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
            "newPath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java",
            "oldMethodName": "newUploadPartRequest",
            "newMethodName": "newUploadPartRequest"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HADOOP-13786 Add S3A committer for zero-rename commits to S3 endpoints.\nContributed by Steve Loughran and Ryan Blue.\n",
          "commitDate": "22/11/17 7:28 AM",
          "commitName": "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "21/11/17 8:42 PM",
          "commitNameOld": "782ba3bf9da52699b27405a3f147464975d1df99",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,40 @@\n-    UploadPartRequest newUploadPartRequest(String uploadId,\n-        int partNumber, int size, InputStream uploadStream, File sourceFile) {\n-      Preconditions.checkNotNull(uploadId);\n-      // exactly one source must be set; xor verifies this\n-      Preconditions.checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n-          \"Data source\");\n-      Preconditions.checkArgument(size \u003e 0, \"Invalid partition size %s\", size);\n-      Preconditions.checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n-          \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n-          partNumber);\n+  public UploadPartRequest newUploadPartRequest(\n+      String destKey,\n+      String uploadId,\n+      int partNumber,\n+      int size,\n+      InputStream uploadStream,\n+      File sourceFile,\n+      Long offset) {\n+    checkNotNull(uploadId);\n+    // exactly one source must be set; xor verifies this\n+    checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n+        \"Data source\");\n+    checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n+    checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n+        \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n+        partNumber);\n \n-      LOG.debug(\"Creating part upload request for {} #{} size {}\",\n-          uploadId, partNumber, size);\n-      UploadPartRequest request \u003d new UploadPartRequest()\n-          .withBucketName(bucket)\n-          .withKey(key)\n-          .withUploadId(uploadId)\n-          .withPartNumber(partNumber)\n-          .withPartSize(size);\n-      if (uploadStream !\u003d null) {\n-        // there\u0027s an upload stream. Bind to it.\n-        request.setInputStream(uploadStream);\n-      } else {\n-        request.setFile(sourceFile);\n-      }\n-      return request;\n-    }\n\\ No newline at end of file\n+    LOG.debug(\"Creating part upload request for {} #{} size {}\",\n+        uploadId, partNumber, size);\n+    UploadPartRequest request \u003d new UploadPartRequest()\n+        .withBucketName(owner.getBucket())\n+        .withKey(destKey)\n+        .withUploadId(uploadId)\n+        .withPartNumber(partNumber)\n+        .withPartSize(size);\n+    if (uploadStream !\u003d null) {\n+      // there\u0027s an upload stream. Bind to it.\n+      request.setInputStream(uploadStream);\n+    } else {\n+      checkArgument(sourceFile.exists(),\n+          \"Source file does not exist: %s\", sourceFile);\n+      checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n+      long length \u003d sourceFile.length();\n+      checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n+          \"Offset %s beyond length of file %s\", offset, length);\n+      request.setFile(sourceFile);\n+      request.setFileOffset(offset);\n+    }\n+    return request;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public UploadPartRequest newUploadPartRequest(\n      String destKey,\n      String uploadId,\n      int partNumber,\n      int size,\n      InputStream uploadStream,\n      File sourceFile,\n      Long offset) {\n    checkNotNull(uploadId);\n    // exactly one source must be set; xor verifies this\n    checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n        \"Data source\");\n    checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n    checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n        \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n        partNumber);\n\n    LOG.debug(\"Creating part upload request for {} #{} size {}\",\n        uploadId, partNumber, size);\n    UploadPartRequest request \u003d new UploadPartRequest()\n        .withBucketName(owner.getBucket())\n        .withKey(destKey)\n        .withUploadId(uploadId)\n        .withPartNumber(partNumber)\n        .withPartSize(size);\n    if (uploadStream !\u003d null) {\n      // there\u0027s an upload stream. Bind to it.\n      request.setInputStream(uploadStream);\n    } else {\n      checkArgument(sourceFile.exists(),\n          \"Source file does not exist: %s\", sourceFile);\n      checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n      long length \u003d sourceFile.length();\n      checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n          \"Offset %s beyond length of file %s\", offset, length);\n      request.setFile(sourceFile);\n      request.setFileOffset(offset);\n    }\n    return request;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-13786 Add S3A committer for zero-rename commits to S3 endpoints.\nContributed by Steve Loughran and Ryan Blue.\n",
          "commitDate": "22/11/17 7:28 AM",
          "commitName": "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "21/11/17 8:42 PM",
          "commitNameOld": "782ba3bf9da52699b27405a3f147464975d1df99",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,40 @@\n-    UploadPartRequest newUploadPartRequest(String uploadId,\n-        int partNumber, int size, InputStream uploadStream, File sourceFile) {\n-      Preconditions.checkNotNull(uploadId);\n-      // exactly one source must be set; xor verifies this\n-      Preconditions.checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n-          \"Data source\");\n-      Preconditions.checkArgument(size \u003e 0, \"Invalid partition size %s\", size);\n-      Preconditions.checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n-          \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n-          partNumber);\n+  public UploadPartRequest newUploadPartRequest(\n+      String destKey,\n+      String uploadId,\n+      int partNumber,\n+      int size,\n+      InputStream uploadStream,\n+      File sourceFile,\n+      Long offset) {\n+    checkNotNull(uploadId);\n+    // exactly one source must be set; xor verifies this\n+    checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n+        \"Data source\");\n+    checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n+    checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n+        \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n+        partNumber);\n \n-      LOG.debug(\"Creating part upload request for {} #{} size {}\",\n-          uploadId, partNumber, size);\n-      UploadPartRequest request \u003d new UploadPartRequest()\n-          .withBucketName(bucket)\n-          .withKey(key)\n-          .withUploadId(uploadId)\n-          .withPartNumber(partNumber)\n-          .withPartSize(size);\n-      if (uploadStream !\u003d null) {\n-        // there\u0027s an upload stream. Bind to it.\n-        request.setInputStream(uploadStream);\n-      } else {\n-        request.setFile(sourceFile);\n-      }\n-      return request;\n-    }\n\\ No newline at end of file\n+    LOG.debug(\"Creating part upload request for {} #{} size {}\",\n+        uploadId, partNumber, size);\n+    UploadPartRequest request \u003d new UploadPartRequest()\n+        .withBucketName(owner.getBucket())\n+        .withKey(destKey)\n+        .withUploadId(uploadId)\n+        .withPartNumber(partNumber)\n+        .withPartSize(size);\n+    if (uploadStream !\u003d null) {\n+      // there\u0027s an upload stream. Bind to it.\n+      request.setInputStream(uploadStream);\n+    } else {\n+      checkArgument(sourceFile.exists(),\n+          \"Source file does not exist: %s\", sourceFile);\n+      checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n+      long length \u003d sourceFile.length();\n+      checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n+          \"Offset %s beyond length of file %s\", offset, length);\n+      request.setFile(sourceFile);\n+      request.setFileOffset(offset);\n+    }\n+    return request;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public UploadPartRequest newUploadPartRequest(\n      String destKey,\n      String uploadId,\n      int partNumber,\n      int size,\n      InputStream uploadStream,\n      File sourceFile,\n      Long offset) {\n    checkNotNull(uploadId);\n    // exactly one source must be set; xor verifies this\n    checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n        \"Data source\");\n    checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n    checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n        \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n        partNumber);\n\n    LOG.debug(\"Creating part upload request for {} #{} size {}\",\n        uploadId, partNumber, size);\n    UploadPartRequest request \u003d new UploadPartRequest()\n        .withBucketName(owner.getBucket())\n        .withKey(destKey)\n        .withUploadId(uploadId)\n        .withPartNumber(partNumber)\n        .withPartSize(size);\n    if (uploadStream !\u003d null) {\n      // there\u0027s an upload stream. Bind to it.\n      request.setInputStream(uploadStream);\n    } else {\n      checkArgument(sourceFile.exists(),\n          \"Source file does not exist: %s\", sourceFile);\n      checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n      long length \u003d sourceFile.length();\n      checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n          \"Offset %s beyond length of file %s\", offset, length);\n      request.setFile(sourceFile);\n      request.setFileOffset(offset);\n    }\n    return request;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-13786 Add S3A committer for zero-rename commits to S3 endpoints.\nContributed by Steve Loughran and Ryan Blue.\n",
          "commitDate": "22/11/17 7:28 AM",
          "commitName": "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "21/11/17 8:42 PM",
          "commitNameOld": "782ba3bf9da52699b27405a3f147464975d1df99",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,40 @@\n-    UploadPartRequest newUploadPartRequest(String uploadId,\n-        int partNumber, int size, InputStream uploadStream, File sourceFile) {\n-      Preconditions.checkNotNull(uploadId);\n-      // exactly one source must be set; xor verifies this\n-      Preconditions.checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n-          \"Data source\");\n-      Preconditions.checkArgument(size \u003e 0, \"Invalid partition size %s\", size);\n-      Preconditions.checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n-          \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n-          partNumber);\n+  public UploadPartRequest newUploadPartRequest(\n+      String destKey,\n+      String uploadId,\n+      int partNumber,\n+      int size,\n+      InputStream uploadStream,\n+      File sourceFile,\n+      Long offset) {\n+    checkNotNull(uploadId);\n+    // exactly one source must be set; xor verifies this\n+    checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n+        \"Data source\");\n+    checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n+    checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n+        \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n+        partNumber);\n \n-      LOG.debug(\"Creating part upload request for {} #{} size {}\",\n-          uploadId, partNumber, size);\n-      UploadPartRequest request \u003d new UploadPartRequest()\n-          .withBucketName(bucket)\n-          .withKey(key)\n-          .withUploadId(uploadId)\n-          .withPartNumber(partNumber)\n-          .withPartSize(size);\n-      if (uploadStream !\u003d null) {\n-        // there\u0027s an upload stream. Bind to it.\n-        request.setInputStream(uploadStream);\n-      } else {\n-        request.setFile(sourceFile);\n-      }\n-      return request;\n-    }\n\\ No newline at end of file\n+    LOG.debug(\"Creating part upload request for {} #{} size {}\",\n+        uploadId, partNumber, size);\n+    UploadPartRequest request \u003d new UploadPartRequest()\n+        .withBucketName(owner.getBucket())\n+        .withKey(destKey)\n+        .withUploadId(uploadId)\n+        .withPartNumber(partNumber)\n+        .withPartSize(size);\n+    if (uploadStream !\u003d null) {\n+      // there\u0027s an upload stream. Bind to it.\n+      request.setInputStream(uploadStream);\n+    } else {\n+      checkArgument(sourceFile.exists(),\n+          \"Source file does not exist: %s\", sourceFile);\n+      checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n+      long length \u003d sourceFile.length();\n+      checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n+          \"Offset %s beyond length of file %s\", offset, length);\n+      request.setFile(sourceFile);\n+      request.setFileOffset(offset);\n+    }\n+    return request;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public UploadPartRequest newUploadPartRequest(\n      String destKey,\n      String uploadId,\n      int partNumber,\n      int size,\n      InputStream uploadStream,\n      File sourceFile,\n      Long offset) {\n    checkNotNull(uploadId);\n    // exactly one source must be set; xor verifies this\n    checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n        \"Data source\");\n    checkArgument(size \u003e\u003d 0, \"Invalid partition size %s\", size);\n    checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n        \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n        partNumber);\n\n    LOG.debug(\"Creating part upload request for {} #{} size {}\",\n        uploadId, partNumber, size);\n    UploadPartRequest request \u003d new UploadPartRequest()\n        .withBucketName(owner.getBucket())\n        .withKey(destKey)\n        .withUploadId(uploadId)\n        .withPartNumber(partNumber)\n        .withPartSize(size);\n    if (uploadStream !\u003d null) {\n      // there\u0027s an upload stream. Bind to it.\n      request.setInputStream(uploadStream);\n    } else {\n      checkArgument(sourceFile.exists(),\n          \"Source file does not exist: %s\", sourceFile);\n      checkArgument(offset \u003e\u003d 0, \"Invalid offset %s\", offset);\n      long length \u003d sourceFile.length();\n      checkArgument(offset \u003d\u003d 0 || offset \u003c length,\n          \"Offset %s beyond length of file %s\", offset, length);\n      request.setFile(sourceFile);\n      request.setFileOffset(offset);\n    }\n    return request;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java",
          "extendedDetails": {
            "oldValue": "[uploadId-String, partNumber-int, size-int, uploadStream-InputStream, sourceFile-File]",
            "newValue": "[destKey-String, uploadId-String, partNumber-int, size-int, uploadStream-InputStream, sourceFile-File, offset-Long]"
          }
        }
      ]
    },
    "dab00da19f25619ccc71c7f803a235b21766bf1e": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-14028. S3A BlockOutputStreams doesn\u0027t delete temporary files in multipart uploads or handle part upload failures.\nContributed by Steve Loughran.\n\n(cherry picked from commit 29fe5af017b945d8750c074ca39031b5b777eddd)\n",
      "commitDate": "25/02/17 7:35 AM",
      "commitName": "dab00da19f25619ccc71c7f803a235b21766bf1e",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-14028. S3A BlockOutputStreams doesn\u0027t delete temporary files in multipart uploads or handle part upload failures.\nContributed by Steve Loughran.\n\n(cherry picked from commit 29fe5af017b945d8750c074ca39031b5b777eddd)\n",
          "commitDate": "25/02/17 7:35 AM",
          "commitName": "dab00da19f25619ccc71c7f803a235b21766bf1e",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "23/02/17 12:55 PM",
          "commitNameOld": "a4d4a23785356e6a19d0db3a2dec8ae8cf861273",
          "commitAuthorOld": "Mingliang Liu",
          "daysBetweenCommits": 1.78,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,27 @@\n     UploadPartRequest newUploadPartRequest(String uploadId,\n-        InputStream uploadStream,\n-        int partNumber,\n-        int size) {\n+        int partNumber, int size, InputStream uploadStream, File sourceFile) {\n       Preconditions.checkNotNull(uploadId);\n-      Preconditions.checkNotNull(uploadStream);\n+      // exactly one source must be set; xor verifies this\n+      Preconditions.checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n+          \"Data source\");\n       Preconditions.checkArgument(size \u003e 0, \"Invalid partition size %s\", size);\n-      Preconditions.checkArgument(partNumber\u003e 0 \u0026\u0026 partNumber \u003c\u003d10000,\n+      Preconditions.checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n           \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n           partNumber);\n \n       LOG.debug(\"Creating part upload request for {} #{} size {}\",\n           uploadId, partNumber, size);\n-      return new UploadPartRequest()\n+      UploadPartRequest request \u003d new UploadPartRequest()\n           .withBucketName(bucket)\n           .withKey(key)\n           .withUploadId(uploadId)\n-          .withInputStream(uploadStream)\n           .withPartNumber(partNumber)\n           .withPartSize(size);\n+      if (uploadStream !\u003d null) {\n+        // there\u0027s an upload stream. Bind to it.\n+        request.setInputStream(uploadStream);\n+      } else {\n+        request.setFile(sourceFile);\n+      }\n+      return request;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    UploadPartRequest newUploadPartRequest(String uploadId,\n        int partNumber, int size, InputStream uploadStream, File sourceFile) {\n      Preconditions.checkNotNull(uploadId);\n      // exactly one source must be set; xor verifies this\n      Preconditions.checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n          \"Data source\");\n      Preconditions.checkArgument(size \u003e 0, \"Invalid partition size %s\", size);\n      Preconditions.checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n          \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n          partNumber);\n\n      LOG.debug(\"Creating part upload request for {} #{} size {}\",\n          uploadId, partNumber, size);\n      UploadPartRequest request \u003d new UploadPartRequest()\n          .withBucketName(bucket)\n          .withKey(key)\n          .withUploadId(uploadId)\n          .withPartNumber(partNumber)\n          .withPartSize(size);\n      if (uploadStream !\u003d null) {\n        // there\u0027s an upload stream. Bind to it.\n        request.setInputStream(uploadStream);\n      } else {\n        request.setFile(sourceFile);\n      }\n      return request;\n    }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
          "extendedDetails": {
            "oldValue": "[uploadId-String, uploadStream-InputStream, partNumber-int, size-int]",
            "newValue": "[uploadId-String, partNumber-int, size-int, uploadStream-InputStream, sourceFile-File]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-14028. S3A BlockOutputStreams doesn\u0027t delete temporary files in multipart uploads or handle part upload failures.\nContributed by Steve Loughran.\n\n(cherry picked from commit 29fe5af017b945d8750c074ca39031b5b777eddd)\n",
          "commitDate": "25/02/17 7:35 AM",
          "commitName": "dab00da19f25619ccc71c7f803a235b21766bf1e",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "23/02/17 12:55 PM",
          "commitNameOld": "a4d4a23785356e6a19d0db3a2dec8ae8cf861273",
          "commitAuthorOld": "Mingliang Liu",
          "daysBetweenCommits": 1.78,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,27 @@\n     UploadPartRequest newUploadPartRequest(String uploadId,\n-        InputStream uploadStream,\n-        int partNumber,\n-        int size) {\n+        int partNumber, int size, InputStream uploadStream, File sourceFile) {\n       Preconditions.checkNotNull(uploadId);\n-      Preconditions.checkNotNull(uploadStream);\n+      // exactly one source must be set; xor verifies this\n+      Preconditions.checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n+          \"Data source\");\n       Preconditions.checkArgument(size \u003e 0, \"Invalid partition size %s\", size);\n-      Preconditions.checkArgument(partNumber\u003e 0 \u0026\u0026 partNumber \u003c\u003d10000,\n+      Preconditions.checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n           \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n           partNumber);\n \n       LOG.debug(\"Creating part upload request for {} #{} size {}\",\n           uploadId, partNumber, size);\n-      return new UploadPartRequest()\n+      UploadPartRequest request \u003d new UploadPartRequest()\n           .withBucketName(bucket)\n           .withKey(key)\n           .withUploadId(uploadId)\n-          .withInputStream(uploadStream)\n           .withPartNumber(partNumber)\n           .withPartSize(size);\n+      if (uploadStream !\u003d null) {\n+        // there\u0027s an upload stream. Bind to it.\n+        request.setInputStream(uploadStream);\n+      } else {\n+        request.setFile(sourceFile);\n+      }\n+      return request;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    UploadPartRequest newUploadPartRequest(String uploadId,\n        int partNumber, int size, InputStream uploadStream, File sourceFile) {\n      Preconditions.checkNotNull(uploadId);\n      // exactly one source must be set; xor verifies this\n      Preconditions.checkArgument((uploadStream !\u003d null) ^ (sourceFile !\u003d null),\n          \"Data source\");\n      Preconditions.checkArgument(size \u003e 0, \"Invalid partition size %s\", size);\n      Preconditions.checkArgument(partNumber \u003e 0 \u0026\u0026 partNumber \u003c\u003d 10000,\n          \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n          partNumber);\n\n      LOG.debug(\"Creating part upload request for {} #{} size {}\",\n          uploadId, partNumber, size);\n      UploadPartRequest request \u003d new UploadPartRequest()\n          .withBucketName(bucket)\n          .withKey(key)\n          .withUploadId(uploadId)\n          .withPartNumber(partNumber)\n          .withPartSize(size);\n      if (uploadStream !\u003d null) {\n        // there\u0027s an upload stream. Bind to it.\n        request.setInputStream(uploadStream);\n      } else {\n        request.setFile(sourceFile);\n      }\n      return request;\n    }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "6c348c56918973fd988b110e79231324a8befe12": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-13560. S3ABlockOutputStream to support huge (many GB) file writes. Contributed by Steve Loughran\n",
      "commitDate": "18/10/16 1:16 PM",
      "commitName": "6c348c56918973fd988b110e79231324a8befe12",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,21 @@\n+    UploadPartRequest newUploadPartRequest(String uploadId,\n+        InputStream uploadStream,\n+        int partNumber,\n+        int size) {\n+      Preconditions.checkNotNull(uploadId);\n+      Preconditions.checkNotNull(uploadStream);\n+      Preconditions.checkArgument(size \u003e 0, \"Invalid partition size %s\", size);\n+      Preconditions.checkArgument(partNumber\u003e 0 \u0026\u0026 partNumber \u003c\u003d10000,\n+          \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n+          partNumber);\n+\n+      LOG.debug(\"Creating part upload request for {} #{} size {}\",\n+          uploadId, partNumber, size);\n+      return new UploadPartRequest()\n+          .withBucketName(bucket)\n+          .withKey(key)\n+          .withUploadId(uploadId)\n+          .withInputStream(uploadStream)\n+          .withPartNumber(partNumber)\n+          .withPartSize(size);\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    UploadPartRequest newUploadPartRequest(String uploadId,\n        InputStream uploadStream,\n        int partNumber,\n        int size) {\n      Preconditions.checkNotNull(uploadId);\n      Preconditions.checkNotNull(uploadStream);\n      Preconditions.checkArgument(size \u003e 0, \"Invalid partition size %s\", size);\n      Preconditions.checkArgument(partNumber\u003e 0 \u0026\u0026 partNumber \u003c\u003d10000,\n          \"partNumber must be between 1 and 10000 inclusive, but is %s\",\n          partNumber);\n\n      LOG.debug(\"Creating part upload request for {} #{} size {}\",\n          uploadId, partNumber, size);\n      return new UploadPartRequest()\n          .withBucketName(bucket)\n          .withKey(key)\n          .withUploadId(uploadId)\n          .withInputStream(uploadStream)\n          .withPartNumber(partNumber)\n          .withPartSize(size);\n    }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java"
    }
  }
}