{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ReplicaCachingGetSpaceUsed.java",
  "functionName": "refresh",
  "functionId": "refresh",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaCachingGetSpaceUsed.java",
  "functionStartLine": 68,
  "functionEndLine": 108,
  "numCommitsSeen": 3,
  "timeTaken": 1649,
  "changeHistory": [
    "1c5d2f1fdc40b77731bc13973876b567865888d1",
    "a5bb1e8ee871df1111ff77d0f6921b13c8ffb50e"
  ],
  "changeHistoryShort": {
    "1c5d2f1fdc40b77731bc13973876b567865888d1": "Ybodychange",
    "a5bb1e8ee871df1111ff77d0f6921b13c8ffb50e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1c5d2f1fdc40b77731bc13973876b567865888d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15174. Optimize ReplicaCachingGetSpaceUsed by reducing unnecessary io operations. Contributed by Lisheng Sun.\n",
      "commitDate": "24/02/20 1:39 PM",
      "commitName": "1c5d2f1fdc40b77731bc13973876b567865888d1",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "27/11/19 6:43 PM",
      "commitNameOld": "2b452b4e6063072b2bec491edd3f412eb7ac21f3",
      "commitAuthorOld": "Yiqun Lin",
      "daysBetweenCommits": 88.79,
      "commitsBetweenForRepo": 289,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   protected void refresh() {\n     long start \u003d Time.monotonicNow();\n     long dfsUsed \u003d 0;\n     long count \u003d 0;\n \n     FsDatasetSpi fsDataset \u003d volume.getDataset();\n     try {\n       Collection\u003cReplicaInfo\u003e replicaInfos \u003d\n           (Collection\u003cReplicaInfo\u003e) fsDataset.deepCopyReplica(bpid);\n       long cost \u003d Time.monotonicNow() - start;\n       if (cost \u003e DEEP_COPY_REPLICA_THRESHOLD_MS) {\n         LOG.debug(\n             \"Copy replica infos, blockPoolId: {}, replicas size: {}, \"\n                 + \"duration: {}ms\",\n             bpid, replicaInfos.size(), Time.monotonicNow() - start);\n       }\n \n       if (CollectionUtils.isNotEmpty(replicaInfos)) {\n         for (ReplicaInfo replicaInfo : replicaInfos) {\n           if (Objects.equals(replicaInfo.getVolume().getStorageID(),\n               volume.getStorageID())) {\n-            dfsUsed +\u003d replicaInfo.getBlockDataLength();\n+            dfsUsed +\u003d replicaInfo.getBytesOnDisk();\n             dfsUsed +\u003d replicaInfo.getMetadataLength();\n             count++;\n           }\n         }\n       }\n \n       this.used.set(dfsUsed);\n       cost \u003d Time.monotonicNow() - start;\n       if (cost \u003e REPLICA_CACHING_GET_SPACE_USED_THRESHOLD_MS) {\n         LOG.debug(\n             \"Refresh dfs used, bpid: {}, replicas size: {}, dfsUsed: {} \"\n                 + \"on volume: {}, duration: {}ms\",\n             bpid, count, used, volume.getStorageID(),\n             Time.monotonicNow() - start);\n       }\n     } catch (Exception e) {\n       LOG.error(\"ReplicaCachingGetSpaceUsed refresh error\", e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void refresh() {\n    long start \u003d Time.monotonicNow();\n    long dfsUsed \u003d 0;\n    long count \u003d 0;\n\n    FsDatasetSpi fsDataset \u003d volume.getDataset();\n    try {\n      Collection\u003cReplicaInfo\u003e replicaInfos \u003d\n          (Collection\u003cReplicaInfo\u003e) fsDataset.deepCopyReplica(bpid);\n      long cost \u003d Time.monotonicNow() - start;\n      if (cost \u003e DEEP_COPY_REPLICA_THRESHOLD_MS) {\n        LOG.debug(\n            \"Copy replica infos, blockPoolId: {}, replicas size: {}, \"\n                + \"duration: {}ms\",\n            bpid, replicaInfos.size(), Time.monotonicNow() - start);\n      }\n\n      if (CollectionUtils.isNotEmpty(replicaInfos)) {\n        for (ReplicaInfo replicaInfo : replicaInfos) {\n          if (Objects.equals(replicaInfo.getVolume().getStorageID(),\n              volume.getStorageID())) {\n            dfsUsed +\u003d replicaInfo.getBytesOnDisk();\n            dfsUsed +\u003d replicaInfo.getMetadataLength();\n            count++;\n          }\n        }\n      }\n\n      this.used.set(dfsUsed);\n      cost \u003d Time.monotonicNow() - start;\n      if (cost \u003e REPLICA_CACHING_GET_SPACE_USED_THRESHOLD_MS) {\n        LOG.debug(\n            \"Refresh dfs used, bpid: {}, replicas size: {}, dfsUsed: {} \"\n                + \"on volume: {}, duration: {}ms\",\n            bpid, count, used, volume.getStorageID(),\n            Time.monotonicNow() - start);\n      }\n    } catch (Exception e) {\n      LOG.error(\"ReplicaCachingGetSpaceUsed refresh error\", e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaCachingGetSpaceUsed.java",
      "extendedDetails": {}
    },
    "a5bb1e8ee871df1111ff77d0f6921b13c8ffb50e": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-14313. Get hdfs used space from FsDatasetImpl#volumeMap#ReplicaInfo in memory instead of df/du. Contributed by Lisheng Sun.\n",
      "commitDate": "06/08/19 7:18 PM",
      "commitName": "a5bb1e8ee871df1111ff77d0f6921b13c8ffb50e",
      "commitAuthor": "Yiqun Lin",
      "diff": "@@ -0,0 +1,41 @@\n+  protected void refresh() {\n+    long start \u003d Time.monotonicNow();\n+    long dfsUsed \u003d 0;\n+    long count \u003d 0;\n+\n+    FsDatasetSpi fsDataset \u003d volume.getDataset();\n+    try {\n+      Collection\u003cReplicaInfo\u003e replicaInfos \u003d\n+          (Collection\u003cReplicaInfo\u003e) fsDataset.deepCopyReplica(bpid);\n+      long cost \u003d Time.monotonicNow() - start;\n+      if (cost \u003e DEEP_COPY_REPLICA_THRESHOLD_MS) {\n+        LOG.debug(\n+            \"Copy replica infos, blockPoolId: {}, replicas size: {}, \"\n+                + \"duration: {}ms\",\n+            bpid, replicaInfos.size(), Time.monotonicNow() - start);\n+      }\n+\n+      if (CollectionUtils.isNotEmpty(replicaInfos)) {\n+        for (ReplicaInfo replicaInfo : replicaInfos) {\n+          if (Objects.equals(replicaInfo.getVolume().getStorageID(),\n+              volume.getStorageID())) {\n+            dfsUsed +\u003d replicaInfo.getBlockDataLength();\n+            dfsUsed +\u003d replicaInfo.getMetadataLength();\n+            count++;\n+          }\n+        }\n+      }\n+\n+      this.used.set(dfsUsed);\n+      cost \u003d Time.monotonicNow() - start;\n+      if (cost \u003e REPLICA_CACHING_GET_SPACE_USED_THRESHOLD_MS) {\n+        LOG.debug(\n+            \"Refresh dfs used, bpid: {}, replicas size: {}, dfsUsed: {} \"\n+                + \"on volume: {}, duration: {}ms\",\n+            bpid, count, used, volume.getStorageID(),\n+            Time.monotonicNow() - start);\n+      }\n+    } catch (Exception e) {\n+      LOG.error(\"ReplicaCachingGetSpaceUsed refresh error\", e);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void refresh() {\n    long start \u003d Time.monotonicNow();\n    long dfsUsed \u003d 0;\n    long count \u003d 0;\n\n    FsDatasetSpi fsDataset \u003d volume.getDataset();\n    try {\n      Collection\u003cReplicaInfo\u003e replicaInfos \u003d\n          (Collection\u003cReplicaInfo\u003e) fsDataset.deepCopyReplica(bpid);\n      long cost \u003d Time.monotonicNow() - start;\n      if (cost \u003e DEEP_COPY_REPLICA_THRESHOLD_MS) {\n        LOG.debug(\n            \"Copy replica infos, blockPoolId: {}, replicas size: {}, \"\n                + \"duration: {}ms\",\n            bpid, replicaInfos.size(), Time.monotonicNow() - start);\n      }\n\n      if (CollectionUtils.isNotEmpty(replicaInfos)) {\n        for (ReplicaInfo replicaInfo : replicaInfos) {\n          if (Objects.equals(replicaInfo.getVolume().getStorageID(),\n              volume.getStorageID())) {\n            dfsUsed +\u003d replicaInfo.getBlockDataLength();\n            dfsUsed +\u003d replicaInfo.getMetadataLength();\n            count++;\n          }\n        }\n      }\n\n      this.used.set(dfsUsed);\n      cost \u003d Time.monotonicNow() - start;\n      if (cost \u003e REPLICA_CACHING_GET_SPACE_USED_THRESHOLD_MS) {\n        LOG.debug(\n            \"Refresh dfs used, bpid: {}, replicas size: {}, dfsUsed: {} \"\n                + \"on volume: {}, duration: {}ms\",\n            bpid, count, used, volume.getStorageID(),\n            Time.monotonicNow() - start);\n      }\n    } catch (Exception e) {\n      LOG.error(\"ReplicaCachingGetSpaceUsed refresh error\", e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaCachingGetSpaceUsed.java"
    }
  }
}