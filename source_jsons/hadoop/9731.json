{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Storage.java",
  "functionName": "writeProperties",
  "functionId": "writeProperties___to-File__sd-StorageDirectory",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java",
  "functionStartLine": 1237,
  "functionEndLine": 1244,
  "numCommitsSeen": 58,
  "timeTaken": 6557,
  "changeHistory": [
    "b668eb91556b8c85c2b4925808ccb1f769031c20",
    "1403b84b122fb76ef2b085a728b5402c32499c1f",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "ffbe9e5972bf3eee9037e2602c1330e0dc744646",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "b668eb91556b8c85c2b4925808ccb1f769031c20": "Ybodychange",
    "1403b84b122fb76ef2b085a728b5402c32499c1f": "Ybodychange",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "ffbe9e5972bf3eee9037e2602c1330e0dc744646": "Ymultichange(Ymovefromfile,Ybodychange,Yrename,Yparameterchange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b668eb91556b8c85c2b4925808ccb1f769031c20": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10675. Datanode support to read from external stores.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "b668eb91556b8c85c2b4925808ccb1f769031c20",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "06/04/17 2:33 PM",
      "commitNameOld": "a49fac5302128a6f5d971f5818d0fd874c3932e3",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 253.18,
      "commitsBetweenForRepo": 1749,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,8 @@\n   public void writeProperties(File to, StorageDirectory sd) throws IOException {\n+    if (to \u003d\u003d null) {\n+      return;\n+    }\n     Properties props \u003d new Properties();\n     setPropertiesFromFields(props, sd);\n     writeProperties(to, props);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n    if (to \u003d\u003d null) {\n      return;\n    }\n    Properties props \u003d new Properties();\n    setPropertiesFromFields(props, sd);\n    writeProperties(to, props);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java",
      "extendedDetails": {}
    },
    "1403b84b122fb76ef2b085a728b5402c32499c1f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8546. Use try with resources in DataStorage and Storage.\n",
      "commitDate": "25/06/15 5:50 PM",
      "commitName": "1403b84b122fb76ef2b085a728b5402c32499c1f",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "10/03/15 6:20 PM",
      "commitNameOld": "5c1036d598051cf6af595740f1ab82092b0b6554",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 106.98,
      "commitsBetweenForRepo": 951,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,5 +1,5 @@\n   public void writeProperties(File to, StorageDirectory sd) throws IOException {\n     Properties props \u003d new Properties();\n     setPropertiesFromFields(props, sd);\n-    writeProperties(to, sd, props);\n+    writeProperties(to, props);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n    Properties props \u003d new Properties();\n    setPropertiesFromFields(props, sd);\n    writeProperties(to, props);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java",
      "extendedDetails": {}
    },
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5138. Support HDFS upgrade in HA. Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1561381 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/14 12:01 PM",
      "commitName": "edb6dc5f303093c2604cd07b0c0dacf12dbce5de",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "22/11/13 12:07 PM",
      "commitNameOld": "97acde2d33967f7f870f7dfe96c6b558e6fe324b",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 64.0,
      "commitsBetweenForRepo": 297,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,5 @@\n   public void writeProperties(File to, StorageDirectory sd) throws IOException {\n     Properties props \u003d new Properties();\n     setPropertiesFromFields(props, sd);\n-    RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n-    FileOutputStream out \u003d null;\n-    try {\n-      file.seek(0);\n-      out \u003d new FileOutputStream(file.getFD());\n-      /*\n-       * If server is interrupted before this line, \n-       * the version file will remain unchanged.\n-       */\n-      props.store(out, null);\n-      /*\n-       * Now the new fields are flushed to the head of the file, but file \n-       * length can still be larger then required and therefore the file can \n-       * contain whole or corrupted fields from its old contents in the end.\n-       * If server is interrupted here and restarted later these extra fields\n-       * either should not effect server behavior or should be handled\n-       * by the server correctly.\n-       */\n-      file.setLength(out.getChannel().position());\n-    } finally {\n-      if (out !\u003d null) {\n-        out.close();\n-      }\n-      file.close();\n-    }\n+    writeProperties(to, sd, props);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n    Properties props \u003d new Properties();\n    setPropertiesFromFields(props, sd);\n    writeProperties(to, sd, props);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n    Properties props \u003d new Properties();\n    setPropertiesFromFields(props, sd);\n    RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n    FileOutputStream out \u003d null;\n    try {\n      file.seek(0);\n      out \u003d new FileOutputStream(file.getFD());\n      /*\n       * If server is interrupted before this line, \n       * the version file will remain unchanged.\n       */\n      props.store(out, null);\n      /*\n       * Now the new fields are flushed to the head of the file, but file \n       * length can still be larger then required and therefore the file can \n       * contain whole or corrupted fields from its old contents in the end.\n       * If server is interrupted here and restarted later these extra fields\n       * either should not effect server behavior or should be handled\n       * by the server correctly.\n       */\n      file.setLength(out.getChannel().position());\n    } finally {\n      if (out !\u003d null) {\n        out.close();\n      }\n      file.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n    Properties props \u003d new Properties();\n    setPropertiesFromFields(props, sd);\n    RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n    FileOutputStream out \u003d null;\n    try {\n      file.seek(0);\n      out \u003d new FileOutputStream(file.getFD());\n      /*\n       * If server is interrupted before this line, \n       * the version file will remain unchanged.\n       */\n      props.store(out, null);\n      /*\n       * Now the new fields are flushed to the head of the file, but file \n       * length can still be larger then required and therefore the file can \n       * contain whole or corrupted fields from its old contents in the end.\n       * If server is interrupted here and restarted later these extra fields\n       * either should not effect server behavior or should be handled\n       * by the server correctly.\n       */\n      file.setLength(out.getChannel().position());\n    } finally {\n      if (out !\u003d null) {\n        out.close();\n      }\n      file.close();\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/common/Storage.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java"
      }
    },
    "ffbe9e5972bf3eee9037e2602c1330e0dc744646": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "HDFS-2195. Refactor StorageDirectory to not be an non-static inner class. Contributed by Todd Lipcon\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151707 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/07/11 8:19 PM",
      "commitName": "ffbe9e5972bf3eee9037e2602c1330e0dc744646",
      "commitAuthor": "Eli Collins",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-2195. Refactor StorageDirectory to not be an non-static inner class. Contributed by Todd Lipcon\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151707 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/07/11 8:19 PM",
          "commitName": "ffbe9e5972bf3eee9037e2602c1330e0dc744646",
          "commitAuthor": "Eli Collins",
          "commitDateOld": "27/07/11 4:28 PM",
          "commitNameOld": "6f1ef980ded061dd75d3368b0fc2fbbed14eea9f",
          "commitAuthorOld": "Konstantin Shvachko",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,29 @@\n-    public void write(File to) throws IOException {\n-      Properties props \u003d new Properties();\n-      setFields(props, this);\n-      RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n-      FileOutputStream out \u003d null;\n-      try {\n-        file.seek(0);\n-        out \u003d new FileOutputStream(file.getFD());\n-        /*\n-         * If server is interrupted before this line, \n-         * the version file will remain unchanged.\n-         */\n-        props.store(out, null);\n-        /*\n-         * Now the new fields are flushed to the head of the file, but file \n-         * length can still be larger then required and therefore the file can \n-         * contain whole or corrupted fields from its old contents in the end.\n-         * If server is interrupted here and restarted later these extra fields\n-         * either should not effect server behavior or should be handled\n-         * by the server correctly.\n-         */\n-        file.setLength(out.getChannel().position());\n-      } finally {\n-        if (out !\u003d null) {\n-          out.close();\n-        }\n-        file.close();\n+  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n+    Properties props \u003d new Properties();\n+    setPropertiesFromFields(props, sd);\n+    RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n+    FileOutputStream out \u003d null;\n+    try {\n+      file.seek(0);\n+      out \u003d new FileOutputStream(file.getFD());\n+      /*\n+       * If server is interrupted before this line, \n+       * the version file will remain unchanged.\n+       */\n+      props.store(out, null);\n+      /*\n+       * Now the new fields are flushed to the head of the file, but file \n+       * length can still be larger then required and therefore the file can \n+       * contain whole or corrupted fields from its old contents in the end.\n+       * If server is interrupted here and restarted later these extra fields\n+       * either should not effect server behavior or should be handled\n+       * by the server correctly.\n+       */\n+      file.setLength(out.getChannel().position());\n+    } finally {\n+      if (out !\u003d null) {\n+        out.close();\n       }\n-    }\n\\ No newline at end of file\n+      file.close();\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n    Properties props \u003d new Properties();\n    setPropertiesFromFields(props, sd);\n    RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n    FileOutputStream out \u003d null;\n    try {\n      file.seek(0);\n      out \u003d new FileOutputStream(file.getFD());\n      /*\n       * If server is interrupted before this line, \n       * the version file will remain unchanged.\n       */\n      props.store(out, null);\n      /*\n       * Now the new fields are flushed to the head of the file, but file \n       * length can still be larger then required and therefore the file can \n       * contain whole or corrupted fields from its old contents in the end.\n       * If server is interrupted here and restarted later these extra fields\n       * either should not effect server behavior or should be handled\n       * by the server correctly.\n       */\n      file.setLength(out.getChannel().position());\n    } finally {\n      if (out !\u003d null) {\n        out.close();\n      }\n      file.close();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/common/Storage.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/common/Storage.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/common/Storage.java",
            "oldMethodName": "write",
            "newMethodName": "writeProperties"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2195. Refactor StorageDirectory to not be an non-static inner class. Contributed by Todd Lipcon\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151707 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/07/11 8:19 PM",
          "commitName": "ffbe9e5972bf3eee9037e2602c1330e0dc744646",
          "commitAuthor": "Eli Collins",
          "commitDateOld": "27/07/11 4:28 PM",
          "commitNameOld": "6f1ef980ded061dd75d3368b0fc2fbbed14eea9f",
          "commitAuthorOld": "Konstantin Shvachko",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,29 @@\n-    public void write(File to) throws IOException {\n-      Properties props \u003d new Properties();\n-      setFields(props, this);\n-      RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n-      FileOutputStream out \u003d null;\n-      try {\n-        file.seek(0);\n-        out \u003d new FileOutputStream(file.getFD());\n-        /*\n-         * If server is interrupted before this line, \n-         * the version file will remain unchanged.\n-         */\n-        props.store(out, null);\n-        /*\n-         * Now the new fields are flushed to the head of the file, but file \n-         * length can still be larger then required and therefore the file can \n-         * contain whole or corrupted fields from its old contents in the end.\n-         * If server is interrupted here and restarted later these extra fields\n-         * either should not effect server behavior or should be handled\n-         * by the server correctly.\n-         */\n-        file.setLength(out.getChannel().position());\n-      } finally {\n-        if (out !\u003d null) {\n-          out.close();\n-        }\n-        file.close();\n+  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n+    Properties props \u003d new Properties();\n+    setPropertiesFromFields(props, sd);\n+    RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n+    FileOutputStream out \u003d null;\n+    try {\n+      file.seek(0);\n+      out \u003d new FileOutputStream(file.getFD());\n+      /*\n+       * If server is interrupted before this line, \n+       * the version file will remain unchanged.\n+       */\n+      props.store(out, null);\n+      /*\n+       * Now the new fields are flushed to the head of the file, but file \n+       * length can still be larger then required and therefore the file can \n+       * contain whole or corrupted fields from its old contents in the end.\n+       * If server is interrupted here and restarted later these extra fields\n+       * either should not effect server behavior or should be handled\n+       * by the server correctly.\n+       */\n+      file.setLength(out.getChannel().position());\n+    } finally {\n+      if (out !\u003d null) {\n+        out.close();\n       }\n-    }\n\\ No newline at end of file\n+      file.close();\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n    Properties props \u003d new Properties();\n    setPropertiesFromFields(props, sd);\n    RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n    FileOutputStream out \u003d null;\n    try {\n      file.seek(0);\n      out \u003d new FileOutputStream(file.getFD());\n      /*\n       * If server is interrupted before this line, \n       * the version file will remain unchanged.\n       */\n      props.store(out, null);\n      /*\n       * Now the new fields are flushed to the head of the file, but file \n       * length can still be larger then required and therefore the file can \n       * contain whole or corrupted fields from its old contents in the end.\n       * If server is interrupted here and restarted later these extra fields\n       * either should not effect server behavior or should be handled\n       * by the server correctly.\n       */\n      file.setLength(out.getChannel().position());\n    } finally {\n      if (out !\u003d null) {\n        out.close();\n      }\n      file.close();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/common/Storage.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "HDFS-2195. Refactor StorageDirectory to not be an non-static inner class. Contributed by Todd Lipcon\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151707 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/07/11 8:19 PM",
          "commitName": "ffbe9e5972bf3eee9037e2602c1330e0dc744646",
          "commitAuthor": "Eli Collins",
          "commitDateOld": "27/07/11 4:28 PM",
          "commitNameOld": "6f1ef980ded061dd75d3368b0fc2fbbed14eea9f",
          "commitAuthorOld": "Konstantin Shvachko",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,29 @@\n-    public void write(File to) throws IOException {\n-      Properties props \u003d new Properties();\n-      setFields(props, this);\n-      RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n-      FileOutputStream out \u003d null;\n-      try {\n-        file.seek(0);\n-        out \u003d new FileOutputStream(file.getFD());\n-        /*\n-         * If server is interrupted before this line, \n-         * the version file will remain unchanged.\n-         */\n-        props.store(out, null);\n-        /*\n-         * Now the new fields are flushed to the head of the file, but file \n-         * length can still be larger then required and therefore the file can \n-         * contain whole or corrupted fields from its old contents in the end.\n-         * If server is interrupted here and restarted later these extra fields\n-         * either should not effect server behavior or should be handled\n-         * by the server correctly.\n-         */\n-        file.setLength(out.getChannel().position());\n-      } finally {\n-        if (out !\u003d null) {\n-          out.close();\n-        }\n-        file.close();\n+  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n+    Properties props \u003d new Properties();\n+    setPropertiesFromFields(props, sd);\n+    RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n+    FileOutputStream out \u003d null;\n+    try {\n+      file.seek(0);\n+      out \u003d new FileOutputStream(file.getFD());\n+      /*\n+       * If server is interrupted before this line, \n+       * the version file will remain unchanged.\n+       */\n+      props.store(out, null);\n+      /*\n+       * Now the new fields are flushed to the head of the file, but file \n+       * length can still be larger then required and therefore the file can \n+       * contain whole or corrupted fields from its old contents in the end.\n+       * If server is interrupted here and restarted later these extra fields\n+       * either should not effect server behavior or should be handled\n+       * by the server correctly.\n+       */\n+      file.setLength(out.getChannel().position());\n+    } finally {\n+      if (out !\u003d null) {\n+        out.close();\n       }\n-    }\n\\ No newline at end of file\n+      file.close();\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n    Properties props \u003d new Properties();\n    setPropertiesFromFields(props, sd);\n    RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n    FileOutputStream out \u003d null;\n    try {\n      file.seek(0);\n      out \u003d new FileOutputStream(file.getFD());\n      /*\n       * If server is interrupted before this line, \n       * the version file will remain unchanged.\n       */\n      props.store(out, null);\n      /*\n       * Now the new fields are flushed to the head of the file, but file \n       * length can still be larger then required and therefore the file can \n       * contain whole or corrupted fields from its old contents in the end.\n       * If server is interrupted here and restarted later these extra fields\n       * either should not effect server behavior or should be handled\n       * by the server correctly.\n       */\n      file.setLength(out.getChannel().position());\n    } finally {\n      if (out !\u003d null) {\n        out.close();\n      }\n      file.close();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/common/Storage.java",
          "extendedDetails": {
            "oldValue": "write",
            "newValue": "writeProperties"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2195. Refactor StorageDirectory to not be an non-static inner class. Contributed by Todd Lipcon\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151707 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/07/11 8:19 PM",
          "commitName": "ffbe9e5972bf3eee9037e2602c1330e0dc744646",
          "commitAuthor": "Eli Collins",
          "commitDateOld": "27/07/11 4:28 PM",
          "commitNameOld": "6f1ef980ded061dd75d3368b0fc2fbbed14eea9f",
          "commitAuthorOld": "Konstantin Shvachko",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,29 @@\n-    public void write(File to) throws IOException {\n-      Properties props \u003d new Properties();\n-      setFields(props, this);\n-      RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n-      FileOutputStream out \u003d null;\n-      try {\n-        file.seek(0);\n-        out \u003d new FileOutputStream(file.getFD());\n-        /*\n-         * If server is interrupted before this line, \n-         * the version file will remain unchanged.\n-         */\n-        props.store(out, null);\n-        /*\n-         * Now the new fields are flushed to the head of the file, but file \n-         * length can still be larger then required and therefore the file can \n-         * contain whole or corrupted fields from its old contents in the end.\n-         * If server is interrupted here and restarted later these extra fields\n-         * either should not effect server behavior or should be handled\n-         * by the server correctly.\n-         */\n-        file.setLength(out.getChannel().position());\n-      } finally {\n-        if (out !\u003d null) {\n-          out.close();\n-        }\n-        file.close();\n+  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n+    Properties props \u003d new Properties();\n+    setPropertiesFromFields(props, sd);\n+    RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n+    FileOutputStream out \u003d null;\n+    try {\n+      file.seek(0);\n+      out \u003d new FileOutputStream(file.getFD());\n+      /*\n+       * If server is interrupted before this line, \n+       * the version file will remain unchanged.\n+       */\n+      props.store(out, null);\n+      /*\n+       * Now the new fields are flushed to the head of the file, but file \n+       * length can still be larger then required and therefore the file can \n+       * contain whole or corrupted fields from its old contents in the end.\n+       * If server is interrupted here and restarted later these extra fields\n+       * either should not effect server behavior or should be handled\n+       * by the server correctly.\n+       */\n+      file.setLength(out.getChannel().position());\n+    } finally {\n+      if (out !\u003d null) {\n+        out.close();\n       }\n-    }\n\\ No newline at end of file\n+      file.close();\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public void writeProperties(File to, StorageDirectory sd) throws IOException {\n    Properties props \u003d new Properties();\n    setPropertiesFromFields(props, sd);\n    RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n    FileOutputStream out \u003d null;\n    try {\n      file.seek(0);\n      out \u003d new FileOutputStream(file.getFD());\n      /*\n       * If server is interrupted before this line, \n       * the version file will remain unchanged.\n       */\n      props.store(out, null);\n      /*\n       * Now the new fields are flushed to the head of the file, but file \n       * length can still be larger then required and therefore the file can \n       * contain whole or corrupted fields from its old contents in the end.\n       * If server is interrupted here and restarted later these extra fields\n       * either should not effect server behavior or should be handled\n       * by the server correctly.\n       */\n      file.setLength(out.getChannel().position());\n    } finally {\n      if (out !\u003d null) {\n        out.close();\n      }\n      file.close();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/common/Storage.java",
          "extendedDetails": {
            "oldValue": "[to-File]",
            "newValue": "[to-File, sd-StorageDirectory]"
          }
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,29 @@\n+    public void write(File to) throws IOException {\n+      Properties props \u003d new Properties();\n+      setFields(props, this);\n+      RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n+      FileOutputStream out \u003d null;\n+      try {\n+        file.seek(0);\n+        out \u003d new FileOutputStream(file.getFD());\n+        /*\n+         * If server is interrupted before this line, \n+         * the version file will remain unchanged.\n+         */\n+        props.store(out, null);\n+        /*\n+         * Now the new fields are flushed to the head of the file, but file \n+         * length can still be larger then required and therefore the file can \n+         * contain whole or corrupted fields from its old contents in the end.\n+         * If server is interrupted here and restarted later these extra fields\n+         * either should not effect server behavior or should be handled\n+         * by the server correctly.\n+         */\n+        file.setLength(out.getChannel().position());\n+      } finally {\n+        if (out !\u003d null) {\n+          out.close();\n+        }\n+        file.close();\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public void write(File to) throws IOException {\n      Properties props \u003d new Properties();\n      setFields(props, this);\n      RandomAccessFile file \u003d new RandomAccessFile(to, \"rws\");\n      FileOutputStream out \u003d null;\n      try {\n        file.seek(0);\n        out \u003d new FileOutputStream(file.getFD());\n        /*\n         * If server is interrupted before this line, \n         * the version file will remain unchanged.\n         */\n        props.store(out, null);\n        /*\n         * Now the new fields are flushed to the head of the file, but file \n         * length can still be larger then required and therefore the file can \n         * contain whole or corrupted fields from its old contents in the end.\n         * If server is interrupted here and restarted later these extra fields\n         * either should not effect server behavior or should be handled\n         * by the server correctly.\n         */\n        file.setLength(out.getChannel().position());\n      } finally {\n        if (out !\u003d null) {\n          out.close();\n        }\n        file.close();\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/common/Storage.java"
    }
  }
}