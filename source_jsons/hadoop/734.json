{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "transfer",
  "functionId": "transfer___src-DatanodeInfo(modifiers-final)__targets-DatanodeInfo[](modifiers-final)__targetStorageTypes-StorageType[](modifiers-final)__targetStorageIDs-String[](modifiers-final)__blockToken-Token__BlockTokenIdentifier__(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 1437,
  "functionEndLine": 1461,
  "numCommitsSeen": 238,
  "timeTaken": 11384,
  "changeHistory": [
    "a3954ccab148bddc290cb96528e63ff19799bcc9",
    "3ae652f82110a52bf239f3c1849b48981558eb19",
    "4c9497cbf02ecc82532a4e79e18912d8e0eb4731",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
    "730f9930a48259f34e48404aee51e8d641cc3d36",
    "4da8490b512a33a255ed27309860859388d7c168",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
    "552b4fb9f9a76b18605322c0b0e8072613d67773",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54",
    "3b54223c0f32d42a84436c670d80b791a8e9696d",
    "1fbb04e367d7c330e6052207f9f11911f4f5f368",
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "a3954ccab148bddc290cb96528e63ff19799bcc9": "Ymultichange(Yparameterchange,Ybodychange)",
    "3ae652f82110a52bf239f3c1849b48981558eb19": "Ybodychange",
    "4c9497cbf02ecc82532a4e79e18912d8e0eb4731": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Ymultichange(Yfilerename,Ybodychange)",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": "Ybodychange",
    "730f9930a48259f34e48404aee51e8d641cc3d36": "Ybodychange",
    "4da8490b512a33a255ed27309860859388d7c168": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": "Ymovefromfile",
    "552b4fb9f9a76b18605322c0b0e8072613d67773": "Ymultichange(Yparameterchange,Ybodychange)",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": "Ymultichange(Yparameterchange,Ybodychange)",
    "3b54223c0f32d42a84436c670d80b791a8e9696d": "Ybodychange",
    "1fbb04e367d7c330e6052207f9f11911f4f5f368": "Ybodychange",
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ybodychange",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": "Ybodychange",
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a3954ccab148bddc290cb96528e63ff19799bcc9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
      "commitDate": "05/05/17 12:01 PM",
      "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthor": "Chris Douglas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "15/02/17 10:44 AM",
          "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 79.01,
          "commitsBetweenForRepo": 465,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,25 @@\n   private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                         final StorageType[] targetStorageTypes,\n+                        final String[] targetStorageIDs,\n                         final Token\u003cBlockTokenIdentifier\u003e blockToken)\n       throws IOException {\n     //transfer replica to the new datanode\n     RefetchEncryptionKeyPolicy policy \u003d new RefetchEncryptionKeyPolicy(src);\n     do {\n       StreamerStreams streams \u003d null;\n       try {\n         final long writeTimeout \u003d computeTransferWriteTimeout();\n         final long readTimeout \u003d computeTransferReadTimeout();\n \n         streams \u003d new StreamerStreams(src, writeTimeout, readTimeout,\n             blockToken);\n-        streams.sendTransferBlock(targets, targetStorageTypes, blockToken);\n+        streams.sendTransferBlock(targets, targetStorageTypes,\n+            targetStorageIDs, blockToken);\n         return;\n       } catch (InvalidEncryptionKeyException e) {\n         policy.recordFailure(e);\n       } finally {\n         IOUtils.closeStream(streams);\n       }\n     } while (policy.continueRetryingOrThrow());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                        final StorageType[] targetStorageTypes,\n                        final String[] targetStorageIDs,\n                        final Token\u003cBlockTokenIdentifier\u003e blockToken)\n      throws IOException {\n    //transfer replica to the new datanode\n    RefetchEncryptionKeyPolicy policy \u003d new RefetchEncryptionKeyPolicy(src);\n    do {\n      StreamerStreams streams \u003d null;\n      try {\n        final long writeTimeout \u003d computeTransferWriteTimeout();\n        final long readTimeout \u003d computeTransferReadTimeout();\n\n        streams \u003d new StreamerStreams(src, writeTimeout, readTimeout,\n            blockToken);\n        streams.sendTransferBlock(targets, targetStorageTypes,\n            targetStorageIDs, blockToken);\n        return;\n      } catch (InvalidEncryptionKeyException e) {\n        policy.recordFailure(e);\n      } finally {\n        IOUtils.closeStream(streams);\n      }\n    } while (policy.continueRetryingOrThrow());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldValue": "[src-DatanodeInfo(modifiers-final), targets-DatanodeInfo[](modifiers-final), targetStorageTypes-StorageType[](modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]",
            "newValue": "[src-DatanodeInfo(modifiers-final), targets-DatanodeInfo[](modifiers-final), targetStorageTypes-StorageType[](modifiers-final), targetStorageIDs-String[](modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "15/02/17 10:44 AM",
          "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 79.01,
          "commitsBetweenForRepo": 465,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,25 @@\n   private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                         final StorageType[] targetStorageTypes,\n+                        final String[] targetStorageIDs,\n                         final Token\u003cBlockTokenIdentifier\u003e blockToken)\n       throws IOException {\n     //transfer replica to the new datanode\n     RefetchEncryptionKeyPolicy policy \u003d new RefetchEncryptionKeyPolicy(src);\n     do {\n       StreamerStreams streams \u003d null;\n       try {\n         final long writeTimeout \u003d computeTransferWriteTimeout();\n         final long readTimeout \u003d computeTransferReadTimeout();\n \n         streams \u003d new StreamerStreams(src, writeTimeout, readTimeout,\n             blockToken);\n-        streams.sendTransferBlock(targets, targetStorageTypes, blockToken);\n+        streams.sendTransferBlock(targets, targetStorageTypes,\n+            targetStorageIDs, blockToken);\n         return;\n       } catch (InvalidEncryptionKeyException e) {\n         policy.recordFailure(e);\n       } finally {\n         IOUtils.closeStream(streams);\n       }\n     } while (policy.continueRetryingOrThrow());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                        final StorageType[] targetStorageTypes,\n                        final String[] targetStorageIDs,\n                        final Token\u003cBlockTokenIdentifier\u003e blockToken)\n      throws IOException {\n    //transfer replica to the new datanode\n    RefetchEncryptionKeyPolicy policy \u003d new RefetchEncryptionKeyPolicy(src);\n    do {\n      StreamerStreams streams \u003d null;\n      try {\n        final long writeTimeout \u003d computeTransferWriteTimeout();\n        final long readTimeout \u003d computeTransferReadTimeout();\n\n        streams \u003d new StreamerStreams(src, writeTimeout, readTimeout,\n            blockToken);\n        streams.sendTransferBlock(targets, targetStorageTypes,\n            targetStorageIDs, blockToken);\n        return;\n      } catch (InvalidEncryptionKeyException e) {\n        policy.recordFailure(e);\n      } finally {\n        IOUtils.closeStream(streams);\n      }\n    } while (policy.continueRetryingOrThrow());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {}
        }
      ]
    },
    "3ae652f82110a52bf239f3c1849b48981558eb19": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10609. Uncaught InvalidEncryptionKeyException during pipeline recovery may abort downstream applications. Contributed by Wei-Chiu Chuang.\n",
      "commitDate": "26/09/16 1:11 PM",
      "commitName": "3ae652f82110a52bf239f3c1849b48981558eb19",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "02/09/16 11:01 AM",
      "commitNameOld": "5a8c5064d1a1d596b1f5c385299a86ec6ab9ad1e",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 24.09,
      "commitsBetweenForRepo": 117,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,23 @@\n   private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                         final StorageType[] targetStorageTypes,\n                         final Token\u003cBlockTokenIdentifier\u003e blockToken)\n       throws IOException {\n     //transfer replica to the new datanode\n-    Socket sock \u003d null;\n-    DataOutputStream out \u003d null;\n-    DataInputStream in \u003d null;\n-    try {\n-      sock \u003d createSocketForPipeline(src, 2, dfsClient);\n-      final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n+    RefetchEncryptionKeyPolicy policy \u003d new RefetchEncryptionKeyPolicy(src);\n+    do {\n+      StreamerStreams streams \u003d null;\n+      try {\n+        final long writeTimeout \u003d computeTransferWriteTimeout();\n+        final long readTimeout \u003d computeTransferReadTimeout();\n \n-      // transfer timeout multiplier based on the transfer size\n-      // One per 200 packets \u003d 12.8MB. Minimum is 2.\n-      int multi \u003d 2 + (int)(bytesSent /dfsClient.getConf().getWritePacketSize())\n-          / 200;\n-      final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(multi);\n-\n-      OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n-      InputStream unbufIn \u003d NetUtils.getInputStream(sock, readTimeout);\n-      IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n-          unbufOut, unbufIn, dfsClient, blockToken, src);\n-      unbufOut \u003d saslStreams.out;\n-      unbufIn \u003d saslStreams.in;\n-      out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-          DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n-      in \u003d new DataInputStream(unbufIn);\n-\n-      //send the TRANSFER_BLOCK request\n-      new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n-          targets, targetStorageTypes);\n-      out.flush();\n-\n-      //ack\n-      BlockOpResponseProto response \u003d\n-          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n-      if (SUCCESS !\u003d response.getStatus()) {\n-        throw new IOException(\"Failed to add a datanode\");\n+        streams \u003d new StreamerStreams(src, writeTimeout, readTimeout,\n+            blockToken);\n+        streams.sendTransferBlock(targets, targetStorageTypes, blockToken);\n+        return;\n+      } catch (InvalidEncryptionKeyException e) {\n+        policy.recordFailure(e);\n+      } finally {\n+        IOUtils.closeStream(streams);\n       }\n-    } finally {\n-      IOUtils.closeStream(in);\n-      IOUtils.closeStream(out);\n-      IOUtils.closeSocket(sock);\n-    }\n+    } while (policy.continueRetryingOrThrow());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                        final StorageType[] targetStorageTypes,\n                        final Token\u003cBlockTokenIdentifier\u003e blockToken)\n      throws IOException {\n    //transfer replica to the new datanode\n    RefetchEncryptionKeyPolicy policy \u003d new RefetchEncryptionKeyPolicy(src);\n    do {\n      StreamerStreams streams \u003d null;\n      try {\n        final long writeTimeout \u003d computeTransferWriteTimeout();\n        final long readTimeout \u003d computeTransferReadTimeout();\n\n        streams \u003d new StreamerStreams(src, writeTimeout, readTimeout,\n            blockToken);\n        streams.sendTransferBlock(targets, targetStorageTypes, blockToken);\n        return;\n      } catch (InvalidEncryptionKeyException e) {\n        policy.recordFailure(e);\n      } finally {\n        IOUtils.closeStream(streams);\n      }\n    } while (policy.continueRetryingOrThrow());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "4c9497cbf02ecc82532a4e79e18912d8e0eb4731": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9106. Transfer failure during pipeline recovery causes permanent write failures. Contributed by Kihwal Lee.\n",
      "commitDate": "28/09/15 11:29 AM",
      "commitName": "4c9497cbf02ecc82532a4e79e18912d8e0eb4731",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.16,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,43 @@\n   private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                         final StorageType[] targetStorageTypes,\n                         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     //transfer replica to the new datanode\n     Socket sock \u003d null;\n     DataOutputStream out \u003d null;\n     DataInputStream in \u003d null;\n     try {\n       sock \u003d createSocketForPipeline(src, 2, dfsClient);\n       final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n-      final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(2);\n+\n+      // transfer timeout multiplier based on the transfer size\n+      // One per 200 packets \u003d 12.8MB. Minimum is 2.\n+      int multi \u003d 2 + (int)(bytesSent/dfsClient.getConf().getWritePacketSize())/200;\n+      final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(multi);\n \n       OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n       InputStream unbufIn \u003d NetUtils.getInputStream(sock, readTimeout);\n       IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n       unbufOut \u003d saslStreams.out;\n       unbufIn \u003d saslStreams.in;\n       out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n           DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n       in \u003d new DataInputStream(unbufIn);\n \n       //send the TRANSFER_BLOCK request\n       new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n           targets, targetStorageTypes);\n       out.flush();\n \n       //ack\n       BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n       if (SUCCESS !\u003d response.getStatus()) {\n         throw new IOException(\"Failed to add a datanode\");\n       }\n     } finally {\n       IOUtils.closeStream(in);\n       IOUtils.closeStream(out);\n       IOUtils.closeSocket(sock);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                        final StorageType[] targetStorageTypes,\n                        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    //transfer replica to the new datanode\n    Socket sock \u003d null;\n    DataOutputStream out \u003d null;\n    DataInputStream in \u003d null;\n    try {\n      sock \u003d createSocketForPipeline(src, 2, dfsClient);\n      final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n\n      // transfer timeout multiplier based on the transfer size\n      // One per 200 packets \u003d 12.8MB. Minimum is 2.\n      int multi \u003d 2 + (int)(bytesSent/dfsClient.getConf().getWritePacketSize())/200;\n      final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(multi);\n\n      OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n      InputStream unbufIn \u003d NetUtils.getInputStream(sock, readTimeout);\n      IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n      unbufOut \u003d saslStreams.out;\n      unbufIn \u003d saslStreams.in;\n      out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n          DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n      in \u003d new DataInputStream(unbufIn);\n\n      //send the TRANSFER_BLOCK request\n      new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n          targets, targetStorageTypes);\n      out.flush();\n\n      //ack\n      BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n      if (SUCCESS !\u003d response.getStatus()) {\n        throw new IOException(\"Failed to add a datanode\");\n      }\n    } finally {\n      IOUtils.closeStream(in);\n      IOUtils.closeStream(out);\n      IOUtils.closeSocket(sock);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/09/15 11:08 AM",
          "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "26/09/15 9:06 AM",
          "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,39 @@\n   private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                         final StorageType[] targetStorageTypes,\n                         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     //transfer replica to the new datanode\n     Socket sock \u003d null;\n     DataOutputStream out \u003d null;\n     DataInputStream in \u003d null;\n     try {\n       sock \u003d createSocketForPipeline(src, 2, dfsClient);\n       final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n       final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(2);\n \n       OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n       InputStream unbufIn \u003d NetUtils.getInputStream(sock, readTimeout);\n       IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n       unbufOut \u003d saslStreams.out;\n       unbufIn \u003d saslStreams.in;\n       out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-          DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n+          DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n       in \u003d new DataInputStream(unbufIn);\n \n       //send the TRANSFER_BLOCK request\n       new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n           targets, targetStorageTypes);\n       out.flush();\n \n       //ack\n       BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n       if (SUCCESS !\u003d response.getStatus()) {\n         throw new IOException(\"Failed to add a datanode\");\n       }\n     } finally {\n       IOUtils.closeStream(in);\n       IOUtils.closeStream(out);\n       IOUtils.closeSocket(sock);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                        final StorageType[] targetStorageTypes,\n                        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    //transfer replica to the new datanode\n    Socket sock \u003d null;\n    DataOutputStream out \u003d null;\n    DataInputStream in \u003d null;\n    try {\n      sock \u003d createSocketForPipeline(src, 2, dfsClient);\n      final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n      final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(2);\n\n      OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n      InputStream unbufIn \u003d NetUtils.getInputStream(sock, readTimeout);\n      IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n      unbufOut \u003d saslStreams.out;\n      unbufIn \u003d saslStreams.in;\n      out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n          DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n      in \u003d new DataInputStream(unbufIn);\n\n      //send the TRANSFER_BLOCK request\n      new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n          targets, targetStorageTypes);\n      out.flush();\n\n      //ack\n      BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n      if (SUCCESS !\u003d response.getStatus()) {\n        throw new IOException(\"Failed to add a datanode\");\n      }\n    } finally {\n      IOUtils.closeStream(in);\n      IOUtils.closeStream(out);\n      IOUtils.closeSocket(sock);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/09/15 11:08 AM",
          "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "26/09/15 9:06 AM",
          "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,39 @@\n   private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                         final StorageType[] targetStorageTypes,\n                         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     //transfer replica to the new datanode\n     Socket sock \u003d null;\n     DataOutputStream out \u003d null;\n     DataInputStream in \u003d null;\n     try {\n       sock \u003d createSocketForPipeline(src, 2, dfsClient);\n       final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n       final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(2);\n \n       OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n       InputStream unbufIn \u003d NetUtils.getInputStream(sock, readTimeout);\n       IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n       unbufOut \u003d saslStreams.out;\n       unbufIn \u003d saslStreams.in;\n       out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-          DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n+          DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n       in \u003d new DataInputStream(unbufIn);\n \n       //send the TRANSFER_BLOCK request\n       new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n           targets, targetStorageTypes);\n       out.flush();\n \n       //ack\n       BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n       if (SUCCESS !\u003d response.getStatus()) {\n         throw new IOException(\"Failed to add a datanode\");\n       }\n     } finally {\n       IOUtils.closeStream(in);\n       IOUtils.closeStream(out);\n       IOUtils.closeSocket(sock);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                        final StorageType[] targetStorageTypes,\n                        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    //transfer replica to the new datanode\n    Socket sock \u003d null;\n    DataOutputStream out \u003d null;\n    DataInputStream in \u003d null;\n    try {\n      sock \u003d createSocketForPipeline(src, 2, dfsClient);\n      final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n      final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(2);\n\n      OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n      InputStream unbufIn \u003d NetUtils.getInputStream(sock, readTimeout);\n      IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n      unbufOut \u003d saslStreams.out;\n      unbufIn \u003d saslStreams.in;\n      out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n          DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n      in \u003d new DataInputStream(unbufIn);\n\n      //send the TRANSFER_BLOCK request\n      new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n          targets, targetStorageTypes);\n      out.flush();\n\n      //ack\n      BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n      if (SUCCESS !\u003d response.getStatus()) {\n        throw new IOException(\"Failed to add a datanode\");\n      }\n    } finally {\n      IOUtils.closeStream(in);\n      IOUtils.closeStream(out);\n      IOUtils.closeSocket(sock);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {}
        }
      ]
    },
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8934. Move ShortCircuitShm to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "22/08/15 1:31 PM",
      "commitName": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "18/06/15 8:48 AM",
      "commitNameOld": "1c13519e1e7588c3e2974138d37bf3449ca8b3df",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 65.2,
      "commitsBetweenForRepo": 382,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,39 @@\n   private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                         final StorageType[] targetStorageTypes,\n                         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     //transfer replica to the new datanode\n     Socket sock \u003d null;\n     DataOutputStream out \u003d null;\n     DataInputStream in \u003d null;\n     try {\n       sock \u003d createSocketForPipeline(src, 2, dfsClient);\n       final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n       final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(2);\n \n       OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n       InputStream unbufIn \u003d NetUtils.getInputStream(sock, readTimeout);\n       IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n       unbufOut \u003d saslStreams.out;\n       unbufIn \u003d saslStreams.in;\n       out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n           DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n       in \u003d new DataInputStream(unbufIn);\n \n       //send the TRANSFER_BLOCK request\n       new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n           targets, targetStorageTypes);\n       out.flush();\n \n       //ack\n       BlockOpResponseProto response \u003d\n-          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n+          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n       if (SUCCESS !\u003d response.getStatus()) {\n         throw new IOException(\"Failed to add a datanode\");\n       }\n     } finally {\n       IOUtils.closeStream(in);\n       IOUtils.closeStream(out);\n       IOUtils.closeSocket(sock);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                        final StorageType[] targetStorageTypes,\n                        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    //transfer replica to the new datanode\n    Socket sock \u003d null;\n    DataOutputStream out \u003d null;\n    DataInputStream in \u003d null;\n    try {\n      sock \u003d createSocketForPipeline(src, 2, dfsClient);\n      final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n      final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(2);\n\n      OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n      InputStream unbufIn \u003d NetUtils.getInputStream(sock, readTimeout);\n      IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n      unbufOut \u003d saslStreams.out;\n      unbufIn \u003d saslStreams.in;\n      out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n          DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n      in \u003d new DataInputStream(unbufIn);\n\n      //send the TRANSFER_BLOCK request\n      new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n          targets, targetStorageTypes);\n      out.flush();\n\n      //ack\n      BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n      if (SUCCESS !\u003d response.getStatus()) {\n        throw new IOException(\"Failed to add a datanode\");\n      }\n    } finally {\n      IOUtils.closeStream(in);\n      IOUtils.closeStream(out);\n      IOUtils.closeSocket(sock);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "730f9930a48259f34e48404aee51e8d641cc3d36": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8311. DataStreamer.transfer() should timeout the socket InputStream. (Esteban Gutierrez via Yongjun Zhang)\n",
      "commitDate": "08/05/15 12:11 AM",
      "commitName": "730f9930a48259f34e48404aee51e8d641cc3d36",
      "commitAuthor": "Yongjun Zhang",
      "commitDateOld": "08/05/15 4:48 AM",
      "commitNameOld": "c648317a68891e1c900f04b7a9c98ba40c5faddb",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": -0.19,
      "commitsBetweenForRepo": 0,
      "commitsBetweenForFile": 0,
      "diff": "@@ -1,38 +1,39 @@\n   private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                         final StorageType[] targetStorageTypes,\n                         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     //transfer replica to the new datanode\n     Socket sock \u003d null;\n     DataOutputStream out \u003d null;\n     DataInputStream in \u003d null;\n     try {\n       sock \u003d createSocketForPipeline(src, 2, dfsClient);\n       final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n+      final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(2);\n \n       OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n-      InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n+      InputStream unbufIn \u003d NetUtils.getInputStream(sock, readTimeout);\n       IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n       unbufOut \u003d saslStreams.out;\n       unbufIn \u003d saslStreams.in;\n       out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n           DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n       in \u003d new DataInputStream(unbufIn);\n \n       //send the TRANSFER_BLOCK request\n       new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n           targets, targetStorageTypes);\n       out.flush();\n \n       //ack\n       BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n       if (SUCCESS !\u003d response.getStatus()) {\n         throw new IOException(\"Failed to add a datanode\");\n       }\n     } finally {\n       IOUtils.closeStream(in);\n       IOUtils.closeStream(out);\n       IOUtils.closeSocket(sock);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                        final StorageType[] targetStorageTypes,\n                        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    //transfer replica to the new datanode\n    Socket sock \u003d null;\n    DataOutputStream out \u003d null;\n    DataInputStream in \u003d null;\n    try {\n      sock \u003d createSocketForPipeline(src, 2, dfsClient);\n      final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n      final long readTimeout \u003d dfsClient.getDatanodeReadTimeout(2);\n\n      OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n      InputStream unbufIn \u003d NetUtils.getInputStream(sock, readTimeout);\n      IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n      unbufOut \u003d saslStreams.out;\n      unbufIn \u003d saslStreams.in;\n      out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n          DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n      in \u003d new DataInputStream(unbufIn);\n\n      //send the TRANSFER_BLOCK request\n      new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n          targets, targetStorageTypes);\n      out.flush();\n\n      //ack\n      BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      if (SUCCESS !\u003d response.getStatus()) {\n        throw new IOException(\"Failed to add a datanode\");\n      }\n    } finally {\n      IOUtils.closeStream(in);\n      IOUtils.closeStream(out);\n      IOUtils.closeSocket(sock);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "4da8490b512a33a255ed27309860859388d7c168": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
      "commitDate": "05/05/15 3:41 PM",
      "commitName": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.23,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                         final StorageType[] targetStorageTypes,\n                         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     //transfer replica to the new datanode\n     Socket sock \u003d null;\n     DataOutputStream out \u003d null;\n     DataInputStream in \u003d null;\n     try {\n       sock \u003d createSocketForPipeline(src, 2, dfsClient);\n       final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n \n       OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n       InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n       IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n       unbufOut \u003d saslStreams.out;\n       unbufIn \u003d saslStreams.in;\n       out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-          HdfsServerConstants.SMALL_BUFFER_SIZE));\n+          DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n       in \u003d new DataInputStream(unbufIn);\n \n       //send the TRANSFER_BLOCK request\n       new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n           targets, targetStorageTypes);\n       out.flush();\n \n       //ack\n       BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n       if (SUCCESS !\u003d response.getStatus()) {\n         throw new IOException(\"Failed to add a datanode\");\n       }\n     } finally {\n       IOUtils.closeStream(in);\n       IOUtils.closeStream(out);\n       IOUtils.closeSocket(sock);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                        final StorageType[] targetStorageTypes,\n                        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    //transfer replica to the new datanode\n    Socket sock \u003d null;\n    DataOutputStream out \u003d null;\n    DataInputStream in \u003d null;\n    try {\n      sock \u003d createSocketForPipeline(src, 2, dfsClient);\n      final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n\n      OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n      InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n      IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n      unbufOut \u003d saslStreams.out;\n      unbufIn \u003d saslStreams.in;\n      out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n          DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n      in \u003d new DataInputStream(unbufIn);\n\n      //send the TRANSFER_BLOCK request\n      new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n          targets, targetStorageTypes);\n      out.flush();\n\n      //ack\n      BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      if (SUCCESS !\u003d response.getStatus()) {\n        throw new IOException(\"Failed to add a datanode\");\n      }\n    } finally {\n      IOUtils.closeStream(in);\n      IOUtils.closeStream(out);\n      IOUtils.closeSocket(sock);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/04/15 7:27 PM",
      "commitNameOld": "98a61766286321468bf801a9f17a843d7eae8d9e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.61,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                         final StorageType[] targetStorageTypes,\n                         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n     //transfer replica to the new datanode\n     Socket sock \u003d null;\n     DataOutputStream out \u003d null;\n     DataInputStream in \u003d null;\n     try {\n       sock \u003d createSocketForPipeline(src, 2, dfsClient);\n       final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n \n       OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n       InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n       IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n       unbufOut \u003d saslStreams.out;\n       unbufIn \u003d saslStreams.in;\n       out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-          HdfsConstants.SMALL_BUFFER_SIZE));\n+          HdfsServerConstants.SMALL_BUFFER_SIZE));\n       in \u003d new DataInputStream(unbufIn);\n \n       //send the TRANSFER_BLOCK request\n       new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n           targets, targetStorageTypes);\n       out.flush();\n \n       //ack\n       BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n       if (SUCCESS !\u003d response.getStatus()) {\n         throw new IOException(\"Failed to add a datanode\");\n       }\n     } finally {\n       IOUtils.closeStream(in);\n       IOUtils.closeStream(out);\n       IOUtils.closeSocket(sock);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                        final StorageType[] targetStorageTypes,\n                        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    //transfer replica to the new datanode\n    Socket sock \u003d null;\n    DataOutputStream out \u003d null;\n    DataInputStream in \u003d null;\n    try {\n      sock \u003d createSocketForPipeline(src, 2, dfsClient);\n      final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n\n      OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n      InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n      IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n      unbufOut \u003d saslStreams.out;\n      unbufIn \u003d saslStreams.in;\n      out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n          HdfsServerConstants.SMALL_BUFFER_SIZE));\n      in \u003d new DataInputStream(unbufIn);\n\n      //send the TRANSFER_BLOCK request\n      new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n          targets, targetStorageTypes);\n      out.flush();\n\n      //ack\n      BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      if (SUCCESS !\u003d response.getStatus()) {\n        throw new IOException(\"Failed to add a datanode\");\n      }\n    } finally {\n      IOUtils.closeStream(in);\n      IOUtils.closeStream(out);\n      IOUtils.closeSocket(sock);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "24/03/15 11:06 AM",
      "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/03/15 10:49 AM",
      "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n-    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n-        final StorageType[] targetStorageTypes,\n-        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n-      //transfer replica to the new datanode\n-      Socket sock \u003d null;\n-      DataOutputStream out \u003d null;\n-      DataInputStream in \u003d null;\n-      try {\n-        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n-        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n-        \n-        OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n-        InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n-        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n+  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n+                        final StorageType[] targetStorageTypes,\n+                        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+    //transfer replica to the new datanode\n+    Socket sock \u003d null;\n+    DataOutputStream out \u003d null;\n+    DataInputStream in \u003d null;\n+    try {\n+      sock \u003d createSocketForPipeline(src, 2, dfsClient);\n+      final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n+\n+      OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n+      InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n+      IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n-        unbufOut \u003d saslStreams.out;\n-        unbufIn \u003d saslStreams.in;\n-        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-            HdfsConstants.SMALL_BUFFER_SIZE));\n-        in \u003d new DataInputStream(unbufIn);\n+      unbufOut \u003d saslStreams.out;\n+      unbufIn \u003d saslStreams.in;\n+      out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n+          HdfsConstants.SMALL_BUFFER_SIZE));\n+      in \u003d new DataInputStream(unbufIn);\n \n-        //send the TRANSFER_BLOCK request\n-        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n-            targets, targetStorageTypes);\n-        out.flush();\n+      //send the TRANSFER_BLOCK request\n+      new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n+          targets, targetStorageTypes);\n+      out.flush();\n \n-        //ack\n-        BlockOpResponseProto response \u003d\n+      //ack\n+      BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n-        if (SUCCESS !\u003d response.getStatus()) {\n-          throw new IOException(\"Failed to add a datanode\");\n-        }\n-      } finally {\n-        IOUtils.closeStream(in);\n-        IOUtils.closeStream(out);\n-        IOUtils.closeSocket(sock);\n+      if (SUCCESS !\u003d response.getStatus()) {\n+        throw new IOException(\"Failed to add a datanode\");\n       }\n-    }\n\\ No newline at end of file\n+    } finally {\n+      IOUtils.closeStream(in);\n+      IOUtils.closeStream(out);\n+      IOUtils.closeSocket(sock);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n                        final StorageType[] targetStorageTypes,\n                        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n    //transfer replica to the new datanode\n    Socket sock \u003d null;\n    DataOutputStream out \u003d null;\n    DataInputStream in \u003d null;\n    try {\n      sock \u003d createSocketForPipeline(src, 2, dfsClient);\n      final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n\n      OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n      InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n      IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n      unbufOut \u003d saslStreams.out;\n      unbufIn \u003d saslStreams.in;\n      out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n          HdfsConstants.SMALL_BUFFER_SIZE));\n      in \u003d new DataInputStream(unbufIn);\n\n      //send the TRANSFER_BLOCK request\n      new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n          targets, targetStorageTypes);\n      out.flush();\n\n      //ack\n      BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      if (SUCCESS !\u003d response.getStatus()) {\n        throw new IOException(\"Failed to add a datanode\");\n      }\n    } finally {\n      IOUtils.closeStream(in);\n      IOUtils.closeStream(out);\n      IOUtils.closeSocket(sock);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
        "oldMethodName": "transfer",
        "newMethodName": "transfer"
      }
    },
    "552b4fb9f9a76b18605322c0b0e8072613d67773": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "Merge from trunk to branch\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612928 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/07/14 12:26 PM",
      "commitName": "552b4fb9f9a76b18605322c0b0e8072613d67773",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "Merge from trunk to branch\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612928 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/07/14 12:26 PM",
          "commitName": "552b4fb9f9a76b18605322c0b0e8072613d67773",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "15/07/14 2:10 PM",
          "commitNameOld": "56c0bd4d37ab13b6cbcf860eda852da603ab2f62",
          "commitAuthorOld": "",
          "daysBetweenCommits": 7.93,
          "commitsBetweenForRepo": 61,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,38 @@\n     private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n+        final StorageType[] targetStorageTypes,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //transfer replica to the new datanode\n       Socket sock \u003d null;\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock \u003d createSocketForPipeline(src, 2, dfsClient);\n         final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n         \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         in \u003d new DataInputStream(unbufIn);\n \n         //send the TRANSFER_BLOCK request\n         new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n-            targets);\n+            targets, targetStorageTypes);\n         out.flush();\n \n         //ack\n         BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n         if (SUCCESS !\u003d response.getStatus()) {\n           throw new IOException(\"Failed to add a datanode\");\n         }\n       } finally {\n         IOUtils.closeStream(in);\n         IOUtils.closeStream(out);\n         IOUtils.closeSocket(sock);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final StorageType[] targetStorageTypes,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        \n        OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        in \u003d new DataInputStream(unbufIn);\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets, targetStorageTypes);\n        out.flush();\n\n        //ack\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[src-DatanodeInfo(modifiers-final), targets-DatanodeInfo[](modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]",
            "newValue": "[src-DatanodeInfo(modifiers-final), targets-DatanodeInfo[](modifiers-final), targetStorageTypes-StorageType[](modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "Merge from trunk to branch\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612928 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/07/14 12:26 PM",
          "commitName": "552b4fb9f9a76b18605322c0b0e8072613d67773",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "15/07/14 2:10 PM",
          "commitNameOld": "56c0bd4d37ab13b6cbcf860eda852da603ab2f62",
          "commitAuthorOld": "",
          "daysBetweenCommits": 7.93,
          "commitsBetweenForRepo": 61,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,38 @@\n     private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n+        final StorageType[] targetStorageTypes,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //transfer replica to the new datanode\n       Socket sock \u003d null;\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock \u003d createSocketForPipeline(src, 2, dfsClient);\n         final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n         \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         in \u003d new DataInputStream(unbufIn);\n \n         //send the TRANSFER_BLOCK request\n         new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n-            targets);\n+            targets, targetStorageTypes);\n         out.flush();\n \n         //ack\n         BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n         if (SUCCESS !\u003d response.getStatus()) {\n           throw new IOException(\"Failed to add a datanode\");\n         }\n       } finally {\n         IOUtils.closeStream(in);\n         IOUtils.closeStream(out);\n         IOUtils.closeSocket(sock);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final StorageType[] targetStorageTypes,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        \n        OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        in \u003d new DataInputStream(unbufIn);\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets, targetStorageTypes);\n        out.flush();\n\n        //ack\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/14 12:41 AM",
      "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/14 12:41 AM",
          "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/07/14 11:10 AM",
          "commitNameOld": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 7.56,
          "commitsBetweenForRepo": 68,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,38 @@\n     private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n+        final StorageType[] targetStorageTypes,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //transfer replica to the new datanode\n       Socket sock \u003d null;\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock \u003d createSocketForPipeline(src, 2, dfsClient);\n         final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n         \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         in \u003d new DataInputStream(unbufIn);\n \n         //send the TRANSFER_BLOCK request\n         new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n-            targets);\n+            targets, targetStorageTypes);\n         out.flush();\n \n         //ack\n         BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n         if (SUCCESS !\u003d response.getStatus()) {\n           throw new IOException(\"Failed to add a datanode\");\n         }\n       } finally {\n         IOUtils.closeStream(in);\n         IOUtils.closeStream(out);\n         IOUtils.closeSocket(sock);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final StorageType[] targetStorageTypes,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        \n        OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        in \u003d new DataInputStream(unbufIn);\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets, targetStorageTypes);\n        out.flush();\n\n        //ack\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[src-DatanodeInfo(modifiers-final), targets-DatanodeInfo[](modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]",
            "newValue": "[src-DatanodeInfo(modifiers-final), targets-DatanodeInfo[](modifiers-final), targetStorageTypes-StorageType[](modifiers-final), blockToken-Token\u003cBlockTokenIdentifier\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/14 12:41 AM",
          "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/07/14 11:10 AM",
          "commitNameOld": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 7.56,
          "commitsBetweenForRepo": 68,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,38 @@\n     private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n+        final StorageType[] targetStorageTypes,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //transfer replica to the new datanode\n       Socket sock \u003d null;\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock \u003d createSocketForPipeline(src, 2, dfsClient);\n         final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n         \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n           unbufOut, unbufIn, dfsClient, blockToken, src);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         in \u003d new DataInputStream(unbufIn);\n \n         //send the TRANSFER_BLOCK request\n         new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n-            targets);\n+            targets, targetStorageTypes);\n         out.flush();\n \n         //ack\n         BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n         if (SUCCESS !\u003d response.getStatus()) {\n           throw new IOException(\"Failed to add a datanode\");\n         }\n       } finally {\n         IOUtils.closeStream(in);\n         IOUtils.closeStream(out);\n         IOUtils.closeSocket(sock);\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final StorageType[] targetStorageTypes,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        \n        OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        in \u003d new DataInputStream(unbufIn);\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets, targetStorageTypes);\n        out.flush();\n\n        //ack\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "3b54223c0f32d42a84436c670d80b791a8e9696d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/07/14 11:10 AM",
      "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "26/05/14 12:38 PM",
      "commitNameOld": "1228f8f6fb16de4f0283dd1c7939e6fc3dfb7aae",
      "commitAuthorOld": "Michael Stack",
      "daysBetweenCommits": 48.94,
      "commitsBetweenForRepo": 301,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,37 @@\n     private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //transfer replica to the new datanode\n       Socket sock \u003d null;\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock \u003d createSocketForPipeline(src, 2, dfsClient);\n         final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n         \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n-        if (dfsClient.shouldEncryptData() \u0026\u0026 \n-            !dfsClient.trustedChannelResolver.isTrusted(sock.getInetAddress())) {\n-          IOStreamPair encryptedStreams \u003d\n-              DataTransferEncryptor.getEncryptedStreams(\n-                  unbufOut, unbufIn, dfsClient.getDataEncryptionKey());\n-          unbufOut \u003d encryptedStreams.out;\n-          unbufIn \u003d encryptedStreams.in;\n-        }\n+        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n+          unbufOut, unbufIn, dfsClient, blockToken, src);\n+        unbufOut \u003d saslStreams.out;\n+        unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         in \u003d new DataInputStream(unbufIn);\n \n         //send the TRANSFER_BLOCK request\n         new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n             targets);\n         out.flush();\n \n         //ack\n         BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n         if (SUCCESS !\u003d response.getStatus()) {\n           throw new IOException(\"Failed to add a datanode\");\n         }\n       } finally {\n         IOUtils.closeStream(in);\n         IOUtils.closeStream(out);\n         IOUtils.closeSocket(sock);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        \n        OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(sock,\n          unbufOut, unbufIn, dfsClient, blockToken, src);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        in \u003d new DataInputStream(unbufIn);\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets);\n        out.flush();\n\n        //ack\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "1fbb04e367d7c330e6052207f9f11911f4f5f368": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5910. Enhance DataTransferProtocol to allow per-connection choice of encryption/plain-text. (Contributed by Benoy Antony)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1581688 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/03/14 9:11 PM",
      "commitName": "1fbb04e367d7c330e6052207f9f11911f4f5f368",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "24/03/14 4:32 PM",
      "commitNameOld": "c2ef7e239eb0e81cf8a3e971378e9e696202de67",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 1.19,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,41 @@\n     private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //transfer replica to the new datanode\n       Socket sock \u003d null;\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock \u003d createSocketForPipeline(src, 2, dfsClient);\n         final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n         \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n-        if (dfsClient.shouldEncryptData()) {\n+        if (dfsClient.shouldEncryptData() \u0026\u0026 \n+            !dfsClient.trustedChannelResolver.isTrusted(sock.getInetAddress())) {\n           IOStreamPair encryptedStreams \u003d\n               DataTransferEncryptor.getEncryptedStreams(\n                   unbufOut, unbufIn, dfsClient.getDataEncryptionKey());\n           unbufOut \u003d encryptedStreams.out;\n           unbufIn \u003d encryptedStreams.in;\n         }\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         in \u003d new DataInputStream(unbufIn);\n \n         //send the TRANSFER_BLOCK request\n         new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n             targets);\n         out.flush();\n \n         //ack\n         BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n         if (SUCCESS !\u003d response.getStatus()) {\n           throw new IOException(\"Failed to add a datanode\");\n         }\n       } finally {\n         IOUtils.closeStream(in);\n         IOUtils.closeStream(out);\n         IOUtils.closeSocket(sock);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        \n        OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n        if (dfsClient.shouldEncryptData() \u0026\u0026 \n            !dfsClient.trustedChannelResolver.isTrusted(sock.getInetAddress())) {\n          IOStreamPair encryptedStreams \u003d\n              DataTransferEncryptor.getEncryptedStreams(\n                  unbufOut, unbufIn, dfsClient.getDataEncryptionKey());\n          unbufOut \u003d encryptedStreams.out;\n          unbufIn \u003d encryptedStreams.in;\n        }\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        in \u003d new DataInputStream(unbufIn);\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets);\n        out.flush();\n\n        //ack\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4363. Combine PBHelper and HdfsProtoUtil and remove redundant methods. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431088 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:20 PM",
      "commitName": "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "30/11/12 11:24 AM",
      "commitNameOld": "571da54179f731eb8421ffc681169799588f76bc",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 40.08,
      "commitsBetweenForRepo": 152,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n     private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //transfer replica to the new datanode\n       Socket sock \u003d null;\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock \u003d createSocketForPipeline(src, 2, dfsClient);\n         final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n         \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n         if (dfsClient.shouldEncryptData()) {\n           IOStreamPair encryptedStreams \u003d\n               DataTransferEncryptor.getEncryptedStreams(\n                   unbufOut, unbufIn, dfsClient.getDataEncryptionKey());\n           unbufOut \u003d encryptedStreams.out;\n           unbufIn \u003d encryptedStreams.in;\n         }\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         in \u003d new DataInputStream(unbufIn);\n \n         //send the TRANSFER_BLOCK request\n         new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n             targets);\n         out.flush();\n \n         //ack\n         BlockOpResponseProto response \u003d\n-          BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n+          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n         if (SUCCESS !\u003d response.getStatus()) {\n           throw new IOException(\"Failed to add a datanode\");\n         }\n       } finally {\n         IOUtils.closeStream(in);\n         IOUtils.closeStream(out);\n         IOUtils.closeSocket(sock);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        \n        OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n        if (dfsClient.shouldEncryptData()) {\n          IOStreamPair encryptedStreams \u003d\n              DataTransferEncryptor.getEncryptedStreams(\n                  unbufOut, unbufIn, dfsClient.getDataEncryptionKey());\n          unbufOut \u003d encryptedStreams.out;\n          unbufIn \u003d encryptedStreams.in;\n        }\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        in \u003d new DataInputStream(unbufIn);\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets);\n        out.flush();\n\n        //ack\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "26/07/12 5:26 PM",
      "commitNameOld": "c1ea9b4490e7d6d030eeaeeff2fad3767d2cfd4a",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 11.68,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,40 @@\n     private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //transfer replica to the new datanode\n       Socket sock \u003d null;\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock \u003d createSocketForPipeline(src, 2, dfsClient);\n         final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n-        out \u003d new DataOutputStream(new BufferedOutputStream(\n-            NetUtils.getOutputStream(sock, writeTimeout),\n+        \n+        OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n+        InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n+        if (dfsClient.shouldEncryptData()) {\n+          IOStreamPair encryptedStreams \u003d\n+              DataTransferEncryptor.getEncryptedStreams(\n+                  unbufOut, unbufIn, dfsClient.getDataEncryptionKey());\n+          unbufOut \u003d encryptedStreams.out;\n+          unbufIn \u003d encryptedStreams.in;\n+        }\n+        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.SMALL_BUFFER_SIZE));\n+        in \u003d new DataInputStream(unbufIn);\n \n         //send the TRANSFER_BLOCK request\n         new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n             targets);\n+        out.flush();\n \n         //ack\n-        in \u003d new DataInputStream(NetUtils.getInputStream(sock));\n         BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n         if (SUCCESS !\u003d response.getStatus()) {\n           throw new IOException(\"Failed to add a datanode\");\n         }\n       } finally {\n         IOUtils.closeStream(in);\n         IOUtils.closeStream(out);\n         IOUtils.closeSocket(sock);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        \n        OutputStream unbufOut \u003d NetUtils.getOutputStream(sock, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(sock);\n        if (dfsClient.shouldEncryptData()) {\n          IOStreamPair encryptedStreams \u003d\n              DataTransferEncryptor.getEncryptedStreams(\n                  unbufOut, unbufIn, dfsClient.getDataEncryptionKey());\n          unbufOut \u003d encryptedStreams.out;\n          unbufIn \u003d encryptedStreams.in;\n        }\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        in \u003d new DataInputStream(unbufIn);\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets);\n        out.flush();\n\n        //ack\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1620. Rename HdfsConstants -\u003e HdfsServerConstants, FSConstants -\u003e HdfsConstants. (Harsh J Chouraria via atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165096 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/11 12:30 PM",
      "commitName": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 10.8,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n     private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //transfer replica to the new datanode\n       Socket sock \u003d null;\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock \u003d createSocketForPipeline(src, 2, dfsClient);\n         final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n         out \u003d new DataOutputStream(new BufferedOutputStream(\n             NetUtils.getOutputStream(sock, writeTimeout),\n-            FSConstants.SMALL_BUFFER_SIZE));\n+            HdfsConstants.SMALL_BUFFER_SIZE));\n \n         //send the TRANSFER_BLOCK request\n         new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n             targets);\n \n         //ack\n         in \u003d new DataInputStream(NetUtils.getInputStream(sock));\n         BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n         if (SUCCESS !\u003d response.getStatus()) {\n           throw new IOException(\"Failed to add a datanode\");\n         }\n       } finally {\n         IOUtils.closeStream(in);\n         IOUtils.closeStream(out);\n         IOUtils.closeSocket(sock);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(sock, writeTimeout),\n            HdfsConstants.SMALL_BUFFER_SIZE));\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets);\n\n        //ack\n        in \u003d new DataInputStream(NetUtils.getInputStream(sock));\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(sock, writeTimeout),\n            FSConstants.SMALL_BUFFER_SIZE));\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets);\n\n        //ack\n        in \u003d new DataInputStream(NetUtils.getInputStream(sock));\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(sock, writeTimeout),\n            FSConstants.SMALL_BUFFER_SIZE));\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets);\n\n        //ack\n        in \u003d new DataInputStream(NetUtils.getInputStream(sock));\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2241. Remove implementing FSConstants interface to just get the constants from the interface. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1156420 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/08/11 5:46 PM",
      "commitName": "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "12/07/11 6:11 PM",
      "commitNameOld": "2c5dd549e31aa5d3377ff2619ede8e92b8dc5d0f",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 28.98,
      "commitsBetweenForRepo": 108,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n     private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //transfer replica to the new datanode\n       Socket sock \u003d null;\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock \u003d createSocketForPipeline(src, 2, dfsClient);\n         final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n         out \u003d new DataOutputStream(new BufferedOutputStream(\n             NetUtils.getOutputStream(sock, writeTimeout),\n-            DataNode.SMALL_BUFFER_SIZE));\n+            FSConstants.SMALL_BUFFER_SIZE));\n \n         //send the TRANSFER_BLOCK request\n         new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n             targets);\n \n         //ack\n         in \u003d new DataInputStream(NetUtils.getInputStream(sock));\n         BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n         if (SUCCESS !\u003d response.getStatus()) {\n           throw new IOException(\"Failed to add a datanode\");\n         }\n       } finally {\n         IOUtils.closeStream(in);\n         IOUtils.closeStream(out);\n         IOUtils.closeSocket(sock);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(sock, writeTimeout),\n            FSConstants.SMALL_BUFFER_SIZE));\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets);\n\n        //ack\n        in \u003d new DataInputStream(NetUtils.getInputStream(sock));\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/06/11 4:57 PM",
      "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "23/06/11 3:24 PM",
      "commitNameOld": "fd9997989c1f1c6f806c57a806e7225ca599fc0c",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n     private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n         final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n       //transfer replica to the new datanode\n       Socket sock \u003d null;\n       DataOutputStream out \u003d null;\n       DataInputStream in \u003d null;\n       try {\n         sock \u003d createSocketForPipeline(src, 2, dfsClient);\n         final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n         out \u003d new DataOutputStream(new BufferedOutputStream(\n             NetUtils.getOutputStream(sock, writeTimeout),\n             DataNode.SMALL_BUFFER_SIZE));\n \n         //send the TRANSFER_BLOCK request\n-        Sender.opTransferBlock(out, block,\n-            dfsClient.clientName, targets, blockToken);\n+        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n+            targets);\n \n         //ack\n         in \u003d new DataInputStream(NetUtils.getInputStream(sock));\n         BlockOpResponseProto response \u003d\n           BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n         if (SUCCESS !\u003d response.getStatus()) {\n           throw new IOException(\"Failed to add a datanode\");\n         }\n       } finally {\n         IOUtils.closeStream(in);\n         IOUtils.closeStream(out);\n         IOUtils.closeSocket(sock);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(sock, writeTimeout),\n            DataNode.SMALL_BUFFER_SIZE));\n\n        //send the TRANSFER_BLOCK request\n        new Sender(out).transferBlock(block, blockToken, dfsClient.clientName,\n            targets);\n\n        //ack\n        in \u003d new DataInputStream(NetUtils.getInputStream(sock));\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,30 @@\n+    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n+        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n+      //transfer replica to the new datanode\n+      Socket sock \u003d null;\n+      DataOutputStream out \u003d null;\n+      DataInputStream in \u003d null;\n+      try {\n+        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n+        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n+        out \u003d new DataOutputStream(new BufferedOutputStream(\n+            NetUtils.getOutputStream(sock, writeTimeout),\n+            DataNode.SMALL_BUFFER_SIZE));\n+\n+        //send the TRANSFER_BLOCK request\n+        Sender.opTransferBlock(out, block,\n+            dfsClient.clientName, targets, blockToken);\n+\n+        //ack\n+        in \u003d new DataInputStream(NetUtils.getInputStream(sock));\n+        BlockOpResponseProto response \u003d\n+          BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n+        if (SUCCESS !\u003d response.getStatus()) {\n+          throw new IOException(\"Failed to add a datanode\");\n+        }\n+      } finally {\n+        IOUtils.closeStream(in);\n+        IOUtils.closeStream(out);\n+        IOUtils.closeSocket(sock);\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void transfer(final DatanodeInfo src, final DatanodeInfo[] targets,\n        final Token\u003cBlockTokenIdentifier\u003e blockToken) throws IOException {\n      //transfer replica to the new datanode\n      Socket sock \u003d null;\n      DataOutputStream out \u003d null;\n      DataInputStream in \u003d null;\n      try {\n        sock \u003d createSocketForPipeline(src, 2, dfsClient);\n        final long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(2);\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(sock, writeTimeout),\n            DataNode.SMALL_BUFFER_SIZE));\n\n        //send the TRANSFER_BLOCK request\n        Sender.opTransferBlock(out, block,\n            dfsClient.clientName, targets, blockToken);\n\n        //ack\n        in \u003d new DataInputStream(NetUtils.getInputStream(sock));\n        BlockOpResponseProto response \u003d\n          BlockOpResponseProto.parseFrom(HdfsProtoUtil.vintPrefixed(in));\n        if (SUCCESS !\u003d response.getStatus()) {\n          throw new IOException(\"Failed to add a datanode\");\n        }\n      } finally {\n        IOUtils.closeStream(in);\n        IOUtils.closeStream(out);\n        IOUtils.closeSocket(sock);\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}