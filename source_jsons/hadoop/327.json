{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSStripedInputStream.java",
  "functionName": "closeReader",
  "functionId": "closeReader___readerInfo-BlockReaderInfo",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
  "functionStartLine": 221,
  "functionEndLine": 231,
  "numCommitsSeen": 25,
  "timeTaken": 1063,
  "changeHistory": [
    "734d54c1a8950446e68098f62d8964e02ecc2890",
    "e54cc2931262bf49682a8323da9811976218c03b"
  ],
  "changeHistoryShort": {
    "734d54c1a8950446e68098f62d8964e02ecc2890": "Ymodifierchange",
    "e54cc2931262bf49682a8323da9811976218c03b": "Ybodychange"
  },
  "changeHistoryDetails": {
    "734d54c1a8950446e68098f62d8964e02ecc2890": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
      "commitDate": "21/09/16 6:34 AM",
      "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "08/09/16 11:54 AM",
      "commitNameOld": "401db4fc65140979fe7665983e36905e886df971",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 12.78,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,11 @@\n-  private void closeReader(BlockReaderInfo readerInfo) {\n+  protected void closeReader(BlockReaderInfo readerInfo) {\n     if (readerInfo !\u003d null) {\n       if (readerInfo.reader !\u003d null) {\n         try {\n           readerInfo.reader.close();\n         } catch (Throwable ignored) {\n         }\n       }\n       readerInfo.skip();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void closeReader(BlockReaderInfo readerInfo) {\n    if (readerInfo !\u003d null) {\n      if (readerInfo.reader !\u003d null) {\n        try {\n          readerInfo.reader.close();\n        } catch (Throwable ignored) {\n        }\n      }\n      readerInfo.skip();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[protected]"
      }
    },
    "e54cc2931262bf49682a8323da9811976218c03b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9818. Correctly handle EC reconstruction work caused by not enough racks. Contributed by Jing Zhao.\n",
      "commitDate": "19/02/16 7:02 PM",
      "commitName": "e54cc2931262bf49682a8323da9811976218c03b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "22/01/16 9:46 AM",
      "commitNameOld": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 28.39,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,6 +1,11 @@\n   private void closeReader(BlockReaderInfo readerInfo) {\n     if (readerInfo !\u003d null) {\n-//      IOUtils.cleanup(null, readerInfo.reader);\n+      if (readerInfo.reader !\u003d null) {\n+        try {\n+          readerInfo.reader.close();\n+        } catch (Throwable ignored) {\n+        }\n+      }\n       readerInfo.skip();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void closeReader(BlockReaderInfo readerInfo) {\n    if (readerInfo !\u003d null) {\n      if (readerInfo.reader !\u003d null) {\n        try {\n          readerInfo.reader.close();\n        } catch (Throwable ignored) {\n        }\n      }\n      readerInfo.skip();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
      "extendedDetails": {}
    }
  }
}