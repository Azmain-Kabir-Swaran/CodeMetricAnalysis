{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ProvidedVolumeImpl.java",
  "functionName": "compileReport",
  "functionId": "compileReport___report-Collection__ScanInfo____reportCompiler-ReportCompiler",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java",
  "functionStartLine": 229,
  "functionEndLine": 245,
  "numCommitsSeen": 32,
  "timeTaken": 4383,
  "changeHistory": [
    "73c660b43f3d5311947d2acc9488f17c1caf4de0",
    "9c35be86e17021202823bfd3c2067ff3b312ce5c",
    "352f994b6484524cdcfcda021046c59905b62f31",
    "cc933cba77c147153e463415fc192cee2d53a1ef",
    "98f5ed5aa377ddd3f35b763b20c499d2ccac2ed5",
    "b668eb91556b8c85c2b4925808ccb1f769031c20"
  ],
  "changeHistoryShort": {
    "73c660b43f3d5311947d2acc9488f17c1caf4de0": "Yparameterchange",
    "9c35be86e17021202823bfd3c2067ff3b312ce5c": "Ybodychange",
    "352f994b6484524cdcfcda021046c59905b62f31": "Ybodychange",
    "cc933cba77c147153e463415fc192cee2d53a1ef": "Ybodychange",
    "98f5ed5aa377ddd3f35b763b20c499d2ccac2ed5": "Ybodychange",
    "b668eb91556b8c85c2b4925808ccb1f769031c20": "Yintroduced"
  },
  "changeHistoryDetails": {
    "73c660b43f3d5311947d2acc9488f17c1caf4de0": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-13958. Miscellaneous Improvements for FsVolumeSpi. Contributed by BELUGA BEHR.\n",
      "commitDate": "05/10/18 9:32 AM",
      "commitName": "73c660b43f3d5311947d2acc9488f17c1caf4de0",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "30/04/18 1:28 PM",
      "commitNameOld": "fc074a359c44e84dd9612be2bd772763f943eb04",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 157.84,
      "commitsBetweenForRepo": 1255,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,17 @@\n-    public void compileReport(LinkedList\u003cScanInfo\u003e report,\n+    public void compileReport(Collection\u003cScanInfo\u003e report,\n         ReportCompiler reportCompiler)\n-            throws IOException, InterruptedException {\n+        throws IOException, InterruptedException {\n       /* refresh the aliasMap and return the list of blocks found.\n        * the assumption here is that the block ids in the external\n        * block map, after the refresh, are consistent with those\n        * from before the refresh, i.e., for blocks which did not change,\n        * the ids remain the same.\n        */\n       aliasMap.refresh();\n       BlockAliasMap.Reader\u003cFileRegion\u003e reader \u003d aliasMap.getReader(null, bpid);\n       for (FileRegion region : reader) {\n         reportCompiler.throttle();\n-        report.add(new ScanInfo(region.getBlock().getBlockId(),\n-            providedVolume, region,\n-            region.getProvidedStorageLocation().getLength()));\n+        report.add(new ScanInfo(region.getBlock().getBlockId(), providedVolume,\n+            region, region.getProvidedStorageLocation().getLength()));\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void compileReport(Collection\u003cScanInfo\u003e report,\n        ReportCompiler reportCompiler)\n        throws IOException, InterruptedException {\n      /* refresh the aliasMap and return the list of blocks found.\n       * the assumption here is that the block ids in the external\n       * block map, after the refresh, are consistent with those\n       * from before the refresh, i.e., for blocks which did not change,\n       * the ids remain the same.\n       */\n      aliasMap.refresh();\n      BlockAliasMap.Reader\u003cFileRegion\u003e reader \u003d aliasMap.getReader(null, bpid);\n      for (FileRegion region : reader) {\n        reportCompiler.throttle();\n        report.add(new ScanInfo(region.getBlock().getBlockId(), providedVolume,\n            region, region.getProvidedStorageLocation().getLength()));\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java",
      "extendedDetails": {
        "oldValue": "[report-LinkedList\u003cScanInfo\u003e, reportCompiler-ReportCompiler]",
        "newValue": "[report-Collection\u003cScanInfo\u003e, reportCompiler-ReportCompiler]"
      }
    },
    "9c35be86e17021202823bfd3c2067ff3b312ce5c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12713. [READ] Refactor FileRegion and BlockAliasMap to separate out HDFS metadata and PROVIDED storage metadata. Contributed by Ewan Higgs\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "9c35be86e17021202823bfd3c2067ff3b312ce5c",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "a027055dd2bf5009fe272e9ceb08305bd0a8cc31",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,18 @@\n     public void compileReport(LinkedList\u003cScanInfo\u003e report,\n         ReportCompiler reportCompiler)\n             throws IOException, InterruptedException {\n       /* refresh the aliasMap and return the list of blocks found.\n        * the assumption here is that the block ids in the external\n        * block map, after the refresh, are consistent with those\n        * from before the refresh, i.e., for blocks which did not change,\n        * the ids remain the same.\n        */\n       aliasMap.refresh();\n-      BlockAliasMap.Reader\u003cFileRegion\u003e reader \u003d aliasMap.getReader(null);\n+      BlockAliasMap.Reader\u003cFileRegion\u003e reader \u003d aliasMap.getReader(null, bpid);\n       for (FileRegion region : reader) {\n         reportCompiler.throttle();\n-        if (region.getBlockPoolId().equals(bpid)) {\n-          report.add(new ScanInfo(region.getBlock().getBlockId(),\n-              providedVolume, region,\n-              region.getProvidedStorageLocation().getLength()));\n-        }\n+        report.add(new ScanInfo(region.getBlock().getBlockId(),\n+            providedVolume, region,\n+            region.getProvidedStorageLocation().getLength()));\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void compileReport(LinkedList\u003cScanInfo\u003e report,\n        ReportCompiler reportCompiler)\n            throws IOException, InterruptedException {\n      /* refresh the aliasMap and return the list of blocks found.\n       * the assumption here is that the block ids in the external\n       * block map, after the refresh, are consistent with those\n       * from before the refresh, i.e., for blocks which did not change,\n       * the ids remain the same.\n       */\n      aliasMap.refresh();\n      BlockAliasMap.Reader\u003cFileRegion\u003e reader \u003d aliasMap.getReader(null, bpid);\n      for (FileRegion region : reader) {\n        reportCompiler.throttle();\n        report.add(new ScanInfo(region.getBlock().getBlockId(),\n            providedVolume, region,\n            region.getProvidedStorageLocation().getLength()));\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java",
      "extendedDetails": {}
    },
    "352f994b6484524cdcfcda021046c59905b62f31": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12665. [AliasMap] Create a version of the AliasMap that runs in memory in the Namenode (leveldb). Contributed by Ewan Higgs.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "352f994b6484524cdcfcda021046c59905b62f31",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "cc933cba77c147153e463415fc192cee2d53a1ef",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,20 @@\n     public void compileReport(LinkedList\u003cScanInfo\u003e report,\n         ReportCompiler reportCompiler)\n             throws IOException, InterruptedException {\n       /* refresh the aliasMap and return the list of blocks found.\n        * the assumption here is that the block ids in the external\n        * block map, after the refresh, are consistent with those\n        * from before the refresh, i.e., for blocks which did not change,\n        * the ids remain the same.\n        */\n       aliasMap.refresh();\n       BlockAliasMap.Reader\u003cFileRegion\u003e reader \u003d aliasMap.getReader(null);\n-      if (reader \u003d\u003d null) {\n-        LOG.warn(\"Got null reader from BlockAliasMap \" + aliasMap\n-            + \"; no blocks will be populated in scan report\");\n-        return;\n-      }\n-      Iterator\u003cFileRegion\u003e iter \u003d reader.iterator();\n-      while(iter.hasNext()) {\n+      for (FileRegion region : reader) {\n         reportCompiler.throttle();\n-        FileRegion region \u003d iter.next();\n         if (region.getBlockPoolId().equals(bpid)) {\n           report.add(new ScanInfo(region.getBlock().getBlockId(),\n-              providedVolume, region, region.getLength()));\n+              providedVolume, region,\n+              region.getProvidedStorageLocation().getLength()));\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void compileReport(LinkedList\u003cScanInfo\u003e report,\n        ReportCompiler reportCompiler)\n            throws IOException, InterruptedException {\n      /* refresh the aliasMap and return the list of blocks found.\n       * the assumption here is that the block ids in the external\n       * block map, after the refresh, are consistent with those\n       * from before the refresh, i.e., for blocks which did not change,\n       * the ids remain the same.\n       */\n      aliasMap.refresh();\n      BlockAliasMap.Reader\u003cFileRegion\u003e reader \u003d aliasMap.getReader(null);\n      for (FileRegion region : reader) {\n        reportCompiler.throttle();\n        if (region.getBlockPoolId().equals(bpid)) {\n          report.add(new ScanInfo(region.getBlock().getBlockId(),\n              providedVolume, region,\n              region.getProvidedStorageLocation().getLength()));\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java",
      "extendedDetails": {}
    },
    "cc933cba77c147153e463415fc192cee2d53a1ef": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12685. [READ] FsVolumeImpl exception when scanning Provided storage volume\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "cc933cba77c147153e463415fc192cee2d53a1ef",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "3b1d30301bcd35bbe525a7e122d3e5acfab92c88",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,26 @@\n     public void compileReport(LinkedList\u003cScanInfo\u003e report,\n         ReportCompiler reportCompiler)\n             throws IOException, InterruptedException {\n       /* refresh the aliasMap and return the list of blocks found.\n        * the assumption here is that the block ids in the external\n        * block map, after the refresh, are consistent with those\n        * from before the refresh, i.e., for blocks which did not change,\n        * the ids remain the same.\n        */\n       aliasMap.refresh();\n       BlockAliasMap.Reader\u003cFileRegion\u003e reader \u003d aliasMap.getReader(null);\n       if (reader \u003d\u003d null) {\n         LOG.warn(\"Got null reader from BlockAliasMap \" + aliasMap\n             + \"; no blocks will be populated in scan report\");\n         return;\n       }\n       Iterator\u003cFileRegion\u003e iter \u003d reader.iterator();\n       while(iter.hasNext()) {\n         reportCompiler.throttle();\n         FileRegion region \u003d iter.next();\n         if (region.getBlockPoolId().equals(bpid)) {\n-          LOG.info(\"Adding ScanInfo for blkid \" +\n-              region.getBlock().getBlockId());\n-          report.add(new ScanInfo(region.getBlock().getBlockId(), null, null,\n+          report.add(new ScanInfo(region.getBlock().getBlockId(),\n               providedVolume, region, region.getLength()));\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void compileReport(LinkedList\u003cScanInfo\u003e report,\n        ReportCompiler reportCompiler)\n            throws IOException, InterruptedException {\n      /* refresh the aliasMap and return the list of blocks found.\n       * the assumption here is that the block ids in the external\n       * block map, after the refresh, are consistent with those\n       * from before the refresh, i.e., for blocks which did not change,\n       * the ids remain the same.\n       */\n      aliasMap.refresh();\n      BlockAliasMap.Reader\u003cFileRegion\u003e reader \u003d aliasMap.getReader(null);\n      if (reader \u003d\u003d null) {\n        LOG.warn(\"Got null reader from BlockAliasMap \" + aliasMap\n            + \"; no blocks will be populated in scan report\");\n        return;\n      }\n      Iterator\u003cFileRegion\u003e iter \u003d reader.iterator();\n      while(iter.hasNext()) {\n        reportCompiler.throttle();\n        FileRegion region \u003d iter.next();\n        if (region.getBlockPoolId().equals(bpid)) {\n          report.add(new ScanInfo(region.getBlock().getBlockId(),\n              providedVolume, region, region.getLength()));\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java",
      "extendedDetails": {}
    },
    "98f5ed5aa377ddd3f35b763b20c499d2ccac2ed5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11902. [READ] Merge BlockFormatProvider and FileRegionProvider.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "98f5ed5aa377ddd3f35b763b20c499d2ccac2ed5",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "2407c9b93aabb021b76c802b19c928fb6cbb0a85",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,28 @@\n     public void compileReport(LinkedList\u003cScanInfo\u003e report,\n         ReportCompiler reportCompiler)\n             throws IOException, InterruptedException {\n-      /* refresh the provider and return the list of blocks found.\n+      /* refresh the aliasMap and return the list of blocks found.\n        * the assumption here is that the block ids in the external\n        * block map, after the refresh, are consistent with those\n        * from before the refresh, i.e., for blocks which did not change,\n        * the ids remain the same.\n        */\n-      provider.refresh();\n-      Iterator\u003cFileRegion\u003e iter \u003d provider.iterator();\n+      aliasMap.refresh();\n+      BlockAliasMap.Reader\u003cFileRegion\u003e reader \u003d aliasMap.getReader(null);\n+      if (reader \u003d\u003d null) {\n+        LOG.warn(\"Got null reader from BlockAliasMap \" + aliasMap\n+            + \"; no blocks will be populated in scan report\");\n+        return;\n+      }\n+      Iterator\u003cFileRegion\u003e iter \u003d reader.iterator();\n       while(iter.hasNext()) {\n         reportCompiler.throttle();\n         FileRegion region \u003d iter.next();\n         if (region.getBlockPoolId().equals(bpid)) {\n           LOG.info(\"Adding ScanInfo for blkid \" +\n               region.getBlock().getBlockId());\n           report.add(new ScanInfo(region.getBlock().getBlockId(), null, null,\n               providedVolume, region, region.getLength()));\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void compileReport(LinkedList\u003cScanInfo\u003e report,\n        ReportCompiler reportCompiler)\n            throws IOException, InterruptedException {\n      /* refresh the aliasMap and return the list of blocks found.\n       * the assumption here is that the block ids in the external\n       * block map, after the refresh, are consistent with those\n       * from before the refresh, i.e., for blocks which did not change,\n       * the ids remain the same.\n       */\n      aliasMap.refresh();\n      BlockAliasMap.Reader\u003cFileRegion\u003e reader \u003d aliasMap.getReader(null);\n      if (reader \u003d\u003d null) {\n        LOG.warn(\"Got null reader from BlockAliasMap \" + aliasMap\n            + \"; no blocks will be populated in scan report\");\n        return;\n      }\n      Iterator\u003cFileRegion\u003e iter \u003d reader.iterator();\n      while(iter.hasNext()) {\n        reportCompiler.throttle();\n        FileRegion region \u003d iter.next();\n        if (region.getBlockPoolId().equals(bpid)) {\n          LOG.info(\"Adding ScanInfo for blkid \" +\n              region.getBlock().getBlockId());\n          report.add(new ScanInfo(region.getBlock().getBlockId(), null, null,\n              providedVolume, region, region.getLength()));\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java",
      "extendedDetails": {}
    },
    "b668eb91556b8c85c2b4925808ccb1f769031c20": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-10675. Datanode support to read from external stores.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "b668eb91556b8c85c2b4925808ccb1f769031c20",
      "commitAuthor": "Virajith Jalaparti",
      "diff": "@@ -0,0 +1,22 @@\n+    public void compileReport(LinkedList\u003cScanInfo\u003e report,\n+        ReportCompiler reportCompiler)\n+            throws IOException, InterruptedException {\n+      /* refresh the provider and return the list of blocks found.\n+       * the assumption here is that the block ids in the external\n+       * block map, after the refresh, are consistent with those\n+       * from before the refresh, i.e., for blocks which did not change,\n+       * the ids remain the same.\n+       */\n+      provider.refresh();\n+      Iterator\u003cFileRegion\u003e iter \u003d provider.iterator();\n+      while(iter.hasNext()) {\n+        reportCompiler.throttle();\n+        FileRegion region \u003d iter.next();\n+        if (region.getBlockPoolId().equals(bpid)) {\n+          LOG.info(\"Adding ScanInfo for blkid \" +\n+              region.getBlock().getBlockId());\n+          report.add(new ScanInfo(region.getBlock().getBlockId(), null, null,\n+              providedVolume, region, region.getLength()));\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public void compileReport(LinkedList\u003cScanInfo\u003e report,\n        ReportCompiler reportCompiler)\n            throws IOException, InterruptedException {\n      /* refresh the provider and return the list of blocks found.\n       * the assumption here is that the block ids in the external\n       * block map, after the refresh, are consistent with those\n       * from before the refresh, i.e., for blocks which did not change,\n       * the ids remain the same.\n       */\n      provider.refresh();\n      Iterator\u003cFileRegion\u003e iter \u003d provider.iterator();\n      while(iter.hasNext()) {\n        reportCompiler.throttle();\n        FileRegion region \u003d iter.next();\n        if (region.getBlockPoolId().equals(bpid)) {\n          LOG.info(\"Adding ScanInfo for blkid \" +\n              region.getBlock().getBlockId());\n          report.add(new ScanInfo(region.getBlock().getBlockId(), null, null,\n              providedVolume, region, region.getLength()));\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java"
    }
  }
}