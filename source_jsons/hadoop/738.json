{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "handleBadDatanode",
  "functionId": "handleBadDatanode",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 1556,
  "functionEndLine": 1591,
  "numCommitsSeen": 53,
  "timeTaken": 2594,
  "changeHistory": [
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "8f378733423a5244461df79a92c00239514b8b93"
  ],
  "changeHistoryShort": {
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "8f378733423a5244461df79a92c00239514b8b93": "Yintroduced"
  },
  "changeHistoryDetails": {
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11856. Ability to re-add Upgrading Nodes to pipeline for future pipeline updates. Contributed by Vinayakumar B.\n",
      "commitDate": "25/05/17 11:05 AM",
      "commitName": "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "05/05/17 12:01 PM",
      "commitNameOld": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 19.96,
      "commitsBetweenForRepo": 106,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,36 @@\n   boolean handleBadDatanode() {\n     final int badNodeIndex \u003d errorState.getBadNodeIndex();\n     if (badNodeIndex \u003e\u003d 0) {\n       if (nodes.length \u003c\u003d 1) {\n         lastException.set(new IOException(\"All datanodes \"\n             + Arrays.toString(nodes) + \" are bad. Aborting...\"));\n         streamerClosed \u003d true;\n         return false;\n       }\n \n+      String reason \u003d \"bad.\";\n+      if (errorState.getRestartingNodeIndex() \u003d\u003d badNodeIndex) {\n+        reason \u003d \"restarting.\";\n+        restartingNodes.add(nodes[badNodeIndex]);\n+      }\n       LOG.warn(\"Error Recovery for \" + block + \" in pipeline \"\n           + Arrays.toString(nodes) + \": datanode \" + badNodeIndex\n-          + \"(\"+ nodes[badNodeIndex] + \") is bad.\");\n+          + \"(\"+ nodes[badNodeIndex] + \") is \" + reason);\n       failed.add(nodes[badNodeIndex]);\n \n       DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n       arraycopy(nodes, newnodes, badNodeIndex);\n \n       final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n       arraycopy(storageTypes, newStorageTypes, badNodeIndex);\n \n       final String[] newStorageIDs \u003d new String[newnodes.length];\n       arraycopy(storageIDs, newStorageIDs, badNodeIndex);\n \n       setPipeline(newnodes, newStorageTypes, newStorageIDs);\n \n       errorState.adjustState4RestartingNode();\n       lastException.clear();\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean handleBadDatanode() {\n    final int badNodeIndex \u003d errorState.getBadNodeIndex();\n    if (badNodeIndex \u003e\u003d 0) {\n      if (nodes.length \u003c\u003d 1) {\n        lastException.set(new IOException(\"All datanodes \"\n            + Arrays.toString(nodes) + \" are bad. Aborting...\"));\n        streamerClosed \u003d true;\n        return false;\n      }\n\n      String reason \u003d \"bad.\";\n      if (errorState.getRestartingNodeIndex() \u003d\u003d badNodeIndex) {\n        reason \u003d \"restarting.\";\n        restartingNodes.add(nodes[badNodeIndex]);\n      }\n      LOG.warn(\"Error Recovery for \" + block + \" in pipeline \"\n          + Arrays.toString(nodes) + \": datanode \" + badNodeIndex\n          + \"(\"+ nodes[badNodeIndex] + \") is \" + reason);\n      failed.add(nodes[badNodeIndex]);\n\n      DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n      arraycopy(nodes, newnodes, badNodeIndex);\n\n      final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n      arraycopy(storageTypes, newStorageTypes, badNodeIndex);\n\n      final String[] newStorageIDs \u003d new String[newnodes.length];\n      arraycopy(storageIDs, newStorageIDs, badNodeIndex);\n\n      setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n      errorState.adjustState4RestartingNode();\n      lastException.clear();\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean handleBadDatanode() {\n    final int badNodeIndex \u003d errorState.getBadNodeIndex();\n    if (badNodeIndex \u003e\u003d 0) {\n      if (nodes.length \u003c\u003d 1) {\n        lastException.set(new IOException(\"All datanodes \"\n            + Arrays.toString(nodes) + \" are bad. Aborting...\"));\n        streamerClosed \u003d true;\n        return false;\n      }\n\n      LOG.warn(\"Error Recovery for \" + block + \" in pipeline \"\n          + Arrays.toString(nodes) + \": datanode \" + badNodeIndex\n          + \"(\"+ nodes[badNodeIndex] + \") is bad.\");\n      failed.add(nodes[badNodeIndex]);\n\n      DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n      arraycopy(nodes, newnodes, badNodeIndex);\n\n      final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n      arraycopy(storageTypes, newStorageTypes, badNodeIndex);\n\n      final String[] newStorageIDs \u003d new String[newnodes.length];\n      arraycopy(storageIDs, newStorageIDs, badNodeIndex);\n\n      setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n      errorState.adjustState4RestartingNode();\n      lastException.clear();\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
      }
    },
    "8f378733423a5244461df79a92c00239514b8b93": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8397. Refactor the error handling code in DataStreamer. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "15/05/15 4:14 PM",
      "commitName": "8f378733423a5244461df79a92c00239514b8b93",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,31 @@\n+  private boolean handleBadDatanode() {\n+    final int badNodeIndex \u003d errorState.getBadNodeIndex();\n+    if (badNodeIndex \u003e\u003d 0) {\n+      if (nodes.length \u003c\u003d 1) {\n+        lastException.set(new IOException(\"All datanodes \"\n+            + Arrays.toString(nodes) + \" are bad. Aborting...\"));\n+        streamerClosed \u003d true;\n+        return false;\n+      }\n+\n+      LOG.warn(\"Error Recovery for \" + block + \" in pipeline \"\n+          + Arrays.toString(nodes) + \": datanode \" + badNodeIndex\n+          + \"(\"+ nodes[badNodeIndex] + \") is bad.\");\n+      failed.add(nodes[badNodeIndex]);\n+\n+      DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n+      arraycopy(nodes, newnodes, badNodeIndex);\n+\n+      final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n+      arraycopy(storageTypes, newStorageTypes, badNodeIndex);\n+\n+      final String[] newStorageIDs \u003d new String[newnodes.length];\n+      arraycopy(storageIDs, newStorageIDs, badNodeIndex);\n+\n+      setPipeline(newnodes, newStorageTypes, newStorageIDs);\n+\n+      errorState.adjustState4RestartingNode();\n+      lastException.clear();\n+    }\n+    return true;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean handleBadDatanode() {\n    final int badNodeIndex \u003d errorState.getBadNodeIndex();\n    if (badNodeIndex \u003e\u003d 0) {\n      if (nodes.length \u003c\u003d 1) {\n        lastException.set(new IOException(\"All datanodes \"\n            + Arrays.toString(nodes) + \" are bad. Aborting...\"));\n        streamerClosed \u003d true;\n        return false;\n      }\n\n      LOG.warn(\"Error Recovery for \" + block + \" in pipeline \"\n          + Arrays.toString(nodes) + \": datanode \" + badNodeIndex\n          + \"(\"+ nodes[badNodeIndex] + \") is bad.\");\n      failed.add(nodes[badNodeIndex]);\n\n      DatanodeInfo[] newnodes \u003d new DatanodeInfo[nodes.length-1];\n      arraycopy(nodes, newnodes, badNodeIndex);\n\n      final StorageType[] newStorageTypes \u003d new StorageType[newnodes.length];\n      arraycopy(storageTypes, newStorageTypes, badNodeIndex);\n\n      final String[] newStorageIDs \u003d new String[newnodes.length];\n      arraycopy(storageIDs, newStorageIDs, badNodeIndex);\n\n      setPipeline(newnodes, newStorageTypes, newStorageIDs);\n\n      errorState.adjustState4RestartingNode();\n      lastException.clear();\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    }
  }
}