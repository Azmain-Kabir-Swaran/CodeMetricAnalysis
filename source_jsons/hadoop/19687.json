{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LineRecordReader.java",
  "functionName": "next",
  "functionId": "next___key-LongWritable__value-Text",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LineRecordReader.java",
  "functionStartLine": 249,
  "functionEndLine": 277,
  "numCommitsSeen": 13,
  "timeTaken": 4857,
  "changeHistory": [
    "40ba8c17c1500703c47a154f06708e5924c24e65",
    "4bb4de93d67873b47fd90c61396f52315165c7bf",
    "18d99c12c371cfd7b9604e321d8bd6a7be9c4977",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "40ba8c17c1500703c47a154f06708e5924c24e65": "Ybodychange",
    "4bb4de93d67873b47fd90c61396f52315165c7bf": "Ybodychange",
    "18d99c12c371cfd7b9604e321d8bd6a7be9c4977": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "40ba8c17c1500703c47a154f06708e5924c24e65": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5777. Support utf-8 text with Byte Order Marker. (Zhihai Xu via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1600977 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/06/14 11:37 AM",
      "commitName": "40ba8c17c1500703c47a154f06708e5924c24e65",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "28/05/14 12:37 PM",
      "commitNameOld": "4bb4de93d67873b47fd90c61396f52315165c7bf",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 8.96,
      "commitsBetweenForRepo": 48,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,29 @@\n   public synchronized boolean next(LongWritable key, Text value)\n     throws IOException {\n \n     // We always read one extra line, which lies outside the upper\n     // split limit i.e. (end - 1)\n     while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n       key.set(pos);\n \n-      int newSize \u003d in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n+      int newSize \u003d 0;\n+      if (pos \u003d\u003d 0) {\n+        newSize \u003d skipUtfByteOrderMark(value);\n+      } else {\n+        newSize \u003d in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n+        pos +\u003d newSize;\n+      }\n+\n       if (newSize \u003d\u003d 0) {\n         return false;\n       }\n-      pos +\u003d newSize;\n       if (newSize \u003c maxLineLength) {\n         return true;\n       }\n \n       // line too long. try again\n       LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized boolean next(LongWritable key, Text value)\n    throws IOException {\n\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n      key.set(pos);\n\n      int newSize \u003d 0;\n      if (pos \u003d\u003d 0) {\n        newSize \u003d skipUtfByteOrderMark(value);\n      } else {\n        newSize \u003d in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n        pos +\u003d newSize;\n      }\n\n      if (newSize \u003d\u003d 0) {\n        return false;\n      }\n      if (newSize \u003c maxLineLength) {\n        return true;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\n    }\n\n    return false;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LineRecordReader.java",
      "extendedDetails": {}
    },
    "4bb4de93d67873b47fd90c61396f52315165c7bf": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5862. Line records longer than 2x split size aren\u0027t handled correctly. Contributed by bc Wong\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1598111 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/05/14 12:37 PM",
      "commitName": "4bb4de93d67873b47fd90c61396f52315165c7bf",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "09/12/13 3:31 PM",
      "commitNameOld": "18d99c12c371cfd7b9604e321d8bd6a7be9c4977",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 169.84,
      "commitsBetweenForRepo": 1155,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,23 @@\n   public synchronized boolean next(LongWritable key, Text value)\n     throws IOException {\n \n     // We always read one extra line, which lies outside the upper\n     // split limit i.e. (end - 1)\n     while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n       key.set(pos);\n \n-      int newSize \u003d in.readLine(value, maxLineLength,\n-          Math.max(maxBytesToConsume(pos), maxLineLength));\n+      int newSize \u003d in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n       if (newSize \u003d\u003d 0) {\n         return false;\n       }\n       pos +\u003d newSize;\n       if (newSize \u003c maxLineLength) {\n         return true;\n       }\n \n       // line too long. try again\n       LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized boolean next(LongWritable key, Text value)\n    throws IOException {\n\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n      key.set(pos);\n\n      int newSize \u003d in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n      if (newSize \u003d\u003d 0) {\n        return false;\n      }\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        return true;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\n    }\n\n    return false;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LineRecordReader.java",
      "extendedDetails": {}
    },
    "18d99c12c371cfd7b9604e321d8bd6a7be9c4977": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5656. bzip2 codec can drop records when reading data in splits. Contributed by Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1549705 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/12/13 3:31 PM",
      "commitName": "18d99c12c371cfd7b9604e321d8bd6a7be9c4977",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "02/02/12 12:37 AM",
      "commitNameOld": "cc74881acb839bbcab7e6d1346093eed3f35c780",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 676.62,
      "commitsBetweenForRepo": 4116,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public synchronized boolean next(LongWritable key, Text value)\n     throws IOException {\n \n     // We always read one extra line, which lies outside the upper\n     // split limit i.e. (end - 1)\n-    while (getFilePosition() \u003c\u003d end) {\n+    while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n       key.set(pos);\n \n       int newSize \u003d in.readLine(value, maxLineLength,\n           Math.max(maxBytesToConsume(pos), maxLineLength));\n       if (newSize \u003d\u003d 0) {\n         return false;\n       }\n       pos +\u003d newSize;\n       if (newSize \u003c maxLineLength) {\n         return true;\n       }\n \n       // line too long. try again\n       LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\n     }\n \n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized boolean next(LongWritable key, Text value)\n    throws IOException {\n\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n      key.set(pos);\n\n      int newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      if (newSize \u003d\u003d 0) {\n        return false;\n      }\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        return true;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\n    }\n\n    return false;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LineRecordReader.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized boolean next(LongWritable key, Text value)\n    throws IOException {\n\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end) {\n      key.set(pos);\n\n      int newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      if (newSize \u003d\u003d 0) {\n        return false;\n      }\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        return true;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\n    }\n\n    return false;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LineRecordReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LineRecordReader.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LineRecordReader.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized boolean next(LongWritable key, Text value)\n    throws IOException {\n\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end) {\n      key.set(pos);\n\n      int newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      if (newSize \u003d\u003d 0) {\n        return false;\n      }\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        return true;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\n    }\n\n    return false;\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LineRecordReader.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/LineRecordReader.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LineRecordReader.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,24 @@\n+  public synchronized boolean next(LongWritable key, Text value)\n+    throws IOException {\n+\n+    // We always read one extra line, which lies outside the upper\n+    // split limit i.e. (end - 1)\n+    while (getFilePosition() \u003c\u003d end) {\n+      key.set(pos);\n+\n+      int newSize \u003d in.readLine(value, maxLineLength,\n+          Math.max(maxBytesToConsume(pos), maxLineLength));\n+      if (newSize \u003d\u003d 0) {\n+        return false;\n+      }\n+      pos +\u003d newSize;\n+      if (newSize \u003c maxLineLength) {\n+        return true;\n+      }\n+\n+      // line too long. try again\n+      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\n+    }\n+\n+    return false;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized boolean next(LongWritable key, Text value)\n    throws IOException {\n\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end) {\n      key.set(pos);\n\n      int newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      if (newSize \u003d\u003d 0) {\n        return false;\n      }\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        return true;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\n    }\n\n    return false;\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/LineRecordReader.java"
    }
  }
}