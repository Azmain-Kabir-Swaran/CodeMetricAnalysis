{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StripedBlockUtil.java",
  "functionName": "getInternalBlockLength",
  "functionId": "getInternalBlockLength___dataSize-long__ecPolicy-ErasureCodingPolicy__idxInBlockGroup-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
  "functionStartLine": 194,
  "functionEndLine": 199,
  "numCommitsSeen": 25,
  "timeTaken": 1056,
  "changeHistory": [
    "3c18a53cbd2efabb2ad108d63a0b0b558424115f"
  ],
  "changeHistoryShort": {
    "3c18a53cbd2efabb2ad108d63a0b0b558424115f": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "3c18a53cbd2efabb2ad108d63a0b0b558424115f": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9719. Refactoring ErasureCodingWorker into smaller reusable constructs. Contributed by Kai Zheng.\n",
      "commitDate": "06/04/16 10:50 PM",
      "commitName": "3c18a53cbd2efabb2ad108d63a0b0b558424115f",
      "commitAuthor": "Uma Maheswara Rao G",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9719. Refactoring ErasureCodingWorker into smaller reusable constructs. Contributed by Kai Zheng.\n",
          "commitDate": "06/04/16 10:50 PM",
          "commitName": "3c18a53cbd2efabb2ad108d63a0b0b558424115f",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "26/03/16 7:58 PM",
          "commitNameOld": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 11.12,
          "commitsBetweenForRepo": 82,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,6 @@\n   public static long getInternalBlockLength(long dataSize,\n-      int cellSize, int numDataBlocks, int i) {\n-    Preconditions.checkArgument(dataSize \u003e\u003d 0);\n-    Preconditions.checkArgument(cellSize \u003e 0);\n-    Preconditions.checkArgument(numDataBlocks \u003e 0);\n-    Preconditions.checkArgument(i \u003e\u003d 0);\n-    // Size of each stripe (only counting data blocks)\n-    final int stripeSize \u003d cellSize * numDataBlocks;\n-    // If block group ends at stripe boundary, each internal block has an equal\n-    // share of the group\n-    final int lastStripeDataLen \u003d (int)(dataSize % stripeSize);\n-    if (lastStripeDataLen \u003d\u003d 0) {\n-      return dataSize / numDataBlocks;\n-    }\n-\n-    final int numStripes \u003d (int) ((dataSize - 1) / stripeSize + 1);\n-    return (numStripes - 1L)*cellSize\n-        + lastCellSize(lastStripeDataLen, cellSize, numDataBlocks, i);\n+                                            ErasureCodingPolicy ecPolicy,\n+                                            int idxInBlockGroup) {\n+    return getInternalBlockLength(dataSize, ecPolicy.getCellSize(),\n+        ecPolicy.getNumDataUnits(), idxInBlockGroup);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static long getInternalBlockLength(long dataSize,\n                                            ErasureCodingPolicy ecPolicy,\n                                            int idxInBlockGroup) {\n    return getInternalBlockLength(dataSize, ecPolicy.getCellSize(),\n        ecPolicy.getNumDataUnits(), idxInBlockGroup);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
          "extendedDetails": {
            "oldValue": "[dataSize-long, cellSize-int, numDataBlocks-int, i-int]",
            "newValue": "[dataSize-long, ecPolicy-ErasureCodingPolicy, idxInBlockGroup-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9719. Refactoring ErasureCodingWorker into smaller reusable constructs. Contributed by Kai Zheng.\n",
          "commitDate": "06/04/16 10:50 PM",
          "commitName": "3c18a53cbd2efabb2ad108d63a0b0b558424115f",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "26/03/16 7:58 PM",
          "commitNameOld": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 11.12,
          "commitsBetweenForRepo": 82,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,6 @@\n   public static long getInternalBlockLength(long dataSize,\n-      int cellSize, int numDataBlocks, int i) {\n-    Preconditions.checkArgument(dataSize \u003e\u003d 0);\n-    Preconditions.checkArgument(cellSize \u003e 0);\n-    Preconditions.checkArgument(numDataBlocks \u003e 0);\n-    Preconditions.checkArgument(i \u003e\u003d 0);\n-    // Size of each stripe (only counting data blocks)\n-    final int stripeSize \u003d cellSize * numDataBlocks;\n-    // If block group ends at stripe boundary, each internal block has an equal\n-    // share of the group\n-    final int lastStripeDataLen \u003d (int)(dataSize % stripeSize);\n-    if (lastStripeDataLen \u003d\u003d 0) {\n-      return dataSize / numDataBlocks;\n-    }\n-\n-    final int numStripes \u003d (int) ((dataSize - 1) / stripeSize + 1);\n-    return (numStripes - 1L)*cellSize\n-        + lastCellSize(lastStripeDataLen, cellSize, numDataBlocks, i);\n+                                            ErasureCodingPolicy ecPolicy,\n+                                            int idxInBlockGroup) {\n+    return getInternalBlockLength(dataSize, ecPolicy.getCellSize(),\n+        ecPolicy.getNumDataUnits(), idxInBlockGroup);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static long getInternalBlockLength(long dataSize,\n                                            ErasureCodingPolicy ecPolicy,\n                                            int idxInBlockGroup) {\n    return getInternalBlockLength(dataSize, ecPolicy.getCellSize(),\n        ecPolicy.getNumDataUnits(), idxInBlockGroup);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}