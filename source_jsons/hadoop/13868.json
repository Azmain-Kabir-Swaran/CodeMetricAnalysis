{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "processMisReplicatedBlocks",
  "functionId": "processMisReplicatedBlocks",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 3605,
  "functionEndLine": 3625,
  "numCommitsSeen": 507,
  "timeTaken": 18260,
  "changeHistory": [
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "788fca4124ecac818a20bfc2607676849cf0d94f",
    "048c416beb42ad27cf0e82b144da1d99e50c62b1",
    "12645ff6c06bc514930177a78610017897e2e0b7",
    "31c91706f7d17da006ef2d6c541f8dd092fae077",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "1dd48b1aee2378c02ee7e78864a757cff3607274",
    "0b12cc822ddd57e6ecf4f7047f6614419c34580b",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ybodychange",
    "788fca4124ecac818a20bfc2607676849cf0d94f": "Ybodychange",
    "048c416beb42ad27cf0e82b144da1d99e50c62b1": "Ybodychange",
    "12645ff6c06bc514930177a78610017897e2e0b7": "Ybodychange",
    "31c91706f7d17da006ef2d6c541f8dd092fae077": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "1dd48b1aee2378c02ee7e78864a757cff3607274": "Ybodychange",
    "0b12cc822ddd57e6ecf4f7047f6614419c34580b": "Ybodychange",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": "Ymultichange(Yfilerename,Ymodifierchange)",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": "Ymultichange(Yfilerename,Ymodifierchange)",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": "Ymultichange(Yfilerename,Ymodifierchange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,21 @@\n   public void processMisReplicatedBlocks() {\n     assert namesystem.hasWriteLock();\n-    stopReplicationInitializer();\n-    neededReplications.clear();\n-    replicationQueuesInitializer \u003d new Daemon() {\n+    stopReconstructionInitializer();\n+    neededReconstruction.clear();\n+    reconstructionQueuesInitializer \u003d new Daemon() {\n \n       @Override\n       public void run() {\n         try {\n           processMisReplicatesAsync();\n         } catch (InterruptedException ie) {\n-          LOG.info(\"Interrupted while processing replication queues.\");\n+          LOG.info(\"Interrupted while processing reconstruction queues.\");\n         } catch (Exception e) {\n-          LOG.error(\"Error while processing replication queues async\", e);\n+          LOG.error(\"Error while processing reconstruction queues async\", e);\n         }\n       }\n     };\n-    replicationQueuesInitializer.setName(\"Replication Queue Initializer\");\n-    replicationQueuesInitializer.start();\n+    reconstructionQueuesInitializer\n+        .setName(\"Reconstruction Queue Initializer\");\n+    reconstructionQueuesInitializer.start();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processMisReplicatedBlocks() {\n    assert namesystem.hasWriteLock();\n    stopReconstructionInitializer();\n    neededReconstruction.clear();\n    reconstructionQueuesInitializer \u003d new Daemon() {\n\n      @Override\n      public void run() {\n        try {\n          processMisReplicatesAsync();\n        } catch (InterruptedException ie) {\n          LOG.info(\"Interrupted while processing reconstruction queues.\");\n        } catch (Exception e) {\n          LOG.error(\"Error while processing reconstruction queues async\", e);\n        }\n      }\n    };\n    reconstructionQueuesInitializer\n        .setName(\"Reconstruction Queue Initializer\");\n    reconstructionQueuesInitializer.start();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "788fca4124ecac818a20bfc2607676849cf0d94f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5496. Make replication queue initialization asynchronous. Contributed by Vinay.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1552109 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/12/13 1:30 PM",
      "commitName": "788fca4124ecac818a20bfc2607676849cf0d94f",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/12/13 12:18 PM",
      "commitNameOld": "38a04a3042c5af455605bd3477358893700e2a9d",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 16.05,
      "commitsBetweenForRepo": 54,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,20 @@\n   public void processMisReplicatedBlocks() {\n     assert namesystem.hasWriteLock();\n-\n-    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0, nrPostponed \u003d 0,\n-         nrUnderConstruction \u003d 0;\n+    stopReplicationInitializer();\n     neededReplications.clear();\n-    for (BlockInfo block : blocksMap.getBlocks()) {\n-      MisReplicationResult res \u003d processMisReplicatedBlock(block);\n-      if (LOG.isTraceEnabled()) {\n-        LOG.trace(\"block \" + block + \": \" + res);\n+    replicationQueuesInitializer \u003d new Daemon() {\n+\n+      @Override\n+      public void run() {\n+        try {\n+          processMisReplicatesAsync();\n+        } catch (InterruptedException ie) {\n+          LOG.info(\"Interrupted while processing replication queues.\");\n+        } catch (Exception e) {\n+          LOG.error(\"Error while processing replication queues async\", e);\n+        }\n       }\n-      switch (res) {\n-      case UNDER_REPLICATED:\n-        nrUnderReplicated++;\n-        break;\n-      case OVER_REPLICATED:\n-        nrOverReplicated++;\n-        break;\n-      case INVALID:\n-        nrInvalid++;\n-        break;\n-      case POSTPONE:\n-        nrPostponed++;\n-        postponeBlock(block);\n-        break;\n-      case UNDER_CONSTRUCTION:\n-        nrUnderConstruction++;\n-        break;\n-      case OK:\n-        break;\n-      default:\n-        throw new AssertionError(\"Invalid enum value: \" + res);\n-      }\n-    }\n-    \n-    LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n-    LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n-    LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n-    LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated +\n-        ((nrPostponed \u003e 0) ? ( \" (\" + nrPostponed + \" postponed)\") : \"\"));\n-    LOG.info(\"Number of blocks being written    \u003d \" + nrUnderConstruction);\n+    };\n+    replicationQueuesInitializer.setName(\"Replication Queue Initializer\");\n+    replicationQueuesInitializer.start();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processMisReplicatedBlocks() {\n    assert namesystem.hasWriteLock();\n    stopReplicationInitializer();\n    neededReplications.clear();\n    replicationQueuesInitializer \u003d new Daemon() {\n\n      @Override\n      public void run() {\n        try {\n          processMisReplicatesAsync();\n        } catch (InterruptedException ie) {\n          LOG.info(\"Interrupted while processing replication queues.\");\n        } catch (Exception e) {\n          LOG.error(\"Error while processing replication queues async\", e);\n        }\n      }\n    };\n    replicationQueuesInitializer.setName(\"Replication Queue Initializer\");\n    replicationQueuesInitializer.start();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "048c416beb42ad27cf0e82b144da1d99e50c62b1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2870. Fix log level for block debug info in processMisReplicatedBlocks. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1239278 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/02/12 11:23 AM",
      "commitName": "048c416beb42ad27cf0e82b144da1d99e50c62b1",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "31/01/12 9:16 PM",
      "commitNameOld": "cf611255d6fcd7016e0ce2a3f80ccd0d4e051d9f",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.59,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,42 @@\n   public void processMisReplicatedBlocks() {\n     assert namesystem.hasWriteLock();\n \n     long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0, nrPostponed \u003d 0,\n          nrUnderConstruction \u003d 0;\n     neededReplications.clear();\n     for (BlockInfo block : blocksMap.getBlocks()) {\n       MisReplicationResult res \u003d processMisReplicatedBlock(block);\n-      LOG.info(\"block \" + block + \": \" + res);\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(\"block \" + block + \": \" + res);\n+      }\n       switch (res) {\n       case UNDER_REPLICATED:\n         nrUnderReplicated++;\n         break;\n       case OVER_REPLICATED:\n         nrOverReplicated++;\n         break;\n       case INVALID:\n         nrInvalid++;\n         break;\n       case POSTPONE:\n         nrPostponed++;\n         postponeBlock(block);\n         break;\n       case UNDER_CONSTRUCTION:\n         nrUnderConstruction++;\n         break;\n       case OK:\n         break;\n       default:\n         throw new AssertionError(\"Invalid enum value: \" + res);\n       }\n     }\n     \n     LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n     LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n     LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n     LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated +\n         ((nrPostponed \u003e 0) ? ( \" (\" + nrPostponed + \" postponed)\") : \"\"));\n     LOG.info(\"Number of blocks being written    \u003d \" + nrUnderConstruction);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processMisReplicatedBlocks() {\n    assert namesystem.hasWriteLock();\n\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0, nrPostponed \u003d 0,\n         nrUnderConstruction \u003d 0;\n    neededReplications.clear();\n    for (BlockInfo block : blocksMap.getBlocks()) {\n      MisReplicationResult res \u003d processMisReplicatedBlock(block);\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"block \" + block + \": \" + res);\n      }\n      switch (res) {\n      case UNDER_REPLICATED:\n        nrUnderReplicated++;\n        break;\n      case OVER_REPLICATED:\n        nrOverReplicated++;\n        break;\n      case INVALID:\n        nrInvalid++;\n        break;\n      case POSTPONE:\n        nrPostponed++;\n        postponeBlock(block);\n        break;\n      case UNDER_CONSTRUCTION:\n        nrUnderConstruction++;\n        break;\n      case OK:\n        break;\n      default:\n        throw new AssertionError(\"Invalid enum value: \" + res);\n      }\n    }\n    \n    LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n    LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n    LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated +\n        ((nrPostponed \u003e 0) ? ( \" (\" + nrPostponed + \" postponed)\") : \"\"));\n    LOG.info(\"Number of blocks being written    \u003d \" + nrUnderConstruction);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "12645ff6c06bc514930177a78610017897e2e0b7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2822. processMisReplicatedBlock incorrectly identifies under-construction blocks as under-replicated. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1234925 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/01/12 10:11 AM",
      "commitName": "12645ff6c06bc514930177a78610017897e2e0b7",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "29/12/11 8:51 AM",
      "commitNameOld": "6c349f9420ff19daa00d1d7bee77942a41d9f358",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 25.05,
      "commitsBetweenForRepo": 97,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,45 @@\n   public void processMisReplicatedBlocks() {\n     assert namesystem.hasWriteLock();\n \n-    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n+    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0,\n+         nrUnderConstruction \u003d 0;\n     neededReplications.clear();\n     for (BlockInfo block : blocksMap.getBlocks()) {\n       INodeFile fileINode \u003d block.getINode();\n       if (fileINode \u003d\u003d null) {\n         // block does not belong to any file\n         nrInvalid++;\n         addToInvalidates(block);\n         continue;\n       }\n+      if (!block.isComplete()) {\n+        // Incomplete blocks are never considered mis-replicated --\n+        // they\u0027ll be reached when they are completed or recovered.\n+        nrUnderConstruction++;\n+        continue;\n+      }\n       // calculate current replication\n       short expectedReplication \u003d fileINode.getReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       // add to under-replicated queue if need to be\n       if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n         if (neededReplications.add(block, numCurrentReplica, num\n             .decommissionedReplicas(), expectedReplication)) {\n           nrUnderReplicated++;\n         }\n       }\n \n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block\n         nrOverReplicated++;\n         processOverReplicatedBlock(block, expectedReplication, null, null);\n       }\n     }\n \n     LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n     LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n     LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n     LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n+    LOG.info(\"Number of blocks being written    \u003d \" + nrUnderConstruction);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processMisReplicatedBlocks() {\n    assert namesystem.hasWriteLock();\n\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0,\n         nrUnderConstruction \u003d 0;\n    neededReplications.clear();\n    for (BlockInfo block : blocksMap.getBlocks()) {\n      INodeFile fileINode \u003d block.getINode();\n      if (fileINode \u003d\u003d null) {\n        // block does not belong to any file\n        nrInvalid++;\n        addToInvalidates(block);\n        continue;\n      }\n      if (!block.isComplete()) {\n        // Incomplete blocks are never considered mis-replicated --\n        // they\u0027ll be reached when they are completed or recovered.\n        nrUnderConstruction++;\n        continue;\n      }\n      // calculate current replication\n      short expectedReplication \u003d fileINode.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      // add to under-replicated queue if need to be\n      if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n        if (neededReplications.add(block, numCurrentReplica, num\n            .decommissionedReplicas(), expectedReplication)) {\n          nrUnderReplicated++;\n        }\n      }\n\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block\n        nrOverReplicated++;\n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n\n    LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n    LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n    LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n    LOG.info(\"Number of blocks being written    \u003d \" + nrUnderConstruction);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "31c91706f7d17da006ef2d6c541f8dd092fae077": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1972. Fencing mechanism for block invalidations and replications. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1221608 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/11 8:32 PM",
      "commitName": "31c91706f7d17da006ef2d6c541f8dd092fae077",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "20/12/11 7:03 PM",
      "commitNameOld": "36d1c49486587c2dbb193e8538b1d4510c462fa6",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,35 @@\n   public void processMisReplicatedBlocks() {\n     assert namesystem.hasWriteLock();\n \n-    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n+    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0, nrPostponed \u003d 0;\n     neededReplications.clear();\n     for (BlockInfo block : blocksMap.getBlocks()) {\n-      INodeFile fileINode \u003d block.getINode();\n-      if (fileINode \u003d\u003d null) {\n-        // block does not belong to any file\n-        nrInvalid++;\n-        addToInvalidates(block);\n-        continue;\n-      }\n-      // calculate current replication\n-      short expectedReplication \u003d fileINode.getReplication();\n-      NumberReplicas num \u003d countNodes(block);\n-      int numCurrentReplica \u003d num.liveReplicas();\n-      // add to under-replicated queue if need to be\n-      if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n-        if (neededReplications.add(block, numCurrentReplica, num\n-            .decommissionedReplicas(), expectedReplication)) {\n-          nrUnderReplicated++;\n-        }\n-      }\n-\n-      if (numCurrentReplica \u003e expectedReplication) {\n-        // over-replicated block\n+      MisReplicationResult res \u003d processMisReplicatedBlock(block);\n+      LOG.info(\"block \" + block + \": \" + res);\n+      switch (res) {\n+      case UNDER_REPLICATED:\n+        nrUnderReplicated++;\n+        break;\n+      case OVER_REPLICATED:\n         nrOverReplicated++;\n-        processOverReplicatedBlock(block, expectedReplication, null, null);\n+        break;\n+      case INVALID:\n+        nrInvalid++;\n+        break;\n+      case POSTPONE:\n+        nrPostponed++;\n+        postponeBlock(block);\n+        break;\n+      case OK:\n+        break;\n+      default:\n+        throw new AssertionError(\"Invalid enum value: \" + res);\n       }\n     }\n-\n+    \n     LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n     LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n     LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n-    LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n+    LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated +\n+        ((nrPostponed \u003e 0) ? ( \" (\" + nrPostponed + \" postponed)\") : \"\"));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processMisReplicatedBlocks() {\n    assert namesystem.hasWriteLock();\n\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0, nrPostponed \u003d 0;\n    neededReplications.clear();\n    for (BlockInfo block : blocksMap.getBlocks()) {\n      MisReplicationResult res \u003d processMisReplicatedBlock(block);\n      LOG.info(\"block \" + block + \": \" + res);\n      switch (res) {\n      case UNDER_REPLICATED:\n        nrUnderReplicated++;\n        break;\n      case OVER_REPLICATED:\n        nrOverReplicated++;\n        break;\n      case INVALID:\n        nrInvalid++;\n        break;\n      case POSTPONE:\n        nrPostponed++;\n        postponeBlock(block);\n        break;\n      case OK:\n        break;\n      default:\n        throw new AssertionError(\"Invalid enum value: \" + res);\n      }\n    }\n    \n    LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n    LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n    LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated +\n        ((nrPostponed \u003e 0) ? ( \" (\" + nrPostponed + \" postponed)\") : \"\"));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void processMisReplicatedBlocks() {\n    assert namesystem.hasWriteLock();\n\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n    neededReplications.clear();\n    for (BlockInfo block : blocksMap.getBlocks()) {\n      INodeFile fileINode \u003d block.getINode();\n      if (fileINode \u003d\u003d null) {\n        // block does not belong to any file\n        nrInvalid++;\n        addToInvalidates(block);\n        continue;\n      }\n      // calculate current replication\n      short expectedReplication \u003d fileINode.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      // add to under-replicated queue if need to be\n      if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n        if (neededReplications.add(block, numCurrentReplica, num\n            .decommissionedReplicas(), expectedReplication)) {\n          nrUnderReplicated++;\n        }\n      }\n\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block\n        nrOverReplicated++;\n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n\n    LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n    LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n    LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void processMisReplicatedBlocks() {\n    assert namesystem.hasWriteLock();\n\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n    neededReplications.clear();\n    for (BlockInfo block : blocksMap.getBlocks()) {\n      INodeFile fileINode \u003d block.getINode();\n      if (fileINode \u003d\u003d null) {\n        // block does not belong to any file\n        nrInvalid++;\n        addToInvalidates(block);\n        continue;\n      }\n      // calculate current replication\n      short expectedReplication \u003d fileINode.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      // add to under-replicated queue if need to be\n      if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n        if (neededReplications.add(block, numCurrentReplica, num\n            .decommissionedReplicas(), expectedReplication)) {\n          nrUnderReplicated++;\n        }\n      }\n\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block\n        nrOverReplicated++;\n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n\n    LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n    LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n    LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "1dd48b1aee2378c02ee7e78864a757cff3607274": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2229. Fix a deadlock in namenode by enforcing lock acquisition ordering.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1156847 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/08/11 3:16 PM",
      "commitName": "1dd48b1aee2378c02ee7e78864a757cff3607274",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "10/08/11 10:24 PM",
      "commitNameOld": "6a7c0306bd9a600a943abb61606aaaf4f9beafee",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.7,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,37 @@\n   public void processMisReplicatedBlocks() {\n-    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n-    namesystem.writeLock();\n-    try {\n-      neededReplications.clear();\n-      for (BlockInfo block : blocksMap.getBlocks()) {\n-        INodeFile fileINode \u003d block.getINode();\n-        if (fileINode \u003d\u003d null) {\n-          // block does not belong to any file\n-          nrInvalid++;\n-          addToInvalidates(block);\n-          continue;\n-        }\n-        // calculate current replication\n-        short expectedReplication \u003d fileINode.getReplication();\n-        NumberReplicas num \u003d countNodes(block);\n-        int numCurrentReplica \u003d num.liveReplicas();\n-        // add to under-replicated queue if need to be\n-        if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n-          if (neededReplications.add(block, numCurrentReplica, num\n-              .decommissionedReplicas(), expectedReplication)) {\n-            nrUnderReplicated++;\n-          }\n-        }\n+    assert namesystem.hasWriteLock();\n \n-        if (numCurrentReplica \u003e expectedReplication) {\n-          // over-replicated block\n-          nrOverReplicated++;\n-          processOverReplicatedBlock(block, expectedReplication, null, null);\n+    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n+    neededReplications.clear();\n+    for (BlockInfo block : blocksMap.getBlocks()) {\n+      INodeFile fileINode \u003d block.getINode();\n+      if (fileINode \u003d\u003d null) {\n+        // block does not belong to any file\n+        nrInvalid++;\n+        addToInvalidates(block);\n+        continue;\n+      }\n+      // calculate current replication\n+      short expectedReplication \u003d fileINode.getReplication();\n+      NumberReplicas num \u003d countNodes(block);\n+      int numCurrentReplica \u003d num.liveReplicas();\n+      // add to under-replicated queue if need to be\n+      if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n+        if (neededReplications.add(block, numCurrentReplica, num\n+            .decommissionedReplicas(), expectedReplication)) {\n+          nrUnderReplicated++;\n         }\n       }\n-    } finally {\n-      namesystem.writeUnlock();\n+\n+      if (numCurrentReplica \u003e expectedReplication) {\n+        // over-replicated block\n+        nrOverReplicated++;\n+        processOverReplicatedBlock(block, expectedReplication, null, null);\n+      }\n     }\n+\n     LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n     LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n     LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n     LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processMisReplicatedBlocks() {\n    assert namesystem.hasWriteLock();\n\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n    neededReplications.clear();\n    for (BlockInfo block : blocksMap.getBlocks()) {\n      INodeFile fileINode \u003d block.getINode();\n      if (fileINode \u003d\u003d null) {\n        // block does not belong to any file\n        nrInvalid++;\n        addToInvalidates(block);\n        continue;\n      }\n      // calculate current replication\n      short expectedReplication \u003d fileINode.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      // add to under-replicated queue if need to be\n      if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n        if (neededReplications.add(block, numCurrentReplica, num\n            .decommissionedReplicas(), expectedReplication)) {\n          nrUnderReplicated++;\n        }\n      }\n\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block\n        nrOverReplicated++;\n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n\n    LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n    LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n    LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "0b12cc822ddd57e6ecf4f7047f6614419c34580b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2200.  Change FSNamesystem.LOG to package private.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151344 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/11 10:59 PM",
      "commitName": "0b12cc822ddd57e6ecf4f7047f6614419c34580b",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "26/07/11 10:46 PM",
      "commitNameOld": "969a263188f7015261719fe45fa1505121ebb80e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,39 @@\n   public void processMisReplicatedBlocks() {\n     long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n     namesystem.writeLock();\n     try {\n       neededReplications.clear();\n       for (BlockInfo block : blocksMap.getBlocks()) {\n         INodeFile fileINode \u003d block.getINode();\n         if (fileINode \u003d\u003d null) {\n           // block does not belong to any file\n           nrInvalid++;\n           addToInvalidates(block);\n           continue;\n         }\n         // calculate current replication\n         short expectedReplication \u003d fileINode.getReplication();\n         NumberReplicas num \u003d countNodes(block);\n         int numCurrentReplica \u003d num.liveReplicas();\n         // add to under-replicated queue if need to be\n         if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n           if (neededReplications.add(block, numCurrentReplica, num\n               .decommissionedReplicas(), expectedReplication)) {\n             nrUnderReplicated++;\n           }\n         }\n \n         if (numCurrentReplica \u003e expectedReplication) {\n           // over-replicated block\n           nrOverReplicated++;\n           processOverReplicatedBlock(block, expectedReplication, null, null);\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n-    FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n-    FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n-    FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n-    FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n+    LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n+    LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n+    LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n+    LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void processMisReplicatedBlocks() {\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n    namesystem.writeLock();\n    try {\n      neededReplications.clear();\n      for (BlockInfo block : blocksMap.getBlocks()) {\n        INodeFile fileINode \u003d block.getINode();\n        if (fileINode \u003d\u003d null) {\n          // block does not belong to any file\n          nrInvalid++;\n          addToInvalidates(block);\n          continue;\n        }\n        // calculate current replication\n        short expectedReplication \u003d fileINode.getReplication();\n        NumberReplicas num \u003d countNodes(block);\n        int numCurrentReplica \u003d num.liveReplicas();\n        // add to under-replicated queue if need to be\n        if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n          if (neededReplications.add(block, numCurrentReplica, num\n              .decommissionedReplicas(), expectedReplication)) {\n            nrUnderReplicated++;\n          }\n        }\n\n        if (numCurrentReplica \u003e expectedReplication) {\n          // over-replicated block\n          nrOverReplicated++;\n          processOverReplicatedBlock(block, expectedReplication, null, null);\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n    LOG.info(\"Total number of blocks            \u003d \" + blocksMap.size());\n    LOG.info(\"Number of invalid blocks          \u003d \" + nrInvalid);\n    LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 6:31 PM",
      "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 6:31 PM",
          "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 5:26 PM",
          "commitNameOld": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,39 @@\n-  void processMisReplicatedBlocks() {\n+  public void processMisReplicatedBlocks() {\n     long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n     namesystem.writeLock();\n     try {\n       neededReplications.clear();\n       for (BlockInfo block : blocksMap.getBlocks()) {\n         INodeFile fileINode \u003d block.getINode();\n         if (fileINode \u003d\u003d null) {\n           // block does not belong to any file\n           nrInvalid++;\n           addToInvalidates(block);\n           continue;\n         }\n         // calculate current replication\n         short expectedReplication \u003d fileINode.getReplication();\n         NumberReplicas num \u003d countNodes(block);\n         int numCurrentReplica \u003d num.liveReplicas();\n         // add to under-replicated queue if need to be\n         if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n           if (neededReplications.add(block, numCurrentReplica, num\n               .decommissionedReplicas(), expectedReplication)) {\n             nrUnderReplicated++;\n           }\n         }\n \n         if (numCurrentReplica \u003e expectedReplication) {\n           // over-replicated block\n           nrOverReplicated++;\n           processOverReplicatedBlock(block, expectedReplication, null, null);\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n     FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n     FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n     FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n     FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void processMisReplicatedBlocks() {\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n    namesystem.writeLock();\n    try {\n      neededReplications.clear();\n      for (BlockInfo block : blocksMap.getBlocks()) {\n        INodeFile fileINode \u003d block.getINode();\n        if (fileINode \u003d\u003d null) {\n          // block does not belong to any file\n          nrInvalid++;\n          addToInvalidates(block);\n          continue;\n        }\n        // calculate current replication\n        short expectedReplication \u003d fileINode.getReplication();\n        NumberReplicas num \u003d countNodes(block);\n        int numCurrentReplica \u003d num.liveReplicas();\n        // add to under-replicated queue if need to be\n        if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n          if (neededReplications.add(block, numCurrentReplica, num\n              .decommissionedReplicas(), expectedReplication)) {\n            nrUnderReplicated++;\n          }\n        }\n\n        if (numCurrentReplica \u003e expectedReplication) {\n          // over-replicated block\n          nrOverReplicated++;\n          processOverReplicatedBlock(block, expectedReplication, null, null);\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n    FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n    FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n    FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 6:31 PM",
          "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 5:26 PM",
          "commitNameOld": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,39 @@\n-  void processMisReplicatedBlocks() {\n+  public void processMisReplicatedBlocks() {\n     long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n     namesystem.writeLock();\n     try {\n       neededReplications.clear();\n       for (BlockInfo block : blocksMap.getBlocks()) {\n         INodeFile fileINode \u003d block.getINode();\n         if (fileINode \u003d\u003d null) {\n           // block does not belong to any file\n           nrInvalid++;\n           addToInvalidates(block);\n           continue;\n         }\n         // calculate current replication\n         short expectedReplication \u003d fileINode.getReplication();\n         NumberReplicas num \u003d countNodes(block);\n         int numCurrentReplica \u003d num.liveReplicas();\n         // add to under-replicated queue if need to be\n         if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n           if (neededReplications.add(block, numCurrentReplica, num\n               .decommissionedReplicas(), expectedReplication)) {\n             nrUnderReplicated++;\n           }\n         }\n \n         if (numCurrentReplica \u003e expectedReplication) {\n           // over-replicated block\n           nrOverReplicated++;\n           processOverReplicatedBlock(block, expectedReplication, null, null);\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n     FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n     FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n     FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n     FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void processMisReplicatedBlocks() {\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n    namesystem.writeLock();\n    try {\n      neededReplications.clear();\n      for (BlockInfo block : blocksMap.getBlocks()) {\n        INodeFile fileINode \u003d block.getINode();\n        if (fileINode \u003d\u003d null) {\n          // block does not belong to any file\n          nrInvalid++;\n          addToInvalidates(block);\n          continue;\n        }\n        // calculate current replication\n        short expectedReplication \u003d fileINode.getReplication();\n        NumberReplicas num \u003d countNodes(block);\n        int numCurrentReplica \u003d num.liveReplicas();\n        // add to under-replicated queue if need to be\n        if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n          if (neededReplications.add(block, numCurrentReplica, num\n              .decommissionedReplicas(), expectedReplication)) {\n            nrUnderReplicated++;\n          }\n        }\n\n        if (numCurrentReplica \u003e expectedReplication) {\n          // over-replicated block\n          nrOverReplicated++;\n          processOverReplicatedBlock(block, expectedReplication, null, null);\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n    FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n    FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n    FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[public]"
          }
        }
      ]
    },
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 5:26 PM",
      "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 5:26 PM",
          "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 4:57 PM",
          "commitNameOld": "d58e3efe9269efe00c309ed0e9726d2f94bcd03a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,39 @@\n-  public void processMisReplicatedBlocks() {\n+  void processMisReplicatedBlocks() {\n     long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n     namesystem.writeLock();\n     try {\n       neededReplications.clear();\n       for (BlockInfo block : blocksMap.getBlocks()) {\n         INodeFile fileINode \u003d block.getINode();\n         if (fileINode \u003d\u003d null) {\n           // block does not belong to any file\n           nrInvalid++;\n           addToInvalidates(block);\n           continue;\n         }\n         // calculate current replication\n         short expectedReplication \u003d fileINode.getReplication();\n         NumberReplicas num \u003d countNodes(block);\n         int numCurrentReplica \u003d num.liveReplicas();\n         // add to under-replicated queue if need to be\n         if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n           if (neededReplications.add(block, numCurrentReplica, num\n               .decommissionedReplicas(), expectedReplication)) {\n             nrUnderReplicated++;\n           }\n         }\n \n         if (numCurrentReplica \u003e expectedReplication) {\n           // over-replicated block\n           nrOverReplicated++;\n           processOverReplicatedBlock(block, expectedReplication, null, null);\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n     FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n     FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n     FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n     FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void processMisReplicatedBlocks() {\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n    namesystem.writeLock();\n    try {\n      neededReplications.clear();\n      for (BlockInfo block : blocksMap.getBlocks()) {\n        INodeFile fileINode \u003d block.getINode();\n        if (fileINode \u003d\u003d null) {\n          // block does not belong to any file\n          nrInvalid++;\n          addToInvalidates(block);\n          continue;\n        }\n        // calculate current replication\n        short expectedReplication \u003d fileINode.getReplication();\n        NumberReplicas num \u003d countNodes(block);\n        int numCurrentReplica \u003d num.liveReplicas();\n        // add to under-replicated queue if need to be\n        if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n          if (neededReplications.add(block, numCurrentReplica, num\n              .decommissionedReplicas(), expectedReplication)) {\n            nrUnderReplicated++;\n          }\n        }\n\n        if (numCurrentReplica \u003e expectedReplication) {\n          // over-replicated block\n          nrOverReplicated++;\n          processOverReplicatedBlock(block, expectedReplication, null, null);\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n    FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n    FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n    FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 5:26 PM",
          "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 4:57 PM",
          "commitNameOld": "d58e3efe9269efe00c309ed0e9726d2f94bcd03a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,39 @@\n-  public void processMisReplicatedBlocks() {\n+  void processMisReplicatedBlocks() {\n     long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n     namesystem.writeLock();\n     try {\n       neededReplications.clear();\n       for (BlockInfo block : blocksMap.getBlocks()) {\n         INodeFile fileINode \u003d block.getINode();\n         if (fileINode \u003d\u003d null) {\n           // block does not belong to any file\n           nrInvalid++;\n           addToInvalidates(block);\n           continue;\n         }\n         // calculate current replication\n         short expectedReplication \u003d fileINode.getReplication();\n         NumberReplicas num \u003d countNodes(block);\n         int numCurrentReplica \u003d num.liveReplicas();\n         // add to under-replicated queue if need to be\n         if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n           if (neededReplications.add(block, numCurrentReplica, num\n               .decommissionedReplicas(), expectedReplication)) {\n             nrUnderReplicated++;\n           }\n         }\n \n         if (numCurrentReplica \u003e expectedReplication) {\n           // over-replicated block\n           nrOverReplicated++;\n           processOverReplicatedBlock(block, expectedReplication, null, null);\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n     FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n     FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n     FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n     FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void processMisReplicatedBlocks() {\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n    namesystem.writeLock();\n    try {\n      neededReplications.clear();\n      for (BlockInfo block : blocksMap.getBlocks()) {\n        INodeFile fileINode \u003d block.getINode();\n        if (fileINode \u003d\u003d null) {\n          // block does not belong to any file\n          nrInvalid++;\n          addToInvalidates(block);\n          continue;\n        }\n        // calculate current replication\n        short expectedReplication \u003d fileINode.getReplication();\n        NumberReplicas num \u003d countNodes(block);\n        int numCurrentReplica \u003d num.liveReplicas();\n        // add to under-replicated queue if need to be\n        if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n          if (neededReplications.add(block, numCurrentReplica, num\n              .decommissionedReplicas(), expectedReplication)) {\n            nrUnderReplicated++;\n          }\n        }\n\n        if (numCurrentReplica \u003e expectedReplication) {\n          // over-replicated block\n          nrOverReplicated++;\n          processOverReplicatedBlock(block, expectedReplication, null, null);\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n    FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n    FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n    FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        }
      ]
    },
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 4:43 PM",
      "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 4:43 PM",
          "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 9:21 AM",
          "commitNameOld": "1834fb99f516b2f2cd5e0ab1f89d407f98a7237a",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.31,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,39 @@\n-  void processMisReplicatedBlocks() {\n+  public void processMisReplicatedBlocks() {\n     long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n     namesystem.writeLock();\n     try {\n       neededReplications.clear();\n       for (BlockInfo block : blocksMap.getBlocks()) {\n         INodeFile fileINode \u003d block.getINode();\n         if (fileINode \u003d\u003d null) {\n           // block does not belong to any file\n           nrInvalid++;\n           addToInvalidates(block);\n           continue;\n         }\n         // calculate current replication\n         short expectedReplication \u003d fileINode.getReplication();\n         NumberReplicas num \u003d countNodes(block);\n         int numCurrentReplica \u003d num.liveReplicas();\n         // add to under-replicated queue if need to be\n         if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n           if (neededReplications.add(block, numCurrentReplica, num\n               .decommissionedReplicas(), expectedReplication)) {\n             nrUnderReplicated++;\n           }\n         }\n \n         if (numCurrentReplica \u003e expectedReplication) {\n           // over-replicated block\n           nrOverReplicated++;\n           processOverReplicatedBlock(block, expectedReplication, null, null);\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n     FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n     FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n     FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n     FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void processMisReplicatedBlocks() {\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n    namesystem.writeLock();\n    try {\n      neededReplications.clear();\n      for (BlockInfo block : blocksMap.getBlocks()) {\n        INodeFile fileINode \u003d block.getINode();\n        if (fileINode \u003d\u003d null) {\n          // block does not belong to any file\n          nrInvalid++;\n          addToInvalidates(block);\n          continue;\n        }\n        // calculate current replication\n        short expectedReplication \u003d fileINode.getReplication();\n        NumberReplicas num \u003d countNodes(block);\n        int numCurrentReplica \u003d num.liveReplicas();\n        // add to under-replicated queue if need to be\n        if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n          if (neededReplications.add(block, numCurrentReplica, num\n              .decommissionedReplicas(), expectedReplication)) {\n            nrUnderReplicated++;\n          }\n        }\n\n        if (numCurrentReplica \u003e expectedReplication) {\n          // over-replicated block\n          nrOverReplicated++;\n          processOverReplicatedBlock(block, expectedReplication, null, null);\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n    FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n    FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n    FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 4:43 PM",
          "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 9:21 AM",
          "commitNameOld": "1834fb99f516b2f2cd5e0ab1f89d407f98a7237a",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.31,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,39 @@\n-  void processMisReplicatedBlocks() {\n+  public void processMisReplicatedBlocks() {\n     long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n     namesystem.writeLock();\n     try {\n       neededReplications.clear();\n       for (BlockInfo block : blocksMap.getBlocks()) {\n         INodeFile fileINode \u003d block.getINode();\n         if (fileINode \u003d\u003d null) {\n           // block does not belong to any file\n           nrInvalid++;\n           addToInvalidates(block);\n           continue;\n         }\n         // calculate current replication\n         short expectedReplication \u003d fileINode.getReplication();\n         NumberReplicas num \u003d countNodes(block);\n         int numCurrentReplica \u003d num.liveReplicas();\n         // add to under-replicated queue if need to be\n         if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n           if (neededReplications.add(block, numCurrentReplica, num\n               .decommissionedReplicas(), expectedReplication)) {\n             nrUnderReplicated++;\n           }\n         }\n \n         if (numCurrentReplica \u003e expectedReplication) {\n           // over-replicated block\n           nrOverReplicated++;\n           processOverReplicatedBlock(block, expectedReplication, null, null);\n         }\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n     FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n     FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n     FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n     FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void processMisReplicatedBlocks() {\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n    namesystem.writeLock();\n    try {\n      neededReplications.clear();\n      for (BlockInfo block : blocksMap.getBlocks()) {\n        INodeFile fileINode \u003d block.getINode();\n        if (fileINode \u003d\u003d null) {\n          // block does not belong to any file\n          nrInvalid++;\n          addToInvalidates(block);\n          continue;\n        }\n        // calculate current replication\n        short expectedReplication \u003d fileINode.getReplication();\n        NumberReplicas num \u003d countNodes(block);\n        int numCurrentReplica \u003d num.liveReplicas();\n        // add to under-replicated queue if need to be\n        if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n          if (neededReplications.add(block, numCurrentReplica, num\n              .decommissionedReplicas(), expectedReplication)) {\n            nrUnderReplicated++;\n          }\n        }\n\n        if (numCurrentReplica \u003e expectedReplication) {\n          // over-replicated block\n          nrOverReplicated++;\n          processOverReplicatedBlock(block, expectedReplication, null, null);\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n    FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n    FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n    FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[public]"
          }
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,39 @@\n+  void processMisReplicatedBlocks() {\n+    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n+    namesystem.writeLock();\n+    try {\n+      neededReplications.clear();\n+      for (BlockInfo block : blocksMap.getBlocks()) {\n+        INodeFile fileINode \u003d block.getINode();\n+        if (fileINode \u003d\u003d null) {\n+          // block does not belong to any file\n+          nrInvalid++;\n+          addToInvalidates(block);\n+          continue;\n+        }\n+        // calculate current replication\n+        short expectedReplication \u003d fileINode.getReplication();\n+        NumberReplicas num \u003d countNodes(block);\n+        int numCurrentReplica \u003d num.liveReplicas();\n+        // add to under-replicated queue if need to be\n+        if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n+          if (neededReplications.add(block, numCurrentReplica, num\n+              .decommissionedReplicas(), expectedReplication)) {\n+            nrUnderReplicated++;\n+          }\n+        }\n+\n+        if (numCurrentReplica \u003e expectedReplication) {\n+          // over-replicated block\n+          nrOverReplicated++;\n+          processOverReplicatedBlock(block, expectedReplication, null, null);\n+        }\n+      }\n+    } finally {\n+      namesystem.writeUnlock();\n+    }\n+    FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n+    FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n+    FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n+    FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void processMisReplicatedBlocks() {\n    long nrInvalid \u003d 0, nrOverReplicated \u003d 0, nrUnderReplicated \u003d 0;\n    namesystem.writeLock();\n    try {\n      neededReplications.clear();\n      for (BlockInfo block : blocksMap.getBlocks()) {\n        INodeFile fileINode \u003d block.getINode();\n        if (fileINode \u003d\u003d null) {\n          // block does not belong to any file\n          nrInvalid++;\n          addToInvalidates(block);\n          continue;\n        }\n        // calculate current replication\n        short expectedReplication \u003d fileINode.getReplication();\n        NumberReplicas num \u003d countNodes(block);\n        int numCurrentReplica \u003d num.liveReplicas();\n        // add to under-replicated queue if need to be\n        if (isNeededReplication(block, expectedReplication, numCurrentReplica)) {\n          if (neededReplications.add(block, numCurrentReplica, num\n              .decommissionedReplicas(), expectedReplication)) {\n            nrUnderReplicated++;\n          }\n        }\n\n        if (numCurrentReplica \u003e expectedReplication) {\n          // over-replicated block\n          nrOverReplicated++;\n          processOverReplicatedBlock(block, expectedReplication, null, null);\n        }\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n    FSNamesystem.LOG.info(\"Total number of blocks \u003d \" + blocksMap.size());\n    FSNamesystem.LOG.info(\"Number of invalid blocks \u003d \" + nrInvalid);\n    FSNamesystem.LOG.info(\"Number of under-replicated blocks \u003d \" + nrUnderReplicated);\n    FSNamesystem.LOG.info(\"Number of  over-replicated blocks \u003d \" + nrOverReplicated);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
    }
  }
}