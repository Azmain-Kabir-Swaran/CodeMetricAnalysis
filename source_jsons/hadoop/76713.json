{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RSRawDecoder.java",
  "functionName": "doDecode",
  "functionId": "doDecode___decodingState-ByteArrayDecodingState",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java",
  "functionStartLine": 87,
  "functionEndLine": 101,
  "numCommitsSeen": 22,
  "timeTaken": 2344,
  "changeHistory": [
    "77202fa1035a54496d11d07472fbc399148ff630",
    "19e8f076919932b17f24ec4090df1926677651e7",
    "c89a14a8a4fe58f01f0cba643f2bc203e1a8701e"
  ],
  "changeHistoryShort": {
    "77202fa1035a54496d11d07472fbc399148ff630": "Ymultichange(Yparameterchange,Ybodychange)",
    "19e8f076919932b17f24ec4090df1926677651e7": "Ymultichange(Yfilerename,Ybodychange)",
    "c89a14a8a4fe58f01f0cba643f2bc203e1a8701e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "77202fa1035a54496d11d07472fbc399148ff630": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
      "commitDate": "26/05/16 10:23 PM",
      "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
          "commitDate": "26/05/16 10:23 PM",
          "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "14/03/16 4:45 PM",
          "commitNameOld": "19e8f076919932b17f24ec4090df1926677651e7",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 73.23,
          "commitsBetweenForRepo": 460,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,15 @@\n-  protected void doDecode(byte[][] inputs, int[] inputOffsets,\n-                          int dataLen, int[] erasedIndexes,\n-                          byte[][] outputs, int[] outputOffsets) {\n-    prepareDecoding(inputs, erasedIndexes);\n+  protected void doDecode(ByteArrayDecodingState decodingState) {\n+    int dataLen \u003d decodingState.decodeLength;\n+    CoderUtil.resetOutputBuffers(decodingState.outputs,\n+        decodingState.outputOffsets, dataLen);\n+    prepareDecoding(decodingState.inputs, decodingState.erasedIndexes);\n \n     byte[][] realInputs \u003d new byte[getNumDataUnits()][];\n     int[] realInputOffsets \u003d new int[getNumDataUnits()];\n     for (int i \u003d 0; i \u003c getNumDataUnits(); i++) {\n-      realInputs[i] \u003d inputs[validIndexes[i]];\n-      realInputOffsets[i] \u003d inputOffsets[validIndexes[i]];\n+      realInputs[i] \u003d decodingState.inputs[validIndexes[i]];\n+      realInputOffsets[i] \u003d decodingState.inputOffsets[validIndexes[i]];\n     }\n     RSUtil.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n-            outputs, outputOffsets);\n+        decodingState.outputs, decodingState.outputOffsets);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void doDecode(ByteArrayDecodingState decodingState) {\n    int dataLen \u003d decodingState.decodeLength;\n    CoderUtil.resetOutputBuffers(decodingState.outputs,\n        decodingState.outputOffsets, dataLen);\n    prepareDecoding(decodingState.inputs, decodingState.erasedIndexes);\n\n    byte[][] realInputs \u003d new byte[getNumDataUnits()][];\n    int[] realInputOffsets \u003d new int[getNumDataUnits()];\n    for (int i \u003d 0; i \u003c getNumDataUnits(); i++) {\n      realInputs[i] \u003d decodingState.inputs[validIndexes[i]];\n      realInputOffsets[i] \u003d decodingState.inputOffsets[validIndexes[i]];\n    }\n    RSUtil.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n        decodingState.outputs, decodingState.outputOffsets);\n  }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java",
          "extendedDetails": {
            "oldValue": "[inputs-byte[][], inputOffsets-int[], dataLen-int, erasedIndexes-int[], outputs-byte[][], outputOffsets-int[]]",
            "newValue": "[decodingState-ByteArrayDecodingState]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
          "commitDate": "26/05/16 10:23 PM",
          "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "14/03/16 4:45 PM",
          "commitNameOld": "19e8f076919932b17f24ec4090df1926677651e7",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 73.23,
          "commitsBetweenForRepo": 460,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,15 @@\n-  protected void doDecode(byte[][] inputs, int[] inputOffsets,\n-                          int dataLen, int[] erasedIndexes,\n-                          byte[][] outputs, int[] outputOffsets) {\n-    prepareDecoding(inputs, erasedIndexes);\n+  protected void doDecode(ByteArrayDecodingState decodingState) {\n+    int dataLen \u003d decodingState.decodeLength;\n+    CoderUtil.resetOutputBuffers(decodingState.outputs,\n+        decodingState.outputOffsets, dataLen);\n+    prepareDecoding(decodingState.inputs, decodingState.erasedIndexes);\n \n     byte[][] realInputs \u003d new byte[getNumDataUnits()][];\n     int[] realInputOffsets \u003d new int[getNumDataUnits()];\n     for (int i \u003d 0; i \u003c getNumDataUnits(); i++) {\n-      realInputs[i] \u003d inputs[validIndexes[i]];\n-      realInputOffsets[i] \u003d inputOffsets[validIndexes[i]];\n+      realInputs[i] \u003d decodingState.inputs[validIndexes[i]];\n+      realInputOffsets[i] \u003d decodingState.inputOffsets[validIndexes[i]];\n     }\n     RSUtil.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n-            outputs, outputOffsets);\n+        decodingState.outputs, decodingState.outputOffsets);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void doDecode(ByteArrayDecodingState decodingState) {\n    int dataLen \u003d decodingState.decodeLength;\n    CoderUtil.resetOutputBuffers(decodingState.outputs,\n        decodingState.outputOffsets, dataLen);\n    prepareDecoding(decodingState.inputs, decodingState.erasedIndexes);\n\n    byte[][] realInputs \u003d new byte[getNumDataUnits()][];\n    int[] realInputOffsets \u003d new int[getNumDataUnits()];\n    for (int i \u003d 0; i \u003c getNumDataUnits(); i++) {\n      realInputs[i] \u003d decodingState.inputs[validIndexes[i]];\n      realInputOffsets[i] \u003d decodingState.inputOffsets[validIndexes[i]];\n    }\n    RSUtil.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n        decodingState.outputs, decodingState.outputOffsets);\n  }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java",
          "extendedDetails": {}
        }
      ]
    },
    "19e8f076919932b17f24ec4090df1926677651e7": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HADOOP-12826. Rename the new Java coder and make it default. Contributed by Rui Li.\n",
      "commitDate": "14/03/16 4:45 PM",
      "commitName": "19e8f076919932b17f24ec4090df1926677651e7",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HADOOP-12826. Rename the new Java coder and make it default. Contributed by Rui Li.\n",
          "commitDate": "14/03/16 4:45 PM",
          "commitName": "19e8f076919932b17f24ec4090df1926677651e7",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "14/03/16 3:48 PM",
          "commitNameOld": "1898810cda83e6d273a2963b56ed499c0fb91118",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,14 @@\n   protected void doDecode(byte[][] inputs, int[] inputOffsets,\n                           int dataLen, int[] erasedIndexes,\n                           byte[][] outputs, int[] outputOffsets) {\n     prepareDecoding(inputs, erasedIndexes);\n \n     byte[][] realInputs \u003d new byte[getNumDataUnits()][];\n     int[] realInputOffsets \u003d new int[getNumDataUnits()];\n     for (int i \u003d 0; i \u003c getNumDataUnits(); i++) {\n       realInputs[i] \u003d inputs[validIndexes[i]];\n       realInputOffsets[i] \u003d inputOffsets[validIndexes[i]];\n     }\n-    RSUtil2.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n+    RSUtil.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n             outputs, outputOffsets);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void doDecode(byte[][] inputs, int[] inputOffsets,\n                          int dataLen, int[] erasedIndexes,\n                          byte[][] outputs, int[] outputOffsets) {\n    prepareDecoding(inputs, erasedIndexes);\n\n    byte[][] realInputs \u003d new byte[getNumDataUnits()][];\n    int[] realInputOffsets \u003d new int[getNumDataUnits()];\n    for (int i \u003d 0; i \u003c getNumDataUnits(); i++) {\n      realInputs[i] \u003d inputs[validIndexes[i]];\n      realInputOffsets[i] \u003d inputOffsets[validIndexes[i]];\n    }\n    RSUtil.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n            outputs, outputOffsets);\n  }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java",
          "extendedDetails": {
            "oldPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder2.java",
            "newPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-12826. Rename the new Java coder and make it default. Contributed by Rui Li.\n",
          "commitDate": "14/03/16 4:45 PM",
          "commitName": "19e8f076919932b17f24ec4090df1926677651e7",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "14/03/16 3:48 PM",
          "commitNameOld": "1898810cda83e6d273a2963b56ed499c0fb91118",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,14 @@\n   protected void doDecode(byte[][] inputs, int[] inputOffsets,\n                           int dataLen, int[] erasedIndexes,\n                           byte[][] outputs, int[] outputOffsets) {\n     prepareDecoding(inputs, erasedIndexes);\n \n     byte[][] realInputs \u003d new byte[getNumDataUnits()][];\n     int[] realInputOffsets \u003d new int[getNumDataUnits()];\n     for (int i \u003d 0; i \u003c getNumDataUnits(); i++) {\n       realInputs[i] \u003d inputs[validIndexes[i]];\n       realInputOffsets[i] \u003d inputOffsets[validIndexes[i]];\n     }\n-    RSUtil2.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n+    RSUtil.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n             outputs, outputOffsets);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void doDecode(byte[][] inputs, int[] inputOffsets,\n                          int dataLen, int[] erasedIndexes,\n                          byte[][] outputs, int[] outputOffsets) {\n    prepareDecoding(inputs, erasedIndexes);\n\n    byte[][] realInputs \u003d new byte[getNumDataUnits()][];\n    int[] realInputOffsets \u003d new int[getNumDataUnits()];\n    for (int i \u003d 0; i \u003c getNumDataUnits(); i++) {\n      realInputs[i] \u003d inputs[validIndexes[i]];\n      realInputOffsets[i] \u003d inputOffsets[validIndexes[i]];\n    }\n    RSUtil.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n            outputs, outputOffsets);\n  }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java",
          "extendedDetails": {}
        }
      ]
    },
    "c89a14a8a4fe58f01f0cba643f2bc203e1a8701e": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-12041. Implement another Reed-Solomon coder in pure Java. Contributed by Kai Zheng.\n\nChange-Id: I35ff2e498d4f988c9a064f74374f7c7258b7a6b7\n",
      "commitDate": "03/02/16 3:05 PM",
      "commitName": "c89a14a8a4fe58f01f0cba643f2bc203e1a8701e",
      "commitAuthor": "zhezhang",
      "diff": "@@ -0,0 +1,14 @@\n+  protected void doDecode(byte[][] inputs, int[] inputOffsets,\n+                          int dataLen, int[] erasedIndexes,\n+                          byte[][] outputs, int[] outputOffsets) {\n+    prepareDecoding(inputs, erasedIndexes);\n+\n+    byte[][] realInputs \u003d new byte[getNumDataUnits()][];\n+    int[] realInputOffsets \u003d new int[getNumDataUnits()];\n+    for (int i \u003d 0; i \u003c getNumDataUnits(); i++) {\n+      realInputs[i] \u003d inputs[validIndexes[i]];\n+      realInputOffsets[i] \u003d inputOffsets[validIndexes[i]];\n+    }\n+    RSUtil2.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n+            outputs, outputOffsets);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(byte[][] inputs, int[] inputOffsets,\n                          int dataLen, int[] erasedIndexes,\n                          byte[][] outputs, int[] outputOffsets) {\n    prepareDecoding(inputs, erasedIndexes);\n\n    byte[][] realInputs \u003d new byte[getNumDataUnits()][];\n    int[] realInputOffsets \u003d new int[getNumDataUnits()];\n    for (int i \u003d 0; i \u003c getNumDataUnits(); i++) {\n      realInputs[i] \u003d inputs[validIndexes[i]];\n      realInputOffsets[i] \u003d inputOffsets[validIndexes[i]];\n    }\n    RSUtil2.encodeData(gfTables, dataLen, realInputs, realInputOffsets,\n            outputs, outputOffsets);\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder2.java"
    }
  }
}