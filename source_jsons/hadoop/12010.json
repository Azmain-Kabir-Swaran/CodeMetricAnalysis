{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "invalidate",
  "functionId": "invalidate___bpid-String__invalidBlks-Block[]__async-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 2131,
  "functionEndLine": 2231,
  "numCommitsSeen": 387,
  "timeTaken": 15520,
  "changeHistory": [
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8",
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
    "6d2da38d16cebe9b82f1048f87127eecee33664c",
    "86c9862bec0248d671e657aa56094a2919b8ac14",
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
    "f2ac132d6a21c215093b7f87acf2843ac8123716",
    "6dae6d12ec5abb716e1501cd4e18b10ae7809b94",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
    "1efd9c98258fbb973d2058dcf0850042e53bd02f",
    "5e8b6973527e5f714652641ed95e8a4509e18cfa",
    "b2d5ed36bcb80e2581191dcdc3976e825c959142",
    "eb448e14399e17f11b9e523e4050de245b9b0408",
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16",
    "e85a3fecc68b48a3dc9af5daa466a24f3b39545b",
    "7ec4308f825f9ca6139431feb64818a6b3f3163c",
    "5df82fa01d26c18685ad7617128dbc2913547cb3",
    "fba994ffe20d387e8ed875e727fc3d93f7097101",
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
    "f39f8c57344ede533ca4363c98230f3a0c401a76",
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce",
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8",
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11",
    "bc13dfb1426944ce45293cb8f444239a7406762c",
    "99a68a14237b4cd1936ba5e9468d25d35dad594c",
    "9e31bf675dd92183a9a74a66b7caf1a080581d65",
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6",
    "dbbfaebb71eb9d69d67fd5becd2e357397d0f68b",
    "221aadbc5b35b043fbc62c417b0edc029db9d004",
    "73451ed2d9fb5eb228d80ad5f3be302a60496527",
    "e680023f8b158db25b45a050236163e9246103f3",
    "e6339be188d3f5c94df3b092d92d201b728163f5",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8": "Ybodychange",
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "6d2da38d16cebe9b82f1048f87127eecee33664c": "Ybodychange",
    "86c9862bec0248d671e657aa56094a2919b8ac14": "Ybodychange",
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c": "Ybodychange",
    "f2ac132d6a21c215093b7f87acf2843ac8123716": "Ybodychange",
    "6dae6d12ec5abb716e1501cd4e18b10ae7809b94": "Ybodychange",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": "Ybodychange",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": "Ybodychange",
    "1efd9c98258fbb973d2058dcf0850042e53bd02f": "Ybodychange",
    "5e8b6973527e5f714652641ed95e8a4509e18cfa": "Ybodychange",
    "b2d5ed36bcb80e2581191dcdc3976e825c959142": "Ybodychange",
    "eb448e14399e17f11b9e523e4050de245b9b0408": "Ybodychange",
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16": "Ybodychange",
    "e85a3fecc68b48a3dc9af5daa466a24f3b39545b": "Ybodychange",
    "7ec4308f825f9ca6139431feb64818a6b3f3163c": "Ybodychange",
    "5df82fa01d26c18685ad7617128dbc2913547cb3": "Ybodychange",
    "fba994ffe20d387e8ed875e727fc3d93f7097101": "Ybodychange",
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc": "Ybodychange",
    "f39f8c57344ede533ca4363c98230f3a0c401a76": "Ybodychange",
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce": "Ybodychange",
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8": "Ybodychange",
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11": "Ybodychange",
    "bc13dfb1426944ce45293cb8f444239a7406762c": "Ymultichange(Ymovefromfile,Ybodychange)",
    "99a68a14237b4cd1936ba5e9468d25d35dad594c": "Ybodychange",
    "9e31bf675dd92183a9a74a66b7caf1a080581d65": "Ybodychange",
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6": "Ybodychange",
    "dbbfaebb71eb9d69d67fd5becd2e357397d0f68b": "Ybodychange",
    "221aadbc5b35b043fbc62c417b0edc029db9d004": "Ybodychange",
    "73451ed2d9fb5eb228d80ad5f3be302a60496527": "Ybodychange",
    "e680023f8b158db25b45a050236163e9246103f3": "Ybodychange",
    "e6339be188d3f5c94df3b092d92d201b728163f5": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15150. Introduce read write lock to Datanode. Contributed Stephen O\u0027Donnell.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "11/02/20 8:00 AM",
      "commitName": "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "28/01/20 10:10 AM",
      "commitNameOld": "1839c467f60cbb8592d446694ec3d7710cda5142",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 13.91,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,101 @@\n   private void invalidate(String bpid, Block[] invalidBlks, boolean async)\n       throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final ReplicaInfo removing;\n       final FsVolumeImpl v;\n-      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n+      try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           ReplicaInfo infoByBlockId \u003d\n               volumeMap.get(bpid, invalidBlks[i].getBlockId());\n           if (infoByBlockId \u003d\u003d null) {\n             // It is okay if the block is not found -- it\n             // may be deleted earlier.\n             LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n                 + \": ReplicaInfo not found.\");\n           } else {\n             errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                 + \": GenerationStamp not matched, existing replica is \"\n                 + Block.toString(infoByBlockId));\n           }\n           continue;\n         }\n \n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for replica \" + info);\n           continue;\n         }\n         try {\n           File blockFile \u003d new File(info.getBlockURI());\n           if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n             errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                 +  \". Parent not found for block file: \" + blockFile);\n             continue;\n           }\n         } catch(IllegalArgumentException e) {\n           LOG.warn(\"Parent directory check failed; replica \" + info\n               + \" is not backed by a local file\");\n         }\n         removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n         addDeletingBlock(bpid, removing.getBlockId());\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Block file \" + removing.getBlockURI()\n               + \" is to be deleted\");\n         }\n         if (removing instanceof ReplicaInPipeline) {\n           ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n         }\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n           if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n           ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n             replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       try {\n         if (async) {\n           // Delete the block asynchronously to make sure we can do it fast\n           // enough.\n           // It\u0027s ok to unlink the block file before the uncache operation\n           // finishes.\n           asyncDiskService.deleteAsync(v.obtainReference(), removing,\n               new ExtendedBlock(bpid, invalidBlks[i]),\n               dataStorage.getTrashDirectoryForReplica(bpid, removing));\n         } else {\n           asyncDiskService.deleteSync(v.obtainReference(), removing,\n               new ExtendedBlock(bpid, invalidBlks[i]),\n               dataStorage.getTrashDirectoryForReplica(bpid, removing));\n         }\n       } catch (ClosedChannelException e) {\n         LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n             \"block \" + invalidBlks[i]);\n       }\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void invalidate(String bpid, Block[] invalidBlks, boolean async)\n      throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final ReplicaInfo removing;\n      final FsVolumeImpl v;\n      try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          ReplicaInfo infoByBlockId \u003d\n              volumeMap.get(bpid, invalidBlks[i].getBlockId());\n          if (infoByBlockId \u003d\u003d null) {\n            // It is okay if the block is not found -- it\n            // may be deleted earlier.\n            LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n                + \": ReplicaInfo not found.\");\n          } else {\n            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                + \": GenerationStamp not matched, existing replica is \"\n                + Block.toString(infoByBlockId));\n          }\n          continue;\n        }\n\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for replica \" + info);\n          continue;\n        }\n        try {\n          File blockFile \u003d new File(info.getBlockURI());\n          if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                +  \". Parent not found for block file: \" + blockFile);\n            continue;\n          }\n        } catch(IllegalArgumentException e) {\n          LOG.warn(\"Parent directory check failed; replica \" + info\n              + \" is not backed by a local file\");\n        }\n        removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n        addDeletingBlock(bpid, removing.getBlockId());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Block file \" + removing.getBlockURI()\n              + \" is to be deleted\");\n        }\n        if (removing instanceof ReplicaInPipeline) {\n          ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n        }\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      try {\n        if (async) {\n          // Delete the block asynchronously to make sure we can do it fast\n          // enough.\n          // It\u0027s ok to unlink the block file before the uncache operation\n          // finishes.\n          asyncDiskService.deleteAsync(v.obtainReference(), removing,\n              new ExtendedBlock(bpid, invalidBlks[i]),\n              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n        } else {\n          asyncDiskService.deleteSync(v.obtainReference(), removing,\n              new ExtendedBlock(bpid, invalidBlks[i]),\n              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n        }\n      } catch (ClosedChannelException e) {\n        LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n            \"block \" + invalidBlks[i]);\n      }\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-11856. Ability to re-add Upgrading Nodes to pipeline for future pipeline updates. Contributed by Vinayakumar B.\n",
      "commitDate": "25/05/17 11:05 AM",
      "commitName": "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-11856. Ability to re-add Upgrading Nodes to pipeline for future pipeline updates. Contributed by Vinayakumar B.\n",
          "commitDate": "25/05/17 11:05 AM",
          "commitName": "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "11/05/17 7:08 PM",
          "commitNameOld": "1411612aa4e70c704b941723217ed4efd8a0125b",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 13.66,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,93 +1,101 @@\n-  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n+  private void invalidate(String bpid, Block[] invalidBlks, boolean async)\n+      throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final ReplicaInfo removing;\n       final FsVolumeImpl v;\n       try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           ReplicaInfo infoByBlockId \u003d\n               volumeMap.get(bpid, invalidBlks[i].getBlockId());\n           if (infoByBlockId \u003d\u003d null) {\n             // It is okay if the block is not found -- it\n             // may be deleted earlier.\n             LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n                 + \": ReplicaInfo not found.\");\n           } else {\n             errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                 + \": GenerationStamp not matched, existing replica is \"\n                 + Block.toString(infoByBlockId));\n           }\n           continue;\n         }\n \n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for replica \" + info);\n           continue;\n         }\n         try {\n           File blockFile \u003d new File(info.getBlockURI());\n           if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n             errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                 +  \". Parent not found for block file: \" + blockFile);\n             continue;\n           }\n         } catch(IllegalArgumentException e) {\n           LOG.warn(\"Parent directory check failed; replica \" + info\n               + \" is not backed by a local file\");\n         }\n         removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n         addDeletingBlock(bpid, removing.getBlockId());\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Block file \" + removing.getBlockURI()\n               + \" is to be deleted\");\n         }\n         if (removing instanceof ReplicaInPipeline) {\n           ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n         }\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n           if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n           ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n             replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n-      // Delete the block asynchronously to make sure we can do it fast enough.\n-      // It\u0027s ok to unlink the block file before the uncache operation\n-      // finishes.\n       try {\n-        asyncDiskService.deleteAsync(v.obtainReference(), removing,\n-            new ExtendedBlock(bpid, invalidBlks[i]),\n-            dataStorage.getTrashDirectoryForReplica(bpid, removing));\n+        if (async) {\n+          // Delete the block asynchronously to make sure we can do it fast\n+          // enough.\n+          // It\u0027s ok to unlink the block file before the uncache operation\n+          // finishes.\n+          asyncDiskService.deleteAsync(v.obtainReference(), removing,\n+              new ExtendedBlock(bpid, invalidBlks[i]),\n+              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n+        } else {\n+          asyncDiskService.deleteSync(v.obtainReference(), removing,\n+              new ExtendedBlock(bpid, invalidBlks[i]),\n+              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n+        }\n       } catch (ClosedChannelException e) {\n         LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n             \"block \" + invalidBlks[i]);\n       }\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void invalidate(String bpid, Block[] invalidBlks, boolean async)\n      throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final ReplicaInfo removing;\n      final FsVolumeImpl v;\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          ReplicaInfo infoByBlockId \u003d\n              volumeMap.get(bpid, invalidBlks[i].getBlockId());\n          if (infoByBlockId \u003d\u003d null) {\n            // It is okay if the block is not found -- it\n            // may be deleted earlier.\n            LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n                + \": ReplicaInfo not found.\");\n          } else {\n            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                + \": GenerationStamp not matched, existing replica is \"\n                + Block.toString(infoByBlockId));\n          }\n          continue;\n        }\n\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for replica \" + info);\n          continue;\n        }\n        try {\n          File blockFile \u003d new File(info.getBlockURI());\n          if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                +  \". Parent not found for block file: \" + blockFile);\n            continue;\n          }\n        } catch(IllegalArgumentException e) {\n          LOG.warn(\"Parent directory check failed; replica \" + info\n              + \" is not backed by a local file\");\n        }\n        removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n        addDeletingBlock(bpid, removing.getBlockId());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Block file \" + removing.getBlockURI()\n              + \" is to be deleted\");\n        }\n        if (removing instanceof ReplicaInPipeline) {\n          ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n        }\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      try {\n        if (async) {\n          // Delete the block asynchronously to make sure we can do it fast\n          // enough.\n          // It\u0027s ok to unlink the block file before the uncache operation\n          // finishes.\n          asyncDiskService.deleteAsync(v.obtainReference(), removing,\n              new ExtendedBlock(bpid, invalidBlks[i]),\n              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n        } else {\n          asyncDiskService.deleteSync(v.obtainReference(), removing,\n              new ExtendedBlock(bpid, invalidBlks[i]),\n              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n        }\n      } catch (ClosedChannelException e) {\n        LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n            \"block \" + invalidBlks[i]);\n      }\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[bpid-String, invalidBlks-Block[]]",
            "newValue": "[bpid-String, invalidBlks-Block[], async-boolean]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-11856. Ability to re-add Upgrading Nodes to pipeline for future pipeline updates. Contributed by Vinayakumar B.\n",
          "commitDate": "25/05/17 11:05 AM",
          "commitName": "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "11/05/17 7:08 PM",
          "commitNameOld": "1411612aa4e70c704b941723217ed4efd8a0125b",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 13.66,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,93 +1,101 @@\n-  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n+  private void invalidate(String bpid, Block[] invalidBlks, boolean async)\n+      throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final ReplicaInfo removing;\n       final FsVolumeImpl v;\n       try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           ReplicaInfo infoByBlockId \u003d\n               volumeMap.get(bpid, invalidBlks[i].getBlockId());\n           if (infoByBlockId \u003d\u003d null) {\n             // It is okay if the block is not found -- it\n             // may be deleted earlier.\n             LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n                 + \": ReplicaInfo not found.\");\n           } else {\n             errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                 + \": GenerationStamp not matched, existing replica is \"\n                 + Block.toString(infoByBlockId));\n           }\n           continue;\n         }\n \n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for replica \" + info);\n           continue;\n         }\n         try {\n           File blockFile \u003d new File(info.getBlockURI());\n           if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n             errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                 +  \". Parent not found for block file: \" + blockFile);\n             continue;\n           }\n         } catch(IllegalArgumentException e) {\n           LOG.warn(\"Parent directory check failed; replica \" + info\n               + \" is not backed by a local file\");\n         }\n         removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n         addDeletingBlock(bpid, removing.getBlockId());\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Block file \" + removing.getBlockURI()\n               + \" is to be deleted\");\n         }\n         if (removing instanceof ReplicaInPipeline) {\n           ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n         }\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n           if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n           ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n             replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n-      // Delete the block asynchronously to make sure we can do it fast enough.\n-      // It\u0027s ok to unlink the block file before the uncache operation\n-      // finishes.\n       try {\n-        asyncDiskService.deleteAsync(v.obtainReference(), removing,\n-            new ExtendedBlock(bpid, invalidBlks[i]),\n-            dataStorage.getTrashDirectoryForReplica(bpid, removing));\n+        if (async) {\n+          // Delete the block asynchronously to make sure we can do it fast\n+          // enough.\n+          // It\u0027s ok to unlink the block file before the uncache operation\n+          // finishes.\n+          asyncDiskService.deleteAsync(v.obtainReference(), removing,\n+              new ExtendedBlock(bpid, invalidBlks[i]),\n+              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n+        } else {\n+          asyncDiskService.deleteSync(v.obtainReference(), removing,\n+              new ExtendedBlock(bpid, invalidBlks[i]),\n+              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n+        }\n       } catch (ClosedChannelException e) {\n         LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n             \"block \" + invalidBlks[i]);\n       }\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void invalidate(String bpid, Block[] invalidBlks, boolean async)\n      throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final ReplicaInfo removing;\n      final FsVolumeImpl v;\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          ReplicaInfo infoByBlockId \u003d\n              volumeMap.get(bpid, invalidBlks[i].getBlockId());\n          if (infoByBlockId \u003d\u003d null) {\n            // It is okay if the block is not found -- it\n            // may be deleted earlier.\n            LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n                + \": ReplicaInfo not found.\");\n          } else {\n            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                + \": GenerationStamp not matched, existing replica is \"\n                + Block.toString(infoByBlockId));\n          }\n          continue;\n        }\n\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for replica \" + info);\n          continue;\n        }\n        try {\n          File blockFile \u003d new File(info.getBlockURI());\n          if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                +  \". Parent not found for block file: \" + blockFile);\n            continue;\n          }\n        } catch(IllegalArgumentException e) {\n          LOG.warn(\"Parent directory check failed; replica \" + info\n              + \" is not backed by a local file\");\n        }\n        removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n        addDeletingBlock(bpid, removing.getBlockId());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Block file \" + removing.getBlockURI()\n              + \" is to be deleted\");\n        }\n        if (removing instanceof ReplicaInPipeline) {\n          ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n        }\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      try {\n        if (async) {\n          // Delete the block asynchronously to make sure we can do it fast\n          // enough.\n          // It\u0027s ok to unlink the block file before the uncache operation\n          // finishes.\n          asyncDiskService.deleteAsync(v.obtainReference(), removing,\n              new ExtendedBlock(bpid, invalidBlks[i]),\n              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n        } else {\n          asyncDiskService.deleteSync(v.obtainReference(), removing,\n              new ExtendedBlock(bpid, invalidBlks[i]),\n              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n        }\n      } catch (ClosedChannelException e) {\n        LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n            \"block \" + invalidBlks[i]);\n      }\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-11856. Ability to re-add Upgrading Nodes to pipeline for future pipeline updates. Contributed by Vinayakumar B.\n",
          "commitDate": "25/05/17 11:05 AM",
          "commitName": "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "11/05/17 7:08 PM",
          "commitNameOld": "1411612aa4e70c704b941723217ed4efd8a0125b",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 13.66,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,93 +1,101 @@\n-  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n+  private void invalidate(String bpid, Block[] invalidBlks, boolean async)\n+      throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final ReplicaInfo removing;\n       final FsVolumeImpl v;\n       try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           ReplicaInfo infoByBlockId \u003d\n               volumeMap.get(bpid, invalidBlks[i].getBlockId());\n           if (infoByBlockId \u003d\u003d null) {\n             // It is okay if the block is not found -- it\n             // may be deleted earlier.\n             LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n                 + \": ReplicaInfo not found.\");\n           } else {\n             errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                 + \": GenerationStamp not matched, existing replica is \"\n                 + Block.toString(infoByBlockId));\n           }\n           continue;\n         }\n \n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for replica \" + info);\n           continue;\n         }\n         try {\n           File blockFile \u003d new File(info.getBlockURI());\n           if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n             errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                 +  \". Parent not found for block file: \" + blockFile);\n             continue;\n           }\n         } catch(IllegalArgumentException e) {\n           LOG.warn(\"Parent directory check failed; replica \" + info\n               + \" is not backed by a local file\");\n         }\n         removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n         addDeletingBlock(bpid, removing.getBlockId());\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Block file \" + removing.getBlockURI()\n               + \" is to be deleted\");\n         }\n         if (removing instanceof ReplicaInPipeline) {\n           ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n         }\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n           if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n           ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n             replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n-      // Delete the block asynchronously to make sure we can do it fast enough.\n-      // It\u0027s ok to unlink the block file before the uncache operation\n-      // finishes.\n       try {\n-        asyncDiskService.deleteAsync(v.obtainReference(), removing,\n-            new ExtendedBlock(bpid, invalidBlks[i]),\n-            dataStorage.getTrashDirectoryForReplica(bpid, removing));\n+        if (async) {\n+          // Delete the block asynchronously to make sure we can do it fast\n+          // enough.\n+          // It\u0027s ok to unlink the block file before the uncache operation\n+          // finishes.\n+          asyncDiskService.deleteAsync(v.obtainReference(), removing,\n+              new ExtendedBlock(bpid, invalidBlks[i]),\n+              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n+        } else {\n+          asyncDiskService.deleteSync(v.obtainReference(), removing,\n+              new ExtendedBlock(bpid, invalidBlks[i]),\n+              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n+        }\n       } catch (ClosedChannelException e) {\n         LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n             \"block \" + invalidBlks[i]);\n       }\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void invalidate(String bpid, Block[] invalidBlks, boolean async)\n      throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final ReplicaInfo removing;\n      final FsVolumeImpl v;\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          ReplicaInfo infoByBlockId \u003d\n              volumeMap.get(bpid, invalidBlks[i].getBlockId());\n          if (infoByBlockId \u003d\u003d null) {\n            // It is okay if the block is not found -- it\n            // may be deleted earlier.\n            LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n                + \": ReplicaInfo not found.\");\n          } else {\n            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                + \": GenerationStamp not matched, existing replica is \"\n                + Block.toString(infoByBlockId));\n          }\n          continue;\n        }\n\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for replica \" + info);\n          continue;\n        }\n        try {\n          File blockFile \u003d new File(info.getBlockURI());\n          if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                +  \". Parent not found for block file: \" + blockFile);\n            continue;\n          }\n        } catch(IllegalArgumentException e) {\n          LOG.warn(\"Parent directory check failed; replica \" + info\n              + \" is not backed by a local file\");\n        }\n        removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n        addDeletingBlock(bpid, removing.getBlockId());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Block file \" + removing.getBlockURI()\n              + \" is to be deleted\");\n        }\n        if (removing instanceof ReplicaInPipeline) {\n          ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n        }\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      try {\n        if (async) {\n          // Delete the block asynchronously to make sure we can do it fast\n          // enough.\n          // It\u0027s ok to unlink the block file before the uncache operation\n          // finishes.\n          asyncDiskService.deleteAsync(v.obtainReference(), removing,\n              new ExtendedBlock(bpid, invalidBlks[i]),\n              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n        } else {\n          asyncDiskService.deleteSync(v.obtainReference(), removing,\n              new ExtendedBlock(bpid, invalidBlks[i]),\n              dataStorage.getTrashDirectoryForReplica(bpid, removing));\n        }\n      } catch (ClosedChannelException e) {\n        LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n            \"block \" + invalidBlks[i]);\n      }\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "6d2da38d16cebe9b82f1048f87127eecee33664c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11018. Incorrect check and message in FsDatasetImpl#invalidate. Contributed by Yiqun Lin.\n",
      "commitDate": "20/10/16 10:56 AM",
      "commitName": "6d2da38d16cebe9b82f1048f87127eecee33664c",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "18/10/16 10:42 PM",
      "commitNameOld": "c5573e6a7599da17cad733cd274e7a9b75b22bb0",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 1.51,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,88 +1,93 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final ReplicaInfo removing;\n       final FsVolumeImpl v;\n       try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n-          // It is okay if the block is not found -- it may be deleted earlier.\n-          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n-              + \": ReplicaInfo not found.\");\n+          ReplicaInfo infoByBlockId \u003d\n+              volumeMap.get(bpid, invalidBlks[i].getBlockId());\n+          if (infoByBlockId \u003d\u003d null) {\n+            // It is okay if the block is not found -- it\n+            // may be deleted earlier.\n+            LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n+                + \": ReplicaInfo not found.\");\n+          } else {\n+            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n+                + \": GenerationStamp not matched, existing replica is \"\n+                + Block.toString(infoByBlockId));\n+          }\n           continue;\n         }\n-        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n-          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n-              + \": GenerationStamp not matched, info\u003d\" + info);\n-          continue;\n-        }\n+\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for replica \" + info);\n           continue;\n         }\n         try {\n           File blockFile \u003d new File(info.getBlockURI());\n           if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n             errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                 +  \". Parent not found for block file: \" + blockFile);\n             continue;\n           }\n         } catch(IllegalArgumentException e) {\n           LOG.warn(\"Parent directory check failed; replica \" + info\n               + \" is not backed by a local file\");\n         }\n         removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n         addDeletingBlock(bpid, removing.getBlockId());\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Block file \" + removing.getBlockURI()\n               + \" is to be deleted\");\n         }\n         if (removing instanceof ReplicaInPipeline) {\n           ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n         }\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n           if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n           ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n             replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       try {\n         asyncDiskService.deleteAsync(v.obtainReference(), removing,\n             new ExtendedBlock(bpid, invalidBlks[i]),\n             dataStorage.getTrashDirectoryForReplica(bpid, removing));\n       } catch (ClosedChannelException e) {\n         LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n             \"block \" + invalidBlks[i]);\n       }\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final ReplicaInfo removing;\n      final FsVolumeImpl v;\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          ReplicaInfo infoByBlockId \u003d\n              volumeMap.get(bpid, invalidBlks[i].getBlockId());\n          if (infoByBlockId \u003d\u003d null) {\n            // It is okay if the block is not found -- it\n            // may be deleted earlier.\n            LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n                + \": ReplicaInfo not found.\");\n          } else {\n            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                + \": GenerationStamp not matched, existing replica is \"\n                + Block.toString(infoByBlockId));\n          }\n          continue;\n        }\n\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for replica \" + info);\n          continue;\n        }\n        try {\n          File blockFile \u003d new File(info.getBlockURI());\n          if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                +  \". Parent not found for block file: \" + blockFile);\n            continue;\n          }\n        } catch(IllegalArgumentException e) {\n          LOG.warn(\"Parent directory check failed; replica \" + info\n              + \" is not backed by a local file\");\n        }\n        removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n        addDeletingBlock(bpid, removing.getBlockId());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Block file \" + removing.getBlockURI()\n              + \" is to be deleted\");\n        }\n        if (removing instanceof ReplicaInPipeline) {\n          ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n        }\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      try {\n        asyncDiskService.deleteAsync(v.obtainReference(), removing,\n            new ExtendedBlock(bpid, invalidBlks[i]),\n            dataStorage.getTrashDirectoryForReplica(bpid, removing));\n      } catch (ClosedChannelException e) {\n        LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n            \"block \" + invalidBlks[i]);\n      }\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "86c9862bec0248d671e657aa56094a2919b8ac14": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "13/09/16 12:54 PM",
      "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "10/09/16 6:22 PM",
      "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.77,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,88 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n-      final File f;\n+      final ReplicaInfo removing;\n       final FsVolumeImpl v;\n       try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n-        f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n-              +  \". No volume for this replica, file\u003d\" + f);\n+              +  \". No volume for replica \" + info);\n           continue;\n         }\n-        File parent \u003d f.getParentFile();\n-        if (parent \u003d\u003d null) {\n-          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n-              +  \". Parent not found for file \" + f);\n-          continue;\n+        try {\n+          File blockFile \u003d new File(info.getBlockURI());\n+          if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n+            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n+                +  \". Parent not found for block file: \" + blockFile);\n+            continue;\n+          }\n+        } catch(IllegalArgumentException e) {\n+          LOG.warn(\"Parent directory check failed; replica \" + info\n+              + \" is not backed by a local file\");\n         }\n-        ReplicaInfo removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n+        removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n         addDeletingBlock(bpid, removing.getBlockId());\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Block file \" + removing.getBlockFile().getName()\n+          LOG.debug(\"Block file \" + removing.getBlockURI()\n               + \" is to be deleted\");\n         }\n-        if (removing instanceof ReplicaInPipelineInterface) {\n-          ((ReplicaInPipelineInterface) removing).releaseAllBytesReserved();\n+        if (removing instanceof ReplicaInPipeline) {\n+          ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n         }\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n           if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n           ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n             replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       try {\n-        asyncDiskService.deleteAsync(v.obtainReference(), f,\n-            FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n+        asyncDiskService.deleteAsync(v.obtainReference(), removing,\n             new ExtendedBlock(bpid, invalidBlks[i]),\n-            dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n+            dataStorage.getTrashDirectoryForReplica(bpid, removing));\n       } catch (ClosedChannelException e) {\n         LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n             \"block \" + invalidBlks[i]);\n       }\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final ReplicaInfo removing;\n      final FsVolumeImpl v;\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for replica \" + info);\n          continue;\n        }\n        try {\n          File blockFile \u003d new File(info.getBlockURI());\n          if (blockFile !\u003d null \u0026\u0026 blockFile.getParentFile() \u003d\u003d null) {\n            errors.add(\"Failed to delete replica \" + invalidBlks[i]\n                +  \". Parent not found for block file: \" + blockFile);\n            continue;\n          }\n        } catch(IllegalArgumentException e) {\n          LOG.warn(\"Parent directory check failed; replica \" + info\n              + \" is not backed by a local file\");\n        }\n        removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n        addDeletingBlock(bpid, removing.getBlockId());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Block file \" + removing.getBlockURI()\n              + \" is to be deleted\");\n        }\n        if (removing instanceof ReplicaInPipeline) {\n          ((ReplicaInPipeline) removing).releaseAllBytesReserved();\n        }\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      try {\n        asyncDiskService.deleteAsync(v.obtainReference(), removing,\n            new ExtendedBlock(bpid, invalidBlks[i]),\n            dataStorage.getTrashDirectoryForReplica(bpid, removing));\n      } catch (ClosedChannelException e) {\n        LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n            \"block \" + invalidBlks[i]);\n      }\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10682. Replace FsDatasetImpl object lock with a separate lock object. (Chen Liang)\n",
      "commitDate": "08/08/16 12:02 PM",
      "commitName": "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "08/07/16 7:40 PM",
      "commitNameOld": "da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 30.68,
      "commitsBetweenForRepo": 320,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,85 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n-      synchronized (this) {\n+      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n         f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f);\n           continue;\n         }\n         ReplicaInfo removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n         addDeletingBlock(bpid, removing.getBlockId());\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Block file \" + removing.getBlockFile().getName()\n               + \" is to be deleted\");\n         }\n         if (removing instanceof ReplicaInPipelineInterface) {\n           ((ReplicaInPipelineInterface) removing).releaseAllBytesReserved();\n         }\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n           if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n           ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n             replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       try {\n         asyncDiskService.deleteAsync(v.obtainReference(), f,\n             FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n             new ExtendedBlock(bpid, invalidBlks[i]),\n             dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n       } catch (ClosedChannelException e) {\n         LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n             \"block \" + invalidBlks[i]);\n       }\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        ReplicaInfo removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n        addDeletingBlock(bpid, removing.getBlockId());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Block file \" + removing.getBlockFile().getName()\n              + \" is to be deleted\");\n        }\n        if (removing instanceof ReplicaInPipelineInterface) {\n          ((ReplicaInPipelineInterface) removing).releaseAllBytesReserved();\n        }\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      try {\n        asyncDiskService.deleteAsync(v.obtainReference(), f,\n            FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n            new ExtendedBlock(bpid, invalidBlks[i]),\n            dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n      } catch (ClosedChannelException e) {\n        LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n            \"block \" + invalidBlks[i]);\n      }\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "f2ac132d6a21c215093b7f87acf2843ac8123716": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9530. ReservedSpace is not cleared for abandoned Blocks (Contributed by Brahma Reddy Battula)\n",
      "commitDate": "21/06/16 3:12 AM",
      "commitName": "f2ac132d6a21c215093b7f87acf2843ac8123716",
      "commitAuthor": "Brahma Reddy Battula",
      "commitDateOld": "20/04/16 1:39 PM",
      "commitNameOld": "63ac2db59af2b50e74dc892cae1dbc4d2e061423",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 61.56,
      "commitsBetweenForRepo": 404,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,85 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n         f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f);\n           continue;\n         }\n         ReplicaInfo removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n         addDeletingBlock(bpid, removing.getBlockId());\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Block file \" + removing.getBlockFile().getName()\n               + \" is to be deleted\");\n         }\n+        if (removing instanceof ReplicaInPipelineInterface) {\n+          ((ReplicaInPipelineInterface) removing).releaseAllBytesReserved();\n+        }\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n           if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n           ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n             replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       try {\n         asyncDiskService.deleteAsync(v.obtainReference(), f,\n             FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n             new ExtendedBlock(bpid, invalidBlks[i]),\n             dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n       } catch (ClosedChannelException e) {\n         LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n             \"block \" + invalidBlks[i]);\n       }\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        ReplicaInfo removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n        addDeletingBlock(bpid, removing.getBlockId());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Block file \" + removing.getBlockFile().getName()\n              + \" is to be deleted\");\n        }\n        if (removing instanceof ReplicaInPipelineInterface) {\n          ((ReplicaInPipelineInterface) removing).releaseAllBytesReserved();\n        }\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      try {\n        asyncDiskService.deleteAsync(v.obtainReference(), f,\n            FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n            new ExtendedBlock(bpid, invalidBlks[i]),\n            dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n      } catch (ClosedChannelException e) {\n        LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n            \"block \" + invalidBlks[i]);\n      }\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "6dae6d12ec5abb716e1501cd4e18b10ae7809b94": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6833.  DirectoryScanner should not register a deleting block with memory of DataNode.  Contributed by Shinichi Yamashita\n",
      "commitDate": "12/03/15 11:25 AM",
      "commitName": "6dae6d12ec5abb716e1501cd4e18b10ae7809b94",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "10/03/15 6:20 PM",
      "commitNameOld": "5c1036d598051cf6af595740f1ab82092b0b6554",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.71,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,82 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n         f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f);\n           continue;\n         }\n-        volumeMap.remove(bpid, invalidBlks[i]);\n+        ReplicaInfo removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n+        addDeletingBlock(bpid, removing.getBlockId());\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Block file \" + removing.getBlockFile().getName()\n+              + \" is to be deleted\");\n+        }\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n           if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n           ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n             replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       try {\n         asyncDiskService.deleteAsync(v.obtainReference(), f,\n             FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n             new ExtendedBlock(bpid, invalidBlks[i]),\n             dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n       } catch (ClosedChannelException e) {\n         LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n             \"block \" + invalidBlks[i]);\n       }\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        ReplicaInfo removing \u003d volumeMap.remove(bpid, invalidBlks[i]);\n        addDeletingBlock(bpid, removing.getBlockId());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Block file \" + removing.getBlockFile().getName()\n              + \" is to be deleted\");\n        }\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      try {\n        asyncDiskService.deleteAsync(v.obtainReference(), f,\n            FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n            new ExtendedBlock(bpid, invalidBlks[i]),\n            dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n      } catch (ClosedChannelException e) {\n        LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n            \"block \" + invalidBlks[i]);\n      }\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)\n",
      "commitDate": "20/01/15 7:05 PM",
      "commitName": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "13/01/15 12:24 AM",
      "commitNameOld": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 7.78,
      "commitsBetweenForRepo": 49,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,72 +1,77 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n         f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f);\n           continue;\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n           if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n           ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n             replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n-      asyncDiskService.deleteAsync(v, f,\n-          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n-          new ExtendedBlock(bpid, invalidBlks[i]),\n-          dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n+      try {\n+        asyncDiskService.deleteAsync(v.obtainReference(), f,\n+            FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n+            new ExtendedBlock(bpid, invalidBlks[i]),\n+            dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n+      } catch (ClosedChannelException e) {\n+        LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n+            \"block \" + invalidBlks[i]);\n+      }\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      try {\n        asyncDiskService.deleteAsync(v.obtainReference(), f,\n            FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n            new ExtendedBlock(bpid, invalidBlks[i]),\n            dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n      } catch (ClosedChannelException e) {\n        LOG.warn(\"Volume \" + v + \" is closed, ignore the deletion task for \" +\n            \"block \" + invalidBlks[i]);\n      }\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.\n",
      "commitDate": "11/12/14 12:36 PM",
      "commitName": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/11/14 9:57 AM",
      "commitNameOld": "058af60c56207907f2bedf76df4284e86d923e0c",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 15.11,
      "commitsBetweenForRepo": 103,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,72 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n         f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n-        if (f \u003d\u003d null) {\n-          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n-              +  \": File not found, volume\u003d\" + v);\n-          continue;\n-        }\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f);\n           continue;\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n           if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n           ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n             replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]),\n           dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]),\n          dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "1efd9c98258fbb973d2058dcf0850042e53bd02f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7112. LazyWriter should use either async IO or one thread per physical disk. Contributed by Xiaoyu Yao.\n",
      "commitDate": "07/10/14 8:25 PM",
      "commitName": "1efd9c98258fbb973d2058dcf0850042e53bd02f",
      "commitAuthor": "cnauroth",
      "commitDateOld": "30/09/14 12:53 AM",
      "commitNameOld": "5e8b6973527e5f714652641ed95e8a4509e18cfa",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 7.81,
      "commitsBetweenForRepo": 80,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,77 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n         f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f);\n           continue;\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n \n       if (v.isTransientStorage()) {\n         RamDiskReplica replicaInfo \u003d\n           ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n         if (replicaInfo !\u003d null) {\n-          if (replicaInfo.getIsPersisted() \u003d\u003d  false) {\n+          if (!replicaInfo.getIsPersisted()) {\n             datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n           }\n-          discardRamDiskReplica(replicaInfo, true);\n+          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n+            replicaInfo.getBlockId(), true);\n         }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]),\n           dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (!replicaInfo.getIsPersisted()) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          ramDiskReplicaTracker.discardReplica(replicaInfo.getBlockPoolId(),\n            replicaInfo.getBlockId(), true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]),\n          dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "5e8b6973527e5f714652641ed95e8a4509e18cfa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7129. Metrics to track usage of memory for writes. (Contributed by Xiaoyu Yao)\n",
      "commitDate": "30/09/14 12:53 AM",
      "commitName": "5e8b6973527e5f714652641ed95e8a4509e18cfa",
      "commitAuthor": "arp",
      "commitDateOld": "29/09/14 10:27 PM",
      "commitNameOld": "bb84f1fccb18c6c7373851e05d2451d55e908242",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,76 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n         f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f);\n           continue;\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n \n       if (v.isTransientStorage()) {\n-        ramDiskReplicaTracker.discardReplica(bpid, invalidBlks[i].getBlockId(), true);\n+        RamDiskReplica replicaInfo \u003d\n+          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n+        if (replicaInfo !\u003d null) {\n+          if (replicaInfo.getIsPersisted() \u003d\u003d  false) {\n+            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n+          }\n+          discardRamDiskReplica(replicaInfo, true);\n+        }\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]),\n           dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      if (v.isTransientStorage()) {\n        RamDiskReplica replicaInfo \u003d\n          ramDiskReplicaTracker.getReplica(bpid, invalidBlks[i].getBlockId());\n        if (replicaInfo !\u003d null) {\n          if (replicaInfo.getIsPersisted() \u003d\u003d  false) {\n            datanode.getMetrics().incrRamDiskBlocksDeletedBeforeLazyPersisted();\n          }\n          discardRamDiskReplica(replicaInfo, true);\n        }\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]),\n          dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "b2d5ed36bcb80e2581191dcdc3976e825c959142": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7100. Make eviction scheme pluggable. (Arpit Agarwal)\n",
      "commitDate": "20/09/14 1:25 PM",
      "commitName": "b2d5ed36bcb80e2581191dcdc3976e825c959142",
      "commitAuthor": "arp",
      "commitDateOld": "19/09/14 10:02 AM",
      "commitNameOld": "222bf0fe6706ee43964fd39b8315c1a339fbc84a",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.14,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,69 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n         f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f);\n           continue;\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n \n       if (v.isTransientStorage()) {\n-        lazyWriteReplicaTracker.discardReplica(bpid, invalidBlks[i].getBlockId(), true);\n+        ramDiskReplicaTracker.discardReplica(bpid, invalidBlks[i].getBlockId(), true);\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]),\n           dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      if (v.isTransientStorage()) {\n        ramDiskReplicaTracker.discardReplica(bpid, invalidBlks[i].getBlockId(), true);\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]),\n          dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "eb448e14399e17f11b9e523e4050de245b9b0408": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6926. DN support for saving replicas to persistent storage and evicting in-memory replicas. (Arpit Agarwal)\n",
      "commitDate": "27/08/14 9:47 PM",
      "commitName": "eb448e14399e17f11b9e523e4050de245b9b0408",
      "commitAuthor": "arp",
      "commitDateOld": "27/08/14 9:47 PM",
      "commitNameOld": "a317bd7b02c37bd57743bfad59593ec12f53f4ed",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,69 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n         f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f);\n           continue;\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n \n+      if (v.isTransientStorage()) {\n+        lazyWriteReplicaTracker.discardReplica(bpid, invalidBlks[i].getBlockId(), true);\n+      }\n+\n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]),\n           dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      if (v.isTransientStorage()) {\n        lazyWriteReplicaTracker.discardReplica(bpid, invalidBlks[i].getBlockId(), true);\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]),\n          dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "1ba3f8971433cdbc3e43fd3605065d811dab5b16": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6482. Use block ID-based block layout on datanodes (James Thomas via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615223 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/14 1:41 PM",
      "commitName": "1ba3f8971433cdbc3e43fd3605065d811dab5b16",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "25/07/14 4:56 PM",
      "commitNameOld": "e85a3fecc68b48a3dc9af5daa466a24f3b39545b",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 6.86,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,72 +1,65 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n         f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f);\n           continue;\n         }\n-        ReplicaState replicaState \u003d info.getState();\n-        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n-            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n-                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n-                  ReplicaState.FINALIZED)) {\n-          v.clearPath(bpid, parent);\n-        }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n \n       // If a DFSClient has the replica in its cache of short-circuit file\n       // descriptors (and the client is using ShortCircuitShm), invalidate it.\n       datanode.getShortCircuitRegistry().processBlockInvalidation(\n                 new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n \n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n \n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]),\n           dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]),\n          dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "e85a3fecc68b48a3dc9af5daa466a24f3b39545b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6750. The DataNode should use its shared memory segment to mark short-circuit replicas that have been unlinked as stale (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1613537 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/07/14 4:56 PM",
      "commitName": "e85a3fecc68b48a3dc9af5daa466a24f3b39545b",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "22/07/14 12:41 AM",
      "commitNameOld": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.68,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,72 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           // It is okay if the block is not found -- it may be deleted earlier.\n           LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           continue;\n         }\n         f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f);\n           continue;\n         }\n         ReplicaState replicaState \u003d info.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n+\n+      // If a DFSClient has the replica in its cache of short-circuit file\n+      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n+      datanode.getShortCircuitRegistry().processBlockInvalidation(\n+                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n+\n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n+\n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]),\n           dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n     }\n     if (!errors.isEmpty()) {\n       StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n         .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n         .append(\") replica(s):\");\n       for(int i \u003d 0; i \u003c errors.size(); i++) {\n         b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n       }\n       throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        ReplicaState replicaState \u003d info.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      // If a DFSClient has the replica in its cache of short-circuit file\n      // descriptors (and the client is using ShortCircuitShm), invalidate it.\n      datanode.getShortCircuitRegistry().processBlockInvalidation(\n                new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));\n\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]),\n          dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "7ec4308f825f9ca6139431feb64818a6b3f3163c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6129.  When a replica is not found for deletion, do not throw an exception.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1579670 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/03/14 8:50 AM",
      "commitName": "7ec4308f825f9ca6139431feb64818a6b3f3163c",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "17/03/14 10:37 AM",
      "commitNameOld": "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.93,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,65 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n-    boolean error \u003d false;\n+    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n-        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n-        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n+        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n-          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+          // It is okay if the block is not found -- it may be deleted earlier.\n+          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n-          error \u003d true;\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n-          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n-          error \u003d true;\n           continue;\n         }\n+        f \u003d info.getBlockFile();\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n-          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n-          error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n-          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n-              +  \". No volume for this replica, file\u003d\" + f + \".\");\n-          error \u003d true;\n+          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n+              +  \". No volume for this replica, file\u003d\" + f);\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n-          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n-              +  \". Parent not found for file \" + f + \".\");\n-          error \u003d true;\n+          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n+              +  \". Parent not found for file \" + f);\n           continue;\n         }\n         ReplicaState replicaState \u003d info.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]),\n           dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n     }\n-    if (error) {\n-      throw new IOException(\"Error in deleting blocks.\");\n+    if (!errors.isEmpty()) {\n+      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n+        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n+        .append(\") replica(s):\");\n+      for(int i \u003d 0; i \u003c errors.size(); i++) {\n+        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n+      }\n+      throw new IOException(b.toString());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    final List\u003cString\u003e errors \u003d new ArrayList\u003cString\u003e();\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        final ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          // It is okay if the block is not found -- it may be deleted earlier.\n          LOG.info(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          continue;\n        }\n        f \u003d info.getBlockFile();\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f);\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          errors.add(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f);\n          continue;\n        }\n        ReplicaState replicaState \u003d info.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]),\n          dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n    }\n    if (!errors.isEmpty()) {\n      StringBuilder b \u003d new StringBuilder(\"Failed to delete \")\n        .append(errors.size()).append(\" (out of \").append(invalidBlks.length)\n        .append(\") replica(s):\");\n      for(int i \u003d 0; i \u003c errors.size(); i++) {\n        b.append(\"\\n\").append(i).append(\") \").append(errors.get(i));\n      }\n      throw new IOException(b.toString());\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "5df82fa01d26c18685ad7617128dbc2913547cb3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5907. BlockPoolSliceStorage trash to handle block deletions during rolling upgrade. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1568346 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/02/14 8:37 AM",
      "commitName": "5df82fa01d26c18685ad7617128dbc2913547cb3",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "06/01/14 9:28 AM",
      "commitNameOld": "f8a9329f2b8e768fe6730fc05436e973344b9132",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 38.96,
      "commitsBetweenForRepo": 231,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,63 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n         ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           error \u003d true;\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           error \u003d true;\n           continue;\n         }\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d info.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n-          new ExtendedBlock(bpid, invalidBlks[i]));\n+          new ExtendedBlock(bpid, invalidBlks[i]),\n+          dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          error \u003d true;\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d info.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]),\n          dataStorage.getTrashDirectoryForBlockFile(bpid, f));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "fba994ffe20d387e8ed875e727fc3d93f7097101": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5648. Get rid of FsDatasetImpl#perVolumeReplicaMap.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1550357 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/13 11:01 PM",
      "commitName": "fba994ffe20d387e8ed875e727fc3d93f7097101",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "03/12/13 8:30 AM",
      "commitNameOld": "a1aa1836fb6831c25efe326cdfdc014370cf5957",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 8.6,
      "commitsBetweenForRepo": 57,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,62 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n         ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           error \u003d true;\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           error \u003d true;\n           continue;\n         }\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d info.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n-        perVolumeReplicaMap.get(v.getStorageID()).remove(bpid, invalidBlks[i]);\n       }\n       // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n       // Delete the block asynchronously to make sure we can do it fast enough.\n       // It\u0027s ok to unlink the block file before the uncache operation\n       // finishes.\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          error \u003d true;\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d info.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5394: Fix race conditions in DN caching and uncaching (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/11/13 7:00 PM",
      "commitName": "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "21/10/13 12:29 PM",
      "commitNameOld": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 17.31,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,62 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n         ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           error \u003d true;\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           error \u003d true;\n           continue;\n         }\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d info.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n-\n-      // Uncache the block synchronously\n+      // If the block is cached, start uncaching it.\n       cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n-      // Delete the block asynchronously to make sure we can do it fast enough\n+      // Delete the block asynchronously to make sure we can do it fast enough.\n+      // It\u0027s ok to unlink the block file before the uncache operation\n+      // finishes.\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          error \u003d true;\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d info.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      // If the block is cached, start uncaching it.\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n      // Delete the block asynchronously to make sure we can do it fast enough.\n      // It\u0027s ok to unlink the block file before the uncache operation\n      // finishes.\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "f39f8c57344ede533ca4363c98230f3a0c401a76": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5401. Fix NPE in Directory Scanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1535158 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/10/13 1:28 PM",
      "commitName": "f39f8c57344ede533ca4363c98230f3a0c401a76",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/10/13 6:28 PM",
      "commitNameOld": "01f37e42f050207b7659bf74e2484cf8bdae2d89",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n         ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           error \u003d true;\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           error \u003d true;\n           continue;\n         }\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d info.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n-        perVolumeReplicaMap.get(v).remove(bpid, invalidBlks[i]);\n+        perVolumeReplicaMap.get(v.getStorageID()).remove(bpid, invalidBlks[i]);\n       }\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          error \u003d true;\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d info.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n        perVolumeReplicaMap.get(v.getStorageID()).remove(bpid, invalidBlks[i]);\n      }\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5349. DNA_CACHE and DNA_UNCACHE should be by blockId only (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532116 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/10/13 3:19 PM",
      "commitName": "15d08c4778350a86d7bae0174aeb48f8d8f61cce",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "23/08/13 8:41 PM",
      "commitNameOld": "b992219fa13ccee2b417d91222fd0c3e8c3ffe11",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 51.78,
      "commitsBetweenForRepo": 114,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n         ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           error \u003d true;\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           error \u003d true;\n           continue;\n         }\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d info.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n \n       // Uncache the block synchronously\n-      cacheManager.uncacheBlock(bpid, invalidBlks[i]);\n+      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n       // Delete the block asynchronously to make sure we can do it fast enough\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          error \u003d true;\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d info.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      // Uncache the block synchronously\n      cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4988. Datanode must support all the volumes as individual storages.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1526969 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 9:05 AM",
      "commitName": "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "10/09/13 11:30 PM",
      "commitNameOld": "b2976af14034c6e2a7e9964535b9f363bfc31150",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 16.4,
      "commitsBetweenForRepo": 92,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,60 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n         ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           error \u003d true;\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           error \u003d true;\n           continue;\n         }\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d info.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n+        perVolumeReplicaMap.get(v).remove(bpid, invalidBlks[i]);\n       }\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          error \u003d true;\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d info.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n        perVolumeReplicaMap.get(v).remove(bpid, invalidBlks[i]);\n      }\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5050.  Add DataNode support for mlock and munlock  (contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1517106 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/08/13 8:41 PM",
      "commitName": "b992219fa13ccee2b417d91222fd0c3e8c3ffe11",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "25/07/13 9:42 PM",
      "commitNameOld": "7723b139d55fc2c3954939559cb4914046a0f81c",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 28.96,
      "commitsBetweenForRepo": 127,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,61 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       final File f;\n       final FsVolumeImpl v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n         ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (info \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": ReplicaInfo not found.\");\n           error \u003d true;\n           continue;\n         }\n         if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               + \": GenerationStamp not matched, info\u003d\" + info);\n           error \u003d true;\n           continue;\n         }\n         v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \": File not found, volume\u003d\" + v);\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". No volume for this replica, file\u003d\" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n               +  \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d info.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n \n+      // Uncache the block synchronously\n+      cacheManager.uncacheBlock(bpid, invalidBlks[i]);\n       // Delete the block asynchronously to make sure we can do it fast enough\n       asyncDiskService.deleteAsync(v, f,\n           FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          error \u003d true;\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d info.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      // Uncache the block synchronously\n      cacheManager.uncacheBlock(bpid, invalidBlks[i]);\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "bc13dfb1426944ce45293cb8f444239a7406762c": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/04/12 10:38 AM",
      "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "02/04/12 10:38 AM",
          "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/04/12 8:48 PM",
          "commitNameOld": "a4ccb8f504e79802f1b3c69acbcbb00b2343c529",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.58,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,59 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n-      File f \u003d null;\n-      final FSVolume v;\n+      final File f;\n+      final FsVolumeImpl v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n-        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n-        if (dinfo \u003d\u003d null || \n-            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n-          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n-                           + invalidBlks[i] + \n-                           \". BlockInfo not found in volumeMap.\");\n+        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n+        if (info \u003d\u003d null) {\n+          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+              + \": ReplicaInfo not found.\");\n           error \u003d true;\n           continue;\n         }\n-        v \u003d (FSVolume)dinfo.getVolume();\n+        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n+          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+              + \": GenerationStamp not matched, info\u003d\" + info);\n+          error \u003d true;\n+          continue;\n+        }\n+        v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n-          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n-                            + invalidBlks[i] + \n-                            \". Block not found in blockMap.\" +\n-                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n+          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+              +  \": File not found, volume\u003d\" + v);\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n-          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n-                            + invalidBlks[i] + \n-                            \". No volume for this block.\" +\n-                            \" Block found in blockMap. \" + f + \".\");\n+          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+              +  \". No volume for this replica, file\u003d\" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n-          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n-                            + invalidBlks[i] + \n-                            \". Parent not found for file \" + f + \".\");\n+          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+              +  \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n-        ReplicaState replicaState \u003d dinfo.getState();\n+        ReplicaState replicaState \u003d info.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n-                ((ReplicaUnderRecovery)dinfo).getOriginalReplica().getState() \u003d\u003d \n+                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n-      File metaFile \u003d DatanodeUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp());\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n-      asyncDiskService.deleteAsync(v, f, metaFile,\n+      asyncDiskService.deleteAsync(v, f,\n+          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          error \u003d true;\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d info.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
            "oldMethodName": "invalidate",
            "newMethodName": "invalidate"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "02/04/12 10:38 AM",
          "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/04/12 8:48 PM",
          "commitNameOld": "a4ccb8f504e79802f1b3c69acbcbb00b2343c529",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.58,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,59 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n-      File f \u003d null;\n-      final FSVolume v;\n+      final File f;\n+      final FsVolumeImpl v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n-        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n-        if (dinfo \u003d\u003d null || \n-            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n-          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n-                           + invalidBlks[i] + \n-                           \". BlockInfo not found in volumeMap.\");\n+        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n+        if (info \u003d\u003d null) {\n+          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+              + \": ReplicaInfo not found.\");\n           error \u003d true;\n           continue;\n         }\n-        v \u003d (FSVolume)dinfo.getVolume();\n+        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n+          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+              + \": GenerationStamp not matched, info\u003d\" + info);\n+          error \u003d true;\n+          continue;\n+        }\n+        v \u003d (FsVolumeImpl)info.getVolume();\n         if (f \u003d\u003d null) {\n-          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n-                            + invalidBlks[i] + \n-                            \". Block not found in blockMap.\" +\n-                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n+          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+              +  \": File not found, volume\u003d\" + v);\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n-          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n-                            + invalidBlks[i] + \n-                            \". No volume for this block.\" +\n-                            \" Block found in blockMap. \" + f + \".\");\n+          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+              +  \". No volume for this replica, file\u003d\" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n-          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n-                            + invalidBlks[i] + \n-                            \". Parent not found for file \" + f + \".\");\n+          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n+              +  \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n-        ReplicaState replicaState \u003d dinfo.getState();\n+        ReplicaState replicaState \u003d info.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n-                ((ReplicaUnderRecovery)dinfo).getOriginalReplica().getState() \u003d\u003d \n+                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n-      File metaFile \u003d DatanodeUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp());\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n-      asyncDiskService.deleteAsync(v, f, metaFile,\n+      asyncDiskService.deleteAsync(v, f,\n+          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      final File f;\n      final FsVolumeImpl v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo info \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (info \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": ReplicaInfo not found.\");\n          error \u003d true;\n          continue;\n        }\n        if (info.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              + \": GenerationStamp not matched, info\u003d\" + info);\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FsVolumeImpl)info.getVolume();\n        if (f \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \": File not found, volume\u003d\" + v);\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". No volume for this replica, file\u003d\" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          LOG.warn(\"Failed to delete replica \" + invalidBlks[i]\n              +  \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d info.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)info).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f,\n          FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()),\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "99a68a14237b4cd1936ba5e9468d25d35dad594c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3155. Clean up FSDataset implemenation related code.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1306582 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/03/12 1:37 PM",
      "commitName": "99a68a14237b4cd1936ba5e9468d25d35dad594c",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "26/03/12 2:14 PM",
      "commitNameOld": "67bdbd60cdebf909a847d56ba0f90b35876af0b3",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.97,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       File f \u003d null;\n       final FSVolume v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n         ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (dinfo \u003d\u003d null || \n             dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". BlockInfo not found in volumeMap.\");\n           error \u003d true;\n           continue;\n         }\n         v \u003d (FSVolume)dinfo.getVolume();\n         if (f \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Block not found in blockMap.\" +\n                             ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". No volume for this block.\" +\n                             \" Block found in blockMap. \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d dinfo.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n-                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n+                ((ReplicaUnderRecovery)dinfo).getOriginalReplica().getState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n       File metaFile \u003d DatanodeUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp());\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n       asyncDiskService.deleteAsync(v, f, metaFile,\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      File f \u003d null;\n      final FSVolume v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo \u003d\u003d null || \n            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FSVolume)dinfo.getVolume();\n        if (f \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d dinfo.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)dinfo).getOriginalReplica().getState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile \u003d DatanodeUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp());\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "9e31bf675dd92183a9a74a66b7caf1a080581d65": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3021. Use generic type to declare FSDatasetInterface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1295929 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/03/12 1:58 PM",
      "commitName": "9e31bf675dd92183a9a74a66b7caf1a080581d65",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "22/02/12 9:47 AM",
      "commitNameOld": "efbc58f30c8e8d9f26c6a82d32d53716fb2b222a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 8.17,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       File f \u003d null;\n-      FSVolume v;\n+      final FSVolume v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n         ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (dinfo \u003d\u003d null || \n             dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". BlockInfo not found in volumeMap.\");\n           error \u003d true;\n           continue;\n         }\n         v \u003d (FSVolume)dinfo.getVolume();\n         if (f \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Block not found in blockMap.\" +\n                             ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". No volume for this block.\" +\n                             \" Block found in blockMap. \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d dinfo.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n       File metaFile \u003d DatanodeUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp());\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n       asyncDiskService.deleteAsync(v, f, metaFile,\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      File f \u003d null;\n      final FSVolume v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo \u003d\u003d null || \n            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FSVolume)dinfo.getVolume();\n        if (f \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d dinfo.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile \u003d DatanodeUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp());\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2887. FSVolume, is a part of FSDatasetInterface implementation, should not be referred outside FSDataset.  A new FSVolumeInterface is defined.  The BlockVolumeChoosingPolicy.chooseVolume(..) method signature is also updated.  (szetszwo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1242087 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/02/12 12:58 PM",
      "commitName": "b6ffb08a467f1b5bc89e5114c462c3117b337be6",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "02/02/12 11:26 PM",
      "commitNameOld": "38ad4b503686a0d18cb2d42ffdecf06f0ba7b98f",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.56,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       File f \u003d null;\n       FSVolume v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n         ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (dinfo \u003d\u003d null || \n             dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". BlockInfo not found in volumeMap.\");\n           error \u003d true;\n           continue;\n         }\n-        v \u003d dinfo.getVolume();\n+        v \u003d (FSVolume)dinfo.getVolume();\n         if (f \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Block not found in blockMap.\" +\n                             ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". No volume for this block.\" +\n                             \" Block found in blockMap. \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d dinfo.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n-      File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n+      File metaFile \u003d DatanodeUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp());\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n       asyncDiskService.deleteAsync(v, f, metaFile,\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      File f \u003d null;\n      FSVolume v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo \u003d\u003d null || \n            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error \u003d true;\n          continue;\n        }\n        v \u003d (FSVolume)dinfo.getVolume();\n        if (f \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d dinfo.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile \u003d DatanodeUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp());\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "dbbfaebb71eb9d69d67fd5becd2e357397d0f68b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2864. Remove some redundant methods and the constant METADATA_VERSION from FSDataset.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1238969 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/01/12 11:46 PM",
      "commitName": "dbbfaebb71eb9d69d67fd5becd2e357397d0f68b",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "21/11/11 6:57 PM",
      "commitNameOld": "2ab10e29d9cca5018064be46a40e3c74423615a8",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 71.2,
      "commitsBetweenForRepo": 356,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       File f \u003d null;\n       FSVolume v;\n       synchronized (this) {\n-        f \u003d getFile(bpid, invalidBlks[i]);\n+        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n         ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (dinfo \u003d\u003d null || \n             dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". BlockInfo not found in volumeMap.\");\n           error \u003d true;\n           continue;\n         }\n         v \u003d dinfo.getVolume();\n         if (f \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Block not found in blockMap.\" +\n                             ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". No volume for this block.\" +\n                             \" Block found in blockMap. \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d dinfo.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n       File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n       asyncDiskService.deleteAsync(v, f, metaFile,\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      File f \u003d null;\n      FSVolume v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i].getBlockId());\n        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo \u003d\u003d null || \n            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error \u003d true;\n          continue;\n        }\n        v \u003d dinfo.getVolume();\n        if (f \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d dinfo.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "221aadbc5b35b043fbc62c417b0edc029db9d004": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2500. Avoid file system operations in BPOfferService thread while processing deletes. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190071 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/10/11 3:47 PM",
      "commitName": "221aadbc5b35b043fbc62c417b0edc029db9d004",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "24/10/11 6:32 PM",
      "commitNameOld": "a268a3c0e45f82c93f1a6f99fd2693a1bd89dbb7",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 2.89,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,60 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       File f \u003d null;\n       FSVolume v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i]);\n         ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (dinfo \u003d\u003d null || \n             dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". BlockInfo not found in volumeMap.\");\n           error \u003d true;\n           continue;\n         }\n         v \u003d dinfo.getVolume();\n         if (f \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Block not found in blockMap.\" +\n                             ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". No volume for this block.\" +\n                             \" Block found in blockMap. \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d dinfo.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n       File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n-      long dfsBytes \u003d f.length() + metaFile.length();\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n-      asyncDiskService.deleteAsync(v, f, metaFile, dfsBytes,\n+      asyncDiskService.deleteAsync(v, f, metaFile,\n           new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      File f \u003d null;\n      FSVolume v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i]);\n        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo \u003d\u003d null || \n            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error \u003d true;\n          continue;\n        }\n        v \u003d dinfo.getVolume();\n        if (f \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d dinfo.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "73451ed2d9fb5eb228d80ad5f3be302a60496527": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-395.  DFS Scalability: Incremental block reports. Contributed by Tomasz Nykiel.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161992 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/08/11 9:46 PM",
      "commitName": "73451ed2d9fb5eb228d80ad5f3be302a60496527",
      "commitAuthor": "Hairong Kuang",
      "commitDateOld": "25/08/11 9:41 PM",
      "commitNameOld": "e680023f8b158db25b45a050236163e9246103f3",
      "commitAuthorOld": "Hairong Kuang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       File f \u003d null;\n       FSVolume v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i]);\n         ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (dinfo \u003d\u003d null || \n             dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". BlockInfo not found in volumeMap.\");\n           error \u003d true;\n           continue;\n         }\n         v \u003d dinfo.getVolume();\n         if (f \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Block not found in blockMap.\" +\n                             ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". No volume for this block.\" +\n                             \" Block found in blockMap. \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d dinfo.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n       File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n       long dfsBytes \u003d f.length() + metaFile.length();\n-      \n+\n       // Delete the block asynchronously to make sure we can do it fast enough\n-      asyncDiskService.deleteAsync(v, bpid, f, metaFile, dfsBytes,\n-          invalidBlks[i].toString());\n+      asyncDiskService.deleteAsync(v, f, metaFile, dfsBytes,\n+          new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      File f \u003d null;\n      FSVolume v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i]);\n        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo \u003d\u003d null || \n            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error \u003d true;\n          continue;\n        }\n        v \u003d dinfo.getVolume();\n        if (f \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d dinfo.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n      long dfsBytes \u003d f.length() + metaFile.length();\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile, dfsBytes,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "e680023f8b158db25b45a050236163e9246103f3": {
      "type": "Ybodychange",
      "commitMessage": "Revert 1161976 since the log message was incorrectly marked the issue as HDFS-349.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161991 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/08/11 9:41 PM",
      "commitName": "e680023f8b158db25b45a050236163e9246103f3",
      "commitAuthor": "Hairong Kuang",
      "commitDateOld": "25/08/11 8:10 PM",
      "commitNameOld": "e6339be188d3f5c94df3b092d92d201b728163f5",
      "commitAuthorOld": "Hairong Kuang",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       File f \u003d null;\n       FSVolume v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i]);\n         ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (dinfo \u003d\u003d null || \n             dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". BlockInfo not found in volumeMap.\");\n           error \u003d true;\n           continue;\n         }\n         v \u003d dinfo.getVolume();\n         if (f \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Block not found in blockMap.\" +\n                             ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". No volume for this block.\" +\n                             \" Block found in blockMap. \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d dinfo.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n       File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n       long dfsBytes \u003d f.length() + metaFile.length();\n-\n+      \n       // Delete the block asynchronously to make sure we can do it fast enough\n-      asyncDiskService.deleteAsync(v, f, metaFile, dfsBytes,\n-          new ExtendedBlock(bpid, invalidBlks[i]));\n+      asyncDiskService.deleteAsync(v, bpid, f, metaFile, dfsBytes,\n+          invalidBlks[i].toString());\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      File f \u003d null;\n      FSVolume v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i]);\n        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo \u003d\u003d null || \n            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error \u003d true;\n          continue;\n        }\n        v \u003d dinfo.getVolume();\n        if (f \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d dinfo.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n      long dfsBytes \u003d f.length() + metaFile.length();\n      \n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, bpid, f, metaFile, dfsBytes,\n          invalidBlks[i].toString());\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "e6339be188d3f5c94df3b092d92d201b728163f5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-349.  DFS Scalability: Incremental block reports. Contributed by Tomasz Nykiel.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161976 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/08/11 8:10 PM",
      "commitName": "e6339be188d3f5c94df3b092d92d201b728163f5",
      "commitAuthor": "Hairong Kuang",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 1.12,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n     boolean error \u003d false;\n     for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n       File f \u003d null;\n       FSVolume v;\n       synchronized (this) {\n         f \u003d getFile(bpid, invalidBlks[i]);\n         ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n         if (dinfo \u003d\u003d null || \n             dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". BlockInfo not found in volumeMap.\");\n           error \u003d true;\n           continue;\n         }\n         v \u003d dinfo.getVolume();\n         if (f \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Block not found in blockMap.\" +\n                             ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n           error \u003d true;\n           continue;\n         }\n         if (v \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". No volume for this block.\" +\n                             \" Block found in blockMap. \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         File parent \u003d f.getParentFile();\n         if (parent \u003d\u003d null) {\n           DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                             + invalidBlks[i] + \n                             \". Parent not found for file \" + f + \".\");\n           error \u003d true;\n           continue;\n         }\n         ReplicaState replicaState \u003d dinfo.getState();\n         if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n             (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                 ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                   ReplicaState.FINALIZED)) {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n       }\n       File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n       long dfsBytes \u003d f.length() + metaFile.length();\n-      \n+\n       // Delete the block asynchronously to make sure we can do it fast enough\n-      asyncDiskService.deleteAsync(v, bpid, f, metaFile, dfsBytes,\n-          invalidBlks[i].toString());\n+      asyncDiskService.deleteAsync(v, f, metaFile, dfsBytes,\n+          new ExtendedBlock(bpid, invalidBlks[i]));\n     }\n     if (error) {\n       throw new IOException(\"Error in deleting blocks.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      File f \u003d null;\n      FSVolume v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i]);\n        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo \u003d\u003d null || \n            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error \u003d true;\n          continue;\n        }\n        v \u003d dinfo.getVolume();\n        if (f \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d dinfo.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n      long dfsBytes \u003d f.length() + metaFile.length();\n\n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, f, metaFile, dfsBytes,\n          new ExtendedBlock(bpid, invalidBlks[i]));\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      File f \u003d null;\n      FSVolume v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i]);\n        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo \u003d\u003d null || \n            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error \u003d true;\n          continue;\n        }\n        v \u003d dinfo.getVolume();\n        if (f \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d dinfo.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n      long dfsBytes \u003d f.length() + metaFile.length();\n      \n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, bpid, f, metaFile, dfsBytes,\n          invalidBlks[i].toString());\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      File f \u003d null;\n      FSVolume v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i]);\n        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo \u003d\u003d null || \n            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error \u003d true;\n          continue;\n        }\n        v \u003d dinfo.getVolume();\n        if (f \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d dinfo.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n      long dfsBytes \u003d f.length() + metaFile.length();\n      \n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, bpid, f, metaFile, dfsBytes,\n          invalidBlks[i].toString());\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,61 @@\n+  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n+    boolean error \u003d false;\n+    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n+      File f \u003d null;\n+      FSVolume v;\n+      synchronized (this) {\n+        f \u003d getFile(bpid, invalidBlks[i]);\n+        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n+        if (dinfo \u003d\u003d null || \n+            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n+          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n+                           + invalidBlks[i] + \n+                           \". BlockInfo not found in volumeMap.\");\n+          error \u003d true;\n+          continue;\n+        }\n+        v \u003d dinfo.getVolume();\n+        if (f \u003d\u003d null) {\n+          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n+                            + invalidBlks[i] + \n+                            \". Block not found in blockMap.\" +\n+                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n+          error \u003d true;\n+          continue;\n+        }\n+        if (v \u003d\u003d null) {\n+          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n+                            + invalidBlks[i] + \n+                            \". No volume for this block.\" +\n+                            \" Block found in blockMap. \" + f + \".\");\n+          error \u003d true;\n+          continue;\n+        }\n+        File parent \u003d f.getParentFile();\n+        if (parent \u003d\u003d null) {\n+          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n+                            + invalidBlks[i] + \n+                            \". Parent not found for file \" + f + \".\");\n+          error \u003d true;\n+          continue;\n+        }\n+        ReplicaState replicaState \u003d dinfo.getState();\n+        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n+            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n+                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n+                  ReplicaState.FINALIZED)) {\n+          v.clearPath(bpid, parent);\n+        }\n+        volumeMap.remove(bpid, invalidBlks[i]);\n+      }\n+      File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n+      long dfsBytes \u003d f.length() + metaFile.length();\n+      \n+      // Delete the block asynchronously to make sure we can do it fast enough\n+      asyncDiskService.deleteAsync(v, bpid, f, metaFile, dfsBytes,\n+          invalidBlks[i].toString());\n+    }\n+    if (error) {\n+      throw new IOException(\"Error in deleting blocks.\");\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n    boolean error \u003d false;\n    for (int i \u003d 0; i \u003c invalidBlks.length; i++) {\n      File f \u003d null;\n      FSVolume v;\n      synchronized (this) {\n        f \u003d getFile(bpid, invalidBlks[i]);\n        ReplicaInfo dinfo \u003d volumeMap.get(bpid, invalidBlks[i]);\n        if (dinfo \u003d\u003d null || \n            dinfo.getGenerationStamp() !\u003d invalidBlks[i].getGenerationStamp()) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                           + invalidBlks[i] + \n                           \". BlockInfo not found in volumeMap.\");\n          error \u003d true;\n          continue;\n        }\n        v \u003d dinfo.getVolume();\n        if (f \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Block not found in blockMap.\" +\n                            ((v \u003d\u003d null) ? \" \" : \" Block found in volumeMap.\"));\n          error \u003d true;\n          continue;\n        }\n        if (v \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". No volume for this block.\" +\n                            \" Block found in blockMap. \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        File parent \u003d f.getParentFile();\n        if (parent \u003d\u003d null) {\n          DataNode.LOG.warn(\"Unexpected error trying to delete block \"\n                            + invalidBlks[i] + \n                            \". Parent not found for file \" + f + \".\");\n          error \u003d true;\n          continue;\n        }\n        ReplicaState replicaState \u003d dinfo.getState();\n        if (replicaState \u003d\u003d ReplicaState.FINALIZED || \n            (replicaState \u003d\u003d ReplicaState.RUR \u0026\u0026 \n                ((ReplicaUnderRecovery)dinfo).getOrignalReplicaState() \u003d\u003d \n                  ReplicaState.FINALIZED)) {\n          v.clearPath(bpid, parent);\n        }\n        volumeMap.remove(bpid, invalidBlks[i]);\n      }\n      File metaFile \u003d getMetaFile(f, invalidBlks[i].getGenerationStamp());\n      long dfsBytes \u003d f.length() + metaFile.length();\n      \n      // Delete the block asynchronously to make sure we can do it fast enough\n      asyncDiskService.deleteAsync(v, bpid, f, metaFile, dfsBytes,\n          invalidBlks[i].toString());\n    }\n    if (error) {\n      throw new IOException(\"Error in deleting blocks.\");\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
    }
  }
}