{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "MapTask.java",
  "functionName": "runOldMapper",
  "functionId": "runOldMapper___job-JobConf(modifiers-final)__splitIndex-TaskSplitIndex(modifiers-final)__umbilical-TaskUmbilicalProtocol(modifiers-final)__reporter-TaskReporter",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
  "functionStartLine": 432,
  "functionEndLine": 483,
  "numCommitsSeen": 36,
  "timeTaken": 9854,
  "changeHistory": [
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e",
    "8329fae686cf7a68679d177c25623311beec3384",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "4796e1adcb912005198c9003305c97cf3a8b523e",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e": "Ybodychange",
    "8329fae686cf7a68679d177c25623311beec3384": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymovefromfile",
    "4796e1adcb912005198c9003305c97cf3a8b523e": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4737. Ensure that mapreduce APIs are semantically consistent with mapred API w.r.t Mapper.cleanup and Reducer.cleanup; in the sense that cleanup is now called even if there is an error. The old mapred API already ensures that Mapper.close and Reducer.close are invoked during error handling. Note that it is an incompatible change, however end-users can override Mapper.run and Reducer.run to get the old (inconsistent) behaviour. Contributed by Arun C. Murthy.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1471556 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/04/13 10:38 AM",
      "commitName": "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "19/03/13 10:56 AM",
      "commitNameOld": "c19633da5b0cc190cc64e812ee89c38f28d5a670",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 35.99,
      "commitsBetweenForRepo": 195,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,52 @@\n   void runOldMapper(final JobConf job,\n                     final TaskSplitIndex splitIndex,\n                     final TaskUmbilicalProtocol umbilical,\n                     TaskReporter reporter\n                     ) throws IOException, InterruptedException,\n                              ClassNotFoundException {\n     InputSplit inputSplit \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n            splitIndex.getStartOffset());\n \n     updateJobWithSplit(job, inputSplit);\n     reporter.setInputSplit(inputSplit);\n \n     RecordReader\u003cINKEY,INVALUE\u003e in \u003d isSkipping() ? \n         new SkippingRecordReader\u003cINKEY,INVALUE\u003e(umbilical, reporter, job) :\n           new TrackedRecordReader\u003cINKEY,INVALUE\u003e(reporter, job);\n     job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n \n \n     int numReduceTasks \u003d conf.getNumReduceTasks();\n     LOG.info(\"numReduceTasks: \" + numReduceTasks);\n     MapOutputCollector\u003cOUTKEY, OUTVALUE\u003e collector \u003d null;\n     if (numReduceTasks \u003e 0) {\n       collector \u003d createSortingCollector(job, reporter);\n     } else { \n       collector \u003d new DirectMapOutputCollector\u003cOUTKEY, OUTVALUE\u003e();\n        MapOutputCollector.Context context \u003d\n                            new MapOutputCollector.Context(this, job, reporter);\n       collector.init(context);\n     }\n     MapRunnable\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e runner \u003d\n       ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n \n     try {\n       runner.run(in, new OldOutputCollector(collector, conf), reporter);\n       mapPhase.complete();\n       // start the sort phase only if there are reducers\n       if (numReduceTasks \u003e 0) {\n         setPhase(TaskStatus.Phase.SORT);\n       }\n       statusUpdate(umbilical);\n       collector.flush();\n-    } finally {\n-      //close\n-      in.close();                               // close input\n+      \n+      in.close();\n+      in \u003d null;\n+      \n       collector.close();\n+      collector \u003d null;\n+    } finally {\n+      closeQuietly(in);\n+      closeQuietly(collector);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runOldMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, InterruptedException,\n                             ClassNotFoundException {\n    InputSplit inputSplit \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n           splitIndex.getStartOffset());\n\n    updateJobWithSplit(job, inputSplit);\n    reporter.setInputSplit(inputSplit);\n\n    RecordReader\u003cINKEY,INVALUE\u003e in \u003d isSkipping() ? \n        new SkippingRecordReader\u003cINKEY,INVALUE\u003e(umbilical, reporter, job) :\n          new TrackedRecordReader\u003cINKEY,INVALUE\u003e(reporter, job);\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n\n    int numReduceTasks \u003d conf.getNumReduceTasks();\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n    MapOutputCollector\u003cOUTKEY, OUTVALUE\u003e collector \u003d null;\n    if (numReduceTasks \u003e 0) {\n      collector \u003d createSortingCollector(job, reporter);\n    } else { \n      collector \u003d new DirectMapOutputCollector\u003cOUTKEY, OUTVALUE\u003e();\n       MapOutputCollector.Context context \u003d\n                           new MapOutputCollector.Context(this, job, reporter);\n      collector.init(context);\n    }\n    MapRunnable\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e runner \u003d\n      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n\n    try {\n      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n      mapPhase.complete();\n      // start the sort phase only if there are reducers\n      if (numReduceTasks \u003e 0) {\n        setPhase(TaskStatus.Phase.SORT);\n      }\n      statusUpdate(umbilical);\n      collector.flush();\n      \n      in.close();\n      in \u003d null;\n      \n      collector.close();\n      collector \u003d null;\n    } finally {\n      closeQuietly(in);\n      closeQuietly(collector);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "8329fae686cf7a68679d177c25623311beec3384": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4807. Allow MapOutputBuffer to be pluggable. (masokan via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1422345 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/12/12 12:23 PM",
      "commitName": "8329fae686cf7a68679d177c25623311beec3384",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "15/12/12 12:18 PM",
      "commitNameOld": "803e5155d1c8c842bed8e2d8624cb17ab11ec53b",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,47 @@\n   void runOldMapper(final JobConf job,\n                     final TaskSplitIndex splitIndex,\n                     final TaskUmbilicalProtocol umbilical,\n                     TaskReporter reporter\n                     ) throws IOException, InterruptedException,\n                              ClassNotFoundException {\n     InputSplit inputSplit \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n            splitIndex.getStartOffset());\n \n     updateJobWithSplit(job, inputSplit);\n     reporter.setInputSplit(inputSplit);\n \n     RecordReader\u003cINKEY,INVALUE\u003e in \u003d isSkipping() ? \n         new SkippingRecordReader\u003cINKEY,INVALUE\u003e(umbilical, reporter, job) :\n           new TrackedRecordReader\u003cINKEY,INVALUE\u003e(reporter, job);\n     job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n \n \n     int numReduceTasks \u003d conf.getNumReduceTasks();\n     LOG.info(\"numReduceTasks: \" + numReduceTasks);\n-    MapOutputCollector collector \u003d null;\n+    MapOutputCollector\u003cOUTKEY, OUTVALUE\u003e collector \u003d null;\n     if (numReduceTasks \u003e 0) {\n-      collector \u003d new MapOutputBuffer(umbilical, job, reporter);\n+      collector \u003d createSortingCollector(job, reporter);\n     } else { \n-      collector \u003d new DirectMapOutputCollector(umbilical, job, reporter);\n+      collector \u003d new DirectMapOutputCollector\u003cOUTKEY, OUTVALUE\u003e();\n+       MapOutputCollector.Context context \u003d\n+                           new MapOutputCollector.Context(this, job, reporter);\n+      collector.init(context);\n     }\n     MapRunnable\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e runner \u003d\n       ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n \n     try {\n       runner.run(in, new OldOutputCollector(collector, conf), reporter);\n       mapPhase.complete();\n       // start the sort phase only if there are reducers\n       if (numReduceTasks \u003e 0) {\n         setPhase(TaskStatus.Phase.SORT);\n       }\n       statusUpdate(umbilical);\n       collector.flush();\n     } finally {\n       //close\n       in.close();                               // close input\n       collector.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runOldMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, InterruptedException,\n                             ClassNotFoundException {\n    InputSplit inputSplit \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n           splitIndex.getStartOffset());\n\n    updateJobWithSplit(job, inputSplit);\n    reporter.setInputSplit(inputSplit);\n\n    RecordReader\u003cINKEY,INVALUE\u003e in \u003d isSkipping() ? \n        new SkippingRecordReader\u003cINKEY,INVALUE\u003e(umbilical, reporter, job) :\n          new TrackedRecordReader\u003cINKEY,INVALUE\u003e(reporter, job);\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n\n    int numReduceTasks \u003d conf.getNumReduceTasks();\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n    MapOutputCollector\u003cOUTKEY, OUTVALUE\u003e collector \u003d null;\n    if (numReduceTasks \u003e 0) {\n      collector \u003d createSortingCollector(job, reporter);\n    } else { \n      collector \u003d new DirectMapOutputCollector\u003cOUTKEY, OUTVALUE\u003e();\n       MapOutputCollector.Context context \u003d\n                           new MapOutputCollector.Context(this, job, reporter);\n      collector.init(context);\n    }\n    MapRunnable\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e runner \u003d\n      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n\n    try {\n      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n      mapPhase.complete();\n      // start the sort phase only if there are reducers\n      if (numReduceTasks \u003e 0) {\n        setPhase(TaskStatus.Phase.SORT);\n      }\n      statusUpdate(umbilical);\n      collector.flush();\n    } finally {\n      //close\n      in.close();                               // close input\n      collector.close();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void runOldMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, InterruptedException,\n                             ClassNotFoundException {\n    InputSplit inputSplit \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n           splitIndex.getStartOffset());\n\n    updateJobWithSplit(job, inputSplit);\n    reporter.setInputSplit(inputSplit);\n\n    RecordReader\u003cINKEY,INVALUE\u003e in \u003d isSkipping() ? \n        new SkippingRecordReader\u003cINKEY,INVALUE\u003e(umbilical, reporter, job) :\n          new TrackedRecordReader\u003cINKEY,INVALUE\u003e(reporter, job);\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n\n    int numReduceTasks \u003d conf.getNumReduceTasks();\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n    MapOutputCollector collector \u003d null;\n    if (numReduceTasks \u003e 0) {\n      collector \u003d new MapOutputBuffer(umbilical, job, reporter);\n    } else { \n      collector \u003d new DirectMapOutputCollector(umbilical, job, reporter);\n    }\n    MapRunnable\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e runner \u003d\n      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n\n    try {\n      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n      mapPhase.complete();\n      // start the sort phase only if there are reducers\n      if (numReduceTasks \u003e 0) {\n        setPhase(TaskStatus.Phase.SORT);\n      }\n      statusUpdate(umbilical);\n      collector.flush();\n    } finally {\n      //close\n      in.close();                               // close input\n      collector.close();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void runOldMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, InterruptedException,\n                             ClassNotFoundException {\n    InputSplit inputSplit \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n           splitIndex.getStartOffset());\n\n    updateJobWithSplit(job, inputSplit);\n    reporter.setInputSplit(inputSplit);\n\n    RecordReader\u003cINKEY,INVALUE\u003e in \u003d isSkipping() ? \n        new SkippingRecordReader\u003cINKEY,INVALUE\u003e(umbilical, reporter, job) :\n          new TrackedRecordReader\u003cINKEY,INVALUE\u003e(reporter, job);\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n\n    int numReduceTasks \u003d conf.getNumReduceTasks();\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n    MapOutputCollector collector \u003d null;\n    if (numReduceTasks \u003e 0) {\n      collector \u003d new MapOutputBuffer(umbilical, job, reporter);\n    } else { \n      collector \u003d new DirectMapOutputCollector(umbilical, job, reporter);\n    }\n    MapRunnable\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e runner \u003d\n      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n\n    try {\n      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n      mapPhase.complete();\n      // start the sort phase only if there are reducers\n      if (numReduceTasks \u003e 0) {\n        setPhase(TaskStatus.Phase.SORT);\n      }\n      statusUpdate(umbilical);\n      collector.flush();\n    } finally {\n      //close\n      in.close();                               // close input\n      collector.close();\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
        "oldMethodName": "runOldMapper",
        "newMethodName": "runOldMapper"
      }
    },
    "4796e1adcb912005198c9003305c97cf3a8b523e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2365. Add counters to track bytes (read,written) via File(Input,Output)Format. Contributed by Siddharth Seth. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1146515 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/07/11 4:36 PM",
      "commitName": "4796e1adcb912005198c9003305c97cf3a8b523e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "11/07/11 5:54 PM",
      "commitNameOld": "ad7cf36d5fd99ecaf29e33d8de437e21f81a32d3",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.95,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,44 @@\n   void runOldMapper(final JobConf job,\n                     final TaskSplitIndex splitIndex,\n                     final TaskUmbilicalProtocol umbilical,\n                     TaskReporter reporter\n                     ) throws IOException, InterruptedException,\n                              ClassNotFoundException {\n     InputSplit inputSplit \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n            splitIndex.getStartOffset());\n \n     updateJobWithSplit(job, inputSplit);\n     reporter.setInputSplit(inputSplit);\n \n-    RecordReader\u003cINKEY,INVALUE\u003e rawIn \u003d                  // open input\n-      job.getInputFormat().getRecordReader(inputSplit, job, reporter);\n     RecordReader\u003cINKEY,INVALUE\u003e in \u003d isSkipping() ? \n-        new SkippingRecordReader\u003cINKEY,INVALUE\u003e(rawIn, umbilical, reporter) :\n-        new TrackedRecordReader\u003cINKEY,INVALUE\u003e(rawIn, reporter);\n+        new SkippingRecordReader\u003cINKEY,INVALUE\u003e(umbilical, reporter, job) :\n+          new TrackedRecordReader\u003cINKEY,INVALUE\u003e(reporter, job);\n     job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n \n \n     int numReduceTasks \u003d conf.getNumReduceTasks();\n     LOG.info(\"numReduceTasks: \" + numReduceTasks);\n     MapOutputCollector collector \u003d null;\n     if (numReduceTasks \u003e 0) {\n       collector \u003d new MapOutputBuffer(umbilical, job, reporter);\n     } else { \n       collector \u003d new DirectMapOutputCollector(umbilical, job, reporter);\n     }\n     MapRunnable\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e runner \u003d\n       ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n \n     try {\n       runner.run(in, new OldOutputCollector(collector, conf), reporter);\n       mapPhase.complete();\n       // start the sort phase only if there are reducers\n       if (numReduceTasks \u003e 0) {\n         setPhase(TaskStatus.Phase.SORT);\n       }\n       statusUpdate(umbilical);\n       collector.flush();\n     } finally {\n       //close\n       in.close();                               // close input\n       collector.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runOldMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, InterruptedException,\n                             ClassNotFoundException {\n    InputSplit inputSplit \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n           splitIndex.getStartOffset());\n\n    updateJobWithSplit(job, inputSplit);\n    reporter.setInputSplit(inputSplit);\n\n    RecordReader\u003cINKEY,INVALUE\u003e in \u003d isSkipping() ? \n        new SkippingRecordReader\u003cINKEY,INVALUE\u003e(umbilical, reporter, job) :\n          new TrackedRecordReader\u003cINKEY,INVALUE\u003e(reporter, job);\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n\n    int numReduceTasks \u003d conf.getNumReduceTasks();\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n    MapOutputCollector collector \u003d null;\n    if (numReduceTasks \u003e 0) {\n      collector \u003d new MapOutputBuffer(umbilical, job, reporter);\n    } else { \n      collector \u003d new DirectMapOutputCollector(umbilical, job, reporter);\n    }\n    MapRunnable\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e runner \u003d\n      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n\n    try {\n      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n      mapPhase.complete();\n      // start the sort phase only if there are reducers\n      if (numReduceTasks \u003e 0) {\n        setPhase(TaskStatus.Phase.SORT);\n      }\n      statusUpdate(umbilical);\n      collector.flush();\n    } finally {\n      //close\n      in.close();                               // close input\n      collector.close();\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,46 @@\n+  void runOldMapper(final JobConf job,\n+                    final TaskSplitIndex splitIndex,\n+                    final TaskUmbilicalProtocol umbilical,\n+                    TaskReporter reporter\n+                    ) throws IOException, InterruptedException,\n+                             ClassNotFoundException {\n+    InputSplit inputSplit \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n+           splitIndex.getStartOffset());\n+\n+    updateJobWithSplit(job, inputSplit);\n+    reporter.setInputSplit(inputSplit);\n+\n+    RecordReader\u003cINKEY,INVALUE\u003e rawIn \u003d                  // open input\n+      job.getInputFormat().getRecordReader(inputSplit, job, reporter);\n+    RecordReader\u003cINKEY,INVALUE\u003e in \u003d isSkipping() ? \n+        new SkippingRecordReader\u003cINKEY,INVALUE\u003e(rawIn, umbilical, reporter) :\n+        new TrackedRecordReader\u003cINKEY,INVALUE\u003e(rawIn, reporter);\n+    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n+\n+\n+    int numReduceTasks \u003d conf.getNumReduceTasks();\n+    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n+    MapOutputCollector collector \u003d null;\n+    if (numReduceTasks \u003e 0) {\n+      collector \u003d new MapOutputBuffer(umbilical, job, reporter);\n+    } else { \n+      collector \u003d new DirectMapOutputCollector(umbilical, job, reporter);\n+    }\n+    MapRunnable\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e runner \u003d\n+      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n+\n+    try {\n+      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n+      mapPhase.complete();\n+      // start the sort phase only if there are reducers\n+      if (numReduceTasks \u003e 0) {\n+        setPhase(TaskStatus.Phase.SORT);\n+      }\n+      statusUpdate(umbilical);\n+      collector.flush();\n+    } finally {\n+      //close\n+      in.close();                               // close input\n+      collector.close();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void runOldMapper(final JobConf job,\n                    final TaskSplitIndex splitIndex,\n                    final TaskUmbilicalProtocol umbilical,\n                    TaskReporter reporter\n                    ) throws IOException, InterruptedException,\n                             ClassNotFoundException {\n    InputSplit inputSplit \u003d getSplitDetails(new Path(splitIndex.getSplitLocation()),\n           splitIndex.getStartOffset());\n\n    updateJobWithSplit(job, inputSplit);\n    reporter.setInputSplit(inputSplit);\n\n    RecordReader\u003cINKEY,INVALUE\u003e rawIn \u003d                  // open input\n      job.getInputFormat().getRecordReader(inputSplit, job, reporter);\n    RecordReader\u003cINKEY,INVALUE\u003e in \u003d isSkipping() ? \n        new SkippingRecordReader\u003cINKEY,INVALUE\u003e(rawIn, umbilical, reporter) :\n        new TrackedRecordReader\u003cINKEY,INVALUE\u003e(rawIn, reporter);\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\n\n\n    int numReduceTasks \u003d conf.getNumReduceTasks();\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\n    MapOutputCollector collector \u003d null;\n    if (numReduceTasks \u003e 0) {\n      collector \u003d new MapOutputBuffer(umbilical, job, reporter);\n    } else { \n      collector \u003d new DirectMapOutputCollector(umbilical, job, reporter);\n    }\n    MapRunnable\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e runner \u003d\n      ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\n\n    try {\n      runner.run(in, new OldOutputCollector(collector, conf), reporter);\n      mapPhase.complete();\n      // start the sort phase only if there are reducers\n      if (numReduceTasks \u003e 0) {\n        setPhase(TaskStatus.Phase.SORT);\n      }\n      statusUpdate(umbilical);\n      collector.flush();\n    } finally {\n      //close\n      in.close();                               // close input\n      collector.close();\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java"
    }
  }
}