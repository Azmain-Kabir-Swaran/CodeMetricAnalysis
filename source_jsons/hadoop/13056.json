{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockPlacementPolicyWithNodeGroup.java",
  "functionName": "chooseRemoteRack",
  "functionId": "chooseRemoteRack___numOfReplicas-int__localMachine-DatanodeDescriptor__excludedNodes-Set__Node____blocksize-long__maxReplicasPerRack-int__results-List__DatanodeStorageInfo____avoidStaleNodes-boolean__storageTypes-EnumMap__StorageType,Integer__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java",
  "functionStartLine": 206,
  "functionEndLine": 225,
  "numCommitsSeen": 60,
  "timeTaken": 3003,
  "changeHistory": [
    "e08701ec71f7c10d8f15122d90c35f9f22e40837",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9",
    "f98c343c7f11c165bcc0f7cdbaa2a3998b12cfd2",
    "d01caeee77f4ea00173db7f20a945f6cbfd0c9f7",
    "0182ea16d359b41c065bf9cbf740f8b23f6381e3",
    "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7",
    "4d0cab2729e2bdb1742b62dba69bd30ab69c868e"
  ],
  "changeHistoryShort": {
    "e08701ec71f7c10d8f15122d90c35f9f22e40837": "Ymultichange(Yparameterchange,Ybodychange)",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": "Ymultichange(Yparameterchange,Ybodychange)",
    "f98c343c7f11c165bcc0f7cdbaa2a3998b12cfd2": "Yparameterchange",
    "d01caeee77f4ea00173db7f20a945f6cbfd0c9f7": "Yparameterchange",
    "0182ea16d359b41c065bf9cbf740f8b23f6381e3": "Ybodychange",
    "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7": "Ymultichange(Yparameterchange,Ybodychange)",
    "4d0cab2729e2bdb1742b62dba69bd30ab69c868e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "e08701ec71f7c10d8f15122d90c35f9f22e40837": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6961. Archival Storage: BlockPlacementPolicy#chooseTarget should check each valid storage type in each choosing round.\n",
      "commitDate": "04/09/14 2:19 PM",
      "commitName": "e08701ec71f7c10d8f15122d90c35f9f22e40837",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6961. Archival Storage: BlockPlacementPolicy#chooseTarget should check each valid storage type in each choosing round.\n",
          "commitDate": "04/09/14 2:19 PM",
          "commitName": "e08701ec71f7c10d8f15122d90c35f9f22e40837",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "21/07/14 4:21 PM",
          "commitNameOld": "44d9bb26d640ca5c1de651563c7993b4ecd6b653",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 44.92,
          "commitsBetweenForRepo": 367,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n   protected void chooseRemoteRack(int numOfReplicas,\n       DatanodeDescriptor localMachine, Set\u003cNode\u003e excludedNodes,\n       long blocksize, int maxReplicasPerRack, List\u003cDatanodeStorageInfo\u003e results,\n-      boolean avoidStaleNodes, StorageType storageType)\n-          throws NotEnoughReplicasException {\n+      boolean avoidStaleNodes, EnumMap\u003cStorageType, Integer\u003e storageTypes)\n+      throws NotEnoughReplicasException {\n     int oldNumOfReplicas \u003d results.size();\n \n     final String rackLocation \u003d NetworkTopology.getFirstHalf(\n         localMachine.getNetworkLocation());\n     try {\n       // randomly choose from remote racks\n       chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n-          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n+          maxReplicasPerRack, results, avoidStaleNodes, storageTypes);\n     } catch (NotEnoughReplicasException e) {\n       // fall back to the local rack\n       chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n           rackLocation, excludedNodes, blocksize,\n-          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n+          maxReplicasPerRack, results, avoidStaleNodes, storageTypes);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void chooseRemoteRack(int numOfReplicas,\n      DatanodeDescriptor localMachine, Set\u003cNode\u003e excludedNodes,\n      long blocksize, int maxReplicasPerRack, List\u003cDatanodeStorageInfo\u003e results,\n      boolean avoidStaleNodes, EnumMap\u003cStorageType, Integer\u003e storageTypes)\n      throws NotEnoughReplicasException {\n    int oldNumOfReplicas \u003d results.size();\n\n    final String rackLocation \u003d NetworkTopology.getFirstHalf(\n        localMachine.getNetworkLocation());\n    try {\n      // randomly choose from remote racks\n      chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes, storageTypes);\n    } catch (NotEnoughReplicasException e) {\n      // fall back to the local rack\n      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n          rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes, storageTypes);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java",
          "extendedDetails": {
            "oldValue": "[numOfReplicas-int, localMachine-DatanodeDescriptor, excludedNodes-Set\u003cNode\u003e, blocksize-long, maxReplicasPerRack-int, results-List\u003cDatanodeStorageInfo\u003e, avoidStaleNodes-boolean, storageType-StorageType]",
            "newValue": "[numOfReplicas-int, localMachine-DatanodeDescriptor, excludedNodes-Set\u003cNode\u003e, blocksize-long, maxReplicasPerRack-int, results-List\u003cDatanodeStorageInfo\u003e, avoidStaleNodes-boolean, storageTypes-EnumMap\u003cStorageType,Integer\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6961. Archival Storage: BlockPlacementPolicy#chooseTarget should check each valid storage type in each choosing round.\n",
          "commitDate": "04/09/14 2:19 PM",
          "commitName": "e08701ec71f7c10d8f15122d90c35f9f22e40837",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "21/07/14 4:21 PM",
          "commitNameOld": "44d9bb26d640ca5c1de651563c7993b4ecd6b653",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 44.92,
          "commitsBetweenForRepo": 367,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n   protected void chooseRemoteRack(int numOfReplicas,\n       DatanodeDescriptor localMachine, Set\u003cNode\u003e excludedNodes,\n       long blocksize, int maxReplicasPerRack, List\u003cDatanodeStorageInfo\u003e results,\n-      boolean avoidStaleNodes, StorageType storageType)\n-          throws NotEnoughReplicasException {\n+      boolean avoidStaleNodes, EnumMap\u003cStorageType, Integer\u003e storageTypes)\n+      throws NotEnoughReplicasException {\n     int oldNumOfReplicas \u003d results.size();\n \n     final String rackLocation \u003d NetworkTopology.getFirstHalf(\n         localMachine.getNetworkLocation());\n     try {\n       // randomly choose from remote racks\n       chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n-          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n+          maxReplicasPerRack, results, avoidStaleNodes, storageTypes);\n     } catch (NotEnoughReplicasException e) {\n       // fall back to the local rack\n       chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n           rackLocation, excludedNodes, blocksize,\n-          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n+          maxReplicasPerRack, results, avoidStaleNodes, storageTypes);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void chooseRemoteRack(int numOfReplicas,\n      DatanodeDescriptor localMachine, Set\u003cNode\u003e excludedNodes,\n      long blocksize, int maxReplicasPerRack, List\u003cDatanodeStorageInfo\u003e results,\n      boolean avoidStaleNodes, EnumMap\u003cStorageType, Integer\u003e storageTypes)\n      throws NotEnoughReplicasException {\n    int oldNumOfReplicas \u003d results.size();\n\n    final String rackLocation \u003d NetworkTopology.getFirstHalf(\n        localMachine.getNetworkLocation());\n    try {\n      // randomly choose from remote racks\n      chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes, storageTypes);\n    } catch (NotEnoughReplicasException e) {\n      // fall back to the local rack\n      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n          rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes, storageTypes);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java",
          "extendedDetails": {}
        }
      ]
    },
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4990. Change BlockPlacementPolicy to choose storages instead of datanodes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1524444 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/13 8:12 AM",
      "commitName": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4990. Change BlockPlacementPolicy to choose storages instead of datanodes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1524444 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/09/13 8:12 AM",
          "commitName": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/09/13 7:38 PM",
          "commitNameOld": "f98c343c7f11c165bcc0f7cdbaa2a3998b12cfd2",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.52,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,20 @@\n   protected void chooseRemoteRack(int numOfReplicas,\n       DatanodeDescriptor localMachine, Set\u003cNode\u003e excludedNodes,\n-      long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n-      boolean avoidStaleNodes) throws NotEnoughReplicasException {\n+      long blocksize, int maxReplicasPerRack, List\u003cDatanodeStorageInfo\u003e results,\n+      boolean avoidStaleNodes, StorageType storageType)\n+          throws NotEnoughReplicasException {\n     int oldNumOfReplicas \u003d results.size();\n \n     final String rackLocation \u003d NetworkTopology.getFirstHalf(\n         localMachine.getNetworkLocation());\n     try {\n       // randomly choose from remote racks\n       chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n-          maxReplicasPerRack, results, avoidStaleNodes);\n+          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n     } catch (NotEnoughReplicasException e) {\n       // fall back to the local rack\n       chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n           rackLocation, excludedNodes, blocksize,\n-          maxReplicasPerRack, results, avoidStaleNodes);\n+          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void chooseRemoteRack(int numOfReplicas,\n      DatanodeDescriptor localMachine, Set\u003cNode\u003e excludedNodes,\n      long blocksize, int maxReplicasPerRack, List\u003cDatanodeStorageInfo\u003e results,\n      boolean avoidStaleNodes, StorageType storageType)\n          throws NotEnoughReplicasException {\n    int oldNumOfReplicas \u003d results.size();\n\n    final String rackLocation \u003d NetworkTopology.getFirstHalf(\n        localMachine.getNetworkLocation());\n    try {\n      // randomly choose from remote racks\n      chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n    } catch (NotEnoughReplicasException e) {\n      // fall back to the local rack\n      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n          rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java",
          "extendedDetails": {
            "oldValue": "[numOfReplicas-int, localMachine-DatanodeDescriptor, excludedNodes-Set\u003cNode\u003e, blocksize-long, maxReplicasPerRack-int, results-List\u003cDatanodeDescriptor\u003e, avoidStaleNodes-boolean]",
            "newValue": "[numOfReplicas-int, localMachine-DatanodeDescriptor, excludedNodes-Set\u003cNode\u003e, blocksize-long, maxReplicasPerRack-int, results-List\u003cDatanodeStorageInfo\u003e, avoidStaleNodes-boolean, storageType-StorageType]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4990. Change BlockPlacementPolicy to choose storages instead of datanodes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1524444 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/09/13 8:12 AM",
          "commitName": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/09/13 7:38 PM",
          "commitNameOld": "f98c343c7f11c165bcc0f7cdbaa2a3998b12cfd2",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.52,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,20 @@\n   protected void chooseRemoteRack(int numOfReplicas,\n       DatanodeDescriptor localMachine, Set\u003cNode\u003e excludedNodes,\n-      long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n-      boolean avoidStaleNodes) throws NotEnoughReplicasException {\n+      long blocksize, int maxReplicasPerRack, List\u003cDatanodeStorageInfo\u003e results,\n+      boolean avoidStaleNodes, StorageType storageType)\n+          throws NotEnoughReplicasException {\n     int oldNumOfReplicas \u003d results.size();\n \n     final String rackLocation \u003d NetworkTopology.getFirstHalf(\n         localMachine.getNetworkLocation());\n     try {\n       // randomly choose from remote racks\n       chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n-          maxReplicasPerRack, results, avoidStaleNodes);\n+          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n     } catch (NotEnoughReplicasException e) {\n       // fall back to the local rack\n       chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n           rackLocation, excludedNodes, blocksize,\n-          maxReplicasPerRack, results, avoidStaleNodes);\n+          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void chooseRemoteRack(int numOfReplicas,\n      DatanodeDescriptor localMachine, Set\u003cNode\u003e excludedNodes,\n      long blocksize, int maxReplicasPerRack, List\u003cDatanodeStorageInfo\u003e results,\n      boolean avoidStaleNodes, StorageType storageType)\n          throws NotEnoughReplicasException {\n    int oldNumOfReplicas \u003d results.size();\n\n    final String rackLocation \u003d NetworkTopology.getFirstHalf(\n        localMachine.getNetworkLocation());\n    try {\n      // randomly choose from remote racks\n      chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n    } catch (NotEnoughReplicasException e) {\n      // fall back to the local rack\n      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n          rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes, storageType);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java",
          "extendedDetails": {}
        }
      ]
    },
    "f98c343c7f11c165bcc0f7cdbaa2a3998b12cfd2": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-5207. In BlockPlacementPolicy.chooseTarget(..), change the writer and the excludedNodes parameter types respectively to Node and Set.  Contributed by Junping Du\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1523875 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/09/13 7:38 PM",
      "commitName": "f98c343c7f11c165bcc0f7cdbaa2a3998b12cfd2",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "14/09/13 9:15 PM",
      "commitNameOld": "d01caeee77f4ea00173db7f20a945f6cbfd0c9f7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.93,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   protected void chooseRemoteRack(int numOfReplicas,\n-      DatanodeDescriptor localMachine, Map\u003cNode, Node\u003e excludedNodes,\n+      DatanodeDescriptor localMachine, Set\u003cNode\u003e excludedNodes,\n       long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n       boolean avoidStaleNodes) throws NotEnoughReplicasException {\n     int oldNumOfReplicas \u003d results.size();\n \n     final String rackLocation \u003d NetworkTopology.getFirstHalf(\n         localMachine.getNetworkLocation());\n     try {\n       // randomly choose from remote racks\n       chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n           maxReplicasPerRack, results, avoidStaleNodes);\n     } catch (NotEnoughReplicasException e) {\n       // fall back to the local rack\n       chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n           rackLocation, excludedNodes, blocksize,\n           maxReplicasPerRack, results, avoidStaleNodes);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void chooseRemoteRack(int numOfReplicas,\n      DatanodeDescriptor localMachine, Set\u003cNode\u003e excludedNodes,\n      long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n      boolean avoidStaleNodes) throws NotEnoughReplicasException {\n    int oldNumOfReplicas \u003d results.size();\n\n    final String rackLocation \u003d NetworkTopology.getFirstHalf(\n        localMachine.getNetworkLocation());\n    try {\n      // randomly choose from remote racks\n      chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes);\n    } catch (NotEnoughReplicasException e) {\n      // fall back to the local rack\n      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n          rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java",
      "extendedDetails": {
        "oldValue": "[numOfReplicas-int, localMachine-DatanodeDescriptor, excludedNodes-Map\u003cNode,Node\u003e, blocksize-long, maxReplicasPerRack-int, results-List\u003cDatanodeDescriptor\u003e, avoidStaleNodes-boolean]",
        "newValue": "[numOfReplicas-int, localMachine-DatanodeDescriptor, excludedNodes-Set\u003cNode\u003e, blocksize-long, maxReplicasPerRack-int, results-List\u003cDatanodeDescriptor\u003e, avoidStaleNodes-boolean]"
      }
    },
    "d01caeee77f4ea00173db7f20a945f6cbfd0c9f7": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-5188. In BlockPlacementPolicy, reduce the number of chooseTarget(..) methods; replace HashMap with Map in parameter declarations and cleanup some related code.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1523400 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/09/13 9:15 PM",
      "commitName": "d01caeee77f4ea00173db7f20a945f6cbfd0c9f7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "14/08/13 9:52 PM",
      "commitNameOld": "0182ea16d359b41c065bf9cbf740f8b23f6381e3",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 30.97,
      "commitsBetweenForRepo": 162,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   protected void chooseRemoteRack(int numOfReplicas,\n-      DatanodeDescriptor localMachine, HashMap\u003cNode, Node\u003e excludedNodes,\n+      DatanodeDescriptor localMachine, Map\u003cNode, Node\u003e excludedNodes,\n       long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n       boolean avoidStaleNodes) throws NotEnoughReplicasException {\n     int oldNumOfReplicas \u003d results.size();\n \n     final String rackLocation \u003d NetworkTopology.getFirstHalf(\n         localMachine.getNetworkLocation());\n     try {\n       // randomly choose from remote racks\n       chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n           maxReplicasPerRack, results, avoidStaleNodes);\n     } catch (NotEnoughReplicasException e) {\n       // fall back to the local rack\n       chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n           rackLocation, excludedNodes, blocksize,\n           maxReplicasPerRack, results, avoidStaleNodes);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void chooseRemoteRack(int numOfReplicas,\n      DatanodeDescriptor localMachine, Map\u003cNode, Node\u003e excludedNodes,\n      long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n      boolean avoidStaleNodes) throws NotEnoughReplicasException {\n    int oldNumOfReplicas \u003d results.size();\n\n    final String rackLocation \u003d NetworkTopology.getFirstHalf(\n        localMachine.getNetworkLocation());\n    try {\n      // randomly choose from remote racks\n      chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes);\n    } catch (NotEnoughReplicasException e) {\n      // fall back to the local rack\n      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n          rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java",
      "extendedDetails": {
        "oldValue": "[numOfReplicas-int, localMachine-DatanodeDescriptor, excludedNodes-HashMap\u003cNode,Node\u003e, blocksize-long, maxReplicasPerRack-int, results-List\u003cDatanodeDescriptor\u003e, avoidStaleNodes-boolean]",
        "newValue": "[numOfReplicas-int, localMachine-DatanodeDescriptor, excludedNodes-Map\u003cNode,Node\u003e, blocksize-long, maxReplicasPerRack-int, results-List\u003cDatanodeDescriptor\u003e, avoidStaleNodes-boolean]"
      }
    },
    "0182ea16d359b41c065bf9cbf740f8b23f6381e3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4898. BlockPlacementPolicyWithNodeGroup.chooseRemoteRack() fails to properly fallback to local rack.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1514156 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/08/13 9:52 PM",
      "commitName": "0182ea16d359b41c065bf9cbf740f8b23f6381e3",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "03/12/12 1:59 PM",
      "commitNameOld": "8f7c92094dfc3ea692e5f893126b89987fce2006",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 254.29,
      "commitsBetweenForRepo": 1460,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,19 @@\n   protected void chooseRemoteRack(int numOfReplicas,\n       DatanodeDescriptor localMachine, HashMap\u003cNode, Node\u003e excludedNodes,\n       long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n       boolean avoidStaleNodes) throws NotEnoughReplicasException {\n     int oldNumOfReplicas \u003d results.size();\n-    // randomly choose one node from remote racks\n+\n+    final String rackLocation \u003d NetworkTopology.getFirstHalf(\n+        localMachine.getNetworkLocation());\n     try {\n-      chooseRandom(\n-          numOfReplicas,\n-          \"~\" + NetworkTopology.getFirstHalf(localMachine.getNetworkLocation()),\n-          excludedNodes, blocksize, maxReplicasPerRack, results,\n-          avoidStaleNodes);\n+      // randomly choose from remote racks\n+      chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n+          maxReplicasPerRack, results, avoidStaleNodes);\n     } catch (NotEnoughReplicasException e) {\n+      // fall back to the local rack\n       chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n-          localMachine.getNetworkLocation(), excludedNodes, blocksize,\n+          rackLocation, excludedNodes, blocksize,\n           maxReplicasPerRack, results, avoidStaleNodes);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void chooseRemoteRack(int numOfReplicas,\n      DatanodeDescriptor localMachine, HashMap\u003cNode, Node\u003e excludedNodes,\n      long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n      boolean avoidStaleNodes) throws NotEnoughReplicasException {\n    int oldNumOfReplicas \u003d results.size();\n\n    final String rackLocation \u003d NetworkTopology.getFirstHalf(\n        localMachine.getNetworkLocation());\n    try {\n      // randomly choose from remote racks\n      chooseRandom(numOfReplicas, \"~\" + rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes);\n    } catch (NotEnoughReplicasException e) {\n      // fall back to the local rack\n      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n          rackLocation, excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java",
      "extendedDetails": {}
    },
    "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-3912. Detect and avoid stale datanodes for writes. Contributed by Jing Zhao\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1397211 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/10/12 11:08 AM",
      "commitName": "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-3912. Detect and avoid stale datanodes for writes. Contributed by Jing Zhao\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1397211 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/10/12 11:08 AM",
          "commitName": "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "24/08/12 6:03 PM",
          "commitNameOld": "deead78e35b0cb81af875b5a8032cbd06c9a2dae",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 47.71,
          "commitsBetweenForRepo": 284,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,18 @@\n   protected void chooseRemoteRack(int numOfReplicas,\n-          DatanodeDescriptor localMachine,\n-          HashMap\u003cNode, Node\u003e excludedNodes,\n-          long blocksize,\n-          int maxReplicasPerRack,\n-          List\u003cDatanodeDescriptor\u003e results)\n-          throws NotEnoughReplicasException {\n+      DatanodeDescriptor localMachine, HashMap\u003cNode, Node\u003e excludedNodes,\n+      long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n+      boolean avoidStaleNodes) throws NotEnoughReplicasException {\n     int oldNumOfReplicas \u003d results.size();\n     // randomly choose one node from remote racks\n     try {\n-      chooseRandom(numOfReplicas, \"~\"+NetworkTopology.getFirstHalf(\n-          localMachine.getNetworkLocation()),\n-      excludedNodes, blocksize, maxReplicasPerRack, results);\n+      chooseRandom(\n+          numOfReplicas,\n+          \"~\" + NetworkTopology.getFirstHalf(localMachine.getNetworkLocation()),\n+          excludedNodes, blocksize, maxReplicasPerRack, results,\n+          avoidStaleNodes);\n     } catch (NotEnoughReplicasException e) {\n-      chooseRandom(numOfReplicas-(results.size()-oldNumOfReplicas),\n-      localMachine.getNetworkLocation(), excludedNodes, blocksize, \n-      maxReplicasPerRack, results);\n+      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n+          localMachine.getNetworkLocation(), excludedNodes, blocksize,\n+          maxReplicasPerRack, results, avoidStaleNodes);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void chooseRemoteRack(int numOfReplicas,\n      DatanodeDescriptor localMachine, HashMap\u003cNode, Node\u003e excludedNodes,\n      long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n      boolean avoidStaleNodes) throws NotEnoughReplicasException {\n    int oldNumOfReplicas \u003d results.size();\n    // randomly choose one node from remote racks\n    try {\n      chooseRandom(\n          numOfReplicas,\n          \"~\" + NetworkTopology.getFirstHalf(localMachine.getNetworkLocation()),\n          excludedNodes, blocksize, maxReplicasPerRack, results,\n          avoidStaleNodes);\n    } catch (NotEnoughReplicasException e) {\n      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n          localMachine.getNetworkLocation(), excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java",
          "extendedDetails": {
            "oldValue": "[numOfReplicas-int, localMachine-DatanodeDescriptor, excludedNodes-HashMap\u003cNode,Node\u003e, blocksize-long, maxReplicasPerRack-int, results-List\u003cDatanodeDescriptor\u003e]",
            "newValue": "[numOfReplicas-int, localMachine-DatanodeDescriptor, excludedNodes-HashMap\u003cNode,Node\u003e, blocksize-long, maxReplicasPerRack-int, results-List\u003cDatanodeDescriptor\u003e, avoidStaleNodes-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3912. Detect and avoid stale datanodes for writes. Contributed by Jing Zhao\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1397211 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/10/12 11:08 AM",
          "commitName": "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "24/08/12 6:03 PM",
          "commitNameOld": "deead78e35b0cb81af875b5a8032cbd06c9a2dae",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 47.71,
          "commitsBetweenForRepo": 284,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,18 @@\n   protected void chooseRemoteRack(int numOfReplicas,\n-          DatanodeDescriptor localMachine,\n-          HashMap\u003cNode, Node\u003e excludedNodes,\n-          long blocksize,\n-          int maxReplicasPerRack,\n-          List\u003cDatanodeDescriptor\u003e results)\n-          throws NotEnoughReplicasException {\n+      DatanodeDescriptor localMachine, HashMap\u003cNode, Node\u003e excludedNodes,\n+      long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n+      boolean avoidStaleNodes) throws NotEnoughReplicasException {\n     int oldNumOfReplicas \u003d results.size();\n     // randomly choose one node from remote racks\n     try {\n-      chooseRandom(numOfReplicas, \"~\"+NetworkTopology.getFirstHalf(\n-          localMachine.getNetworkLocation()),\n-      excludedNodes, blocksize, maxReplicasPerRack, results);\n+      chooseRandom(\n+          numOfReplicas,\n+          \"~\" + NetworkTopology.getFirstHalf(localMachine.getNetworkLocation()),\n+          excludedNodes, blocksize, maxReplicasPerRack, results,\n+          avoidStaleNodes);\n     } catch (NotEnoughReplicasException e) {\n-      chooseRandom(numOfReplicas-(results.size()-oldNumOfReplicas),\n-      localMachine.getNetworkLocation(), excludedNodes, blocksize, \n-      maxReplicasPerRack, results);\n+      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n+          localMachine.getNetworkLocation(), excludedNodes, blocksize,\n+          maxReplicasPerRack, results, avoidStaleNodes);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void chooseRemoteRack(int numOfReplicas,\n      DatanodeDescriptor localMachine, HashMap\u003cNode, Node\u003e excludedNodes,\n      long blocksize, int maxReplicasPerRack, List\u003cDatanodeDescriptor\u003e results,\n      boolean avoidStaleNodes) throws NotEnoughReplicasException {\n    int oldNumOfReplicas \u003d results.size();\n    // randomly choose one node from remote racks\n    try {\n      chooseRandom(\n          numOfReplicas,\n          \"~\" + NetworkTopology.getFirstHalf(localMachine.getNetworkLocation()),\n          excludedNodes, blocksize, maxReplicasPerRack, results,\n          avoidStaleNodes);\n    } catch (NotEnoughReplicasException e) {\n      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),\n          localMachine.getNetworkLocation(), excludedNodes, blocksize,\n          maxReplicasPerRack, results, avoidStaleNodes);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java",
          "extendedDetails": {}
        }
      ]
    },
    "4d0cab2729e2bdb1742b62dba69bd30ab69c868e": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3601. Add BlockPlacementPolicyWithNodeGroup to support block placement with 4-layer network topology.  Contributed by Junping Du\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1357442 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/07/12 6:31 PM",
      "commitName": "4d0cab2729e2bdb1742b62dba69bd30ab69c868e",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,19 @@\n+  protected void chooseRemoteRack(int numOfReplicas,\n+          DatanodeDescriptor localMachine,\n+          HashMap\u003cNode, Node\u003e excludedNodes,\n+          long blocksize,\n+          int maxReplicasPerRack,\n+          List\u003cDatanodeDescriptor\u003e results)\n+          throws NotEnoughReplicasException {\n+    int oldNumOfReplicas \u003d results.size();\n+    // randomly choose one node from remote racks\n+    try {\n+      chooseRandom(numOfReplicas, \"~\"+NetworkTopology.getFirstHalf(\n+          localMachine.getNetworkLocation()),\n+      excludedNodes, blocksize, maxReplicasPerRack, results);\n+    } catch (NotEnoughReplicasException e) {\n+      chooseRandom(numOfReplicas-(results.size()-oldNumOfReplicas),\n+      localMachine.getNetworkLocation(), excludedNodes, blocksize, \n+      maxReplicasPerRack, results);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void chooseRemoteRack(int numOfReplicas,\n          DatanodeDescriptor localMachine,\n          HashMap\u003cNode, Node\u003e excludedNodes,\n          long blocksize,\n          int maxReplicasPerRack,\n          List\u003cDatanodeDescriptor\u003e results)\n          throws NotEnoughReplicasException {\n    int oldNumOfReplicas \u003d results.size();\n    // randomly choose one node from remote racks\n    try {\n      chooseRandom(numOfReplicas, \"~\"+NetworkTopology.getFirstHalf(\n          localMachine.getNetworkLocation()),\n      excludedNodes, blocksize, maxReplicasPerRack, results);\n    } catch (NotEnoughReplicasException e) {\n      chooseRandom(numOfReplicas-(results.size()-oldNumOfReplicas),\n      localMachine.getNetworkLocation(), excludedNodes, blocksize, \n      maxReplicasPerRack, results);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java"
    }
  }
}