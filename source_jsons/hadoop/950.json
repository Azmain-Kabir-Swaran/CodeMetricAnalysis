{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSStripedOutputStream.java",
  "functionName": "allocateNewBlock",
  "functionId": "allocateNewBlock",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
  "functionStartLine": 484,
  "functionEndLine": 539,
  "numCommitsSeen": 38,
  "timeTaken": 2661,
  "changeHistory": [
    "f940ab242da80a22bae95509d5c282d7e2f7ecdb",
    "0ab7fc92009fec2f0ab341f3d878e1b8864b8ea9",
    "fbe06b58805aac4861fb27dfa273914b69e8bdc6",
    "de9994bd893af70fffdd68af6252fc45020e0e69",
    "3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd",
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
    "5104077e1f431ad3675d0b1c5c3cf53936902d8e",
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd"
  ],
  "changeHistoryShort": {
    "f940ab242da80a22bae95509d5c282d7e2f7ecdb": "Ybodychange",
    "0ab7fc92009fec2f0ab341f3d878e1b8864b8ea9": "Ybodychange",
    "fbe06b58805aac4861fb27dfa273914b69e8bdc6": "Ybodychange",
    "de9994bd893af70fffdd68af6252fc45020e0e69": "Ybodychange",
    "3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd": "Ybodychange",
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7": "Ybodychange",
    "5104077e1f431ad3675d0b1c5c3cf53936902d8e": "Ybodychange",
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd": "Ybodychange"
  },
  "changeHistoryDetails": {
    "f940ab242da80a22bae95509d5c282d7e2f7ecdb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7663. Erasure Coding: Append on striped file. Contributed by Ayush Saxena.\n",
      "commitDate": "05/03/19 5:56 AM",
      "commitName": "f940ab242da80a22bae95509d5c282d7e2f7ecdb",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "31/01/19 4:07 PM",
      "commitNameOld": "0ab7fc92009fec2f0ab341f3d878e1b8864b8ea9",
      "commitAuthorOld": "Kitti Nanasi",
      "daysBetweenCommits": 32.58,
      "commitsBetweenForRepo": 268,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,56 @@\n   private void allocateNewBlock() throws IOException {\n     if (currentBlockGroup !\u003d null) {\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         // sync all the healthy streamers before writing to the new block\n         waitEndBlocks(i);\n       }\n     }\n     failedStreamers.clear();\n     DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n     LOG.debug(\"Excluding DataNodes when allocating new block: \"\n         + Arrays.asList(excludedNodes));\n \n     // replace failed streamers\n+    ExtendedBlock prevBlockGroup \u003d currentBlockGroup;\n+    if (prevBlockGroup4Append !\u003d null) {\n+      prevBlockGroup \u003d prevBlockGroup4Append;\n+      prevBlockGroup4Append \u003d null;\n+    }\n     replaceFailedStreamers();\n \n     LOG.debug(\"Allocating new block group. The previous block group: \"\n-        + currentBlockGroup);\n+        + prevBlockGroup);\n     final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n-        currentBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n+         prevBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n     assert lb.isStriped();\n     // assign the new block to the current block group\n     currentBlockGroup \u003d lb.getBlock();\n     blockGroupIndex++;\n \n     final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n         (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n         numAllBlocks - numDataBlocks);\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       StripedDataStreamer si \u003d getStripedDataStreamer(i);\n       assert si.isHealthy();\n       if (blocks[i] \u003d\u003d null) {\n         // allocBlock() should guarantee that all data blocks are successfully\n         // allocated.\n         assert i \u003e\u003d numDataBlocks;\n         // Set exception and close streamer as there is no block locations\n         // found for the parity block.\n         LOG.warn(\"Cannot allocate parity block(index\u003d{}, policy\u003d{}). \" +\n                 \"Exclude nodes\u003d{}. There may not be enough datanodes or \" +\n                 \"racks. You can check if the cluster topology supports \" +\n                 \"the enabled erasure coding policies by running the command \" +\n                 \"\u0027hdfs ec -verifyClusterSetup\u0027.\", i,  ecPolicy.getName(),\n             excludedNodes);\n         si.getLastException().set(\n             new IOException(\"Failed to get parity block, index\u003d\" + i));\n         si.getErrorState().setInternalError();\n         si.close(true);\n       } else {\n         coordinator.getFollowingBlocks().offer(i, blocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateNewBlock() throws IOException {\n    if (currentBlockGroup !\u003d null) {\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        // sync all the healthy streamers before writing to the new block\n        waitEndBlocks(i);\n      }\n    }\n    failedStreamers.clear();\n    DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n    LOG.debug(\"Excluding DataNodes when allocating new block: \"\n        + Arrays.asList(excludedNodes));\n\n    // replace failed streamers\n    ExtendedBlock prevBlockGroup \u003d currentBlockGroup;\n    if (prevBlockGroup4Append !\u003d null) {\n      prevBlockGroup \u003d prevBlockGroup4Append;\n      prevBlockGroup4Append \u003d null;\n    }\n    replaceFailedStreamers();\n\n    LOG.debug(\"Allocating new block group. The previous block group: \"\n        + prevBlockGroup);\n    final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n         prevBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n    assert lb.isStriped();\n    // assign the new block to the current block group\n    currentBlockGroup \u003d lb.getBlock();\n    blockGroupIndex++;\n\n    final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n        (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n        numAllBlocks - numDataBlocks);\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      StripedDataStreamer si \u003d getStripedDataStreamer(i);\n      assert si.isHealthy();\n      if (blocks[i] \u003d\u003d null) {\n        // allocBlock() should guarantee that all data blocks are successfully\n        // allocated.\n        assert i \u003e\u003d numDataBlocks;\n        // Set exception and close streamer as there is no block locations\n        // found for the parity block.\n        LOG.warn(\"Cannot allocate parity block(index\u003d{}, policy\u003d{}). \" +\n                \"Exclude nodes\u003d{}. There may not be enough datanodes or \" +\n                \"racks. You can check if the cluster topology supports \" +\n                \"the enabled erasure coding policies by running the command \" +\n                \"\u0027hdfs ec -verifyClusterSetup\u0027.\", i,  ecPolicy.getName(),\n            excludedNodes);\n        si.getLastException().set(\n            new IOException(\"Failed to get parity block, index\u003d\" + i));\n        si.getErrorState().setInternalError();\n        si.close(true);\n      } else {\n        coordinator.getFollowingBlocks().offer(i, blocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "0ab7fc92009fec2f0ab341f3d878e1b8864b8ea9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14187. Make warning message more clear when there are not enough data nodes for EC write. Contributed by Kitti Nanasi.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "31/01/19 4:07 PM",
      "commitName": "0ab7fc92009fec2f0ab341f3d878e1b8864b8ea9",
      "commitAuthor": "Kitti Nanasi",
      "commitDateOld": "29/10/18 7:06 PM",
      "commitNameOld": "db7e636824a36b90ba1c8e9b2fba1162771700fe",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 93.92,
      "commitsBetweenForRepo": 624,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,51 @@\n   private void allocateNewBlock() throws IOException {\n     if (currentBlockGroup !\u003d null) {\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         // sync all the healthy streamers before writing to the new block\n         waitEndBlocks(i);\n       }\n     }\n     failedStreamers.clear();\n     DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n     LOG.debug(\"Excluding DataNodes when allocating new block: \"\n         + Arrays.asList(excludedNodes));\n \n     // replace failed streamers\n     replaceFailedStreamers();\n \n     LOG.debug(\"Allocating new block group. The previous block group: \"\n         + currentBlockGroup);\n     final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n         currentBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n     assert lb.isStriped();\n     // assign the new block to the current block group\n     currentBlockGroup \u003d lb.getBlock();\n     blockGroupIndex++;\n \n     final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n         (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n         numAllBlocks - numDataBlocks);\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       StripedDataStreamer si \u003d getStripedDataStreamer(i);\n       assert si.isHealthy();\n       if (blocks[i] \u003d\u003d null) {\n         // allocBlock() should guarantee that all data blocks are successfully\n         // allocated.\n         assert i \u003e\u003d numDataBlocks;\n         // Set exception and close streamer as there is no block locations\n         // found for the parity block.\n         LOG.warn(\"Cannot allocate parity block(index\u003d{}, policy\u003d{}). \" +\n-            \"Not enough datanodes? Exclude nodes\u003d{}\", i,  ecPolicy.getName(),\n+                \"Exclude nodes\u003d{}. There may not be enough datanodes or \" +\n+                \"racks. You can check if the cluster topology supports \" +\n+                \"the enabled erasure coding policies by running the command \" +\n+                \"\u0027hdfs ec -verifyClusterSetup\u0027.\", i,  ecPolicy.getName(),\n             excludedNodes);\n         si.getLastException().set(\n             new IOException(\"Failed to get parity block, index\u003d\" + i));\n         si.getErrorState().setInternalError();\n         si.close(true);\n       } else {\n         coordinator.getFollowingBlocks().offer(i, blocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateNewBlock() throws IOException {\n    if (currentBlockGroup !\u003d null) {\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        // sync all the healthy streamers before writing to the new block\n        waitEndBlocks(i);\n      }\n    }\n    failedStreamers.clear();\n    DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n    LOG.debug(\"Excluding DataNodes when allocating new block: \"\n        + Arrays.asList(excludedNodes));\n\n    // replace failed streamers\n    replaceFailedStreamers();\n\n    LOG.debug(\"Allocating new block group. The previous block group: \"\n        + currentBlockGroup);\n    final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n        currentBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n    assert lb.isStriped();\n    // assign the new block to the current block group\n    currentBlockGroup \u003d lb.getBlock();\n    blockGroupIndex++;\n\n    final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n        (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n        numAllBlocks - numDataBlocks);\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      StripedDataStreamer si \u003d getStripedDataStreamer(i);\n      assert si.isHealthy();\n      if (blocks[i] \u003d\u003d null) {\n        // allocBlock() should guarantee that all data blocks are successfully\n        // allocated.\n        assert i \u003e\u003d numDataBlocks;\n        // Set exception and close streamer as there is no block locations\n        // found for the parity block.\n        LOG.warn(\"Cannot allocate parity block(index\u003d{}, policy\u003d{}). \" +\n                \"Exclude nodes\u003d{}. There may not be enough datanodes or \" +\n                \"racks. You can check if the cluster topology supports \" +\n                \"the enabled erasure coding policies by running the command \" +\n                \"\u0027hdfs ec -verifyClusterSetup\u0027.\", i,  ecPolicy.getName(),\n            excludedNodes);\n        si.getLastException().set(\n            new IOException(\"Failed to get parity block, index\u003d\" + i));\n        si.getErrorState().setInternalError();\n        si.close(true);\n      } else {\n        coordinator.getFollowingBlocks().offer(i, blocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "fbe06b58805aac4861fb27dfa273914b69e8bdc6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12349. Improve log message when it could not alloc enough blocks for EC. (Lei (Eddy) Xu)\n",
      "commitDate": "15/09/17 12:12 PM",
      "commitName": "fbe06b58805aac4861fb27dfa273914b69e8bdc6",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "11/09/17 10:06 AM",
      "commitNameOld": "de9994bd893af70fffdd68af6252fc45020e0e69",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 4.09,
      "commitsBetweenForRepo": 94,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,48 @@\n   private void allocateNewBlock() throws IOException {\n     if (currentBlockGroup !\u003d null) {\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         // sync all the healthy streamers before writing to the new block\n         waitEndBlocks(i);\n       }\n     }\n     failedStreamers.clear();\n     DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n     LOG.debug(\"Excluding DataNodes when allocating new block: \"\n         + Arrays.asList(excludedNodes));\n \n     // replace failed streamers\n     replaceFailedStreamers();\n \n     LOG.debug(\"Allocating new block group. The previous block group: \"\n         + currentBlockGroup);\n     final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n         currentBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n     assert lb.isStriped();\n-    if (lb.getLocations().length \u003c numDataBlocks) {\n-      throw new IOException(\"Failed to get \" + numDataBlocks\n-          + \" nodes from namenode: blockGroupSize\u003d \" + numAllBlocks\n-          + \", blocks.length\u003d \" + lb.getLocations().length);\n-    }\n     // assign the new block to the current block group\n     currentBlockGroup \u003d lb.getBlock();\n     blockGroupIndex++;\n \n     final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n         (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n         numAllBlocks - numDataBlocks);\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       StripedDataStreamer si \u003d getStripedDataStreamer(i);\n       assert si.isHealthy();\n       if (blocks[i] \u003d\u003d null) {\n+        // allocBlock() should guarantee that all data blocks are successfully\n+        // allocated.\n+        assert i \u003e\u003d numDataBlocks;\n         // Set exception and close streamer as there is no block locations\n         // found for the parity block.\n-        LOG.warn(\"Failed to get block location for parity block, index\u003d\" + i);\n+        LOG.warn(\"Cannot allocate parity block(index\u003d{}, policy\u003d{}). \" +\n+            \"Not enough datanodes? Exclude nodes\u003d{}\", i,  ecPolicy.getName(),\n+            excludedNodes);\n         si.getLastException().set(\n-            new IOException(\"Failed to get following block, i\u003d\" + i));\n+            new IOException(\"Failed to get parity block, index\u003d\" + i));\n         si.getErrorState().setInternalError();\n         si.close(true);\n       } else {\n         coordinator.getFollowingBlocks().offer(i, blocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateNewBlock() throws IOException {\n    if (currentBlockGroup !\u003d null) {\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        // sync all the healthy streamers before writing to the new block\n        waitEndBlocks(i);\n      }\n    }\n    failedStreamers.clear();\n    DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n    LOG.debug(\"Excluding DataNodes when allocating new block: \"\n        + Arrays.asList(excludedNodes));\n\n    // replace failed streamers\n    replaceFailedStreamers();\n\n    LOG.debug(\"Allocating new block group. The previous block group: \"\n        + currentBlockGroup);\n    final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n        currentBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n    assert lb.isStriped();\n    // assign the new block to the current block group\n    currentBlockGroup \u003d lb.getBlock();\n    blockGroupIndex++;\n\n    final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n        (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n        numAllBlocks - numDataBlocks);\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      StripedDataStreamer si \u003d getStripedDataStreamer(i);\n      assert si.isHealthy();\n      if (blocks[i] \u003d\u003d null) {\n        // allocBlock() should guarantee that all data blocks are successfully\n        // allocated.\n        assert i \u003e\u003d numDataBlocks;\n        // Set exception and close streamer as there is no block locations\n        // found for the parity block.\n        LOG.warn(\"Cannot allocate parity block(index\u003d{}, policy\u003d{}). \" +\n            \"Not enough datanodes? Exclude nodes\u003d{}\", i,  ecPolicy.getName(),\n            excludedNodes);\n        si.getLastException().set(\n            new IOException(\"Failed to get parity block, index\u003d\" + i));\n        si.getErrorState().setInternalError();\n        si.close(true);\n      } else {\n        coordinator.getFollowingBlocks().offer(i, blocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "de9994bd893af70fffdd68af6252fc45020e0e69": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-12349. Improve log message when it could not alloc enough blocks for EC. (lei)\"\n\nThis reverts commit 3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd.\n",
      "commitDate": "11/09/17 10:06 AM",
      "commitName": "de9994bd893af70fffdd68af6252fc45020e0e69",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "07/09/17 6:01 PM",
      "commitNameOld": "3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 3.67,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,48 @@\n   private void allocateNewBlock() throws IOException {\n     if (currentBlockGroup !\u003d null) {\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         // sync all the healthy streamers before writing to the new block\n         waitEndBlocks(i);\n       }\n     }\n     failedStreamers.clear();\n     DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n     LOG.debug(\"Excluding DataNodes when allocating new block: \"\n         + Arrays.asList(excludedNodes));\n \n     // replace failed streamers\n     replaceFailedStreamers();\n \n     LOG.debug(\"Allocating new block group. The previous block group: \"\n         + currentBlockGroup);\n     final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n         currentBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n     assert lb.isStriped();\n+    if (lb.getLocations().length \u003c numDataBlocks) {\n+      throw new IOException(\"Failed to get \" + numDataBlocks\n+          + \" nodes from namenode: blockGroupSize\u003d \" + numAllBlocks\n+          + \", blocks.length\u003d \" + lb.getLocations().length);\n+    }\n     // assign the new block to the current block group\n     currentBlockGroup \u003d lb.getBlock();\n     blockGroupIndex++;\n \n     final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n         (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n         numAllBlocks - numDataBlocks);\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       StripedDataStreamer si \u003d getStripedDataStreamer(i);\n       assert si.isHealthy();\n       if (blocks[i] \u003d\u003d null) {\n-        // allocBlock() should guarantee that all data blocks are successfully\n-        // allocated.\n-        assert i \u003e\u003d numDataBlocks;\n         // Set exception and close streamer as there is no block locations\n         // found for the parity block.\n-        LOG.warn(\"Cannot allocate parity block(index\u003d{}, policy\u003d{}). \" +\n-            \"Not enough datanodes? Excluded nodes\u003d{}\", i,  ecPolicy.getName(),\n-            excludedNodes);\n+        LOG.warn(\"Failed to get block location for parity block, index\u003d\" + i);\n         si.getLastException().set(\n-            new IOException(\"Failed to get parity block, index\u003d\" + i));\n+            new IOException(\"Failed to get following block, i\u003d\" + i));\n         si.getErrorState().setInternalError();\n         si.close(true);\n       } else {\n         coordinator.getFollowingBlocks().offer(i, blocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateNewBlock() throws IOException {\n    if (currentBlockGroup !\u003d null) {\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        // sync all the healthy streamers before writing to the new block\n        waitEndBlocks(i);\n      }\n    }\n    failedStreamers.clear();\n    DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n    LOG.debug(\"Excluding DataNodes when allocating new block: \"\n        + Arrays.asList(excludedNodes));\n\n    // replace failed streamers\n    replaceFailedStreamers();\n\n    LOG.debug(\"Allocating new block group. The previous block group: \"\n        + currentBlockGroup);\n    final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n        currentBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n    assert lb.isStriped();\n    if (lb.getLocations().length \u003c numDataBlocks) {\n      throw new IOException(\"Failed to get \" + numDataBlocks\n          + \" nodes from namenode: blockGroupSize\u003d \" + numAllBlocks\n          + \", blocks.length\u003d \" + lb.getLocations().length);\n    }\n    // assign the new block to the current block group\n    currentBlockGroup \u003d lb.getBlock();\n    blockGroupIndex++;\n\n    final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n        (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n        numAllBlocks - numDataBlocks);\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      StripedDataStreamer si \u003d getStripedDataStreamer(i);\n      assert si.isHealthy();\n      if (blocks[i] \u003d\u003d null) {\n        // Set exception and close streamer as there is no block locations\n        // found for the parity block.\n        LOG.warn(\"Failed to get block location for parity block, index\u003d\" + i);\n        si.getLastException().set(\n            new IOException(\"Failed to get following block, i\u003d\" + i));\n        si.getErrorState().setInternalError();\n        si.close(true);\n      } else {\n        coordinator.getFollowingBlocks().offer(i, blocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12349. Improve log message when it could not alloc enough blocks for EC. (lei)\n",
      "commitDate": "07/09/17 6:01 PM",
      "commitName": "3e6d0ca2b2f79bfa87faa7bbd46d814a48334fbd",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "05/09/17 10:29 PM",
      "commitNameOld": "d7f27043ce034d69003b054dfe4f9e8e317ec543",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 1.81,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,48 @@\n   private void allocateNewBlock() throws IOException {\n     if (currentBlockGroup !\u003d null) {\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         // sync all the healthy streamers before writing to the new block\n         waitEndBlocks(i);\n       }\n     }\n     failedStreamers.clear();\n     DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n     LOG.debug(\"Excluding DataNodes when allocating new block: \"\n         + Arrays.asList(excludedNodes));\n \n     // replace failed streamers\n     replaceFailedStreamers();\n \n     LOG.debug(\"Allocating new block group. The previous block group: \"\n         + currentBlockGroup);\n     final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n         currentBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n     assert lb.isStriped();\n-    if (lb.getLocations().length \u003c numDataBlocks) {\n-      throw new IOException(\"Failed to get \" + numDataBlocks\n-          + \" nodes from namenode: blockGroupSize\u003d \" + numAllBlocks\n-          + \", blocks.length\u003d \" + lb.getLocations().length);\n-    }\n     // assign the new block to the current block group\n     currentBlockGroup \u003d lb.getBlock();\n     blockGroupIndex++;\n \n     final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n         (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n         numAllBlocks - numDataBlocks);\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       StripedDataStreamer si \u003d getStripedDataStreamer(i);\n       assert si.isHealthy();\n       if (blocks[i] \u003d\u003d null) {\n+        // allocBlock() should guarantee that all data blocks are successfully\n+        // allocated.\n+        assert i \u003e\u003d numDataBlocks;\n         // Set exception and close streamer as there is no block locations\n         // found for the parity block.\n-        LOG.warn(\"Failed to get block location for parity block, index\u003d\" + i);\n+        LOG.warn(\"Cannot allocate parity block(index\u003d{}, policy\u003d{}). \" +\n+            \"Not enough datanodes? Excluded nodes\u003d{}\", i,  ecPolicy.getName(),\n+            excludedNodes);\n         si.getLastException().set(\n-            new IOException(\"Failed to get following block, i\u003d\" + i));\n+            new IOException(\"Failed to get parity block, index\u003d\" + i));\n         si.getErrorState().setInternalError();\n         si.close(true);\n       } else {\n         coordinator.getFollowingBlocks().offer(i, blocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateNewBlock() throws IOException {\n    if (currentBlockGroup !\u003d null) {\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        // sync all the healthy streamers before writing to the new block\n        waitEndBlocks(i);\n      }\n    }\n    failedStreamers.clear();\n    DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n    LOG.debug(\"Excluding DataNodes when allocating new block: \"\n        + Arrays.asList(excludedNodes));\n\n    // replace failed streamers\n    replaceFailedStreamers();\n\n    LOG.debug(\"Allocating new block group. The previous block group: \"\n        + currentBlockGroup);\n    final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n        currentBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n    assert lb.isStriped();\n    // assign the new block to the current block group\n    currentBlockGroup \u003d lb.getBlock();\n    blockGroupIndex++;\n\n    final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n        (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n        numAllBlocks - numDataBlocks);\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      StripedDataStreamer si \u003d getStripedDataStreamer(i);\n      assert si.isHealthy();\n      if (blocks[i] \u003d\u003d null) {\n        // allocBlock() should guarantee that all data blocks are successfully\n        // allocated.\n        assert i \u003e\u003d numDataBlocks;\n        // Set exception and close streamer as there is no block locations\n        // found for the parity block.\n        LOG.warn(\"Cannot allocate parity block(index\u003d{}, policy\u003d{}). \" +\n            \"Not enough datanodes? Excluded nodes\u003d{}\", i,  ecPolicy.getName(),\n            excludedNodes);\n        si.getLastException().set(\n            new IOException(\"Failed to get parity block, index\u003d\" + i));\n        si.getErrorState().setInternalError();\n        si.close(true);\n      } else {\n        coordinator.getFollowingBlocks().offer(i, blocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3702. Add an option for NOT writing the blocks locally if there is a datanode on the same box as the client. (Contributed by Lei (Eddy) Xu)\n",
      "commitDate": "27/04/16 2:22 PM",
      "commitName": "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "20/04/16 12:47 AM",
      "commitNameOld": "b5d4c7dc76ddb3e0af95d792c2cbc0f99353a42a",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 7.57,
      "commitsBetweenForRepo": 55,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,48 @@\n   private void allocateNewBlock() throws IOException {\n     if (currentBlockGroup !\u003d null) {\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         // sync all the healthy streamers before writing to the new block\n         waitEndBlocks(i);\n       }\n     }\n     failedStreamers.clear();\n     DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n     LOG.debug(\"Excluding DataNodes when allocating new block: \"\n         + Arrays.asList(excludedNodes));\n \n     // replace failed streamers\n     replaceFailedStreamers();\n \n     LOG.debug(\"Allocating new block group. The previous block group: \"\n         + currentBlockGroup);\n     final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n-        currentBlockGroup, fileId, favoredNodes);\n+        currentBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n     assert lb.isStriped();\n     if (lb.getLocations().length \u003c numDataBlocks) {\n       throw new IOException(\"Failed to get \" + numDataBlocks\n           + \" nodes from namenode: blockGroupSize\u003d \" + numAllBlocks\n           + \", blocks.length\u003d \" + lb.getLocations().length);\n     }\n     // assign the new block to the current block group\n     currentBlockGroup \u003d lb.getBlock();\n     blockGroupIndex++;\n \n     final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n         (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n         numAllBlocks - numDataBlocks);\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       StripedDataStreamer si \u003d getStripedDataStreamer(i);\n       assert si.isHealthy();\n       if (blocks[i] \u003d\u003d null) {\n         // Set exception and close streamer as there is no block locations\n         // found for the parity block.\n         LOG.warn(\"Failed to get block location for parity block, index\u003d\" + i);\n         si.getLastException().set(\n             new IOException(\"Failed to get following block, i\u003d\" + i));\n         si.getErrorState().setInternalError();\n         si.close(true);\n       } else {\n         coordinator.getFollowingBlocks().offer(i, blocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateNewBlock() throws IOException {\n    if (currentBlockGroup !\u003d null) {\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        // sync all the healthy streamers before writing to the new block\n        waitEndBlocks(i);\n      }\n    }\n    failedStreamers.clear();\n    DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n    LOG.debug(\"Excluding DataNodes when allocating new block: \"\n        + Arrays.asList(excludedNodes));\n\n    // replace failed streamers\n    replaceFailedStreamers();\n\n    LOG.debug(\"Allocating new block group. The previous block group: \"\n        + currentBlockGroup);\n    final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n        currentBlockGroup, fileId, favoredNodes, getAddBlockFlags());\n    assert lb.isStriped();\n    if (lb.getLocations().length \u003c numDataBlocks) {\n      throw new IOException(\"Failed to get \" + numDataBlocks\n          + \" nodes from namenode: blockGroupSize\u003d \" + numAllBlocks\n          + \", blocks.length\u003d \" + lb.getLocations().length);\n    }\n    // assign the new block to the current block group\n    currentBlockGroup \u003d lb.getBlock();\n    blockGroupIndex++;\n\n    final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n        (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n        numAllBlocks - numDataBlocks);\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      StripedDataStreamer si \u003d getStripedDataStreamer(i);\n      assert si.isHealthy();\n      if (blocks[i] \u003d\u003d null) {\n        // Set exception and close streamer as there is no block locations\n        // found for the parity block.\n        LOG.warn(\"Failed to get block location for parity block, index\u003d\" + i);\n        si.getLastException().set(\n            new IOException(\"Failed to get following block, i\u003d\" + i));\n        si.getErrorState().setInternalError();\n        si.close(true);\n      } else {\n        coordinator.getFollowingBlocks().offer(i, blocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "5104077e1f431ad3675d0b1c5c3cf53936902d8e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9373. Erasure coding: friendly log information for write operations with some failed streamers. Contributed by Li Bo.\n\nChange-Id: Ie8ab4ae00e9ee0eb03c32a54bea26a3524308038\n",
      "commitDate": "17/12/15 1:05 PM",
      "commitName": "5104077e1f431ad3675d0b1c5c3cf53936902d8e",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "02/12/15 5:39 PM",
      "commitNameOld": "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 14.81,
      "commitsBetweenForRepo": 93,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,48 @@\n   private void allocateNewBlock() throws IOException {\n     if (currentBlockGroup !\u003d null) {\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         // sync all the healthy streamers before writing to the new block\n         waitEndBlocks(i);\n       }\n     }\n     failedStreamers.clear();\n     DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n     LOG.debug(\"Excluding DataNodes when allocating new block: \"\n         + Arrays.asList(excludedNodes));\n \n     // replace failed streamers\n     replaceFailedStreamers();\n \n     LOG.debug(\"Allocating new block group. The previous block group: \"\n         + currentBlockGroup);\n     final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n         currentBlockGroup, fileId, favoredNodes);\n     assert lb.isStriped();\n     if (lb.getLocations().length \u003c numDataBlocks) {\n       throw new IOException(\"Failed to get \" + numDataBlocks\n           + \" nodes from namenode: blockGroupSize\u003d \" + numAllBlocks\n           + \", blocks.length\u003d \" + lb.getLocations().length);\n     }\n     // assign the new block to the current block group\n     currentBlockGroup \u003d lb.getBlock();\n+    blockGroupIndex++;\n \n     final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n         (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n         numAllBlocks - numDataBlocks);\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       StripedDataStreamer si \u003d getStripedDataStreamer(i);\n       assert si.isHealthy();\n       if (blocks[i] \u003d\u003d null) {\n         // Set exception and close streamer as there is no block locations\n         // found for the parity block.\n         LOG.warn(\"Failed to get block location for parity block, index\u003d\" + i);\n         si.getLastException().set(\n             new IOException(\"Failed to get following block, i\u003d\" + i));\n         si.getErrorState().setInternalError();\n         si.close(true);\n       } else {\n         coordinator.getFollowingBlocks().offer(i, blocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateNewBlock() throws IOException {\n    if (currentBlockGroup !\u003d null) {\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        // sync all the healthy streamers before writing to the new block\n        waitEndBlocks(i);\n      }\n    }\n    failedStreamers.clear();\n    DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n    LOG.debug(\"Excluding DataNodes when allocating new block: \"\n        + Arrays.asList(excludedNodes));\n\n    // replace failed streamers\n    replaceFailedStreamers();\n\n    LOG.debug(\"Allocating new block group. The previous block group: \"\n        + currentBlockGroup);\n    final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n        currentBlockGroup, fileId, favoredNodes);\n    assert lb.isStriped();\n    if (lb.getLocations().length \u003c numDataBlocks) {\n      throw new IOException(\"Failed to get \" + numDataBlocks\n          + \" nodes from namenode: blockGroupSize\u003d \" + numAllBlocks\n          + \", blocks.length\u003d \" + lb.getLocations().length);\n    }\n    // assign the new block to the current block group\n    currentBlockGroup \u003d lb.getBlock();\n    blockGroupIndex++;\n\n    final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n        (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n        numAllBlocks - numDataBlocks);\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      StripedDataStreamer si \u003d getStripedDataStreamer(i);\n      assert si.isHealthy();\n      if (blocks[i] \u003d\u003d null) {\n        // Set exception and close streamer as there is no block locations\n        // found for the parity block.\n        LOG.warn(\"Failed to get block location for parity block, index\u003d\" + i);\n        si.getLastException().set(\n            new IOException(\"Failed to get following block, i\u003d\" + i));\n        si.getErrorState().setInternalError();\n        si.close(true);\n      } else {\n        coordinator.getFollowingBlocks().offer(i, blocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    },
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9180. Update excluded DataNodes in DFSStripedOutputStream based on failures in data streamers. Contributed by Jing Zhao.\n",
      "commitDate": "06/10/15 10:56 AM",
      "commitName": "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 2.97,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,47 @@\n   private void allocateNewBlock() throws IOException {\n     if (currentBlockGroup !\u003d null) {\n       for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n         // sync all the healthy streamers before writing to the new block\n         waitEndBlocks(i);\n       }\n     }\n     failedStreamers.clear();\n+    DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n+    LOG.debug(\"Excluding DataNodes when allocating new block: \"\n+        + Arrays.asList(excludedNodes));\n+\n     // replace failed streamers\n     replaceFailedStreamers();\n \n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Allocating new block group. The previous block group: \"\n-          + currentBlockGroup);\n-    }\n-\n-    // TODO collect excludedNodes from all the data streamers\n-    final LocatedBlock lb \u003d addBlock(null, dfsClient, src, currentBlockGroup,\n-        fileId, favoredNodes);\n+    LOG.debug(\"Allocating new block group. The previous block group: \"\n+        + currentBlockGroup);\n+    final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n+        currentBlockGroup, fileId, favoredNodes);\n     assert lb.isStriped();\n     if (lb.getLocations().length \u003c numDataBlocks) {\n       throw new IOException(\"Failed to get \" + numDataBlocks\n           + \" nodes from namenode: blockGroupSize\u003d \" + numAllBlocks\n           + \", blocks.length\u003d \" + lb.getLocations().length);\n     }\n     // assign the new block to the current block group\n     currentBlockGroup \u003d lb.getBlock();\n \n     final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n         (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n         numAllBlocks - numDataBlocks);\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       StripedDataStreamer si \u003d getStripedDataStreamer(i);\n-      if (si.isHealthy()) { // skipping failed data streamer\n-        if (blocks[i] \u003d\u003d null) {\n-          // Set exception and close streamer as there is no block locations\n-          // found for the parity block.\n-          LOG.warn(\"Failed to get block location for parity block, index\u003d\" + i);\n-          si.getLastException().set(\n-              new IOException(\"Failed to get following block, i\u003d\" + i));\n-          si.getErrorState().setInternalError();\n-          si.close(true);\n-        } else {\n-          coordinator.getFollowingBlocks().offer(i, blocks[i]);\n-        }\n+      assert si.isHealthy();\n+      if (blocks[i] \u003d\u003d null) {\n+        // Set exception and close streamer as there is no block locations\n+        // found for the parity block.\n+        LOG.warn(\"Failed to get block location for parity block, index\u003d\" + i);\n+        si.getLastException().set(\n+            new IOException(\"Failed to get following block, i\u003d\" + i));\n+        si.getErrorState().setInternalError();\n+        si.close(true);\n+      } else {\n+        coordinator.getFollowingBlocks().offer(i, blocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateNewBlock() throws IOException {\n    if (currentBlockGroup !\u003d null) {\n      for (int i \u003d 0; i \u003c numAllBlocks; i++) {\n        // sync all the healthy streamers before writing to the new block\n        waitEndBlocks(i);\n      }\n    }\n    failedStreamers.clear();\n    DatanodeInfo[] excludedNodes \u003d getExcludedNodes();\n    LOG.debug(\"Excluding DataNodes when allocating new block: \"\n        + Arrays.asList(excludedNodes));\n\n    // replace failed streamers\n    replaceFailedStreamers();\n\n    LOG.debug(\"Allocating new block group. The previous block group: \"\n        + currentBlockGroup);\n    final LocatedBlock lb \u003d addBlock(excludedNodes, dfsClient, src,\n        currentBlockGroup, fileId, favoredNodes);\n    assert lb.isStriped();\n    if (lb.getLocations().length \u003c numDataBlocks) {\n      throw new IOException(\"Failed to get \" + numDataBlocks\n          + \" nodes from namenode: blockGroupSize\u003d \" + numAllBlocks\n          + \", blocks.length\u003d \" + lb.getLocations().length);\n    }\n    // assign the new block to the current block group\n    currentBlockGroup \u003d lb.getBlock();\n\n    final LocatedBlock[] blocks \u003d StripedBlockUtil.parseStripedBlockGroup(\n        (LocatedStripedBlock) lb, cellSize, numDataBlocks,\n        numAllBlocks - numDataBlocks);\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      StripedDataStreamer si \u003d getStripedDataStreamer(i);\n      assert si.isHealthy();\n      if (blocks[i] \u003d\u003d null) {\n        // Set exception and close streamer as there is no block locations\n        // found for the parity block.\n        LOG.warn(\"Failed to get block location for parity block, index\u003d\" + i);\n        si.getLastException().set(\n            new IOException(\"Failed to get following block, i\u003d\" + i));\n        si.getErrorState().setInternalError();\n        si.close(true);\n      } else {\n        coordinator.getFollowingBlocks().offer(i, blocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {}
    }
  }
}