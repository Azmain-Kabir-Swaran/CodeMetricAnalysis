{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "S3AFileSystem.java",
  "functionName": "stopAllServices",
  "functionId": "stopAllServices",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
  "functionStartLine": 3288,
  "functionEndLine": 3313,
  "numCommitsSeen": 141,
  "timeTaken": 2195,
  "changeHistory": [
    "49df83899543586bbcaf80f01399ade031cf68b0",
    "990063d2af0a37e9474949f33128805e34c3f016"
  ],
  "changeHistoryShort": {
    "49df83899543586bbcaf80f01399ade031cf68b0": "Ybodychange",
    "990063d2af0a37e9474949f33128805e34c3f016": "Yintroduced"
  },
  "changeHistoryDetails": {
    "49df83899543586bbcaf80f01399ade031cf68b0": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
      "commitDate": "10/01/20 3:11 AM",
      "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "07/01/20 3:17 AM",
      "commitNameOld": "2bbf73f1df96ebe7e6fcf64a724ae846230a8487",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 3.0,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   protected synchronized void stopAllServices() {\n     if (transfers !\u003d null) {\n       try {\n         transfers.shutdownNow(true);\n       } catch (RuntimeException e) {\n         // catch and swallow for resilience.\n         LOG.debug(\"When shutting down\", e);\n       }\n       transfers \u003d null;\n     }\n     HadoopExecutors.shutdown(boundedThreadPool, LOG,\n         THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\n     boundedThreadPool \u003d null;\n     HadoopExecutors.shutdown(unboundedThreadPool, LOG,\n         THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\n     unboundedThreadPool \u003d null;\n-    closeAutocloseables(LOG, credentials);\n     cleanupWithLogger(LOG,\n         metadataStore,\n         instrumentation,\n         delegationTokens.orElse(null),\n         signerManager);\n+    closeAutocloseables(LOG, credentials);\n     delegationTokens \u003d Optional.empty();\n     signerManager \u003d null;\n     credentials \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void stopAllServices() {\n    if (transfers !\u003d null) {\n      try {\n        transfers.shutdownNow(true);\n      } catch (RuntimeException e) {\n        // catch and swallow for resilience.\n        LOG.debug(\"When shutting down\", e);\n      }\n      transfers \u003d null;\n    }\n    HadoopExecutors.shutdown(boundedThreadPool, LOG,\n        THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\n    boundedThreadPool \u003d null;\n    HadoopExecutors.shutdown(unboundedThreadPool, LOG,\n        THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\n    unboundedThreadPool \u003d null;\n    cleanupWithLogger(LOG,\n        metadataStore,\n        instrumentation,\n        delegationTokens.orElse(null),\n        signerManager);\n    closeAutocloseables(LOG, credentials);\n    delegationTokens \u003d Optional.empty();\n    signerManager \u003d null;\n    credentials \u003d null;\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "990063d2af0a37e9474949f33128805e34c3f016": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-16665. Filesystems to be closed if they failed during initialize().\n\nContributed by Steve Loughran.\n\nThis FileSystem instantiation so if an IOException or RuntimeException is\nraised in the invocation of FileSystem.initialize() then a best-effort\nattempt is made to close the FS instance; exceptions raised that there\nare swallowed.\n\nThe S3AFileSystem is also modified to do its own cleanup if an\nIOException is raised during its initialize() process, it being the\nFS we know has the \"potential\" to leak threads, especially in\nextension points (e.g AWS Authenticators) which spawn threads.\n\nChange-Id: Ib84073a606c9d53bf53cbfca4629876a03894f04\n",
      "commitDate": "12/11/19 10:17 AM",
      "commitName": "990063d2af0a37e9474949f33128805e34c3f016",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,26 @@\n+  protected synchronized void stopAllServices() {\n+    if (transfers !\u003d null) {\n+      try {\n+        transfers.shutdownNow(true);\n+      } catch (RuntimeException e) {\n+        // catch and swallow for resilience.\n+        LOG.debug(\"When shutting down\", e);\n+      }\n+      transfers \u003d null;\n+    }\n+    HadoopExecutors.shutdown(boundedThreadPool, LOG,\n+        THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\n+    boundedThreadPool \u003d null;\n+    HadoopExecutors.shutdown(unboundedThreadPool, LOG,\n+        THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\n+    unboundedThreadPool \u003d null;\n+    closeAutocloseables(LOG, credentials);\n+    cleanupWithLogger(LOG,\n+        metadataStore,\n+        instrumentation,\n+        delegationTokens.orElse(null),\n+        signerManager);\n+    delegationTokens \u003d Optional.empty();\n+    signerManager \u003d null;\n+    credentials \u003d null;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void stopAllServices() {\n    if (transfers !\u003d null) {\n      try {\n        transfers.shutdownNow(true);\n      } catch (RuntimeException e) {\n        // catch and swallow for resilience.\n        LOG.debug(\"When shutting down\", e);\n      }\n      transfers \u003d null;\n    }\n    HadoopExecutors.shutdown(boundedThreadPool, LOG,\n        THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\n    boundedThreadPool \u003d null;\n    HadoopExecutors.shutdown(unboundedThreadPool, LOG,\n        THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\n    unboundedThreadPool \u003d null;\n    closeAutocloseables(LOG, credentials);\n    cleanupWithLogger(LOG,\n        metadataStore,\n        instrumentation,\n        delegationTokens.orElse(null),\n        signerManager);\n    delegationTokens \u003d Optional.empty();\n    signerManager \u003d null;\n    credentials \u003d null;\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java"
    }
  }
}