{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CacheManager.java",
  "functionName": "removeCachePool",
  "functionId": "removeCachePool___poolName-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
  "functionStartLine": 887,
  "functionEndLine": 911,
  "numCommitsSeen": 61,
  "timeTaken": 4434,
  "changeHistory": [
    "ca379e1c43fd733a34f3ece6172c96d74c890422",
    "d85c017d0488930d806f267141057fc73e68c728",
    "991c453ca3ac141a3f286f74af8401f83c38b230",
    "13edb391d06c479720202eb5ac81f1c71fe64748",
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
    "ce35e0950cef9250ce2ceffb3b8bfcff533c6b92",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
    "eb2175db1a99348c80457e3ffda172cc461de8bc",
    "a0d9a155a4a4258f628e927e096ecf6673f788ec",
    "e202d4d1548a0be2f5c61ff82be8b52bd0cfce04",
    "02e0e158a26f81ce8375426ba0ea56db09ee36be",
    "f41f8b8842c3f26d19f7fa928070c7c07f760e4c",
    "d56d0b46e1b82ae068083ddb99872d314684dc82",
    "97b7267977ef42201e5844df49bc37ec3d10ce16"
  ],
  "changeHistoryShort": {
    "ca379e1c43fd733a34f3ece6172c96d74c890422": "Ybodychange",
    "d85c017d0488930d806f267141057fc73e68c728": "Ybodychange",
    "991c453ca3ac141a3f286f74af8401f83c38b230": "Ybodychange",
    "13edb391d06c479720202eb5ac81f1c71fe64748": "Ybodychange",
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde": "Ybodychange",
    "ce35e0950cef9250ce2ceffb3b8bfcff533c6b92": "Ybodychange",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": "Ymultichange(Ymodifierchange,Ybodychange)",
    "eb2175db1a99348c80457e3ffda172cc461de8bc": "Ybodychange",
    "a0d9a155a4a4258f628e927e096ecf6673f788ec": "Ybodychange",
    "e202d4d1548a0be2f5c61ff82be8b52bd0cfce04": "Ybodychange",
    "02e0e158a26f81ce8375426ba0ea56db09ee36be": "Ybodychange",
    "f41f8b8842c3f26d19f7fa928070c7c07f760e4c": "Ymultichange(Yparameterchange,Ybodychange)",
    "d56d0b46e1b82ae068083ddb99872d314684dc82": "Ymultichange(Yparameterchange,Ybodychange)",
    "97b7267977ef42201e5844df49bc37ec3d10ce16": "Yintroduced"
  },
  "changeHistoryDetails": {
    "ca379e1c43fd733a34f3ece6172c96d74c890422": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13970. Use MultiMap for CacheManager Directives to simplify the code. Contributed by BELUGA BEHR.\n",
      "commitDate": "13/12/18 9:36 PM",
      "commitName": "ca379e1c43fd733a34f3ece6172c96d74c890422",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "07/09/18 2:59 PM",
      "commitNameOld": "335a8139f5b9004414b2942eeac5a008283a6f75",
      "commitAuthorOld": "Hrishikesh Gadre",
      "daysBetweenCommits": 97.32,
      "commitsBetweenForRepo": 849,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   public void removeCachePool(String poolName)\n       throws IOException {\n     assert namesystem.hasWriteLock();\n     try {\n       CachePoolInfo.validateName(poolName);\n       CachePool pool \u003d cachePools.remove(poolName);\n       if (pool \u003d\u003d null) {\n         throw new InvalidRequestException(\n             \"Cannot remove non-existent cache pool \" + poolName);\n       }\n       // Remove all directives in this pool.\n       Iterator\u003cCacheDirective\u003e iter \u003d pool.getDirectiveList().iterator();\n       while (iter.hasNext()) {\n         CacheDirective directive \u003d iter.next();\n-        directivesByPath.remove(directive.getPath());\n+        directivesByPath.removeAll(directive.getPath());\n         directivesById.remove(directive.getId());\n         iter.remove();\n       }\n       setNeedsRescan();\n     } catch (IOException e) {\n       LOG.info(\"removeCachePool of \" + poolName + \" failed: \", e);\n       throw e;\n     }\n     LOG.info(\"removeCachePool of \" + poolName + \" successful.\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeCachePool(String poolName)\n      throws IOException {\n    assert namesystem.hasWriteLock();\n    try {\n      CachePoolInfo.validateName(poolName);\n      CachePool pool \u003d cachePools.remove(poolName);\n      if (pool \u003d\u003d null) {\n        throw new InvalidRequestException(\n            \"Cannot remove non-existent cache pool \" + poolName);\n      }\n      // Remove all directives in this pool.\n      Iterator\u003cCacheDirective\u003e iter \u003d pool.getDirectiveList().iterator();\n      while (iter.hasNext()) {\n        CacheDirective directive \u003d iter.next();\n        directivesByPath.removeAll(directive.getPath());\n        directivesById.remove(directive.getId());\n        iter.remove();\n      }\n      setNeedsRescan();\n    } catch (IOException e) {\n      LOG.info(\"removeCachePool of \" + poolName + \" failed: \", e);\n      throw e;\n    }\n    LOG.info(\"removeCachePool of \" + poolName + \" successful.\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "d85c017d0488930d806f267141057fc73e68c728": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5651. Remove dfs.namenode.caching.enabled and improve CRM locking. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1555002 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/01/14 6:45 PM",
      "commitName": "d85c017d0488930d806f267141057fc73e68c728",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "31/12/13 4:01 PM",
      "commitNameOld": "07e4fb1455abc33584fc666ef745abe256ebd7d1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.11,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,25 @@\n   public void removeCachePool(String poolName)\n       throws IOException {\n     assert namesystem.hasWriteLock();\n     try {\n       CachePoolInfo.validateName(poolName);\n       CachePool pool \u003d cachePools.remove(poolName);\n       if (pool \u003d\u003d null) {\n         throw new InvalidRequestException(\n             \"Cannot remove non-existent cache pool \" + poolName);\n       }\n       // Remove all directives in this pool.\n       Iterator\u003cCacheDirective\u003e iter \u003d pool.getDirectiveList().iterator();\n       while (iter.hasNext()) {\n         CacheDirective directive \u003d iter.next();\n         directivesByPath.remove(directive.getPath());\n         directivesById.remove(directive.getId());\n         iter.remove();\n       }\n-      if (monitor !\u003d null) {\n-        monitor.setNeedsRescan();\n-      }\n+      setNeedsRescan();\n     } catch (IOException e) {\n       LOG.info(\"removeCachePool of \" + poolName + \" failed: \", e);\n       throw e;\n     }\n     LOG.info(\"removeCachePool of \" + poolName + \" successful.\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeCachePool(String poolName)\n      throws IOException {\n    assert namesystem.hasWriteLock();\n    try {\n      CachePoolInfo.validateName(poolName);\n      CachePool pool \u003d cachePools.remove(poolName);\n      if (pool \u003d\u003d null) {\n        throw new InvalidRequestException(\n            \"Cannot remove non-existent cache pool \" + poolName);\n      }\n      // Remove all directives in this pool.\n      Iterator\u003cCacheDirective\u003e iter \u003d pool.getDirectiveList().iterator();\n      while (iter.hasNext()) {\n        CacheDirective directive \u003d iter.next();\n        directivesByPath.remove(directive.getPath());\n        directivesById.remove(directive.getId());\n        iter.remove();\n      }\n      setNeedsRescan();\n    } catch (IOException e) {\n      LOG.info(\"removeCachePool of \" + poolName + \" failed: \", e);\n      throw e;\n    }\n    LOG.info(\"removeCachePool of \" + poolName + \" successful.\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "991c453ca3ac141a3f286f74af8401f83c38b230": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 10:47 AM",
      "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "05/12/13 1:09 PM",
      "commitNameOld": "55e5b0653c34a5f4146ce5a97a5b4a88a976d88a",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 11.9,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,27 @@\n   public void removeCachePool(String poolName)\n       throws IOException {\n     assert namesystem.hasWriteLock();\n-    CachePoolInfo.validateName(poolName);\n-    CachePool pool \u003d cachePools.remove(poolName);\n-    if (pool \u003d\u003d null) {\n-      throw new InvalidRequestException(\n-          \"Cannot remove non-existent cache pool \" + poolName);\n+    try {\n+      CachePoolInfo.validateName(poolName);\n+      CachePool pool \u003d cachePools.remove(poolName);\n+      if (pool \u003d\u003d null) {\n+        throw new InvalidRequestException(\n+            \"Cannot remove non-existent cache pool \" + poolName);\n+      }\n+      // Remove all directives in this pool.\n+      Iterator\u003cCacheDirective\u003e iter \u003d pool.getDirectiveList().iterator();\n+      while (iter.hasNext()) {\n+        CacheDirective directive \u003d iter.next();\n+        directivesByPath.remove(directive.getPath());\n+        directivesById.remove(directive.getId());\n+        iter.remove();\n+      }\n+      if (monitor !\u003d null) {\n+        monitor.setNeedsRescan();\n+      }\n+    } catch (IOException e) {\n+      LOG.info(\"removeCachePool of \" + poolName + \" failed: \", e);\n+      throw e;\n     }\n-    // Remove all directives in this pool.\n-    Iterator\u003cCacheDirective\u003e iter \u003d pool.getDirectiveList().iterator();\n-    while (iter.hasNext()) {\n-      CacheDirective directive \u003d iter.next();\n-      directivesByPath.remove(directive.getPath());\n-      directivesById.remove(directive.getId());\n-      iter.remove();\n-    }\n-    if (monitor !\u003d null) {\n-      monitor.kick();\n-    }\n+    LOG.info(\"removeCachePool of \" + poolName + \" successful.\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeCachePool(String poolName)\n      throws IOException {\n    assert namesystem.hasWriteLock();\n    try {\n      CachePoolInfo.validateName(poolName);\n      CachePool pool \u003d cachePools.remove(poolName);\n      if (pool \u003d\u003d null) {\n        throw new InvalidRequestException(\n            \"Cannot remove non-existent cache pool \" + poolName);\n      }\n      // Remove all directives in this pool.\n      Iterator\u003cCacheDirective\u003e iter \u003d pool.getDirectiveList().iterator();\n      while (iter.hasNext()) {\n        CacheDirective directive \u003d iter.next();\n        directivesByPath.remove(directive.getPath());\n        directivesById.remove(directive.getId());\n        iter.remove();\n      }\n      if (monitor !\u003d null) {\n        monitor.setNeedsRescan();\n      }\n    } catch (IOException e) {\n      LOG.info(\"removeCachePool of \" + poolName + \" failed: \", e);\n      throw e;\n    }\n    LOG.info(\"removeCachePool of \" + poolName + \" successful.\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "13edb391d06c479720202eb5ac81f1c71fe64748": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 9:55 AM",
      "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "21/11/13 9:12 AM",
      "commitNameOld": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 6.03,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,21 @@\n   public void removeCachePool(String poolName)\n       throws IOException {\n     assert namesystem.hasWriteLock();\n     CachePoolInfo.validateName(poolName);\n     CachePool pool \u003d cachePools.remove(poolName);\n     if (pool \u003d\u003d null) {\n       throw new InvalidRequestException(\n           \"Cannot remove non-existent cache pool \" + poolName);\n     }\n-    \n-    // Remove entries using this pool\n-    // TODO: could optimize this somewhat to avoid the need to iterate\n-    // over all entries in entriesById\n-    Iterator\u003cEntry\u003cLong, CacheDirective\u003e\u003e iter \u003d \n-        entriesById.entrySet().iterator();\n+    // Remove all directives in this pool.\n+    Iterator\u003cCacheDirective\u003e iter \u003d pool.getDirectiveList().iterator();\n     while (iter.hasNext()) {\n-      Entry\u003cLong, CacheDirective\u003e entry \u003d iter.next();\n-      if (entry.getValue().getPool() \u003d\u003d pool) {\n-        entriesByPath.remove(entry.getValue().getPath());\n-        iter.remove();\n-      }\n+      CacheDirective directive \u003d iter.next();\n+      directivesByPath.remove(directive.getPath());\n+      directivesById.remove(directive.getId());\n+      iter.remove();\n     }\n     if (monitor !\u003d null) {\n       monitor.kick();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeCachePool(String poolName)\n      throws IOException {\n    assert namesystem.hasWriteLock();\n    CachePoolInfo.validateName(poolName);\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new InvalidRequestException(\n          \"Cannot remove non-existent cache pool \" + poolName);\n    }\n    // Remove all directives in this pool.\n    Iterator\u003cCacheDirective\u003e iter \u003d pool.getDirectiveList().iterator();\n    while (iter.hasNext()) {\n      CacheDirective directive \u003d iter.next();\n      directivesByPath.remove(directive.getPath());\n      directivesById.remove(directive.getId());\n      iter.remove();\n    }\n    if (monitor !\u003d null) {\n      monitor.kick();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5473. Consistent naming of user-visible caching classes and methods (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544252 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/13 9:12 AM",
      "commitName": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "18/11/13 6:01 PM",
      "commitNameOld": "4f15d0af4f3633bfa35f7cb7c1cc15ef545597d0",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 2.63,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   public void removeCachePool(String poolName)\n       throws IOException {\n     assert namesystem.hasWriteLock();\n     CachePoolInfo.validateName(poolName);\n     CachePool pool \u003d cachePools.remove(poolName);\n     if (pool \u003d\u003d null) {\n       throw new InvalidRequestException(\n           \"Cannot remove non-existent cache pool \" + poolName);\n     }\n     \n     // Remove entries using this pool\n     // TODO: could optimize this somewhat to avoid the need to iterate\n     // over all entries in entriesById\n-    Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n+    Iterator\u003cEntry\u003cLong, CacheDirective\u003e\u003e iter \u003d \n         entriesById.entrySet().iterator();\n     while (iter.hasNext()) {\n-      Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n+      Entry\u003cLong, CacheDirective\u003e entry \u003d iter.next();\n       if (entry.getValue().getPool() \u003d\u003d pool) {\n         entriesByPath.remove(entry.getValue().getPath());\n         iter.remove();\n       }\n     }\n     if (monitor !\u003d null) {\n       monitor.kick();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeCachePool(String poolName)\n      throws IOException {\n    assert namesystem.hasWriteLock();\n    CachePoolInfo.validateName(poolName);\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new InvalidRequestException(\n          \"Cannot remove non-existent cache pool \" + poolName);\n    }\n    \n    // Remove entries using this pool\n    // TODO: could optimize this somewhat to avoid the need to iterate\n    // over all entries in entriesById\n    Iterator\u003cEntry\u003cLong, CacheDirective\u003e\u003e iter \u003d \n        entriesById.entrySet().iterator();\n    while (iter.hasNext()) {\n      Entry\u003cLong, CacheDirective\u003e entry \u003d iter.next();\n      if (entry.getValue().getPool() \u003d\u003d pool) {\n        entriesByPath.remove(entry.getValue().getPath());\n        iter.remove();\n      }\n    }\n    if (monitor !\u003d null) {\n      monitor.kick();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "ce35e0950cef9250ce2ceffb3b8bfcff533c6b92": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5471. CacheAdmin -listPools fails when user lacks permissions to view all pools (Andrew Wang via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1541323 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/11/13 3:52 PM",
      "commitName": "ce35e0950cef9250ce2ceffb3b8bfcff533c6b92",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "07/11/13 2:07 PM",
      "commitNameOld": "f79b3e6b17450e9d34c483046b7437b09dd72016",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 5.07,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,26 @@\n   public void removeCachePool(String poolName)\n       throws IOException {\n     assert namesystem.hasWriteLock();\n     CachePoolInfo.validateName(poolName);\n     CachePool pool \u003d cachePools.remove(poolName);\n     if (pool \u003d\u003d null) {\n-      throw new IOException(\"can\u0027t remove non-existent cache pool \" + poolName);\n+      throw new InvalidRequestException(\n+          \"Cannot remove non-existent cache pool \" + poolName);\n     }\n     \n     // Remove entries using this pool\n     // TODO: could optimize this somewhat to avoid the need to iterate\n     // over all entries in entriesById\n     Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n         entriesById.entrySet().iterator();\n     while (iter.hasNext()) {\n       Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n       if (entry.getValue().getPool() \u003d\u003d pool) {\n         entriesByPath.remove(entry.getValue().getPath());\n         iter.remove();\n       }\n     }\n     if (monitor !\u003d null) {\n       monitor.kick();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeCachePool(String poolName)\n      throws IOException {\n    assert namesystem.hasWriteLock();\n    CachePoolInfo.validateName(poolName);\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new InvalidRequestException(\n          \"Cannot remove non-existent cache pool \" + poolName);\n    }\n    \n    // Remove entries using this pool\n    // TODO: could optimize this somewhat to avoid the need to iterate\n    // over all entries in entriesById\n    Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n        entriesById.entrySet().iterator();\n    while (iter.hasNext()) {\n      Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n      if (entry.getValue().getPool() \u003d\u003d pool) {\n        entriesByPath.remove(entry.getValue().getPath());\n        iter.remove();\n      }\n    }\n    if (monitor !\u003d null) {\n      monitor.kick();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 3:15 PM",
      "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "14/10/13 3:56 PM",
          "commitNameOld": "efe545b0c219eeba61ac5259aee4d518beb74316",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 1.97,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,25 @@\n-  public synchronized void removeCachePool(String poolName)\n+  public void removeCachePool(String poolName)\n       throws IOException {\n+    assert namesystem.hasWriteLock();\n     CachePoolInfo.validateName(poolName);\n     CachePool pool \u003d cachePools.remove(poolName);\n     if (pool \u003d\u003d null) {\n       throw new IOException(\"can\u0027t remove non-existent cache pool \" + poolName);\n     }\n     \n     // Remove entries using this pool\n     // TODO: could optimize this somewhat to avoid the need to iterate\n     // over all entries in entriesById\n     Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n         entriesById.entrySet().iterator();\n     while (iter.hasNext()) {\n       Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n       if (entry.getValue().getPool() \u003d\u003d pool) {\n         entriesByPath.remove(entry.getValue().getPath());\n         iter.remove();\n       }\n     }\n+    if (monitor !\u003d null) {\n+      monitor.kick();\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void removeCachePool(String poolName)\n      throws IOException {\n    assert namesystem.hasWriteLock();\n    CachePoolInfo.validateName(poolName);\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new IOException(\"can\u0027t remove non-existent cache pool \" + poolName);\n    }\n    \n    // Remove entries using this pool\n    // TODO: could optimize this somewhat to avoid the need to iterate\n    // over all entries in entriesById\n    Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n        entriesById.entrySet().iterator();\n    while (iter.hasNext()) {\n      Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n      if (entry.getValue().getPool() \u003d\u003d pool) {\n        entriesByPath.remove(entry.getValue().getPath());\n        iter.remove();\n      }\n    }\n    if (monitor !\u003d null) {\n      monitor.kick();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "14/10/13 3:56 PM",
          "commitNameOld": "efe545b0c219eeba61ac5259aee4d518beb74316",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 1.97,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,25 @@\n-  public synchronized void removeCachePool(String poolName)\n+  public void removeCachePool(String poolName)\n       throws IOException {\n+    assert namesystem.hasWriteLock();\n     CachePoolInfo.validateName(poolName);\n     CachePool pool \u003d cachePools.remove(poolName);\n     if (pool \u003d\u003d null) {\n       throw new IOException(\"can\u0027t remove non-existent cache pool \" + poolName);\n     }\n     \n     // Remove entries using this pool\n     // TODO: could optimize this somewhat to avoid the need to iterate\n     // over all entries in entriesById\n     Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n         entriesById.entrySet().iterator();\n     while (iter.hasNext()) {\n       Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n       if (entry.getValue().getPool() \u003d\u003d pool) {\n         entriesByPath.remove(entry.getValue().getPath());\n         iter.remove();\n       }\n     }\n+    if (monitor !\u003d null) {\n+      monitor.kick();\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void removeCachePool(String poolName)\n      throws IOException {\n    assert namesystem.hasWriteLock();\n    CachePoolInfo.validateName(poolName);\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new IOException(\"can\u0027t remove non-existent cache pool \" + poolName);\n    }\n    \n    // Remove entries using this pool\n    // TODO: could optimize this somewhat to avoid the need to iterate\n    // over all entries in entriesById\n    Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n        entriesById.entrySet().iterator();\n    while (iter.hasNext()) {\n      Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n      if (entry.getValue().getPool() \u003d\u003d pool) {\n        entriesByPath.remove(entry.getValue().getPath());\n        iter.remove();\n      }\n    }\n    if (monitor !\u003d null) {\n      monitor.kick();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "eb2175db1a99348c80457e3ffda172cc461de8bc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5190. Move cache pool related CLI commands to CacheAdmin. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1529334 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/10/13 3:28 PM",
      "commitName": "eb2175db1a99348c80457e3ffda172cc461de8bc",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "04/10/13 10:46 AM",
      "commitNameOld": "af1ac9a5e8d8d97a855940d853dd59ab4666f6e2",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.2,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   public synchronized void removeCachePool(String poolName)\n       throws IOException {\n     CachePoolInfo.validateName(poolName);\n     CachePool pool \u003d cachePools.remove(poolName);\n     if (pool \u003d\u003d null) {\n       throw new IOException(\"can\u0027t remove non-existent cache pool \" + poolName);\n     }\n     \n     // Remove entries using this pool\n     // TODO: could optimize this somewhat to avoid the need to iterate\n     // over all entries in entriesById\n     Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n         entriesById.entrySet().iterator();\n     while (iter.hasNext()) {\n       Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n       if (entry.getValue().getPool() \u003d\u003d pool) {\n-        entriesById.remove(entry.getValue().getEntryId());\n+        entriesByPath.remove(entry.getValue().getPath());\n         iter.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void removeCachePool(String poolName)\n      throws IOException {\n    CachePoolInfo.validateName(poolName);\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new IOException(\"can\u0027t remove non-existent cache pool \" + poolName);\n    }\n    \n    // Remove entries using this pool\n    // TODO: could optimize this somewhat to avoid the need to iterate\n    // over all entries in entriesById\n    Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n        entriesById.entrySet().iterator();\n    while (iter.hasNext()) {\n      Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n      if (entry.getValue().getPool() \u003d\u003d pool) {\n        entriesByPath.remove(entry.getValue().getPath());\n        iter.remove();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "a0d9a155a4a4258f628e927e096ecf6673f788ec": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5236. Change PathBasedCacheDirective APIs to be a single value rather than batch. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1525183 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/09/13 5:20 PM",
      "commitName": "a0d9a155a4a4258f628e927e096ecf6673f788ec",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "18/09/13 1:43 PM",
      "commitNameOld": "e202d4d1548a0be2f5c61ff82be8b52bd0cfce04",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.15,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,21 @@\n   public synchronized void removeCachePool(String poolName)\n       throws IOException {\n+    CachePoolInfo.validateName(poolName);\n     CachePool pool \u003d cachePools.remove(poolName);\n     if (pool \u003d\u003d null) {\n-      throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n+      throw new IOException(\"can\u0027t remove non-existent cache pool \" + poolName);\n     }\n     \n     // Remove entries using this pool\n     // TODO: could optimize this somewhat to avoid the need to iterate\n     // over all entries in entriesById\n     Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n         entriesById.entrySet().iterator();\n     while (iter.hasNext()) {\n       Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n       if (entry.getValue().getPool() \u003d\u003d pool) {\n         entriesById.remove(entry.getValue().getEntryId());\n         iter.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void removeCachePool(String poolName)\n      throws IOException {\n    CachePoolInfo.validateName(poolName);\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new IOException(\"can\u0027t remove non-existent cache pool \" + poolName);\n    }\n    \n    // Remove entries using this pool\n    // TODO: could optimize this somewhat to avoid the need to iterate\n    // over all entries in entriesById\n    Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n        entriesById.entrySet().iterator();\n    while (iter.hasNext()) {\n      Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n      if (entry.getValue().getPool() \u003d\u003d pool) {\n        entriesById.remove(entry.getValue().getEntryId());\n        iter.remove();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "e202d4d1548a0be2f5c61ff82be8b52bd0cfce04": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5213. Separate PathBasedCacheEntry and PathBasedCacheDirectiveWithId. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1524561 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/13 1:43 PM",
      "commitName": "e202d4d1548a0be2f5c61ff82be8b52bd0cfce04",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "16/09/13 11:41 AM",
      "commitNameOld": "85c203602993a946fb5f41eadf1cf1484a0ce686",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   public synchronized void removeCachePool(String poolName)\n       throws IOException {\n     CachePool pool \u003d cachePools.remove(poolName);\n     if (pool \u003d\u003d null) {\n       throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n     }\n     \n     // Remove entries using this pool\n     // TODO: could optimize this somewhat to avoid the need to iterate\n-    // over all entries in entriesByDirective\n-    Iterator\u003cEntry\u003cPathBasedCacheDirective, PathBasedCacheEntry\u003e\u003e iter \u003d \n-        entriesByDirective.entrySet().iterator();\n+    // over all entries in entriesById\n+    Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n+        entriesById.entrySet().iterator();\n     while (iter.hasNext()) {\n-      Entry\u003cPathBasedCacheDirective, PathBasedCacheEntry\u003e entry \u003d iter.next();\n-      if (entry.getKey().getPool().equals(poolName)) {\n+      Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n+      if (entry.getValue().getPool() \u003d\u003d pool) {\n         entriesById.remove(entry.getValue().getEntryId());\n         iter.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void removeCachePool(String poolName)\n      throws IOException {\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n    }\n    \n    // Remove entries using this pool\n    // TODO: could optimize this somewhat to avoid the need to iterate\n    // over all entries in entriesById\n    Iterator\u003cEntry\u003cLong, PathBasedCacheEntry\u003e\u003e iter \u003d \n        entriesById.entrySet().iterator();\n    while (iter.hasNext()) {\n      Entry\u003cLong, PathBasedCacheEntry\u003e entry \u003d iter.next();\n      if (entry.getValue().getPool() \u003d\u003d pool) {\n        entriesById.remove(entry.getValue().getEntryId());\n        iter.remove();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "02e0e158a26f81ce8375426ba0ea56db09ee36be": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5158. Add command-line support for manipulating cache directives\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1522272 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/09/13 8:55 PM",
      "commitName": "02e0e158a26f81ce8375426ba0ea56db09ee36be",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "09/09/13 11:53 AM",
      "commitNameOld": "3a9cd79e9ddd5a9499e28633ccccdc9eef22b813",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 2.38,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   public synchronized void removeCachePool(String poolName)\n       throws IOException {\n     CachePool pool \u003d cachePools.remove(poolName);\n     if (pool \u003d\u003d null) {\n       throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n     }\n     \n     // Remove entries using this pool\n     // TODO: could optimize this somewhat to avoid the need to iterate\n     // over all entries in entriesByDirective\n-    Iterator\u003cEntry\u003cPathCacheDirective, PathCacheEntry\u003e\u003e iter \u003d \n+    Iterator\u003cEntry\u003cPathBasedCacheDirective, PathBasedCacheEntry\u003e\u003e iter \u003d \n         entriesByDirective.entrySet().iterator();\n     while (iter.hasNext()) {\n-      Entry\u003cPathCacheDirective, PathCacheEntry\u003e entry \u003d iter.next();\n+      Entry\u003cPathBasedCacheDirective, PathBasedCacheEntry\u003e entry \u003d iter.next();\n       if (entry.getKey().getPool().equals(poolName)) {\n         entriesById.remove(entry.getValue().getEntryId());\n         iter.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void removeCachePool(String poolName)\n      throws IOException {\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n    }\n    \n    // Remove entries using this pool\n    // TODO: could optimize this somewhat to avoid the need to iterate\n    // over all entries in entriesByDirective\n    Iterator\u003cEntry\u003cPathBasedCacheDirective, PathBasedCacheEntry\u003e\u003e iter \u003d \n        entriesByDirective.entrySet().iterator();\n    while (iter.hasNext()) {\n      Entry\u003cPathBasedCacheDirective, PathBasedCacheEntry\u003e entry \u003d iter.next();\n      if (entry.getKey().getPool().equals(poolName)) {\n        entriesById.remove(entry.getValue().getEntryId());\n        iter.remove();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "f41f8b8842c3f26d19f7fa928070c7c07f760e4c": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5163. Miscellaneous cache pool RPC fixes (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520665 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/09/13 11:52 AM",
      "commitName": "f41f8b8842c3f26d19f7fa928070c7c07f760e4c",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5163. Miscellaneous cache pool RPC fixes (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520665 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/09/13 11:52 AM",
          "commitName": "f41f8b8842c3f26d19f7fa928070c7c07f760e4c",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "04/09/13 11:23 AM",
          "commitNameOld": "d56d0b46e1b82ae068083ddb99872d314684dc82",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 2.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,20 @@\n-  public synchronized void removeCachePool(long poolId) throws IOException {\n-    if (!cachePoolsById.containsKey(poolId)) {\n-      throw new IOException(\"can\u0027t remove nonexistent cache pool id \" + poolId);\n+  public synchronized void removeCachePool(String poolName)\n+      throws IOException {\n+    CachePool pool \u003d cachePools.remove(poolName);\n+    if (pool \u003d\u003d null) {\n+      throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n     }\n-    // Remove all the entries associated with the pool\n-    Iterator\u003cMap.Entry\u003cLong, PathCacheEntry\u003e\u003e it \u003d\n-        entriesById.entrySet().iterator();\n-    while (it.hasNext()) {\n-      Map.Entry\u003cLong, PathCacheEntry\u003e entry \u003d it.next();\n-      if (entry.getValue().getDirective().getPoolId() \u003d\u003d poolId) {\n-        it.remove();\n-        entriesByDirective.remove(entry.getValue().getDirective());\n+    \n+    // Remove entries using this pool\n+    // TODO: could optimize this somewhat to avoid the need to iterate\n+    // over all entries in entriesByDirective\n+    Iterator\u003cEntry\u003cPathCacheDirective, PathCacheEntry\u003e\u003e iter \u003d \n+        entriesByDirective.entrySet().iterator();\n+    while (iter.hasNext()) {\n+      Entry\u003cPathCacheDirective, PathCacheEntry\u003e entry \u003d iter.next();\n+      if (entry.getKey().getPool().equals(poolName)) {\n+        entriesById.remove(entry.getValue().getEntryId());\n+        iter.remove();\n       }\n     }\n-    // Remove the pool\n-    CachePool pool \u003d cachePoolsById.remove(poolId);\n-    cachePoolsByName.remove(pool.getInfo().getPoolName());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void removeCachePool(String poolName)\n      throws IOException {\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n    }\n    \n    // Remove entries using this pool\n    // TODO: could optimize this somewhat to avoid the need to iterate\n    // over all entries in entriesByDirective\n    Iterator\u003cEntry\u003cPathCacheDirective, PathCacheEntry\u003e\u003e iter \u003d \n        entriesByDirective.entrySet().iterator();\n    while (iter.hasNext()) {\n      Entry\u003cPathCacheDirective, PathCacheEntry\u003e entry \u003d iter.next();\n      if (entry.getKey().getPool().equals(poolName)) {\n        entriesById.remove(entry.getValue().getEntryId());\n        iter.remove();\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[poolId-long]",
            "newValue": "[poolName-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5163. Miscellaneous cache pool RPC fixes (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520665 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/09/13 11:52 AM",
          "commitName": "f41f8b8842c3f26d19f7fa928070c7c07f760e4c",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "04/09/13 11:23 AM",
          "commitNameOld": "d56d0b46e1b82ae068083ddb99872d314684dc82",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 2.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,20 @@\n-  public synchronized void removeCachePool(long poolId) throws IOException {\n-    if (!cachePoolsById.containsKey(poolId)) {\n-      throw new IOException(\"can\u0027t remove nonexistent cache pool id \" + poolId);\n+  public synchronized void removeCachePool(String poolName)\n+      throws IOException {\n+    CachePool pool \u003d cachePools.remove(poolName);\n+    if (pool \u003d\u003d null) {\n+      throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n     }\n-    // Remove all the entries associated with the pool\n-    Iterator\u003cMap.Entry\u003cLong, PathCacheEntry\u003e\u003e it \u003d\n-        entriesById.entrySet().iterator();\n-    while (it.hasNext()) {\n-      Map.Entry\u003cLong, PathCacheEntry\u003e entry \u003d it.next();\n-      if (entry.getValue().getDirective().getPoolId() \u003d\u003d poolId) {\n-        it.remove();\n-        entriesByDirective.remove(entry.getValue().getDirective());\n+    \n+    // Remove entries using this pool\n+    // TODO: could optimize this somewhat to avoid the need to iterate\n+    // over all entries in entriesByDirective\n+    Iterator\u003cEntry\u003cPathCacheDirective, PathCacheEntry\u003e\u003e iter \u003d \n+        entriesByDirective.entrySet().iterator();\n+    while (iter.hasNext()) {\n+      Entry\u003cPathCacheDirective, PathCacheEntry\u003e entry \u003d iter.next();\n+      if (entry.getKey().getPool().equals(poolName)) {\n+        entriesById.remove(entry.getValue().getEntryId());\n+        iter.remove();\n       }\n     }\n-    // Remove the pool\n-    CachePool pool \u003d cachePoolsById.remove(poolId);\n-    cachePoolsByName.remove(pool.getInfo().getPoolName());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void removeCachePool(String poolName)\n      throws IOException {\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n    }\n    \n    // Remove entries using this pool\n    // TODO: could optimize this somewhat to avoid the need to iterate\n    // over all entries in entriesByDirective\n    Iterator\u003cEntry\u003cPathCacheDirective, PathCacheEntry\u003e\u003e iter \u003d \n        entriesByDirective.entrySet().iterator();\n    while (iter.hasNext()) {\n      Entry\u003cPathCacheDirective, PathCacheEntry\u003e entry \u003d iter.next();\n      if (entry.getKey().getPool().equals(poolName)) {\n        entriesById.remove(entry.getValue().getEntryId());\n        iter.remove();\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "d56d0b46e1b82ae068083ddb99872d314684dc82": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "commit correct version of HDFS-5121\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520090 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/13 11:23 AM",
      "commitName": "d56d0b46e1b82ae068083ddb99872d314684dc82",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "commit correct version of HDFS-5121\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520090 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/09/13 11:23 AM",
          "commitName": "d56d0b46e1b82ae068083ddb99872d314684dc82",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "03/09/13 1:38 PM",
          "commitNameOld": "97b7267977ef42201e5844df49bc37ec3d10ce16",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,18 @@\n-  public synchronized void removeCachePool(String poolName)\n-      throws IOException {\n-    CachePool pool \u003d cachePools.remove(poolName);\n-    if (pool \u003d\u003d null) {\n-      throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n+  public synchronized void removeCachePool(long poolId) throws IOException {\n+    if (!cachePoolsById.containsKey(poolId)) {\n+      throw new IOException(\"can\u0027t remove nonexistent cache pool id \" + poolId);\n     }\n+    // Remove all the entries associated with the pool\n+    Iterator\u003cMap.Entry\u003cLong, PathCacheEntry\u003e\u003e it \u003d\n+        entriesById.entrySet().iterator();\n+    while (it.hasNext()) {\n+      Map.Entry\u003cLong, PathCacheEntry\u003e entry \u003d it.next();\n+      if (entry.getValue().getDirective().getPoolId() \u003d\u003d poolId) {\n+        it.remove();\n+        entriesByDirective.remove(entry.getValue().getDirective());\n+      }\n+    }\n+    // Remove the pool\n+    CachePool pool \u003d cachePoolsById.remove(poolId);\n+    cachePoolsByName.remove(pool.getInfo().getPoolName());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void removeCachePool(long poolId) throws IOException {\n    if (!cachePoolsById.containsKey(poolId)) {\n      throw new IOException(\"can\u0027t remove nonexistent cache pool id \" + poolId);\n    }\n    // Remove all the entries associated with the pool\n    Iterator\u003cMap.Entry\u003cLong, PathCacheEntry\u003e\u003e it \u003d\n        entriesById.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry\u003cLong, PathCacheEntry\u003e entry \u003d it.next();\n      if (entry.getValue().getDirective().getPoolId() \u003d\u003d poolId) {\n        it.remove();\n        entriesByDirective.remove(entry.getValue().getDirective());\n      }\n    }\n    // Remove the pool\n    CachePool pool \u003d cachePoolsById.remove(poolId);\n    cachePoolsByName.remove(pool.getInfo().getPoolName());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[poolName-String]",
            "newValue": "[poolId-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "commit correct version of HDFS-5121\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520090 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/09/13 11:23 AM",
          "commitName": "d56d0b46e1b82ae068083ddb99872d314684dc82",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "03/09/13 1:38 PM",
          "commitNameOld": "97b7267977ef42201e5844df49bc37ec3d10ce16",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,18 @@\n-  public synchronized void removeCachePool(String poolName)\n-      throws IOException {\n-    CachePool pool \u003d cachePools.remove(poolName);\n-    if (pool \u003d\u003d null) {\n-      throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n+  public synchronized void removeCachePool(long poolId) throws IOException {\n+    if (!cachePoolsById.containsKey(poolId)) {\n+      throw new IOException(\"can\u0027t remove nonexistent cache pool id \" + poolId);\n     }\n+    // Remove all the entries associated with the pool\n+    Iterator\u003cMap.Entry\u003cLong, PathCacheEntry\u003e\u003e it \u003d\n+        entriesById.entrySet().iterator();\n+    while (it.hasNext()) {\n+      Map.Entry\u003cLong, PathCacheEntry\u003e entry \u003d it.next();\n+      if (entry.getValue().getDirective().getPoolId() \u003d\u003d poolId) {\n+        it.remove();\n+        entriesByDirective.remove(entry.getValue().getDirective());\n+      }\n+    }\n+    // Remove the pool\n+    CachePool pool \u003d cachePoolsById.remove(poolId);\n+    cachePoolsByName.remove(pool.getInfo().getPoolName());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void removeCachePool(long poolId) throws IOException {\n    if (!cachePoolsById.containsKey(poolId)) {\n      throw new IOException(\"can\u0027t remove nonexistent cache pool id \" + poolId);\n    }\n    // Remove all the entries associated with the pool\n    Iterator\u003cMap.Entry\u003cLong, PathCacheEntry\u003e\u003e it \u003d\n        entriesById.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry\u003cLong, PathCacheEntry\u003e entry \u003d it.next();\n      if (entry.getValue().getDirective().getPoolId() \u003d\u003d poolId) {\n        it.remove();\n        entriesByDirective.remove(entry.getValue().getDirective());\n      }\n    }\n    // Remove the pool\n    CachePool pool \u003d cachePoolsById.remove(poolId);\n    cachePoolsByName.remove(pool.getInfo().getPoolName());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "97b7267977ef42201e5844df49bc37ec3d10ce16": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5121.  Add RPCs for creating and manipulating cache pools.  (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1519841 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/09/13 1:38 PM",
      "commitName": "97b7267977ef42201e5844df49bc37ec3d10ce16",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,7 @@\n+  public synchronized void removeCachePool(String poolName)\n+      throws IOException {\n+    CachePool pool \u003d cachePools.remove(poolName);\n+    if (pool \u003d\u003d null) {\n+      throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void removeCachePool(String poolName)\n      throws IOException {\n    CachePool pool \u003d cachePools.remove(poolName);\n    if (pool \u003d\u003d null) {\n      throw new IOException(\"can\u0027t remove nonexistent cache pool \" + poolName);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java"
    }
  }
}