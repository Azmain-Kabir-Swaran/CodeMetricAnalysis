{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PeerCache.java",
  "functionName": "evictExpired",
  "functionId": "evictExpired___expiryPeriod-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
  "functionStartLine": 214,
  "functionEndLine": 227,
  "numCommitsSeen": 29,
  "timeTaken": 3080,
  "changeHistory": [
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
    "e2c9b288b223b9fd82dc12018936e13128413492",
    "d12f465c674b3bb5102671b6d6c2746261602d7e",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
    "837e17b2eac1471d93e2eff395272063b265fee7",
    "239b2742d0e80d13c970fd062af4930e672fe903",
    "f6f71529958e2d2aa579046a1b7d7bdf263b584c",
    "a7bcdcc0518595b7d94383606ab8e9aa711292b0"
  ],
  "changeHistoryShort": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": "Ybodychange",
    "e2c9b288b223b9fd82dc12018936e13128413492": "Ymultichange(Yfilerename,Ybodychange)",
    "d12f465c674b3bb5102671b6d6c2746261602d7e": "Ybodychange",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": "Ymultichange(Yfilerename,Ybodychange)",
    "837e17b2eac1471d93e2eff395272063b265fee7": "Ymultichange(Yfilerename,Ybodychange)",
    "239b2742d0e80d13c970fd062af4930e672fe903": "Ymultichange(Yfilerename,Ybodychange)",
    "f6f71529958e2d2aa579046a1b7d7bdf263b584c": "Ybodychange",
    "a7bcdcc0518595b7d94383606ab8e9aa711292b0": "Yintroduced"
  },
  "changeHistoryDetails": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13695. Move logging to slf4j in HDFS package. Contributed by Ian Pickering.\n",
      "commitDate": "06/09/18 2:48 PM",
      "commitName": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "22/11/15 3:03 PM",
      "commitNameOld": "6039059c37626d3d1d231986440623a593e2726b",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1018.95,
      "commitsBetweenForRepo": 7497,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   private synchronized void evictExpired(long expiryPeriod) {\n     while (multimap.size() !\u003d 0) {\n       Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d\n           multimap.entries().iterator();\n       Entry\u003cKey, Value\u003e entry \u003d iter.next();\n       // if oldest socket expired, remove it\n       if (entry \u003d\u003d null ||\n           Time.monotonicNow() - entry.getValue().getTime() \u003c expiryPeriod) {\n         break;\n       }\n-      IOUtilsClient.cleanup(LOG, entry.getValue().getPeer());\n+      IOUtilsClient.cleanupWithLogger(LOG, entry.getValue().getPeer());\n       iter.remove();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d\n          multimap.entries().iterator();\n      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null ||\n          Time.monotonicNow() - entry.getValue().getTime() \u003c expiryPeriod) {\n        break;\n      }\n      IOUtilsClient.cleanupWithLogger(LOG, entry.getValue().getPeer());\n      iter.remove();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
      "extendedDetails": {}
    },
    "e2c9b288b223b9fd82dc12018936e13128413492": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "28/08/15 2:38 PM",
      "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "28/08/15 2:38 PM",
          "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "28/08/15 2:21 PM",
          "commitNameOld": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,15 @@\n   private synchronized void evictExpired(long expiryPeriod) {\n     while (multimap.size() !\u003d 0) {\n       Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d\n         multimap.entries().iterator();\n       Entry\u003cKey, Value\u003e entry \u003d iter.next();\n       // if oldest socket expired, remove it\n       if (entry \u003d\u003d null || \n         Time.monotonicNow() - entry.getValue().getTime() \u003c\n         expiryPeriod) {\n         break;\n       }\n-      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n+      IOUtilsClient.cleanup(LOG, entry.getValue().getPeer());\n       iter.remove();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d\n        multimap.entries().iterator();\n      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null || \n        Time.monotonicNow() - entry.getValue().getTime() \u003c\n        expiryPeriod) {\n        break;\n      }\n      IOUtilsClient.cleanup(LOG, entry.getValue().getPeer());\n      iter.remove();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PeerCache.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "28/08/15 2:38 PM",
          "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "28/08/15 2:21 PM",
          "commitNameOld": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,15 @@\n   private synchronized void evictExpired(long expiryPeriod) {\n     while (multimap.size() !\u003d 0) {\n       Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d\n         multimap.entries().iterator();\n       Entry\u003cKey, Value\u003e entry \u003d iter.next();\n       // if oldest socket expired, remove it\n       if (entry \u003d\u003d null || \n         Time.monotonicNow() - entry.getValue().getTime() \u003c\n         expiryPeriod) {\n         break;\n       }\n-      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n+      IOUtilsClient.cleanup(LOG, entry.getValue().getPeer());\n       iter.remove();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d\n        multimap.entries().iterator();\n      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null || \n        Time.monotonicNow() - entry.getValue().getTime() \u003c\n        expiryPeriod) {\n        break;\n      }\n      IOUtilsClient.cleanup(LOG, entry.getValue().getPeer());\n      iter.remove();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {}
        }
      ]
    },
    "d12f465c674b3bb5102671b6d6c2746261602d7e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4417. Fix case where local reads get disabled incorrectly. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1437616 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/01/13 10:38 AM",
      "commitName": "d12f465c674b3bb5102671b6d6c2746261602d7e",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "09/01/13 1:34 PM",
      "commitNameOld": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 13.88,
      "commitsBetweenForRepo": 46,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   private synchronized void evictExpired(long expiryPeriod) {\n     while (multimap.size() !\u003d 0) {\n-      Iterator\u003cEntry\u003cDatanodeID, Value\u003e\u003e iter \u003d\n+      Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d\n         multimap.entries().iterator();\n-      Entry\u003cDatanodeID, Value\u003e entry \u003d iter.next();\n+      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n       // if oldest socket expired, remove it\n       if (entry \u003d\u003d null || \n         Time.monotonicNow() - entry.getValue().getTime() \u003c\n         expiryPeriod) {\n         break;\n       }\n       IOUtils.cleanup(LOG, entry.getValue().getPeer());\n       iter.remove();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d\n        multimap.entries().iterator();\n      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null || \n        Time.monotonicNow() - entry.getValue().getTime() \u003c\n        expiryPeriod) {\n        break;\n      }\n      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n      iter.remove();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
      "extendedDetails": {}
    },
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:34 PM",
      "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/01/13 1:34 PM",
          "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "09/01/13 10:12 AM",
          "commitNameOld": "f6c28639005f46bc171a9a990e2ad4d7afb4ce73",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,15 @@\n   private synchronized void evictExpired(long expiryPeriod) {\n     while (multimap.size() !\u003d 0) {\n-      Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n+      Iterator\u003cEntry\u003cDatanodeID, Value\u003e\u003e iter \u003d\n         multimap.entries().iterator();\n-      Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n+      Entry\u003cDatanodeID, Value\u003e entry \u003d iter.next();\n       // if oldest socket expired, remove it\n       if (entry \u003d\u003d null || \n-        Time.monotonicNow() - entry.getValue().getCreateTime() \u003c \n+        Time.monotonicNow() - entry.getValue().getTime() \u003c\n         expiryPeriod) {\n         break;\n       }\n+      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n       iter.remove();\n-      SocketAndStreams s \u003d entry.getValue();\n-      s.close();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cDatanodeID, Value\u003e\u003e iter \u003d\n        multimap.entries().iterator();\n      Entry\u003cDatanodeID, Value\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null || \n        Time.monotonicNow() - entry.getValue().getTime() \u003c\n        expiryPeriod) {\n        break;\n      }\n      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n      iter.remove();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/01/13 1:34 PM",
          "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "09/01/13 10:12 AM",
          "commitNameOld": "f6c28639005f46bc171a9a990e2ad4d7afb4ce73",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.14,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,15 @@\n   private synchronized void evictExpired(long expiryPeriod) {\n     while (multimap.size() !\u003d 0) {\n-      Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n+      Iterator\u003cEntry\u003cDatanodeID, Value\u003e\u003e iter \u003d\n         multimap.entries().iterator();\n-      Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n+      Entry\u003cDatanodeID, Value\u003e entry \u003d iter.next();\n       // if oldest socket expired, remove it\n       if (entry \u003d\u003d null || \n-        Time.monotonicNow() - entry.getValue().getCreateTime() \u003c \n+        Time.monotonicNow() - entry.getValue().getTime() \u003c\n         expiryPeriod) {\n         break;\n       }\n+      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n       iter.remove();\n-      SocketAndStreams s \u003d entry.getValue();\n-      s.close();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cDatanodeID, Value\u003e\u003e iter \u003d\n        multimap.entries().iterator();\n      Entry\u003cDatanodeID, Value\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null || \n        Time.monotonicNow() - entry.getValue().getTime() \u003c\n        expiryPeriod) {\n        break;\n      }\n      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n      iter.remove();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {}
        }
      ]
    },
    "837e17b2eac1471d93e2eff395272063b265fee7": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 6:39 PM",
      "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 6:39 PM",
          "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "08/01/13 2:43 PM",
          "commitNameOld": "4ca58bd57c11fe328ff03d52a3cf6d848f6daa00",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,16 @@\n   private synchronized void evictExpired(long expiryPeriod) {\n     while (multimap.size() !\u003d 0) {\n-      Iterator\u003cEntry\u003cDatanodeID, Value\u003e\u003e iter \u003d\n+      Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n         multimap.entries().iterator();\n-      Entry\u003cDatanodeID, Value\u003e entry \u003d iter.next();\n+      Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n       // if oldest socket expired, remove it\n       if (entry \u003d\u003d null || \n-        Time.monotonicNow() - entry.getValue().getTime() \u003c\n+        Time.monotonicNow() - entry.getValue().getCreateTime() \u003c \n         expiryPeriod) {\n         break;\n       }\n-      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n       iter.remove();\n+      SocketAndStreams s \u003d entry.getValue();\n+      s.close();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n        multimap.entries().iterator();\n      Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null || \n        Time.monotonicNow() - entry.getValue().getCreateTime() \u003c \n        expiryPeriod) {\n        break;\n      }\n      iter.remove();\n      SocketAndStreams s \u003d entry.getValue();\n      s.close();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 6:39 PM",
          "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "08/01/13 2:43 PM",
          "commitNameOld": "4ca58bd57c11fe328ff03d52a3cf6d848f6daa00",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,16 @@\n   private synchronized void evictExpired(long expiryPeriod) {\n     while (multimap.size() !\u003d 0) {\n-      Iterator\u003cEntry\u003cDatanodeID, Value\u003e\u003e iter \u003d\n+      Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n         multimap.entries().iterator();\n-      Entry\u003cDatanodeID, Value\u003e entry \u003d iter.next();\n+      Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n       // if oldest socket expired, remove it\n       if (entry \u003d\u003d null || \n-        Time.monotonicNow() - entry.getValue().getTime() \u003c\n+        Time.monotonicNow() - entry.getValue().getCreateTime() \u003c \n         expiryPeriod) {\n         break;\n       }\n-      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n       iter.remove();\n+      SocketAndStreams s \u003d entry.getValue();\n+      s.close();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n        multimap.entries().iterator();\n      Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null || \n        Time.monotonicNow() - entry.getValue().getCreateTime() \u003c \n        expiryPeriod) {\n        break;\n      }\n      iter.remove();\n      SocketAndStreams s \u003d entry.getValue();\n      s.close();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
          "extendedDetails": {}
        }
      ]
    },
    "239b2742d0e80d13c970fd062af4930e672fe903": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 12:44 PM",
      "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 12:44 PM",
          "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "08/01/13 11:51 AM",
          "commitNameOld": "db99f7f67d173de63e5601e401b7d4daf1585288",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,15 @@\n   private synchronized void evictExpired(long expiryPeriod) {\n     while (multimap.size() !\u003d 0) {\n-      Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n+      Iterator\u003cEntry\u003cDatanodeID, Value\u003e\u003e iter \u003d\n         multimap.entries().iterator();\n-      Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n+      Entry\u003cDatanodeID, Value\u003e entry \u003d iter.next();\n       // if oldest socket expired, remove it\n       if (entry \u003d\u003d null || \n-        Time.monotonicNow() - entry.getValue().getCreateTime() \u003c \n+        Time.monotonicNow() - entry.getValue().getTime() \u003c\n         expiryPeriod) {\n         break;\n       }\n+      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n       iter.remove();\n-      SocketAndStreams s \u003d entry.getValue();\n-      s.close();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cDatanodeID, Value\u003e\u003e iter \u003d\n        multimap.entries().iterator();\n      Entry\u003cDatanodeID, Value\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null || \n        Time.monotonicNow() - entry.getValue().getTime() \u003c\n        expiryPeriod) {\n        break;\n      }\n      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n      iter.remove();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/01/13 12:44 PM",
          "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "08/01/13 11:51 AM",
          "commitNameOld": "db99f7f67d173de63e5601e401b7d4daf1585288",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,15 @@\n   private synchronized void evictExpired(long expiryPeriod) {\n     while (multimap.size() !\u003d 0) {\n-      Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n+      Iterator\u003cEntry\u003cDatanodeID, Value\u003e\u003e iter \u003d\n         multimap.entries().iterator();\n-      Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n+      Entry\u003cDatanodeID, Value\u003e entry \u003d iter.next();\n       // if oldest socket expired, remove it\n       if (entry \u003d\u003d null || \n-        Time.monotonicNow() - entry.getValue().getCreateTime() \u003c \n+        Time.monotonicNow() - entry.getValue().getTime() \u003c\n         expiryPeriod) {\n         break;\n       }\n+      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n       iter.remove();\n-      SocketAndStreams s \u003d entry.getValue();\n-      s.close();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cDatanodeID, Value\u003e\u003e iter \u003d\n        multimap.entries().iterator();\n      Entry\u003cDatanodeID, Value\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null || \n        Time.monotonicNow() - entry.getValue().getTime() \u003c\n        expiryPeriod) {\n        break;\n      }\n      IOUtils.cleanup(LOG, entry.getValue().getPeer());\n      iter.remove();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/PeerCache.java",
          "extendedDetails": {}
        }
      ]
    },
    "f6f71529958e2d2aa579046a1b7d7bdf263b584c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4307. SocketCache should use monotonic time. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1421572 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/12 2:09 PM",
      "commitName": "f6f71529958e2d2aa579046a1b7d7bdf263b584c",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "26/09/12 6:23 AM",
      "commitNameOld": "a7bcdcc0518595b7d94383606ab8e9aa711292b0",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 78.37,
      "commitsBetweenForRepo": 399,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   private synchronized void evictExpired(long expiryPeriod) {\n     while (multimap.size() !\u003d 0) {\n       Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n         multimap.entries().iterator();\n       Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n       // if oldest socket expired, remove it\n       if (entry \u003d\u003d null || \n-        System.currentTimeMillis() - entry.getValue().getCreateTime() \u003c \n+        Time.monotonicNow() - entry.getValue().getCreateTime() \u003c \n         expiryPeriod) {\n         break;\n       }\n       iter.remove();\n       SocketAndStreams s \u003d entry.getValue();\n       s.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n        multimap.entries().iterator();\n      Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null || \n        Time.monotonicNow() - entry.getValue().getCreateTime() \u003c \n        expiryPeriod) {\n        break;\n      }\n      iter.remove();\n      SocketAndStreams s \u003d entry.getValue();\n      s.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java",
      "extendedDetails": {}
    },
    "a7bcdcc0518595b7d94383606ab8e9aa711292b0": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3373. Change DFSClient input stream socket cache to global static and add a thread to cleanup expired cache entries.  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1390466 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/09/12 6:23 AM",
      "commitName": "a7bcdcc0518595b7d94383606ab8e9aa711292b0",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,16 @@\n+  private synchronized void evictExpired(long expiryPeriod) {\n+    while (multimap.size() !\u003d 0) {\n+      Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n+        multimap.entries().iterator();\n+      Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n+      // if oldest socket expired, remove it\n+      if (entry \u003d\u003d null || \n+        System.currentTimeMillis() - entry.getValue().getCreateTime() \u003c \n+        expiryPeriod) {\n+        break;\n+      }\n+      iter.remove();\n+      SocketAndStreams s \u003d entry.getValue();\n+      s.close();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void evictExpired(long expiryPeriod) {\n    while (multimap.size() !\u003d 0) {\n      Iterator\u003cEntry\u003cSocketAddress, SocketAndStreams\u003e\u003e iter \u003d\n        multimap.entries().iterator();\n      Entry\u003cSocketAddress, SocketAndStreams\u003e entry \u003d iter.next();\n      // if oldest socket expired, remove it\n      if (entry \u003d\u003d null || \n        System.currentTimeMillis() - entry.getValue().getCreateTime() \u003c \n        expiryPeriod) {\n        break;\n      }\n      iter.remove();\n      SocketAndStreams s \u003d entry.getValue();\n      s.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java"
    }
  }
}