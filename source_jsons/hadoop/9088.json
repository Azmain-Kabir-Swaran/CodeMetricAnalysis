{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSNamesystem.java",
  "functionName": "getBatchedListing",
  "functionId": "getBatchedListing___srcs-String[]__startAfter-byte[]__needLocation-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
  "functionStartLine": 4110,
  "functionEndLine": 4241,
  "numCommitsSeen": 873,
  "timeTaken": 3906,
  "changeHistory": [
    "1824aee9da4056de0fb638906b2172e486bbebe7",
    "d7c4f8ab21c56a52afcfbd0a56d9120e61376d0c"
  ],
  "changeHistoryShort": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": "Ybodychange",
    "d7c4f8ab21c56a52afcfbd0a56d9120e61376d0c": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15217 Add more information to longest write/read lock held log\n\n",
      "commitDate": "18/04/20 1:52 PM",
      "commitName": "1824aee9da4056de0fb638906b2172e486bbebe7",
      "commitAuthor": "Toshihiro Suzuki",
      "commitDateOld": "25/03/20 10:28 AM",
      "commitNameOld": "a700803a18fb957d2799001a2ce1dcb70f75c080",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 24.14,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,131 +1,132 @@\n   BatchedDirectoryListing getBatchedListing(String[] srcs, byte[] startAfter,\n       boolean needLocation) throws IOException {\n \n     if (srcs.length \u003e this.batchedListingLimit) {\n       String msg \u003d String.format(\"Too many source paths (%d \u003e %d)\",\n           srcs.length, batchedListingLimit);\n       throw new IllegalArgumentException(msg);\n     }\n \n     // Parse the startAfter key if present\n     int srcsIndex \u003d 0;\n     byte[] indexStartAfter \u003d new byte[0];\n \n     if (startAfter.length \u003e 0) {\n       BatchedListingKeyProto startAfterProto \u003d\n           BatchedListingKeyProto.parseFrom(startAfter);\n       // Validate that the passed paths match the checksum from key\n       Preconditions.checkArgument(\n           Arrays.equals(\n               startAfterProto.getChecksum().toByteArray(),\n               getSrcPathsHash(srcs)));\n       srcsIndex \u003d startAfterProto.getPathIndex();\n       indexStartAfter \u003d startAfterProto.getStartAfter().toByteArray();\n       // Special case: if the indexStartAfter key is an empty array, it\n       // means the last element we listed was a file, not a directory.\n       // Skip it so we don\u0027t list it twice.\n       if (indexStartAfter.length \u003d\u003d 0) {\n         srcsIndex++;\n       }\n     }\n     final int startSrcsIndex \u003d srcsIndex;\n     final String operationName \u003d \"listStatus\";\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n \n     BatchedDirectoryListing bdl;\n \n     checkOperation(OperationCategory.READ);\n     readLock();\n     try {\n       checkOperation(NameNode.OperationCategory.READ);\n \n       // List all directories from the starting index until we\u0027ve reached\n       // ls limit OR finished listing all srcs.\n       LinkedHashMap\u003cInteger, HdfsPartialListing\u003e listings \u003d\n           Maps.newLinkedHashMap();\n       DirectoryListing lastListing \u003d null;\n       int numEntries \u003d 0;\n       for (; srcsIndex \u003c srcs.length; srcsIndex++) {\n         String src \u003d srcs[srcsIndex];\n         HdfsPartialListing listing;\n         try {\n           DirectoryListing dirListing \u003d\n               getListingInt(dir, pc, src, indexStartAfter, needLocation);\n           if (dirListing \u003d\u003d null) {\n             throw new FileNotFoundException(\"Path \" + src + \" does not exist\");\n           }\n           listing \u003d new HdfsPartialListing(\n               srcsIndex, Lists.newArrayList(dirListing.getPartialListing()));\n           numEntries +\u003d listing.getPartialListing().size();\n           lastListing \u003d dirListing;\n         } catch (Exception e) {\n           if (e instanceof AccessControlException) {\n             logAuditEvent(false, operationName, src);\n           }\n           listing \u003d new HdfsPartialListing(\n               srcsIndex,\n               new RemoteException(\n                   e.getClass().getCanonicalName(),\n                   e.getMessage()));\n           lastListing \u003d null;\n           LOG.info(\"Exception listing src {}\", src, e);\n         }\n \n         listings.put(srcsIndex, listing);\n         // Null out the indexStartAfter after the first time.\n         // If we get a partial result, we\u0027re done iterating because we\u0027re also\n         // over the list limit.\n         if (indexStartAfter.length !\u003d 0) {\n           indexStartAfter \u003d new byte[0];\n         }\n         // Terminate if we\u0027ve reached the maximum listing size\n         if (numEntries \u003e\u003d dir.getListLimit()) {\n           break;\n         }\n       }\n \n       HdfsPartialListing[] partialListingArray \u003d\n           listings.values().toArray(new HdfsPartialListing[] {});\n \n       // Check whether there are more dirs/files to be listed, and if so setting\n       // up the index to start within the first dir to be listed next time.\n       if (srcsIndex \u003e\u003d srcs.length) {\n         // If the loop finished normally, there are no more srcs and we\u0027re done.\n         bdl \u003d new BatchedDirectoryListing(\n             partialListingArray,\n             false,\n             new byte[0]);\n       } else if (srcsIndex \u003d\u003d srcs.length-1 \u0026\u0026\n           lastListing !\u003d null \u0026\u0026\n           !lastListing.hasMore()) {\n         // If we\u0027re on the last srcsIndex, then we might be done exactly on an\n         // lsLimit boundary.\n         bdl \u003d new BatchedDirectoryListing(\n             partialListingArray,\n             false,\n             new byte[0]\n         );\n       } else {\n         byte[] lastName \u003d lastListing !\u003d null \u0026\u0026 lastListing.getLastName() !\u003d\n             null ? lastListing.getLastName() : new byte[0];\n         BatchedListingKeyProto proto \u003d BatchedListingKeyProto.newBuilder()\n             .setChecksum(ByteString.copyFrom(getSrcPathsHash(srcs)))\n             .setPathIndex(srcsIndex)\n             .setStartAfter(ByteString.copyFrom(lastName))\n             .build();\n         byte[] returnedStartAfter \u003d proto.toByteArray();\n \n         // Set the startAfter key if the last listing has more entries\n         bdl \u003d new BatchedDirectoryListing(\n             partialListingArray,\n             true,\n             returnedStartAfter);\n       }\n     } finally {\n-      readUnlock(operationName);\n+      readUnlock(operationName,\n+          getLockReportInfoSupplier(Arrays.toString(srcs)));\n     }\n     for (int i \u003d startSrcsIndex; i \u003c srcsIndex; i++) {\n       logAuditEvent(true, operationName, srcs[i]);\n     }\n     return bdl;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  BatchedDirectoryListing getBatchedListing(String[] srcs, byte[] startAfter,\n      boolean needLocation) throws IOException {\n\n    if (srcs.length \u003e this.batchedListingLimit) {\n      String msg \u003d String.format(\"Too many source paths (%d \u003e %d)\",\n          srcs.length, batchedListingLimit);\n      throw new IllegalArgumentException(msg);\n    }\n\n    // Parse the startAfter key if present\n    int srcsIndex \u003d 0;\n    byte[] indexStartAfter \u003d new byte[0];\n\n    if (startAfter.length \u003e 0) {\n      BatchedListingKeyProto startAfterProto \u003d\n          BatchedListingKeyProto.parseFrom(startAfter);\n      // Validate that the passed paths match the checksum from key\n      Preconditions.checkArgument(\n          Arrays.equals(\n              startAfterProto.getChecksum().toByteArray(),\n              getSrcPathsHash(srcs)));\n      srcsIndex \u003d startAfterProto.getPathIndex();\n      indexStartAfter \u003d startAfterProto.getStartAfter().toByteArray();\n      // Special case: if the indexStartAfter key is an empty array, it\n      // means the last element we listed was a file, not a directory.\n      // Skip it so we don\u0027t list it twice.\n      if (indexStartAfter.length \u003d\u003d 0) {\n        srcsIndex++;\n      }\n    }\n    final int startSrcsIndex \u003d srcsIndex;\n    final String operationName \u003d \"listStatus\";\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n\n    BatchedDirectoryListing bdl;\n\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      checkOperation(NameNode.OperationCategory.READ);\n\n      // List all directories from the starting index until we\u0027ve reached\n      // ls limit OR finished listing all srcs.\n      LinkedHashMap\u003cInteger, HdfsPartialListing\u003e listings \u003d\n          Maps.newLinkedHashMap();\n      DirectoryListing lastListing \u003d null;\n      int numEntries \u003d 0;\n      for (; srcsIndex \u003c srcs.length; srcsIndex++) {\n        String src \u003d srcs[srcsIndex];\n        HdfsPartialListing listing;\n        try {\n          DirectoryListing dirListing \u003d\n              getListingInt(dir, pc, src, indexStartAfter, needLocation);\n          if (dirListing \u003d\u003d null) {\n            throw new FileNotFoundException(\"Path \" + src + \" does not exist\");\n          }\n          listing \u003d new HdfsPartialListing(\n              srcsIndex, Lists.newArrayList(dirListing.getPartialListing()));\n          numEntries +\u003d listing.getPartialListing().size();\n          lastListing \u003d dirListing;\n        } catch (Exception e) {\n          if (e instanceof AccessControlException) {\n            logAuditEvent(false, operationName, src);\n          }\n          listing \u003d new HdfsPartialListing(\n              srcsIndex,\n              new RemoteException(\n                  e.getClass().getCanonicalName(),\n                  e.getMessage()));\n          lastListing \u003d null;\n          LOG.info(\"Exception listing src {}\", src, e);\n        }\n\n        listings.put(srcsIndex, listing);\n        // Null out the indexStartAfter after the first time.\n        // If we get a partial result, we\u0027re done iterating because we\u0027re also\n        // over the list limit.\n        if (indexStartAfter.length !\u003d 0) {\n          indexStartAfter \u003d new byte[0];\n        }\n        // Terminate if we\u0027ve reached the maximum listing size\n        if (numEntries \u003e\u003d dir.getListLimit()) {\n          break;\n        }\n      }\n\n      HdfsPartialListing[] partialListingArray \u003d\n          listings.values().toArray(new HdfsPartialListing[] {});\n\n      // Check whether there are more dirs/files to be listed, and if so setting\n      // up the index to start within the first dir to be listed next time.\n      if (srcsIndex \u003e\u003d srcs.length) {\n        // If the loop finished normally, there are no more srcs and we\u0027re done.\n        bdl \u003d new BatchedDirectoryListing(\n            partialListingArray,\n            false,\n            new byte[0]);\n      } else if (srcsIndex \u003d\u003d srcs.length-1 \u0026\u0026\n          lastListing !\u003d null \u0026\u0026\n          !lastListing.hasMore()) {\n        // If we\u0027re on the last srcsIndex, then we might be done exactly on an\n        // lsLimit boundary.\n        bdl \u003d new BatchedDirectoryListing(\n            partialListingArray,\n            false,\n            new byte[0]\n        );\n      } else {\n        byte[] lastName \u003d lastListing !\u003d null \u0026\u0026 lastListing.getLastName() !\u003d\n            null ? lastListing.getLastName() : new byte[0];\n        BatchedListingKeyProto proto \u003d BatchedListingKeyProto.newBuilder()\n            .setChecksum(ByteString.copyFrom(getSrcPathsHash(srcs)))\n            .setPathIndex(srcsIndex)\n            .setStartAfter(ByteString.copyFrom(lastName))\n            .build();\n        byte[] returnedStartAfter \u003d proto.toByteArray();\n\n        // Set the startAfter key if the last listing has more entries\n        bdl \u003d new BatchedDirectoryListing(\n            partialListingArray,\n            true,\n            returnedStartAfter);\n      }\n    } finally {\n      readUnlock(operationName,\n          getLockReportInfoSupplier(Arrays.toString(srcs)));\n    }\n    for (int i \u003d startSrcsIndex; i \u003c srcsIndex; i++) {\n      logAuditEvent(true, operationName, srcs[i]);\n    }\n    return bdl;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "d7c4f8ab21c56a52afcfbd0a56d9120e61376d0c": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13616. Batch listing of multiple directories (#1725)\n\n",
      "commitDate": "15/01/20 5:22 PM",
      "commitName": "d7c4f8ab21c56a52afcfbd0a56d9120e61376d0c",
      "commitAuthor": "Chao Sun",
      "diff": "@@ -0,0 +1,131 @@\n+  BatchedDirectoryListing getBatchedListing(String[] srcs, byte[] startAfter,\n+      boolean needLocation) throws IOException {\n+\n+    if (srcs.length \u003e this.batchedListingLimit) {\n+      String msg \u003d String.format(\"Too many source paths (%d \u003e %d)\",\n+          srcs.length, batchedListingLimit);\n+      throw new IllegalArgumentException(msg);\n+    }\n+\n+    // Parse the startAfter key if present\n+    int srcsIndex \u003d 0;\n+    byte[] indexStartAfter \u003d new byte[0];\n+\n+    if (startAfter.length \u003e 0) {\n+      BatchedListingKeyProto startAfterProto \u003d\n+          BatchedListingKeyProto.parseFrom(startAfter);\n+      // Validate that the passed paths match the checksum from key\n+      Preconditions.checkArgument(\n+          Arrays.equals(\n+              startAfterProto.getChecksum().toByteArray(),\n+              getSrcPathsHash(srcs)));\n+      srcsIndex \u003d startAfterProto.getPathIndex();\n+      indexStartAfter \u003d startAfterProto.getStartAfter().toByteArray();\n+      // Special case: if the indexStartAfter key is an empty array, it\n+      // means the last element we listed was a file, not a directory.\n+      // Skip it so we don\u0027t list it twice.\n+      if (indexStartAfter.length \u003d\u003d 0) {\n+        srcsIndex++;\n+      }\n+    }\n+    final int startSrcsIndex \u003d srcsIndex;\n+    final String operationName \u003d \"listStatus\";\n+    final FSPermissionChecker pc \u003d getPermissionChecker();\n+\n+    BatchedDirectoryListing bdl;\n+\n+    checkOperation(OperationCategory.READ);\n+    readLock();\n+    try {\n+      checkOperation(NameNode.OperationCategory.READ);\n+\n+      // List all directories from the starting index until we\u0027ve reached\n+      // ls limit OR finished listing all srcs.\n+      LinkedHashMap\u003cInteger, HdfsPartialListing\u003e listings \u003d\n+          Maps.newLinkedHashMap();\n+      DirectoryListing lastListing \u003d null;\n+      int numEntries \u003d 0;\n+      for (; srcsIndex \u003c srcs.length; srcsIndex++) {\n+        String src \u003d srcs[srcsIndex];\n+        HdfsPartialListing listing;\n+        try {\n+          DirectoryListing dirListing \u003d\n+              getListingInt(dir, pc, src, indexStartAfter, needLocation);\n+          if (dirListing \u003d\u003d null) {\n+            throw new FileNotFoundException(\"Path \" + src + \" does not exist\");\n+          }\n+          listing \u003d new HdfsPartialListing(\n+              srcsIndex, Lists.newArrayList(dirListing.getPartialListing()));\n+          numEntries +\u003d listing.getPartialListing().size();\n+          lastListing \u003d dirListing;\n+        } catch (Exception e) {\n+          if (e instanceof AccessControlException) {\n+            logAuditEvent(false, operationName, src);\n+          }\n+          listing \u003d new HdfsPartialListing(\n+              srcsIndex,\n+              new RemoteException(\n+                  e.getClass().getCanonicalName(),\n+                  e.getMessage()));\n+          lastListing \u003d null;\n+          LOG.info(\"Exception listing src {}\", src, e);\n+        }\n+\n+        listings.put(srcsIndex, listing);\n+        // Null out the indexStartAfter after the first time.\n+        // If we get a partial result, we\u0027re done iterating because we\u0027re also\n+        // over the list limit.\n+        if (indexStartAfter.length !\u003d 0) {\n+          indexStartAfter \u003d new byte[0];\n+        }\n+        // Terminate if we\u0027ve reached the maximum listing size\n+        if (numEntries \u003e\u003d dir.getListLimit()) {\n+          break;\n+        }\n+      }\n+\n+      HdfsPartialListing[] partialListingArray \u003d\n+          listings.values().toArray(new HdfsPartialListing[] {});\n+\n+      // Check whether there are more dirs/files to be listed, and if so setting\n+      // up the index to start within the first dir to be listed next time.\n+      if (srcsIndex \u003e\u003d srcs.length) {\n+        // If the loop finished normally, there are no more srcs and we\u0027re done.\n+        bdl \u003d new BatchedDirectoryListing(\n+            partialListingArray,\n+            false,\n+            new byte[0]);\n+      } else if (srcsIndex \u003d\u003d srcs.length-1 \u0026\u0026\n+          lastListing !\u003d null \u0026\u0026\n+          !lastListing.hasMore()) {\n+        // If we\u0027re on the last srcsIndex, then we might be done exactly on an\n+        // lsLimit boundary.\n+        bdl \u003d new BatchedDirectoryListing(\n+            partialListingArray,\n+            false,\n+            new byte[0]\n+        );\n+      } else {\n+        byte[] lastName \u003d lastListing !\u003d null \u0026\u0026 lastListing.getLastName() !\u003d\n+            null ? lastListing.getLastName() : new byte[0];\n+        BatchedListingKeyProto proto \u003d BatchedListingKeyProto.newBuilder()\n+            .setChecksum(ByteString.copyFrom(getSrcPathsHash(srcs)))\n+            .setPathIndex(srcsIndex)\n+            .setStartAfter(ByteString.copyFrom(lastName))\n+            .build();\n+        byte[] returnedStartAfter \u003d proto.toByteArray();\n+\n+        // Set the startAfter key if the last listing has more entries\n+        bdl \u003d new BatchedDirectoryListing(\n+            partialListingArray,\n+            true,\n+            returnedStartAfter);\n+      }\n+    } finally {\n+      readUnlock(operationName);\n+    }\n+    for (int i \u003d startSrcsIndex; i \u003c srcsIndex; i++) {\n+      logAuditEvent(true, operationName, srcs[i]);\n+    }\n+    return bdl;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  BatchedDirectoryListing getBatchedListing(String[] srcs, byte[] startAfter,\n      boolean needLocation) throws IOException {\n\n    if (srcs.length \u003e this.batchedListingLimit) {\n      String msg \u003d String.format(\"Too many source paths (%d \u003e %d)\",\n          srcs.length, batchedListingLimit);\n      throw new IllegalArgumentException(msg);\n    }\n\n    // Parse the startAfter key if present\n    int srcsIndex \u003d 0;\n    byte[] indexStartAfter \u003d new byte[0];\n\n    if (startAfter.length \u003e 0) {\n      BatchedListingKeyProto startAfterProto \u003d\n          BatchedListingKeyProto.parseFrom(startAfter);\n      // Validate that the passed paths match the checksum from key\n      Preconditions.checkArgument(\n          Arrays.equals(\n              startAfterProto.getChecksum().toByteArray(),\n              getSrcPathsHash(srcs)));\n      srcsIndex \u003d startAfterProto.getPathIndex();\n      indexStartAfter \u003d startAfterProto.getStartAfter().toByteArray();\n      // Special case: if the indexStartAfter key is an empty array, it\n      // means the last element we listed was a file, not a directory.\n      // Skip it so we don\u0027t list it twice.\n      if (indexStartAfter.length \u003d\u003d 0) {\n        srcsIndex++;\n      }\n    }\n    final int startSrcsIndex \u003d srcsIndex;\n    final String operationName \u003d \"listStatus\";\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n\n    BatchedDirectoryListing bdl;\n\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      checkOperation(NameNode.OperationCategory.READ);\n\n      // List all directories from the starting index until we\u0027ve reached\n      // ls limit OR finished listing all srcs.\n      LinkedHashMap\u003cInteger, HdfsPartialListing\u003e listings \u003d\n          Maps.newLinkedHashMap();\n      DirectoryListing lastListing \u003d null;\n      int numEntries \u003d 0;\n      for (; srcsIndex \u003c srcs.length; srcsIndex++) {\n        String src \u003d srcs[srcsIndex];\n        HdfsPartialListing listing;\n        try {\n          DirectoryListing dirListing \u003d\n              getListingInt(dir, pc, src, indexStartAfter, needLocation);\n          if (dirListing \u003d\u003d null) {\n            throw new FileNotFoundException(\"Path \" + src + \" does not exist\");\n          }\n          listing \u003d new HdfsPartialListing(\n              srcsIndex, Lists.newArrayList(dirListing.getPartialListing()));\n          numEntries +\u003d listing.getPartialListing().size();\n          lastListing \u003d dirListing;\n        } catch (Exception e) {\n          if (e instanceof AccessControlException) {\n            logAuditEvent(false, operationName, src);\n          }\n          listing \u003d new HdfsPartialListing(\n              srcsIndex,\n              new RemoteException(\n                  e.getClass().getCanonicalName(),\n                  e.getMessage()));\n          lastListing \u003d null;\n          LOG.info(\"Exception listing src {}\", src, e);\n        }\n\n        listings.put(srcsIndex, listing);\n        // Null out the indexStartAfter after the first time.\n        // If we get a partial result, we\u0027re done iterating because we\u0027re also\n        // over the list limit.\n        if (indexStartAfter.length !\u003d 0) {\n          indexStartAfter \u003d new byte[0];\n        }\n        // Terminate if we\u0027ve reached the maximum listing size\n        if (numEntries \u003e\u003d dir.getListLimit()) {\n          break;\n        }\n      }\n\n      HdfsPartialListing[] partialListingArray \u003d\n          listings.values().toArray(new HdfsPartialListing[] {});\n\n      // Check whether there are more dirs/files to be listed, and if so setting\n      // up the index to start within the first dir to be listed next time.\n      if (srcsIndex \u003e\u003d srcs.length) {\n        // If the loop finished normally, there are no more srcs and we\u0027re done.\n        bdl \u003d new BatchedDirectoryListing(\n            partialListingArray,\n            false,\n            new byte[0]);\n      } else if (srcsIndex \u003d\u003d srcs.length-1 \u0026\u0026\n          lastListing !\u003d null \u0026\u0026\n          !lastListing.hasMore()) {\n        // If we\u0027re on the last srcsIndex, then we might be done exactly on an\n        // lsLimit boundary.\n        bdl \u003d new BatchedDirectoryListing(\n            partialListingArray,\n            false,\n            new byte[0]\n        );\n      } else {\n        byte[] lastName \u003d lastListing !\u003d null \u0026\u0026 lastListing.getLastName() !\u003d\n            null ? lastListing.getLastName() : new byte[0];\n        BatchedListingKeyProto proto \u003d BatchedListingKeyProto.newBuilder()\n            .setChecksum(ByteString.copyFrom(getSrcPathsHash(srcs)))\n            .setPathIndex(srcsIndex)\n            .setStartAfter(ByteString.copyFrom(lastName))\n            .build();\n        byte[] returnedStartAfter \u003d proto.toByteArray();\n\n        // Set the startAfter key if the last listing has more entries\n        bdl \u003d new BatchedDirectoryListing(\n            partialListingArray,\n            true,\n            returnedStartAfter);\n      }\n    } finally {\n      readUnlock(operationName);\n    }\n    for (int i \u003d startSrcsIndex; i \u003c srcsIndex; i++) {\n      logAuditEvent(true, operationName, srcs[i]);\n    }\n    return bdl;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}