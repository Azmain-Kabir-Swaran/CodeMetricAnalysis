{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockPoolSlice.java",
  "functionName": "validateIntegrityAndSetLength",
  "functionId": "validateIntegrityAndSetLength___blockFile-File__genStamp-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
  "functionStartLine": 809,
  "functionEndLine": 874,
  "numCommitsSeen": 92,
  "timeTaken": 10309,
  "changeHistory": [
    "3a145e2918b66b5776a22eeffba41fc000611936",
    "1a598479a9faec787706bcf924dfbd88a80e1b82",
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389",
    "df983b524ab68ea0c70cee9033bfff2d28052cbf",
    "dcedb72af468128458e597f08d22f5c34b744ae5",
    "aeecfa24f4fb6af289920cbf8830c394e66bd78e",
    "4da8490b512a33a255ed27309860859388d7c168",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "463aec11718e47d4aabb86a7a539cb973460aae6",
    "dd1bc7e1c7e0712a690034f044ab4cf5eaf98ca6",
    "bc13dfb1426944ce45293cb8f444239a7406762c",
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6",
    "dbbfaebb71eb9d69d67fd5becd2e357397d0f68b",
    "b7cd8c0f865e88e40eee75fd2690b1fdc4155071",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "3a145e2918b66b5776a22eeffba41fc000611936": "Ybodychange",
    "1a598479a9faec787706bcf924dfbd88a80e1b82": "Ybodychange",
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389": "Ybodychange",
    "df983b524ab68ea0c70cee9033bfff2d28052cbf": "Ybodychange",
    "dcedb72af468128458e597f08d22f5c34b744ae5": "Ybodychange",
    "aeecfa24f4fb6af289920cbf8830c394e66bd78e": "Ybodychange",
    "4da8490b512a33a255ed27309860859388d7c168": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "463aec11718e47d4aabb86a7a539cb973460aae6": "Ybodychange",
    "dd1bc7e1c7e0712a690034f044ab4cf5eaf98ca6": "Ymultichange(Yrename,Ybodychange)",
    "bc13dfb1426944ce45293cb8f444239a7406762c": "Ymultichange(Ymovefromfile,Ybodychange)",
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6": "Ybodychange",
    "dbbfaebb71eb9d69d67fd5becd2e357397d0f68b": "Ybodychange",
    "b7cd8c0f865e88e40eee75fd2690b1fdc4155071": "Ybodychange",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3a145e2918b66b5776a22eeffba41fc000611936": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14582. Failed to start DN with ArithmeticException when NULL checksum used. Contributed by Surendra Singh Lilhore.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "20/08/19 3:55 PM",
      "commitName": "3a145e2918b66b5776a22eeffba41fc000611936",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "06/08/19 7:18 PM",
      "commitNameOld": "a5bb1e8ee871df1111ff77d0f6921b13c8ffb50e",
      "commitAuthorOld": "Yiqun Lin",
      "daysBetweenCommits": 13.86,
      "commitsBetweenForRepo": 151,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,66 @@\n   private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n     try {\n       final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n       long blockFileLen \u003d blockFile.length();\n       long metaFileLen \u003d metaFile.length();\n       int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n       if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n           !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n       }\n       try (DataInputStream checksumIn \u003d new DataInputStream(\n           new BufferedInputStream(\n               fileIoProvider.getFileInputStream(volume, metaFile),\n               ioFileBufferSize))) {\n         // read and handle the common header here. For now just a version\n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             checksumIn, metaFile);\n+        if (Type.NULL.equals(checksum.getChecksumType())) {\n+          // in case of NULL checksum type consider full file as valid\n+          return blockFileLen;\n+        }\n         int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n         int checksumSize \u003d checksum.getChecksumSize();\n         long numChunks \u003d Math.min(\n             (blockFileLen + bytesPerChecksum - 1) / bytesPerChecksum,\n             (metaFileLen - crcHeaderLen) / checksumSize);\n         if (numChunks \u003d\u003d 0) {\n           return 0;\n         }\n         try (InputStream blockIn \u003d fileIoProvider.getFileInputStream(\n                  volume, blockFile);\n              ReplicaInputStreams ris \u003d new ReplicaInputStreams(blockIn,\n                  checksumIn, volume.obtainReference(), fileIoProvider)) {\n           ris.skipChecksumFully((numChunks - 1) * checksumSize);\n           long lastChunkStartPos \u003d (numChunks - 1) * bytesPerChecksum;\n           ris.skipDataFully(lastChunkStartPos);\n           int lastChunkSize \u003d (int) Math.min(\n               bytesPerChecksum, blockFileLen - lastChunkStartPos);\n           byte[] buf \u003d new byte[lastChunkSize + checksumSize];\n           ris.readChecksumFully(buf, lastChunkSize, checksumSize);\n           ris.readDataFully(buf, 0, lastChunkSize);\n           checksum.update(buf, 0, lastChunkSize);\n           long validFileLength;\n           if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n             validFileLength \u003d lastChunkStartPos + lastChunkSize;\n           } else { // last chunk is corrupt\n             validFileLength \u003d lastChunkStartPos;\n           }\n           // truncate if extra bytes are present without CRC\n           if (blockFile.length() \u003e validFileLength) {\n             try (RandomAccessFile blockRAF \u003d\n                      fileIoProvider.getRandomAccessFile(\n                          volume, blockFile, \"rw\")) {\n               // truncate blockFile\n               blockRAF.setLength(validFileLength);\n             }\n           }\n           return validFileLength;\n         }\n       }\n     } catch (IOException e) {\n       FsDatasetImpl.LOG.warn(\"Getting exception while validating integrity \" +\n               \"and setting length for blockFile\", e);\n       return 0;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      try (DataInputStream checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(\n              fileIoProvider.getFileInputStream(volume, metaFile),\n              ioFileBufferSize))) {\n        // read and handle the common header here. For now just a version\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            checksumIn, metaFile);\n        if (Type.NULL.equals(checksum.getChecksumType())) {\n          // in case of NULL checksum type consider full file as valid\n          return blockFileLen;\n        }\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1) / bytesPerChecksum,\n            (metaFileLen - crcHeaderLen) / checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        try (InputStream blockIn \u003d fileIoProvider.getFileInputStream(\n                 volume, blockFile);\n             ReplicaInputStreams ris \u003d new ReplicaInputStreams(blockIn,\n                 checksumIn, volume.obtainReference(), fileIoProvider)) {\n          ris.skipChecksumFully((numChunks - 1) * checksumSize);\n          long lastChunkStartPos \u003d (numChunks - 1) * bytesPerChecksum;\n          ris.skipDataFully(lastChunkStartPos);\n          int lastChunkSize \u003d (int) Math.min(\n              bytesPerChecksum, blockFileLen - lastChunkStartPos);\n          byte[] buf \u003d new byte[lastChunkSize + checksumSize];\n          ris.readChecksumFully(buf, lastChunkSize, checksumSize);\n          ris.readDataFully(buf, 0, lastChunkSize);\n          checksum.update(buf, 0, lastChunkSize);\n          long validFileLength;\n          if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n            validFileLength \u003d lastChunkStartPos + lastChunkSize;\n          } else { // last chunk is corrupt\n            validFileLength \u003d lastChunkStartPos;\n          }\n          // truncate if extra bytes are present without CRC\n          if (blockFile.length() \u003e validFileLength) {\n            try (RandomAccessFile blockRAF \u003d\n                     fileIoProvider.getRandomAccessFile(\n                         volume, blockFile, \"rw\")) {\n              // truncate blockFile\n              blockRAF.setLength(validFileLength);\n            }\n          }\n          return validFileLength;\n        }\n      }\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(\"Getting exception while validating integrity \" +\n              \"and setting length for blockFile\", e);\n      return 0;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "1a598479a9faec787706bcf924dfbd88a80e1b82": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11992. Replace commons-logging APIs with slf4j in FsDatasetImpl. Contributed by hu xiaodong.\n",
      "commitDate": "20/06/17 7:19 PM",
      "commitName": "1a598479a9faec787706bcf924dfbd88a80e1b82",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "03/04/17 8:13 PM",
      "commitNameOld": "6eba79232f36b36e0196163adc8fe4219a6b6bf9",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 77.96,
      "commitsBetweenForRepo": 410,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,62 @@\n   private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n     try {\n       final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n       long blockFileLen \u003d blockFile.length();\n       long metaFileLen \u003d metaFile.length();\n       int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n       if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n           !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n       }\n       try (DataInputStream checksumIn \u003d new DataInputStream(\n           new BufferedInputStream(\n               fileIoProvider.getFileInputStream(volume, metaFile),\n               ioFileBufferSize))) {\n         // read and handle the common header here. For now just a version\n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             checksumIn, metaFile);\n         int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n         int checksumSize \u003d checksum.getChecksumSize();\n         long numChunks \u003d Math.min(\n             (blockFileLen + bytesPerChecksum - 1) / bytesPerChecksum,\n             (metaFileLen - crcHeaderLen) / checksumSize);\n         if (numChunks \u003d\u003d 0) {\n           return 0;\n         }\n         try (InputStream blockIn \u003d fileIoProvider.getFileInputStream(\n                  volume, blockFile);\n              ReplicaInputStreams ris \u003d new ReplicaInputStreams(blockIn,\n                  checksumIn, volume.obtainReference(), fileIoProvider)) {\n           ris.skipChecksumFully((numChunks - 1) * checksumSize);\n           long lastChunkStartPos \u003d (numChunks - 1) * bytesPerChecksum;\n           ris.skipDataFully(lastChunkStartPos);\n           int lastChunkSize \u003d (int) Math.min(\n               bytesPerChecksum, blockFileLen - lastChunkStartPos);\n           byte[] buf \u003d new byte[lastChunkSize + checksumSize];\n           ris.readChecksumFully(buf, lastChunkSize, checksumSize);\n           ris.readDataFully(buf, 0, lastChunkSize);\n           checksum.update(buf, 0, lastChunkSize);\n           long validFileLength;\n           if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n             validFileLength \u003d lastChunkStartPos + lastChunkSize;\n           } else { // last chunk is corrupt\n             validFileLength \u003d lastChunkStartPos;\n           }\n           // truncate if extra bytes are present without CRC\n           if (blockFile.length() \u003e validFileLength) {\n             try (RandomAccessFile blockRAF \u003d\n                      fileIoProvider.getRandomAccessFile(\n                          volume, blockFile, \"rw\")) {\n               // truncate blockFile\n               blockRAF.setLength(validFileLength);\n             }\n           }\n           return validFileLength;\n         }\n       }\n     } catch (IOException e) {\n-      FsDatasetImpl.LOG.warn(e);\n+      FsDatasetImpl.LOG.warn(\"Getting exception while validating integrity \" +\n+              \"and setting length for blockFile\", e);\n       return 0;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      try (DataInputStream checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(\n              fileIoProvider.getFileInputStream(volume, metaFile),\n              ioFileBufferSize))) {\n        // read and handle the common header here. For now just a version\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            checksumIn, metaFile);\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1) / bytesPerChecksum,\n            (metaFileLen - crcHeaderLen) / checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        try (InputStream blockIn \u003d fileIoProvider.getFileInputStream(\n                 volume, blockFile);\n             ReplicaInputStreams ris \u003d new ReplicaInputStreams(blockIn,\n                 checksumIn, volume.obtainReference(), fileIoProvider)) {\n          ris.skipChecksumFully((numChunks - 1) * checksumSize);\n          long lastChunkStartPos \u003d (numChunks - 1) * bytesPerChecksum;\n          ris.skipDataFully(lastChunkStartPos);\n          int lastChunkSize \u003d (int) Math.min(\n              bytesPerChecksum, blockFileLen - lastChunkStartPos);\n          byte[] buf \u003d new byte[lastChunkSize + checksumSize];\n          ris.readChecksumFully(buf, lastChunkSize, checksumSize);\n          ris.readDataFully(buf, 0, lastChunkSize);\n          checksum.update(buf, 0, lastChunkSize);\n          long validFileLength;\n          if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n            validFileLength \u003d lastChunkStartPos + lastChunkSize;\n          } else { // last chunk is corrupt\n            validFileLength \u003d lastChunkStartPos;\n          }\n          // truncate if extra bytes are present without CRC\n          if (blockFile.length() \u003e validFileLength) {\n            try (RandomAccessFile blockRAF \u003d\n                     fileIoProvider.getRandomAccessFile(\n                         volume, blockFile, \"rw\")) {\n              // truncate blockFile\n              blockRAF.setLength(validFileLength);\n            }\n          }\n          return validFileLength;\n        }\n      }\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(\"Getting exception while validating integrity \" +\n              \"and setting length for blockFile\", e);\n      return 0;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10958. Add instrumentation hooks around Datanode disk IO.\n",
      "commitDate": "14/12/16 11:18 AM",
      "commitName": "6ba9587d370fbf39c129c08c00ebbb894ccc1389",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "06/12/16 11:05 AM",
      "commitNameOld": "df983b524ab68ea0c70cee9033bfff2d28052cbf",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 8.01,
      "commitsBetweenForRepo": 51,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,61 @@\n   private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n     try {\n       final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n       long blockFileLen \u003d blockFile.length();\n       long metaFileLen \u003d metaFile.length();\n       int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n       if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n           !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n       }\n       try (DataInputStream checksumIn \u003d new DataInputStream(\n-          new BufferedInputStream(new FileInputStream(metaFile),\n+          new BufferedInputStream(\n+              fileIoProvider.getFileInputStream(volume, metaFile),\n               ioFileBufferSize))) {\n         // read and handle the common header here. For now just a version\n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             checksumIn, metaFile);\n         int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n         int checksumSize \u003d checksum.getChecksumSize();\n         long numChunks \u003d Math.min(\n             (blockFileLen + bytesPerChecksum - 1) / bytesPerChecksum,\n             (metaFileLen - crcHeaderLen) / checksumSize);\n         if (numChunks \u003d\u003d 0) {\n           return 0;\n         }\n-        try (InputStream blockIn \u003d new FileInputStream(blockFile);\n+        try (InputStream blockIn \u003d fileIoProvider.getFileInputStream(\n+                 volume, blockFile);\n              ReplicaInputStreams ris \u003d new ReplicaInputStreams(blockIn,\n-                 checksumIn, volume.obtainReference())) {\n+                 checksumIn, volume.obtainReference(), fileIoProvider)) {\n           ris.skipChecksumFully((numChunks - 1) * checksumSize);\n           long lastChunkStartPos \u003d (numChunks - 1) * bytesPerChecksum;\n           ris.skipDataFully(lastChunkStartPos);\n           int lastChunkSize \u003d (int) Math.min(\n               bytesPerChecksum, blockFileLen - lastChunkStartPos);\n           byte[] buf \u003d new byte[lastChunkSize + checksumSize];\n           ris.readChecksumFully(buf, lastChunkSize, checksumSize);\n           ris.readDataFully(buf, 0, lastChunkSize);\n           checksum.update(buf, 0, lastChunkSize);\n           long validFileLength;\n           if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n             validFileLength \u003d lastChunkStartPos + lastChunkSize;\n           } else { // last chunk is corrupt\n             validFileLength \u003d lastChunkStartPos;\n           }\n           // truncate if extra bytes are present without CRC\n           if (blockFile.length() \u003e validFileLength) {\n             try (RandomAccessFile blockRAF \u003d\n-                     new RandomAccessFile(blockFile, \"rw\")) {\n+                     fileIoProvider.getRandomAccessFile(\n+                         volume, blockFile, \"rw\")) {\n               // truncate blockFile\n               blockRAF.setLength(validFileLength);\n             }\n           }\n           return validFileLength;\n         }\n       }\n     } catch (IOException e) {\n       FsDatasetImpl.LOG.warn(e);\n       return 0;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      try (DataInputStream checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(\n              fileIoProvider.getFileInputStream(volume, metaFile),\n              ioFileBufferSize))) {\n        // read and handle the common header here. For now just a version\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            checksumIn, metaFile);\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1) / bytesPerChecksum,\n            (metaFileLen - crcHeaderLen) / checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        try (InputStream blockIn \u003d fileIoProvider.getFileInputStream(\n                 volume, blockFile);\n             ReplicaInputStreams ris \u003d new ReplicaInputStreams(blockIn,\n                 checksumIn, volume.obtainReference(), fileIoProvider)) {\n          ris.skipChecksumFully((numChunks - 1) * checksumSize);\n          long lastChunkStartPos \u003d (numChunks - 1) * bytesPerChecksum;\n          ris.skipDataFully(lastChunkStartPos);\n          int lastChunkSize \u003d (int) Math.min(\n              bytesPerChecksum, blockFileLen - lastChunkStartPos);\n          byte[] buf \u003d new byte[lastChunkSize + checksumSize];\n          ris.readChecksumFully(buf, lastChunkSize, checksumSize);\n          ris.readDataFully(buf, 0, lastChunkSize);\n          checksum.update(buf, 0, lastChunkSize);\n          long validFileLength;\n          if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n            validFileLength \u003d lastChunkStartPos + lastChunkSize;\n          } else { // last chunk is corrupt\n            validFileLength \u003d lastChunkStartPos;\n          }\n          // truncate if extra bytes are present without CRC\n          if (blockFile.length() \u003e validFileLength) {\n            try (RandomAccessFile blockRAF \u003d\n                     fileIoProvider.getRandomAccessFile(\n                         volume, blockFile, \"rw\")) {\n              // truncate blockFile\n              blockRAF.setLength(validFileLength);\n            }\n          }\n          return validFileLength;\n        }\n      }\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(e);\n      return 0;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "df983b524ab68ea0c70cee9033bfff2d28052cbf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10930. Refactor: Wrap Datanode IO related operations. Contributed by Xiaoyu Yao.\n",
      "commitDate": "06/12/16 11:05 AM",
      "commitName": "df983b524ab68ea0c70cee9033bfff2d28052cbf",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "05/12/16 12:44 PM",
      "commitNameOld": "dcedb72af468128458e597f08d22f5c34b744ae5",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 0.93,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,58 @@\n   private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n-    DataInputStream checksumIn \u003d null;\n-    InputStream blockIn \u003d null;\n     try {\n       final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n       long blockFileLen \u003d blockFile.length();\n       long metaFileLen \u003d metaFile.length();\n       int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n       if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n           !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n       }\n-      checksumIn \u003d new DataInputStream(\n+      try (DataInputStream checksumIn \u003d new DataInputStream(\n           new BufferedInputStream(new FileInputStream(metaFile),\n-              ioFileBufferSize));\n-\n-      // read and handle the common header here. For now just a version\n-      final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n-          checksumIn, metaFile);\n-      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n-      int checksumSize \u003d checksum.getChecksumSize();\n-      long numChunks \u003d Math.min(\n-          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum,\n-          (metaFileLen - crcHeaderLen)/checksumSize);\n-      if (numChunks \u003d\u003d 0) {\n-        return 0;\n-      }\n-      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n-      blockIn \u003d new FileInputStream(blockFile);\n-      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n-      IOUtils.skipFully(blockIn, lastChunkStartPos);\n-      int lastChunkSize \u003d (int)Math.min(\n-          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n-      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n-      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n-      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n-\n-      checksum.update(buf, 0, lastChunkSize);\n-      long validFileLength;\n-      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n-        validFileLength \u003d lastChunkStartPos + lastChunkSize;\n-      } else { // last chunck is corrupt\n-        validFileLength \u003d lastChunkStartPos;\n-      }\n-\n-      // truncate if extra bytes are present without CRC\n-      if (blockFile.length() \u003e validFileLength) {\n-        RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n-        try {\n-          // truncate blockFile\n-          blockRAF.setLength(validFileLength);\n-        } finally {\n-          blockRAF.close();\n+              ioFileBufferSize))) {\n+        // read and handle the common header here. For now just a version\n+        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n+            checksumIn, metaFile);\n+        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n+        int checksumSize \u003d checksum.getChecksumSize();\n+        long numChunks \u003d Math.min(\n+            (blockFileLen + bytesPerChecksum - 1) / bytesPerChecksum,\n+            (metaFileLen - crcHeaderLen) / checksumSize);\n+        if (numChunks \u003d\u003d 0) {\n+          return 0;\n+        }\n+        try (InputStream blockIn \u003d new FileInputStream(blockFile);\n+             ReplicaInputStreams ris \u003d new ReplicaInputStreams(blockIn,\n+                 checksumIn, volume.obtainReference())) {\n+          ris.skipChecksumFully((numChunks - 1) * checksumSize);\n+          long lastChunkStartPos \u003d (numChunks - 1) * bytesPerChecksum;\n+          ris.skipDataFully(lastChunkStartPos);\n+          int lastChunkSize \u003d (int) Math.min(\n+              bytesPerChecksum, blockFileLen - lastChunkStartPos);\n+          byte[] buf \u003d new byte[lastChunkSize + checksumSize];\n+          ris.readChecksumFully(buf, lastChunkSize, checksumSize);\n+          ris.readDataFully(buf, 0, lastChunkSize);\n+          checksum.update(buf, 0, lastChunkSize);\n+          long validFileLength;\n+          if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n+            validFileLength \u003d lastChunkStartPos + lastChunkSize;\n+          } else { // last chunk is corrupt\n+            validFileLength \u003d lastChunkStartPos;\n+          }\n+          // truncate if extra bytes are present without CRC\n+          if (blockFile.length() \u003e validFileLength) {\n+            try (RandomAccessFile blockRAF \u003d\n+                     new RandomAccessFile(blockFile, \"rw\")) {\n+              // truncate blockFile\n+              blockRAF.setLength(validFileLength);\n+            }\n+          }\n+          return validFileLength;\n         }\n       }\n-\n-      return validFileLength;\n     } catch (IOException e) {\n       FsDatasetImpl.LOG.warn(e);\n       return 0;\n-    } finally {\n-      IOUtils.closeStream(checksumIn);\n-      IOUtils.closeStream(blockIn);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      try (DataInputStream checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(new FileInputStream(metaFile),\n              ioFileBufferSize))) {\n        // read and handle the common header here. For now just a version\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            checksumIn, metaFile);\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1) / bytesPerChecksum,\n            (metaFileLen - crcHeaderLen) / checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        try (InputStream blockIn \u003d new FileInputStream(blockFile);\n             ReplicaInputStreams ris \u003d new ReplicaInputStreams(blockIn,\n                 checksumIn, volume.obtainReference())) {\n          ris.skipChecksumFully((numChunks - 1) * checksumSize);\n          long lastChunkStartPos \u003d (numChunks - 1) * bytesPerChecksum;\n          ris.skipDataFully(lastChunkStartPos);\n          int lastChunkSize \u003d (int) Math.min(\n              bytesPerChecksum, blockFileLen - lastChunkStartPos);\n          byte[] buf \u003d new byte[lastChunkSize + checksumSize];\n          ris.readChecksumFully(buf, lastChunkSize, checksumSize);\n          ris.readDataFully(buf, 0, lastChunkSize);\n          checksum.update(buf, 0, lastChunkSize);\n          long validFileLength;\n          if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n            validFileLength \u003d lastChunkStartPos + lastChunkSize;\n          } else { // last chunk is corrupt\n            validFileLength \u003d lastChunkStartPos;\n          }\n          // truncate if extra bytes are present without CRC\n          if (blockFile.length() \u003e validFileLength) {\n            try (RandomAccessFile blockRAF \u003d\n                     new RandomAccessFile(blockFile, \"rw\")) {\n              // truncate blockFile\n              blockRAF.setLength(validFileLength);\n            }\n          }\n          return validFileLength;\n        }\n      }\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(e);\n      return 0;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "dcedb72af468128458e597f08d22f5c34b744ae5": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HADOOP-10930. Refactor: Wrap Datanode IO related operations. Contributed by Xiaoyu Yao.\"\n\nThis reverts commit aeecfa24f4fb6af289920cbf8830c394e66bd78e.\n",
      "commitDate": "05/12/16 12:44 PM",
      "commitName": "dcedb72af468128458e597f08d22f5c34b744ae5",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "05/12/16 10:54 AM",
      "commitNameOld": "1b5cceaffbdde50a87ede81552dc380832db8e79",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,65 @@\n   private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n-    ReplicaInputStreams ris \u003d null;\n     DataInputStream checksumIn \u003d null;\n     InputStream blockIn \u003d null;\n     try {\n       final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n       long blockFileLen \u003d blockFile.length();\n       long metaFileLen \u003d metaFile.length();\n       int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n       if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n           !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n       }\n       checksumIn \u003d new DataInputStream(\n           new BufferedInputStream(new FileInputStream(metaFile),\n               ioFileBufferSize));\n \n       // read and handle the common header here. For now just a version\n       final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n           checksumIn, metaFile);\n       int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       int checksumSize \u003d checksum.getChecksumSize();\n       long numChunks \u003d Math.min(\n           (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum,\n           (metaFileLen - crcHeaderLen)/checksumSize);\n       if (numChunks \u003d\u003d 0) {\n         return 0;\n       }\n+      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n       blockIn \u003d new FileInputStream(blockFile);\n-      ris \u003d new ReplicaInputStreams(blockIn, checksumIn,\n-          volume.obtainReference());\n-      ris.skipChecksumFully((numChunks-1)*checksumSize);\n       long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n-      ris.skipDataFully(lastChunkStartPos);\n+      IOUtils.skipFully(blockIn, lastChunkStartPos);\n       int lastChunkSize \u003d (int)Math.min(\n           bytesPerChecksum, blockFileLen-lastChunkStartPos);\n       byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n-      ris.readChecksumFully(buf, lastChunkSize, checksumSize);\n-      ris.readDataFully(buf, 0, lastChunkSize);\n+      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n+      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n+\n       checksum.update(buf, 0, lastChunkSize);\n       long validFileLength;\n       if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n         validFileLength \u003d lastChunkStartPos + lastChunkSize;\n-      } else { // last chunk is corrupt\n+      } else { // last chunck is corrupt\n         validFileLength \u003d lastChunkStartPos;\n       }\n \n       // truncate if extra bytes are present without CRC\n       if (blockFile.length() \u003e validFileLength) {\n         RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n         try {\n           // truncate blockFile\n           blockRAF.setLength(validFileLength);\n         } finally {\n           blockRAF.close();\n         }\n       }\n \n       return validFileLength;\n     } catch (IOException e) {\n       FsDatasetImpl.LOG.warn(e);\n       return 0;\n     } finally {\n-      if (ris !\u003d null) {\n-        ris.close();\n-      } else {\n-        IOUtils.closeStream(checksumIn);\n-        IOUtils.closeStream(blockIn);\n-      }\n+      IOUtils.closeStream(checksumIn);\n+      IOUtils.closeStream(blockIn);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n    DataInputStream checksumIn \u003d null;\n    InputStream blockIn \u003d null;\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(new FileInputStream(metaFile),\n              ioFileBufferSize));\n\n      // read and handle the common header here. For now just a version\n      final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n          checksumIn, metaFile);\n      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      int checksumSize \u003d checksum.getChecksumSize();\n      long numChunks \u003d Math.min(\n          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum,\n          (metaFileLen - crcHeaderLen)/checksumSize);\n      if (numChunks \u003d\u003d 0) {\n        return 0;\n      }\n      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n      blockIn \u003d new FileInputStream(blockFile);\n      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n      IOUtils.skipFully(blockIn, lastChunkStartPos);\n      int lastChunkSize \u003d (int)Math.min(\n          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n      checksum.update(buf, 0, lastChunkSize);\n      long validFileLength;\n      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n        validFileLength \u003d lastChunkStartPos + lastChunkSize;\n      } else { // last chunck is corrupt\n        validFileLength \u003d lastChunkStartPos;\n      }\n\n      // truncate if extra bytes are present without CRC\n      if (blockFile.length() \u003e validFileLength) {\n        RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n        try {\n          // truncate blockFile\n          blockRAF.setLength(validFileLength);\n        } finally {\n          blockRAF.close();\n        }\n      }\n\n      return validFileLength;\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(e);\n      return 0;\n    } finally {\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(blockIn);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "aeecfa24f4fb6af289920cbf8830c394e66bd78e": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10930. Refactor: Wrap Datanode IO related operations. Contributed by Xiaoyu Yao.\n",
      "commitDate": "29/11/16 8:52 PM",
      "commitName": "aeecfa24f4fb6af289920cbf8830c394e66bd78e",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "27/09/16 10:02 AM",
      "commitNameOld": "8ae4729107d33c6001cf1fdc8837afb71ea6c0d3",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 63.49,
      "commitsBetweenForRepo": 518,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,71 @@\n   private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n+    ReplicaInputStreams ris \u003d null;\n     DataInputStream checksumIn \u003d null;\n     InputStream blockIn \u003d null;\n     try {\n       final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n       long blockFileLen \u003d blockFile.length();\n       long metaFileLen \u003d metaFile.length();\n       int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n       if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n           !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n       }\n       checksumIn \u003d new DataInputStream(\n           new BufferedInputStream(new FileInputStream(metaFile),\n               ioFileBufferSize));\n \n       // read and handle the common header here. For now just a version\n       final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n           checksumIn, metaFile);\n       int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       int checksumSize \u003d checksum.getChecksumSize();\n       long numChunks \u003d Math.min(\n           (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum,\n           (metaFileLen - crcHeaderLen)/checksumSize);\n       if (numChunks \u003d\u003d 0) {\n         return 0;\n       }\n-      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n       blockIn \u003d new FileInputStream(blockFile);\n+      ris \u003d new ReplicaInputStreams(blockIn, checksumIn,\n+          volume.obtainReference());\n+      ris.skipChecksumFully((numChunks-1)*checksumSize);\n       long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n-      IOUtils.skipFully(blockIn, lastChunkStartPos);\n+      ris.skipDataFully(lastChunkStartPos);\n       int lastChunkSize \u003d (int)Math.min(\n           bytesPerChecksum, blockFileLen-lastChunkStartPos);\n       byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n-      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n-      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n-\n+      ris.readChecksumFully(buf, lastChunkSize, checksumSize);\n+      ris.readDataFully(buf, 0, lastChunkSize);\n       checksum.update(buf, 0, lastChunkSize);\n       long validFileLength;\n       if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n         validFileLength \u003d lastChunkStartPos + lastChunkSize;\n-      } else { // last chunck is corrupt\n+      } else { // last chunk is corrupt\n         validFileLength \u003d lastChunkStartPos;\n       }\n \n       // truncate if extra bytes are present without CRC\n       if (blockFile.length() \u003e validFileLength) {\n         RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n         try {\n           // truncate blockFile\n           blockRAF.setLength(validFileLength);\n         } finally {\n           blockRAF.close();\n         }\n       }\n \n       return validFileLength;\n     } catch (IOException e) {\n       FsDatasetImpl.LOG.warn(e);\n       return 0;\n     } finally {\n-      IOUtils.closeStream(checksumIn);\n-      IOUtils.closeStream(blockIn);\n+      if (ris !\u003d null) {\n+        ris.close();\n+      } else {\n+        IOUtils.closeStream(checksumIn);\n+        IOUtils.closeStream(blockIn);\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n    ReplicaInputStreams ris \u003d null;\n    DataInputStream checksumIn \u003d null;\n    InputStream blockIn \u003d null;\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(new FileInputStream(metaFile),\n              ioFileBufferSize));\n\n      // read and handle the common header here. For now just a version\n      final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n          checksumIn, metaFile);\n      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      int checksumSize \u003d checksum.getChecksumSize();\n      long numChunks \u003d Math.min(\n          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum,\n          (metaFileLen - crcHeaderLen)/checksumSize);\n      if (numChunks \u003d\u003d 0) {\n        return 0;\n      }\n      blockIn \u003d new FileInputStream(blockFile);\n      ris \u003d new ReplicaInputStreams(blockIn, checksumIn,\n          volume.obtainReference());\n      ris.skipChecksumFully((numChunks-1)*checksumSize);\n      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n      ris.skipDataFully(lastChunkStartPos);\n      int lastChunkSize \u003d (int)Math.min(\n          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n      ris.readChecksumFully(buf, lastChunkSize, checksumSize);\n      ris.readDataFully(buf, 0, lastChunkSize);\n      checksum.update(buf, 0, lastChunkSize);\n      long validFileLength;\n      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n        validFileLength \u003d lastChunkStartPos + lastChunkSize;\n      } else { // last chunk is corrupt\n        validFileLength \u003d lastChunkStartPos;\n      }\n\n      // truncate if extra bytes are present without CRC\n      if (blockFile.length() \u003e validFileLength) {\n        RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n        try {\n          // truncate blockFile\n          blockRAF.setLength(validFileLength);\n        } finally {\n          blockRAF.close();\n        }\n      }\n\n      return validFileLength;\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(e);\n      return 0;\n    } finally {\n      if (ris !\u003d null) {\n        ris.close();\n      } else {\n        IOUtils.closeStream(checksumIn);\n        IOUtils.closeStream(blockIn);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "4da8490b512a33a255ed27309860859388d7c168": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
      "commitDate": "05/05/15 3:41 PM",
      "commitName": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.23,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,65 @@\n   private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n     DataInputStream checksumIn \u003d null;\n     InputStream blockIn \u003d null;\n     try {\n       final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n       long blockFileLen \u003d blockFile.length();\n       long metaFileLen \u003d metaFile.length();\n       int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n       if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n           !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n       }\n       checksumIn \u003d new DataInputStream(\n           new BufferedInputStream(new FileInputStream(metaFile),\n-              HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n+              ioFileBufferSize));\n \n       // read and handle the common header here. For now just a version\n       final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n           checksumIn, metaFile);\n       int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       int checksumSize \u003d checksum.getChecksumSize();\n       long numChunks \u003d Math.min(\n           (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n           (metaFileLen - crcHeaderLen)/checksumSize);\n       if (numChunks \u003d\u003d 0) {\n         return 0;\n       }\n       IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n       blockIn \u003d new FileInputStream(blockFile);\n       long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n       IOUtils.skipFully(blockIn, lastChunkStartPos);\n       int lastChunkSize \u003d (int)Math.min(\n           bytesPerChecksum, blockFileLen-lastChunkStartPos);\n       byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n       checksumIn.readFully(buf, lastChunkSize, checksumSize);\n       IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n \n       checksum.update(buf, 0, lastChunkSize);\n       long validFileLength;\n       if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n         validFileLength \u003d lastChunkStartPos + lastChunkSize;\n       } else { // last chunck is corrupt\n         validFileLength \u003d lastChunkStartPos;\n       }\n \n       // truncate if extra bytes are present without CRC\n       if (blockFile.length() \u003e validFileLength) {\n         RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n         try {\n           // truncate blockFile\n           blockRAF.setLength(validFileLength);\n         } finally {\n           blockRAF.close();\n         }\n       }\n \n       return validFileLength;\n     } catch (IOException e) {\n       FsDatasetImpl.LOG.warn(e);\n       return 0;\n     } finally {\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(blockIn);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n    DataInputStream checksumIn \u003d null;\n    InputStream blockIn \u003d null;\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(new FileInputStream(metaFile),\n              ioFileBufferSize));\n\n      // read and handle the common header here. For now just a version\n      final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n          checksumIn, metaFile);\n      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      int checksumSize \u003d checksum.getChecksumSize();\n      long numChunks \u003d Math.min(\n          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n          (metaFileLen - crcHeaderLen)/checksumSize);\n      if (numChunks \u003d\u003d 0) {\n        return 0;\n      }\n      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n      blockIn \u003d new FileInputStream(blockFile);\n      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n      IOUtils.skipFully(blockIn, lastChunkStartPos);\n      int lastChunkSize \u003d (int)Math.min(\n          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n      checksum.update(buf, 0, lastChunkSize);\n      long validFileLength;\n      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n        validFileLength \u003d lastChunkStartPos + lastChunkSize;\n      } else { // last chunck is corrupt\n        validFileLength \u003d lastChunkStartPos;\n      }\n\n      // truncate if extra bytes are present without CRC\n      if (blockFile.length() \u003e validFileLength) {\n        RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n        try {\n          // truncate blockFile\n          blockRAF.setLength(validFileLength);\n        } finally {\n          blockRAF.close();\n        }\n      }\n\n      return validFileLength;\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(e);\n      return 0;\n    } finally {\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(blockIn);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "25/03/15 12:42 PM",
      "commitNameOld": "fc1031af749435dc95efea6745b1b2300ce29446",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 37.89,
      "commitsBetweenForRepo": 321,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,65 @@\n   private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n     DataInputStream checksumIn \u003d null;\n     InputStream blockIn \u003d null;\n     try {\n       final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n       long blockFileLen \u003d blockFile.length();\n       long metaFileLen \u003d metaFile.length();\n       int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n       if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n           !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n       }\n       checksumIn \u003d new DataInputStream(\n           new BufferedInputStream(new FileInputStream(metaFile),\n-              HdfsConstants.IO_FILE_BUFFER_SIZE));\n+              HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n \n       // read and handle the common header here. For now just a version\n       final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n           checksumIn, metaFile);\n       int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       int checksumSize \u003d checksum.getChecksumSize();\n       long numChunks \u003d Math.min(\n           (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n           (metaFileLen - crcHeaderLen)/checksumSize);\n       if (numChunks \u003d\u003d 0) {\n         return 0;\n       }\n       IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n       blockIn \u003d new FileInputStream(blockFile);\n       long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n       IOUtils.skipFully(blockIn, lastChunkStartPos);\n       int lastChunkSize \u003d (int)Math.min(\n           bytesPerChecksum, blockFileLen-lastChunkStartPos);\n       byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n       checksumIn.readFully(buf, lastChunkSize, checksumSize);\n       IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n \n       checksum.update(buf, 0, lastChunkSize);\n       long validFileLength;\n       if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n         validFileLength \u003d lastChunkStartPos + lastChunkSize;\n       } else { // last chunck is corrupt\n         validFileLength \u003d lastChunkStartPos;\n       }\n \n       // truncate if extra bytes are present without CRC\n       if (blockFile.length() \u003e validFileLength) {\n         RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n         try {\n           // truncate blockFile\n           blockRAF.setLength(validFileLength);\n         } finally {\n           blockRAF.close();\n         }\n       }\n \n       return validFileLength;\n     } catch (IOException e) {\n       FsDatasetImpl.LOG.warn(e);\n       return 0;\n     } finally {\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(blockIn);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n    DataInputStream checksumIn \u003d null;\n    InputStream blockIn \u003d null;\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(new FileInputStream(metaFile),\n              HdfsServerConstants.IO_FILE_BUFFER_SIZE));\n\n      // read and handle the common header here. For now just a version\n      final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n          checksumIn, metaFile);\n      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      int checksumSize \u003d checksum.getChecksumSize();\n      long numChunks \u003d Math.min(\n          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n          (metaFileLen - crcHeaderLen)/checksumSize);\n      if (numChunks \u003d\u003d 0) {\n        return 0;\n      }\n      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n      blockIn \u003d new FileInputStream(blockFile);\n      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n      IOUtils.skipFully(blockIn, lastChunkStartPos);\n      int lastChunkSize \u003d (int)Math.min(\n          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n      checksum.update(buf, 0, lastChunkSize);\n      long validFileLength;\n      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n        validFileLength \u003d lastChunkStartPos + lastChunkSize;\n      } else { // last chunck is corrupt\n        validFileLength \u003d lastChunkStartPos;\n      }\n\n      // truncate if extra bytes are present without CRC\n      if (blockFile.length() \u003e validFileLength) {\n        RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n        try {\n          // truncate blockFile\n          blockRAF.setLength(validFileLength);\n        } finally {\n          blockRAF.close();\n        }\n      }\n\n      return validFileLength;\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(e);\n      return 0;\n    } finally {\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(blockIn);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "463aec11718e47d4aabb86a7a539cb973460aae6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
      "commitDate": "27/10/14 9:38 AM",
      "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthor": "cnauroth",
      "commitDateOld": "07/10/14 8:25 PM",
      "commitNameOld": "1efd9c98258fbb973d2058dcf0850042e53bd02f",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 19.55,
      "commitsBetweenForRepo": 152,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,65 @@\n   private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n     DataInputStream checksumIn \u003d null;\n     InputStream blockIn \u003d null;\n     try {\n       final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n       long blockFileLen \u003d blockFile.length();\n       long metaFileLen \u003d metaFile.length();\n       int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n       if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n           !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n       }\n       checksumIn \u003d new DataInputStream(\n           new BufferedInputStream(new FileInputStream(metaFile),\n               HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n       // read and handle the common header here. For now just a version\n-      BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n-      short version \u003d header.getVersion();\n-      if (version !\u003d BlockMetadataHeader.VERSION) {\n-        FsDatasetImpl.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n-            + metaFile + \" ignoring ...\");\n-      }\n-      DataChecksum checksum \u003d header.getChecksum();\n+      final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n+          checksumIn, metaFile);\n       int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       int checksumSize \u003d checksum.getChecksumSize();\n       long numChunks \u003d Math.min(\n           (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n           (metaFileLen - crcHeaderLen)/checksumSize);\n       if (numChunks \u003d\u003d 0) {\n         return 0;\n       }\n       IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n       blockIn \u003d new FileInputStream(blockFile);\n       long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n       IOUtils.skipFully(blockIn, lastChunkStartPos);\n       int lastChunkSize \u003d (int)Math.min(\n           bytesPerChecksum, blockFileLen-lastChunkStartPos);\n       byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n       checksumIn.readFully(buf, lastChunkSize, checksumSize);\n       IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n \n       checksum.update(buf, 0, lastChunkSize);\n       long validFileLength;\n       if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n         validFileLength \u003d lastChunkStartPos + lastChunkSize;\n       } else { // last chunck is corrupt\n         validFileLength \u003d lastChunkStartPos;\n       }\n \n       // truncate if extra bytes are present without CRC\n       if (blockFile.length() \u003e validFileLength) {\n         RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n         try {\n           // truncate blockFile\n           blockRAF.setLength(validFileLength);\n         } finally {\n           blockRAF.close();\n         }\n       }\n \n       return validFileLength;\n     } catch (IOException e) {\n       FsDatasetImpl.LOG.warn(e);\n       return 0;\n     } finally {\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(blockIn);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n    DataInputStream checksumIn \u003d null;\n    InputStream blockIn \u003d null;\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(new FileInputStream(metaFile),\n              HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n      // read and handle the common header here. For now just a version\n      final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n          checksumIn, metaFile);\n      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      int checksumSize \u003d checksum.getChecksumSize();\n      long numChunks \u003d Math.min(\n          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n          (metaFileLen - crcHeaderLen)/checksumSize);\n      if (numChunks \u003d\u003d 0) {\n        return 0;\n      }\n      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n      blockIn \u003d new FileInputStream(blockFile);\n      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n      IOUtils.skipFully(blockIn, lastChunkStartPos);\n      int lastChunkSize \u003d (int)Math.min(\n          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n      checksum.update(buf, 0, lastChunkSize);\n      long validFileLength;\n      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n        validFileLength \u003d lastChunkStartPos + lastChunkSize;\n      } else { // last chunck is corrupt\n        validFileLength \u003d lastChunkStartPos;\n      }\n\n      // truncate if extra bytes are present without CRC\n      if (blockFile.length() \u003e validFileLength) {\n        RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n        try {\n          // truncate blockFile\n          blockRAF.setLength(validFileLength);\n        } finally {\n          blockRAF.close();\n        }\n      }\n\n      return validFileLength;\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(e);\n      return 0;\n    } finally {\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(blockIn);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "dd1bc7e1c7e0712a690034f044ab4cf5eaf98ca6": {
      "type": "Ymultichange(Yrename,Ybodychange)",
      "commitMessage": "HDFS-5728. Block recovery will fail if the metafile does not have crc for all chunks of the block. Contributed by Vinay.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1561223 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/01/14 2:56 PM",
      "commitName": "dd1bc7e1c7e0712a690034f044ab4cf5eaf98ca6",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-5728. Block recovery will fail if the metafile does not have crc for all chunks of the block. Contributed by Vinay.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1561223 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "24/01/14 2:56 PM",
          "commitName": "dd1bc7e1c7e0712a690034f044ab4cf5eaf98ca6",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "02/04/12 10:38 AM",
          "commitNameOld": "bc13dfb1426944ce45293cb8f444239a7406762c",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 662.22,
          "commitsBetweenForRepo": 3926,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,56 +1,70 @@\n-  private long validateIntegrity(File blockFile, long genStamp) {\n+  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n     DataInputStream checksumIn \u003d null;\n     InputStream blockIn \u003d null;\n     try {\n       final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n       long blockFileLen \u003d blockFile.length();\n       long metaFileLen \u003d metaFile.length();\n       int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n       if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n           !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n       }\n       checksumIn \u003d new DataInputStream(\n           new BufferedInputStream(new FileInputStream(metaFile),\n               HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n       // read and handle the common header here. For now just a version\n       BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       short version \u003d header.getVersion();\n       if (version !\u003d BlockMetadataHeader.VERSION) {\n         FsDatasetImpl.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n             + metaFile + \" ignoring ...\");\n       }\n       DataChecksum checksum \u003d header.getChecksum();\n       int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       int checksumSize \u003d checksum.getChecksumSize();\n       long numChunks \u003d Math.min(\n           (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n           (metaFileLen - crcHeaderLen)/checksumSize);\n       if (numChunks \u003d\u003d 0) {\n         return 0;\n       }\n       IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n       blockIn \u003d new FileInputStream(blockFile);\n       long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n       IOUtils.skipFully(blockIn, lastChunkStartPos);\n       int lastChunkSize \u003d (int)Math.min(\n           bytesPerChecksum, blockFileLen-lastChunkStartPos);\n       byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n       checksumIn.readFully(buf, lastChunkSize, checksumSize);\n       IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n \n       checksum.update(buf, 0, lastChunkSize);\n+      long validFileLength;\n       if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n-        return lastChunkStartPos + lastChunkSize;\n+        validFileLength \u003d lastChunkStartPos + lastChunkSize;\n       } else { // last chunck is corrupt\n-        return lastChunkStartPos;\n+        validFileLength \u003d lastChunkStartPos;\n       }\n+\n+      // truncate if extra bytes are present without CRC\n+      if (blockFile.length() \u003e validFileLength) {\n+        RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n+        try {\n+          // truncate blockFile\n+          blockRAF.setLength(validFileLength);\n+        } finally {\n+          blockRAF.close();\n+        }\n+      }\n+\n+      return validFileLength;\n     } catch (IOException e) {\n       FsDatasetImpl.LOG.warn(e);\n       return 0;\n     } finally {\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(blockIn);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n    DataInputStream checksumIn \u003d null;\n    InputStream blockIn \u003d null;\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(new FileInputStream(metaFile),\n              HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n      // read and handle the common header here. For now just a version\n      BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      short version \u003d header.getVersion();\n      if (version !\u003d BlockMetadataHeader.VERSION) {\n        FsDatasetImpl.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n            + metaFile + \" ignoring ...\");\n      }\n      DataChecksum checksum \u003d header.getChecksum();\n      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      int checksumSize \u003d checksum.getChecksumSize();\n      long numChunks \u003d Math.min(\n          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n          (metaFileLen - crcHeaderLen)/checksumSize);\n      if (numChunks \u003d\u003d 0) {\n        return 0;\n      }\n      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n      blockIn \u003d new FileInputStream(blockFile);\n      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n      IOUtils.skipFully(blockIn, lastChunkStartPos);\n      int lastChunkSize \u003d (int)Math.min(\n          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n      checksum.update(buf, 0, lastChunkSize);\n      long validFileLength;\n      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n        validFileLength \u003d lastChunkStartPos + lastChunkSize;\n      } else { // last chunck is corrupt\n        validFileLength \u003d lastChunkStartPos;\n      }\n\n      // truncate if extra bytes are present without CRC\n      if (blockFile.length() \u003e validFileLength) {\n        RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n        try {\n          // truncate blockFile\n          blockRAF.setLength(validFileLength);\n        } finally {\n          blockRAF.close();\n        }\n      }\n\n      return validFileLength;\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(e);\n      return 0;\n    } finally {\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(blockIn);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
          "extendedDetails": {
            "oldValue": "validateIntegrity",
            "newValue": "validateIntegrityAndSetLength"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5728. Block recovery will fail if the metafile does not have crc for all chunks of the block. Contributed by Vinay.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1561223 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "24/01/14 2:56 PM",
          "commitName": "dd1bc7e1c7e0712a690034f044ab4cf5eaf98ca6",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "02/04/12 10:38 AM",
          "commitNameOld": "bc13dfb1426944ce45293cb8f444239a7406762c",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 662.22,
          "commitsBetweenForRepo": 3926,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,56 +1,70 @@\n-  private long validateIntegrity(File blockFile, long genStamp) {\n+  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n     DataInputStream checksumIn \u003d null;\n     InputStream blockIn \u003d null;\n     try {\n       final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n       long blockFileLen \u003d blockFile.length();\n       long metaFileLen \u003d metaFile.length();\n       int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n       if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n           !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n       }\n       checksumIn \u003d new DataInputStream(\n           new BufferedInputStream(new FileInputStream(metaFile),\n               HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n       // read and handle the common header here. For now just a version\n       BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n       short version \u003d header.getVersion();\n       if (version !\u003d BlockMetadataHeader.VERSION) {\n         FsDatasetImpl.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n             + metaFile + \" ignoring ...\");\n       }\n       DataChecksum checksum \u003d header.getChecksum();\n       int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n       int checksumSize \u003d checksum.getChecksumSize();\n       long numChunks \u003d Math.min(\n           (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n           (metaFileLen - crcHeaderLen)/checksumSize);\n       if (numChunks \u003d\u003d 0) {\n         return 0;\n       }\n       IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n       blockIn \u003d new FileInputStream(blockFile);\n       long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n       IOUtils.skipFully(blockIn, lastChunkStartPos);\n       int lastChunkSize \u003d (int)Math.min(\n           bytesPerChecksum, blockFileLen-lastChunkStartPos);\n       byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n       checksumIn.readFully(buf, lastChunkSize, checksumSize);\n       IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n \n       checksum.update(buf, 0, lastChunkSize);\n+      long validFileLength;\n       if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n-        return lastChunkStartPos + lastChunkSize;\n+        validFileLength \u003d lastChunkStartPos + lastChunkSize;\n       } else { // last chunck is corrupt\n-        return lastChunkStartPos;\n+        validFileLength \u003d lastChunkStartPos;\n       }\n+\n+      // truncate if extra bytes are present without CRC\n+      if (blockFile.length() \u003e validFileLength) {\n+        RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n+        try {\n+          // truncate blockFile\n+          blockRAF.setLength(validFileLength);\n+        } finally {\n+          blockRAF.close();\n+        }\n+      }\n+\n+      return validFileLength;\n     } catch (IOException e) {\n       FsDatasetImpl.LOG.warn(e);\n       return 0;\n     } finally {\n       IOUtils.closeStream(checksumIn);\n       IOUtils.closeStream(blockIn);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long validateIntegrityAndSetLength(File blockFile, long genStamp) {\n    DataInputStream checksumIn \u003d null;\n    InputStream blockIn \u003d null;\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(new FileInputStream(metaFile),\n              HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n      // read and handle the common header here. For now just a version\n      BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      short version \u003d header.getVersion();\n      if (version !\u003d BlockMetadataHeader.VERSION) {\n        FsDatasetImpl.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n            + metaFile + \" ignoring ...\");\n      }\n      DataChecksum checksum \u003d header.getChecksum();\n      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      int checksumSize \u003d checksum.getChecksumSize();\n      long numChunks \u003d Math.min(\n          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n          (metaFileLen - crcHeaderLen)/checksumSize);\n      if (numChunks \u003d\u003d 0) {\n        return 0;\n      }\n      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n      blockIn \u003d new FileInputStream(blockFile);\n      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n      IOUtils.skipFully(blockIn, lastChunkStartPos);\n      int lastChunkSize \u003d (int)Math.min(\n          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n      checksum.update(buf, 0, lastChunkSize);\n      long validFileLength;\n      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n        validFileLength \u003d lastChunkStartPos + lastChunkSize;\n      } else { // last chunck is corrupt\n        validFileLength \u003d lastChunkStartPos;\n      }\n\n      // truncate if extra bytes are present without CRC\n      if (blockFile.length() \u003e validFileLength) {\n        RandomAccessFile blockRAF \u003d new RandomAccessFile(blockFile, \"rw\");\n        try {\n          // truncate blockFile\n          blockRAF.setLength(validFileLength);\n        } finally {\n          blockRAF.close();\n        }\n      }\n\n      return validFileLength;\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(e);\n      return 0;\n    } finally {\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(blockIn);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
          "extendedDetails": {}
        }
      ]
    },
    "bc13dfb1426944ce45293cb8f444239a7406762c": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/04/12 10:38 AM",
      "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "02/04/12 10:38 AM",
          "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/04/12 8:48 PM",
          "commitNameOld": "a4ccb8f504e79802f1b3c69acbcbb00b2343c529",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.58,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,56 +1,56 @@\n-    private long validateIntegrity(File blockFile, long genStamp) {\n-      DataInputStream checksumIn \u003d null;\n-      InputStream blockIn \u003d null;\n-      try {\n-        final File metaFile \u003d DatanodeUtil.getMetaFile(blockFile, genStamp);\n-        long blockFileLen \u003d blockFile.length();\n-        long metaFileLen \u003d metaFile.length();\n-        int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n-        if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n-            !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n-          return 0;\n-        }\n-        checksumIn \u003d new DataInputStream(\n-            new BufferedInputStream(new FileInputStream(metaFile),\n-                HdfsConstants.IO_FILE_BUFFER_SIZE));\n-\n-        // read and handle the common header here. For now just a version\n-        BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n-        short version \u003d header.getVersion();\n-        if (version !\u003d BlockMetadataHeader.VERSION) {\n-          DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n-              + metaFile + \" ignoring ...\");\n-        }\n-        DataChecksum checksum \u003d header.getChecksum();\n-        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n-        int checksumSize \u003d checksum.getChecksumSize();\n-        long numChunks \u003d Math.min(\n-            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n-            (metaFileLen - crcHeaderLen)/checksumSize);\n-        if (numChunks \u003d\u003d 0) {\n-          return 0;\n-        }\n-        IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n-        blockIn \u003d new FileInputStream(blockFile);\n-        long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n-        IOUtils.skipFully(blockIn, lastChunkStartPos);\n-        int lastChunkSize \u003d (int)Math.min(\n-            bytesPerChecksum, blockFileLen-lastChunkStartPos);\n-        byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n-        checksumIn.readFully(buf, lastChunkSize, checksumSize);\n-        IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n-\n-        checksum.update(buf, 0, lastChunkSize);\n-        if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n-          return lastChunkStartPos + lastChunkSize;\n-        } else { // last chunck is corrupt\n-          return lastChunkStartPos;\n-        }\n-      } catch (IOException e) {\n-        DataNode.LOG.warn(e);\n+  private long validateIntegrity(File blockFile, long genStamp) {\n+    DataInputStream checksumIn \u003d null;\n+    InputStream blockIn \u003d null;\n+    try {\n+      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n+      long blockFileLen \u003d blockFile.length();\n+      long metaFileLen \u003d metaFile.length();\n+      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n+      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n+          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n-      } finally {\n-        IOUtils.closeStream(checksumIn);\n-        IOUtils.closeStream(blockIn);\n       }\n-    }\n\\ No newline at end of file\n+      checksumIn \u003d new DataInputStream(\n+          new BufferedInputStream(new FileInputStream(metaFile),\n+              HdfsConstants.IO_FILE_BUFFER_SIZE));\n+\n+      // read and handle the common header here. For now just a version\n+      BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n+      short version \u003d header.getVersion();\n+      if (version !\u003d BlockMetadataHeader.VERSION) {\n+        FsDatasetImpl.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n+            + metaFile + \" ignoring ...\");\n+      }\n+      DataChecksum checksum \u003d header.getChecksum();\n+      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n+      int checksumSize \u003d checksum.getChecksumSize();\n+      long numChunks \u003d Math.min(\n+          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n+          (metaFileLen - crcHeaderLen)/checksumSize);\n+      if (numChunks \u003d\u003d 0) {\n+        return 0;\n+      }\n+      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n+      blockIn \u003d new FileInputStream(blockFile);\n+      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n+      IOUtils.skipFully(blockIn, lastChunkStartPos);\n+      int lastChunkSize \u003d (int)Math.min(\n+          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n+      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n+      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n+      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n+\n+      checksum.update(buf, 0, lastChunkSize);\n+      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n+        return lastChunkStartPos + lastChunkSize;\n+      } else { // last chunck is corrupt\n+        return lastChunkStartPos;\n+      }\n+    } catch (IOException e) {\n+      FsDatasetImpl.LOG.warn(e);\n+      return 0;\n+    } finally {\n+      IOUtils.closeStream(checksumIn);\n+      IOUtils.closeStream(blockIn);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private long validateIntegrity(File blockFile, long genStamp) {\n    DataInputStream checksumIn \u003d null;\n    InputStream blockIn \u003d null;\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(new FileInputStream(metaFile),\n              HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n      // read and handle the common header here. For now just a version\n      BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      short version \u003d header.getVersion();\n      if (version !\u003d BlockMetadataHeader.VERSION) {\n        FsDatasetImpl.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n            + metaFile + \" ignoring ...\");\n      }\n      DataChecksum checksum \u003d header.getChecksum();\n      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      int checksumSize \u003d checksum.getChecksumSize();\n      long numChunks \u003d Math.min(\n          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n          (metaFileLen - crcHeaderLen)/checksumSize);\n      if (numChunks \u003d\u003d 0) {\n        return 0;\n      }\n      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n      blockIn \u003d new FileInputStream(blockFile);\n      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n      IOUtils.skipFully(blockIn, lastChunkStartPos);\n      int lastChunkSize \u003d (int)Math.min(\n          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n      checksum.update(buf, 0, lastChunkSize);\n      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n        return lastChunkStartPos + lastChunkSize;\n      } else { // last chunck is corrupt\n        return lastChunkStartPos;\n      }\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(e);\n      return 0;\n    } finally {\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(blockIn);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
            "oldMethodName": "validateIntegrity",
            "newMethodName": "validateIntegrity"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "02/04/12 10:38 AM",
          "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/04/12 8:48 PM",
          "commitNameOld": "a4ccb8f504e79802f1b3c69acbcbb00b2343c529",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.58,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,56 +1,56 @@\n-    private long validateIntegrity(File blockFile, long genStamp) {\n-      DataInputStream checksumIn \u003d null;\n-      InputStream blockIn \u003d null;\n-      try {\n-        final File metaFile \u003d DatanodeUtil.getMetaFile(blockFile, genStamp);\n-        long blockFileLen \u003d blockFile.length();\n-        long metaFileLen \u003d metaFile.length();\n-        int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n-        if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n-            !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n-          return 0;\n-        }\n-        checksumIn \u003d new DataInputStream(\n-            new BufferedInputStream(new FileInputStream(metaFile),\n-                HdfsConstants.IO_FILE_BUFFER_SIZE));\n-\n-        // read and handle the common header here. For now just a version\n-        BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n-        short version \u003d header.getVersion();\n-        if (version !\u003d BlockMetadataHeader.VERSION) {\n-          DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n-              + metaFile + \" ignoring ...\");\n-        }\n-        DataChecksum checksum \u003d header.getChecksum();\n-        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n-        int checksumSize \u003d checksum.getChecksumSize();\n-        long numChunks \u003d Math.min(\n-            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n-            (metaFileLen - crcHeaderLen)/checksumSize);\n-        if (numChunks \u003d\u003d 0) {\n-          return 0;\n-        }\n-        IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n-        blockIn \u003d new FileInputStream(blockFile);\n-        long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n-        IOUtils.skipFully(blockIn, lastChunkStartPos);\n-        int lastChunkSize \u003d (int)Math.min(\n-            bytesPerChecksum, blockFileLen-lastChunkStartPos);\n-        byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n-        checksumIn.readFully(buf, lastChunkSize, checksumSize);\n-        IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n-\n-        checksum.update(buf, 0, lastChunkSize);\n-        if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n-          return lastChunkStartPos + lastChunkSize;\n-        } else { // last chunck is corrupt\n-          return lastChunkStartPos;\n-        }\n-      } catch (IOException e) {\n-        DataNode.LOG.warn(e);\n+  private long validateIntegrity(File blockFile, long genStamp) {\n+    DataInputStream checksumIn \u003d null;\n+    InputStream blockIn \u003d null;\n+    try {\n+      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n+      long blockFileLen \u003d blockFile.length();\n+      long metaFileLen \u003d metaFile.length();\n+      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n+      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n+          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n         return 0;\n-      } finally {\n-        IOUtils.closeStream(checksumIn);\n-        IOUtils.closeStream(blockIn);\n       }\n-    }\n\\ No newline at end of file\n+      checksumIn \u003d new DataInputStream(\n+          new BufferedInputStream(new FileInputStream(metaFile),\n+              HdfsConstants.IO_FILE_BUFFER_SIZE));\n+\n+      // read and handle the common header here. For now just a version\n+      BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n+      short version \u003d header.getVersion();\n+      if (version !\u003d BlockMetadataHeader.VERSION) {\n+        FsDatasetImpl.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n+            + metaFile + \" ignoring ...\");\n+      }\n+      DataChecksum checksum \u003d header.getChecksum();\n+      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n+      int checksumSize \u003d checksum.getChecksumSize();\n+      long numChunks \u003d Math.min(\n+          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n+          (metaFileLen - crcHeaderLen)/checksumSize);\n+      if (numChunks \u003d\u003d 0) {\n+        return 0;\n+      }\n+      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n+      blockIn \u003d new FileInputStream(blockFile);\n+      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n+      IOUtils.skipFully(blockIn, lastChunkStartPos);\n+      int lastChunkSize \u003d (int)Math.min(\n+          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n+      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n+      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n+      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n+\n+      checksum.update(buf, 0, lastChunkSize);\n+      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n+        return lastChunkStartPos + lastChunkSize;\n+      } else { // last chunck is corrupt\n+        return lastChunkStartPos;\n+      }\n+    } catch (IOException e) {\n+      FsDatasetImpl.LOG.warn(e);\n+      return 0;\n+    } finally {\n+      IOUtils.closeStream(checksumIn);\n+      IOUtils.closeStream(blockIn);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private long validateIntegrity(File blockFile, long genStamp) {\n    DataInputStream checksumIn \u003d null;\n    InputStream blockIn \u003d null;\n    try {\n      final File metaFile \u003d FsDatasetUtil.getMetaFile(blockFile, genStamp);\n      long blockFileLen \u003d blockFile.length();\n      long metaFileLen \u003d metaFile.length();\n      int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n      if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n          !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n        return 0;\n      }\n      checksumIn \u003d new DataInputStream(\n          new BufferedInputStream(new FileInputStream(metaFile),\n              HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n      // read and handle the common header here. For now just a version\n      BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n      short version \u003d header.getVersion();\n      if (version !\u003d BlockMetadataHeader.VERSION) {\n        FsDatasetImpl.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n            + metaFile + \" ignoring ...\");\n      }\n      DataChecksum checksum \u003d header.getChecksum();\n      int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n      int checksumSize \u003d checksum.getChecksumSize();\n      long numChunks \u003d Math.min(\n          (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n          (metaFileLen - crcHeaderLen)/checksumSize);\n      if (numChunks \u003d\u003d 0) {\n        return 0;\n      }\n      IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n      blockIn \u003d new FileInputStream(blockFile);\n      long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n      IOUtils.skipFully(blockIn, lastChunkStartPos);\n      int lastChunkSize \u003d (int)Math.min(\n          bytesPerChecksum, blockFileLen-lastChunkStartPos);\n      byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n      checksumIn.readFully(buf, lastChunkSize, checksumSize);\n      IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n      checksum.update(buf, 0, lastChunkSize);\n      if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n        return lastChunkStartPos + lastChunkSize;\n      } else { // last chunck is corrupt\n        return lastChunkStartPos;\n      }\n    } catch (IOException e) {\n      FsDatasetImpl.LOG.warn(e);\n      return 0;\n    } finally {\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(blockIn);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
          "extendedDetails": {}
        }
      ]
    },
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2887. FSVolume, is a part of FSDatasetInterface implementation, should not be referred outside FSDataset.  A new FSVolumeInterface is defined.  The BlockVolumeChoosingPolicy.chooseVolume(..) method signature is also updated.  (szetszwo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1242087 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/02/12 12:58 PM",
      "commitName": "b6ffb08a467f1b5bc89e5114c462c3117b337be6",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "02/02/12 11:26 PM",
      "commitNameOld": "38ad4b503686a0d18cb2d42ffdecf06f0ba7b98f",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.56,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n     private long validateIntegrity(File blockFile, long genStamp) {\n       DataInputStream checksumIn \u003d null;\n       InputStream blockIn \u003d null;\n       try {\n-        File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n+        final File metaFile \u003d DatanodeUtil.getMetaFile(blockFile, genStamp);\n         long blockFileLen \u003d blockFile.length();\n         long metaFileLen \u003d metaFile.length();\n         int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n         if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n             !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n           return 0;\n         }\n         checksumIn \u003d new DataInputStream(\n             new BufferedInputStream(new FileInputStream(metaFile),\n                 HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n         short version \u003d header.getVersion();\n         if (version !\u003d BlockMetadataHeader.VERSION) {\n           DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n               + metaFile + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n         int checksumSize \u003d checksum.getChecksumSize();\n         long numChunks \u003d Math.min(\n             (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n             (metaFileLen - crcHeaderLen)/checksumSize);\n         if (numChunks \u003d\u003d 0) {\n           return 0;\n         }\n         IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n         blockIn \u003d new FileInputStream(blockFile);\n         long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n         IOUtils.skipFully(blockIn, lastChunkStartPos);\n         int lastChunkSize \u003d (int)Math.min(\n             bytesPerChecksum, blockFileLen-lastChunkStartPos);\n         byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n         checksumIn.readFully(buf, lastChunkSize, checksumSize);\n         IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n \n         checksum.update(buf, 0, lastChunkSize);\n         if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n           return lastChunkStartPos + lastChunkSize;\n         } else { // last chunck is corrupt\n           return lastChunkStartPos;\n         }\n       } catch (IOException e) {\n         DataNode.LOG.warn(e);\n         return 0;\n       } finally {\n         IOUtils.closeStream(checksumIn);\n         IOUtils.closeStream(blockIn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long validateIntegrity(File blockFile, long genStamp) {\n      DataInputStream checksumIn \u003d null;\n      InputStream blockIn \u003d null;\n      try {\n        final File metaFile \u003d DatanodeUtil.getMetaFile(blockFile, genStamp);\n        long blockFileLen \u003d blockFile.length();\n        long metaFileLen \u003d metaFile.length();\n        int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n        if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n            !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n          return 0;\n        }\n        checksumIn \u003d new DataInputStream(\n            new BufferedInputStream(new FileInputStream(metaFile),\n                HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n        short version \u003d header.getVersion();\n        if (version !\u003d BlockMetadataHeader.VERSION) {\n          DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n              + metaFile + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n            (metaFileLen - crcHeaderLen)/checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n        blockIn \u003d new FileInputStream(blockFile);\n        long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n        IOUtils.skipFully(blockIn, lastChunkStartPos);\n        int lastChunkSize \u003d (int)Math.min(\n            bytesPerChecksum, blockFileLen-lastChunkStartPos);\n        byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n        checksumIn.readFully(buf, lastChunkSize, checksumSize);\n        IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n        checksum.update(buf, 0, lastChunkSize);\n        if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n          return lastChunkStartPos + lastChunkSize;\n        } else { // last chunck is corrupt\n          return lastChunkStartPos;\n        }\n      } catch (IOException e) {\n        DataNode.LOG.warn(e);\n        return 0;\n      } finally {\n        IOUtils.closeStream(checksumIn);\n        IOUtils.closeStream(blockIn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "dbbfaebb71eb9d69d67fd5becd2e357397d0f68b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2864. Remove some redundant methods and the constant METADATA_VERSION from FSDataset.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1238969 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/01/12 11:46 PM",
      "commitName": "dbbfaebb71eb9d69d67fd5becd2e357397d0f68b",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "21/11/11 6:57 PM",
      "commitNameOld": "2ab10e29d9cca5018064be46a40e3c74423615a8",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 71.2,
      "commitsBetweenForRepo": 356,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n     private long validateIntegrity(File blockFile, long genStamp) {\n       DataInputStream checksumIn \u003d null;\n       InputStream blockIn \u003d null;\n       try {\n         File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n         long blockFileLen \u003d blockFile.length();\n         long metaFileLen \u003d metaFile.length();\n         int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n         if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n             !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n           return 0;\n         }\n         checksumIn \u003d new DataInputStream(\n             new BufferedInputStream(new FileInputStream(metaFile),\n                 HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n         short version \u003d header.getVersion();\n-        if (version !\u003d FSDataset.METADATA_VERSION) {\n+        if (version !\u003d BlockMetadataHeader.VERSION) {\n           DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n               + metaFile + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n         int checksumSize \u003d checksum.getChecksumSize();\n         long numChunks \u003d Math.min(\n             (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n             (metaFileLen - crcHeaderLen)/checksumSize);\n         if (numChunks \u003d\u003d 0) {\n           return 0;\n         }\n         IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n         blockIn \u003d new FileInputStream(blockFile);\n         long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n         IOUtils.skipFully(blockIn, lastChunkStartPos);\n         int lastChunkSize \u003d (int)Math.min(\n             bytesPerChecksum, blockFileLen-lastChunkStartPos);\n         byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n         checksumIn.readFully(buf, lastChunkSize, checksumSize);\n         IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n \n         checksum.update(buf, 0, lastChunkSize);\n         if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n           return lastChunkStartPos + lastChunkSize;\n         } else { // last chunck is corrupt\n           return lastChunkStartPos;\n         }\n       } catch (IOException e) {\n         DataNode.LOG.warn(e);\n         return 0;\n       } finally {\n         IOUtils.closeStream(checksumIn);\n         IOUtils.closeStream(blockIn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long validateIntegrity(File blockFile, long genStamp) {\n      DataInputStream checksumIn \u003d null;\n      InputStream blockIn \u003d null;\n      try {\n        File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n        long blockFileLen \u003d blockFile.length();\n        long metaFileLen \u003d metaFile.length();\n        int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n        if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n            !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n          return 0;\n        }\n        checksumIn \u003d new DataInputStream(\n            new BufferedInputStream(new FileInputStream(metaFile),\n                HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n        short version \u003d header.getVersion();\n        if (version !\u003d BlockMetadataHeader.VERSION) {\n          DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n              + metaFile + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n            (metaFileLen - crcHeaderLen)/checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n        blockIn \u003d new FileInputStream(blockFile);\n        long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n        IOUtils.skipFully(blockIn, lastChunkStartPos);\n        int lastChunkSize \u003d (int)Math.min(\n            bytesPerChecksum, blockFileLen-lastChunkStartPos);\n        byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n        checksumIn.readFully(buf, lastChunkSize, checksumSize);\n        IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n        checksum.update(buf, 0, lastChunkSize);\n        if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n          return lastChunkStartPos + lastChunkSize;\n        } else { // last chunck is corrupt\n          return lastChunkStartPos;\n        }\n      } catch (IOException e) {\n        DataNode.LOG.warn(e);\n        return 0;\n      } finally {\n        IOUtils.closeStream(checksumIn);\n        IOUtils.closeStream(blockIn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "b7cd8c0f865e88e40eee75fd2690b1fdc4155071": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2564. Cleanup unnecessary exceptions thrown and unnecessary casts. Contributed by Hari Mankude\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203950 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 6:34 PM",
      "commitName": "b7cd8c0f865e88e40eee75fd2690b1fdc4155071",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "02/11/11 6:00 PM",
      "commitNameOld": "11cf658d0a8c3fb0f0822c9fc60f18f2ae5bf629",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 16.07,
      "commitsBetweenForRepo": 60,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n     private long validateIntegrity(File blockFile, long genStamp) {\n       DataInputStream checksumIn \u003d null;\n       InputStream blockIn \u003d null;\n       try {\n         File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n         long blockFileLen \u003d blockFile.length();\n         long metaFileLen \u003d metaFile.length();\n         int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n         if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n-            !metaFile.exists() || metaFileLen \u003c (long)crcHeaderLen) {\n+            !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n           return 0;\n         }\n         checksumIn \u003d new DataInputStream(\n             new BufferedInputStream(new FileInputStream(metaFile),\n                 HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n         short version \u003d header.getVersion();\n         if (version !\u003d FSDataset.METADATA_VERSION) {\n           DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n               + metaFile + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n         int checksumSize \u003d checksum.getChecksumSize();\n         long numChunks \u003d Math.min(\n             (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n             (metaFileLen - crcHeaderLen)/checksumSize);\n         if (numChunks \u003d\u003d 0) {\n           return 0;\n         }\n         IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n         blockIn \u003d new FileInputStream(blockFile);\n         long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n         IOUtils.skipFully(blockIn, lastChunkStartPos);\n         int lastChunkSize \u003d (int)Math.min(\n             bytesPerChecksum, blockFileLen-lastChunkStartPos);\n         byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n         checksumIn.readFully(buf, lastChunkSize, checksumSize);\n         IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n \n         checksum.update(buf, 0, lastChunkSize);\n         if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n           return lastChunkStartPos + lastChunkSize;\n         } else { // last chunck is corrupt\n           return lastChunkStartPos;\n         }\n       } catch (IOException e) {\n         DataNode.LOG.warn(e);\n         return 0;\n       } finally {\n         IOUtils.closeStream(checksumIn);\n         IOUtils.closeStream(blockIn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long validateIntegrity(File blockFile, long genStamp) {\n      DataInputStream checksumIn \u003d null;\n      InputStream blockIn \u003d null;\n      try {\n        File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n        long blockFileLen \u003d blockFile.length();\n        long metaFileLen \u003d metaFile.length();\n        int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n        if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n            !metaFile.exists() || metaFileLen \u003c crcHeaderLen) {\n          return 0;\n        }\n        checksumIn \u003d new DataInputStream(\n            new BufferedInputStream(new FileInputStream(metaFile),\n                HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n        short version \u003d header.getVersion();\n        if (version !\u003d FSDataset.METADATA_VERSION) {\n          DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n              + metaFile + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n            (metaFileLen - crcHeaderLen)/checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n        blockIn \u003d new FileInputStream(blockFile);\n        long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n        IOUtils.skipFully(blockIn, lastChunkStartPos);\n        int lastChunkSize \u003d (int)Math.min(\n            bytesPerChecksum, blockFileLen-lastChunkStartPos);\n        byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n        checksumIn.readFully(buf, lastChunkSize, checksumSize);\n        IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n        checksum.update(buf, 0, lastChunkSize);\n        if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n          return lastChunkStartPos + lastChunkSize;\n        } else { // last chunck is corrupt\n          return lastChunkStartPos;\n        }\n      } catch (IOException e) {\n        DataNode.LOG.warn(e);\n        return 0;\n      } finally {\n        IOUtils.closeStream(checksumIn);\n        IOUtils.closeStream(blockIn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1620. Rename HdfsConstants -\u003e HdfsServerConstants, FSConstants -\u003e HdfsConstants. (Harsh J Chouraria via atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165096 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/11 12:30 PM",
      "commitName": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "25/08/11 9:46 PM",
      "commitNameOld": "73451ed2d9fb5eb228d80ad5f3be302a60496527",
      "commitAuthorOld": "Hairong Kuang",
      "daysBetweenCommits": 9.61,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n     private long validateIntegrity(File blockFile, long genStamp) {\n       DataInputStream checksumIn \u003d null;\n       InputStream blockIn \u003d null;\n       try {\n         File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n         long blockFileLen \u003d blockFile.length();\n         long metaFileLen \u003d metaFile.length();\n         int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n         if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n             !metaFile.exists() || metaFileLen \u003c (long)crcHeaderLen) {\n           return 0;\n         }\n         checksumIn \u003d new DataInputStream(\n             new BufferedInputStream(new FileInputStream(metaFile),\n-                FSConstants.IO_FILE_BUFFER_SIZE));\n+                HdfsConstants.IO_FILE_BUFFER_SIZE));\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n         short version \u003d header.getVersion();\n         if (version !\u003d FSDataset.METADATA_VERSION) {\n           DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n               + metaFile + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n         int checksumSize \u003d checksum.getChecksumSize();\n         long numChunks \u003d Math.min(\n             (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n             (metaFileLen - crcHeaderLen)/checksumSize);\n         if (numChunks \u003d\u003d 0) {\n           return 0;\n         }\n         IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n         blockIn \u003d new FileInputStream(blockFile);\n         long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n         IOUtils.skipFully(blockIn, lastChunkStartPos);\n         int lastChunkSize \u003d (int)Math.min(\n             bytesPerChecksum, blockFileLen-lastChunkStartPos);\n         byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n         checksumIn.readFully(buf, lastChunkSize, checksumSize);\n         IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n \n         checksum.update(buf, 0, lastChunkSize);\n         if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n           return lastChunkStartPos + lastChunkSize;\n         } else { // last chunck is corrupt\n           return lastChunkStartPos;\n         }\n       } catch (IOException e) {\n         DataNode.LOG.warn(e);\n         return 0;\n       } finally {\n         IOUtils.closeStream(checksumIn);\n         IOUtils.closeStream(blockIn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long validateIntegrity(File blockFile, long genStamp) {\n      DataInputStream checksumIn \u003d null;\n      InputStream blockIn \u003d null;\n      try {\n        File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n        long blockFileLen \u003d blockFile.length();\n        long metaFileLen \u003d metaFile.length();\n        int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n        if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n            !metaFile.exists() || metaFileLen \u003c (long)crcHeaderLen) {\n          return 0;\n        }\n        checksumIn \u003d new DataInputStream(\n            new BufferedInputStream(new FileInputStream(metaFile),\n                HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n        short version \u003d header.getVersion();\n        if (version !\u003d FSDataset.METADATA_VERSION) {\n          DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n              + metaFile + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n            (metaFileLen - crcHeaderLen)/checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n        blockIn \u003d new FileInputStream(blockFile);\n        long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n        IOUtils.skipFully(blockIn, lastChunkStartPos);\n        int lastChunkSize \u003d (int)Math.min(\n            bytesPerChecksum, blockFileLen-lastChunkStartPos);\n        byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n        checksumIn.readFully(buf, lastChunkSize, checksumSize);\n        IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n        checksum.update(buf, 0, lastChunkSize);\n        if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n          return lastChunkStartPos + lastChunkSize;\n        } else { // last chunck is corrupt\n          return lastChunkStartPos;\n        }\n      } catch (IOException e) {\n        DataNode.LOG.warn(e);\n        return 0;\n      } finally {\n        IOUtils.closeStream(checksumIn);\n        IOUtils.closeStream(blockIn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private long validateIntegrity(File blockFile, long genStamp) {\n      DataInputStream checksumIn \u003d null;\n      InputStream blockIn \u003d null;\n      try {\n        File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n        long blockFileLen \u003d blockFile.length();\n        long metaFileLen \u003d metaFile.length();\n        int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n        if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n            !metaFile.exists() || metaFileLen \u003c (long)crcHeaderLen) {\n          return 0;\n        }\n        checksumIn \u003d new DataInputStream(\n            new BufferedInputStream(new FileInputStream(metaFile),\n                FSConstants.IO_FILE_BUFFER_SIZE));\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n        short version \u003d header.getVersion();\n        if (version !\u003d FSDataset.METADATA_VERSION) {\n          DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n              + metaFile + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n            (metaFileLen - crcHeaderLen)/checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n        blockIn \u003d new FileInputStream(blockFile);\n        long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n        IOUtils.skipFully(blockIn, lastChunkStartPos);\n        int lastChunkSize \u003d (int)Math.min(\n            bytesPerChecksum, blockFileLen-lastChunkStartPos);\n        byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n        checksumIn.readFully(buf, lastChunkSize, checksumSize);\n        IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n        checksum.update(buf, 0, lastChunkSize);\n        if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n          return lastChunkStartPos + lastChunkSize;\n        } else { // last chunck is corrupt\n          return lastChunkStartPos;\n        }\n      } catch (IOException e) {\n        DataNode.LOG.warn(e);\n        return 0;\n      } finally {\n        IOUtils.closeStream(checksumIn);\n        IOUtils.closeStream(blockIn);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private long validateIntegrity(File blockFile, long genStamp) {\n      DataInputStream checksumIn \u003d null;\n      InputStream blockIn \u003d null;\n      try {\n        File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n        long blockFileLen \u003d blockFile.length();\n        long metaFileLen \u003d metaFile.length();\n        int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n        if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n            !metaFile.exists() || metaFileLen \u003c (long)crcHeaderLen) {\n          return 0;\n        }\n        checksumIn \u003d new DataInputStream(\n            new BufferedInputStream(new FileInputStream(metaFile),\n                FSConstants.IO_FILE_BUFFER_SIZE));\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n        short version \u003d header.getVersion();\n        if (version !\u003d FSDataset.METADATA_VERSION) {\n          DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n              + metaFile + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n            (metaFileLen - crcHeaderLen)/checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n        blockIn \u003d new FileInputStream(blockFile);\n        long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n        IOUtils.skipFully(blockIn, lastChunkStartPos);\n        int lastChunkSize \u003d (int)Math.min(\n            bytesPerChecksum, blockFileLen-lastChunkStartPos);\n        byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n        checksumIn.readFully(buf, lastChunkSize, checksumSize);\n        IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n        checksum.update(buf, 0, lastChunkSize);\n        if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n          return lastChunkStartPos + lastChunkSize;\n        } else { // last chunck is corrupt\n          return lastChunkStartPos;\n        }\n      } catch (IOException e) {\n        DataNode.LOG.warn(e);\n        return 0;\n      } finally {\n        IOUtils.closeStream(checksumIn);\n        IOUtils.closeStream(blockIn);\n      }\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2241. Remove implementing FSConstants interface to just get the constants from the interface. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1156420 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/08/11 5:46 PM",
      "commitName": "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "20/07/11 11:52 AM",
      "commitNameOld": "cda43d71be5ff7b2d67008f6ab611e3c4c22f3aa",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 21.25,
      "commitsBetweenForRepo": 75,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n     private long validateIntegrity(File blockFile, long genStamp) {\n       DataInputStream checksumIn \u003d null;\n       InputStream blockIn \u003d null;\n       try {\n         File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n         long blockFileLen \u003d blockFile.length();\n         long metaFileLen \u003d metaFile.length();\n         int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n         if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n             !metaFile.exists() || metaFileLen \u003c (long)crcHeaderLen) {\n           return 0;\n         }\n         checksumIn \u003d new DataInputStream(\n             new BufferedInputStream(new FileInputStream(metaFile),\n-                BUFFER_SIZE));\n+                FSConstants.IO_FILE_BUFFER_SIZE));\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n         short version \u003d header.getVersion();\n         if (version !\u003d FSDataset.METADATA_VERSION) {\n           DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n               + metaFile + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n         int checksumSize \u003d checksum.getChecksumSize();\n         long numChunks \u003d Math.min(\n             (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n             (metaFileLen - crcHeaderLen)/checksumSize);\n         if (numChunks \u003d\u003d 0) {\n           return 0;\n         }\n         IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n         blockIn \u003d new FileInputStream(blockFile);\n         long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n         IOUtils.skipFully(blockIn, lastChunkStartPos);\n         int lastChunkSize \u003d (int)Math.min(\n             bytesPerChecksum, blockFileLen-lastChunkStartPos);\n         byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n         checksumIn.readFully(buf, lastChunkSize, checksumSize);\n         IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n \n         checksum.update(buf, 0, lastChunkSize);\n         if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n           return lastChunkStartPos + lastChunkSize;\n         } else { // last chunck is corrupt\n           return lastChunkStartPos;\n         }\n       } catch (IOException e) {\n         DataNode.LOG.warn(e);\n         return 0;\n       } finally {\n         IOUtils.closeStream(checksumIn);\n         IOUtils.closeStream(blockIn);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private long validateIntegrity(File blockFile, long genStamp) {\n      DataInputStream checksumIn \u003d null;\n      InputStream blockIn \u003d null;\n      try {\n        File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n        long blockFileLen \u003d blockFile.length();\n        long metaFileLen \u003d metaFile.length();\n        int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n        if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n            !metaFile.exists() || metaFileLen \u003c (long)crcHeaderLen) {\n          return 0;\n        }\n        checksumIn \u003d new DataInputStream(\n            new BufferedInputStream(new FileInputStream(metaFile),\n                FSConstants.IO_FILE_BUFFER_SIZE));\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n        short version \u003d header.getVersion();\n        if (version !\u003d FSDataset.METADATA_VERSION) {\n          DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n              + metaFile + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n            (metaFileLen - crcHeaderLen)/checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n        blockIn \u003d new FileInputStream(blockFile);\n        long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n        IOUtils.skipFully(blockIn, lastChunkStartPos);\n        int lastChunkSize \u003d (int)Math.min(\n            bytesPerChecksum, blockFileLen-lastChunkStartPos);\n        byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n        checksumIn.readFully(buf, lastChunkSize, checksumSize);\n        IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n        checksum.update(buf, 0, lastChunkSize);\n        if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n          return lastChunkStartPos + lastChunkSize;\n        } else { // last chunck is corrupt\n          return lastChunkStartPos;\n        }\n      } catch (IOException e) {\n        DataNode.LOG.warn(e);\n        return 0;\n      } finally {\n        IOUtils.closeStream(checksumIn);\n        IOUtils.closeStream(blockIn);\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,56 @@\n+    private long validateIntegrity(File blockFile, long genStamp) {\n+      DataInputStream checksumIn \u003d null;\n+      InputStream blockIn \u003d null;\n+      try {\n+        File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n+        long blockFileLen \u003d blockFile.length();\n+        long metaFileLen \u003d metaFile.length();\n+        int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n+        if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n+            !metaFile.exists() || metaFileLen \u003c (long)crcHeaderLen) {\n+          return 0;\n+        }\n+        checksumIn \u003d new DataInputStream(\n+            new BufferedInputStream(new FileInputStream(metaFile),\n+                BUFFER_SIZE));\n+\n+        // read and handle the common header here. For now just a version\n+        BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n+        short version \u003d header.getVersion();\n+        if (version !\u003d FSDataset.METADATA_VERSION) {\n+          DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n+              + metaFile + \" ignoring ...\");\n+        }\n+        DataChecksum checksum \u003d header.getChecksum();\n+        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n+        int checksumSize \u003d checksum.getChecksumSize();\n+        long numChunks \u003d Math.min(\n+            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n+            (metaFileLen - crcHeaderLen)/checksumSize);\n+        if (numChunks \u003d\u003d 0) {\n+          return 0;\n+        }\n+        IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n+        blockIn \u003d new FileInputStream(blockFile);\n+        long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n+        IOUtils.skipFully(blockIn, lastChunkStartPos);\n+        int lastChunkSize \u003d (int)Math.min(\n+            bytesPerChecksum, blockFileLen-lastChunkStartPos);\n+        byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n+        checksumIn.readFully(buf, lastChunkSize, checksumSize);\n+        IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n+\n+        checksum.update(buf, 0, lastChunkSize);\n+        if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n+          return lastChunkStartPos + lastChunkSize;\n+        } else { // last chunck is corrupt\n+          return lastChunkStartPos;\n+        }\n+      } catch (IOException e) {\n+        DataNode.LOG.warn(e);\n+        return 0;\n+      } finally {\n+        IOUtils.closeStream(checksumIn);\n+        IOUtils.closeStream(blockIn);\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private long validateIntegrity(File blockFile, long genStamp) {\n      DataInputStream checksumIn \u003d null;\n      InputStream blockIn \u003d null;\n      try {\n        File metaFile \u003d new File(getMetaFileName(blockFile.toString(), genStamp));\n        long blockFileLen \u003d blockFile.length();\n        long metaFileLen \u003d metaFile.length();\n        int crcHeaderLen \u003d DataChecksum.getChecksumHeaderSize();\n        if (!blockFile.exists() || blockFileLen \u003d\u003d 0 ||\n            !metaFile.exists() || metaFileLen \u003c (long)crcHeaderLen) {\n          return 0;\n        }\n        checksumIn \u003d new DataInputStream(\n            new BufferedInputStream(new FileInputStream(metaFile),\n                BUFFER_SIZE));\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader.readHeader(checksumIn);\n        short version \u003d header.getVersion();\n        if (version !\u003d FSDataset.METADATA_VERSION) {\n          DataNode.LOG.warn(\"Wrong version (\" + version + \") for metadata file \"\n              + metaFile + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        int bytesPerChecksum \u003d checksum.getBytesPerChecksum();\n        int checksumSize \u003d checksum.getChecksumSize();\n        long numChunks \u003d Math.min(\n            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, \n            (metaFileLen - crcHeaderLen)/checksumSize);\n        if (numChunks \u003d\u003d 0) {\n          return 0;\n        }\n        IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);\n        blockIn \u003d new FileInputStream(blockFile);\n        long lastChunkStartPos \u003d (numChunks-1)*bytesPerChecksum;\n        IOUtils.skipFully(blockIn, lastChunkStartPos);\n        int lastChunkSize \u003d (int)Math.min(\n            bytesPerChecksum, blockFileLen-lastChunkStartPos);\n        byte[] buf \u003d new byte[lastChunkSize+checksumSize];\n        checksumIn.readFully(buf, lastChunkSize, checksumSize);\n        IOUtils.readFully(blockIn, buf, 0, lastChunkSize);\n\n        checksum.update(buf, 0, lastChunkSize);\n        if (checksum.compare(buf, lastChunkSize)) { // last chunk matches crc\n          return lastChunkStartPos + lastChunkSize;\n        } else { // last chunck is corrupt\n          return lastChunkStartPos;\n        }\n      } catch (IOException e) {\n        DataNode.LOG.warn(e);\n        return 0;\n      } finally {\n        IOUtils.closeStream(checksumIn);\n        IOUtils.closeStream(blockIn);\n      }\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
    }
  }
}