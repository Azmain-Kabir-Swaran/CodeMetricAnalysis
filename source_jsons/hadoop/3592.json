{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "close",
  "functionId": "close",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 759,
  "functionEndLine": 794,
  "numCommitsSeen": 143,
  "timeTaken": 8591,
  "changeHistory": [
    "b3119b9ab60a19d624db476c4e1c53410870c7a6",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "bff5999d07e9416a22846c849487e509ede55040",
    "30acb7372ab97adf9bc86ead529c96cfe36e2396",
    "871cb56152e6039ff56c6fabfcd45451029471c3",
    "a9dc5cd7069f721e8c55794b877026ba02537167",
    "a97a1e73177974cff8afafad6ca43a96563f3c61",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
    "9a4030e0e84a688c12daa21fe9a165808c3eca70",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
    "837e17b2eac1471d93e2eff395272063b265fee7",
    "239b2742d0e80d13c970fd062af4930e672fe903",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "b3119b9ab60a19d624db476c4e1c53410870c7a6": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "bff5999d07e9416a22846c849487e509ede55040": "Ybodychange",
    "30acb7372ab97adf9bc86ead529c96cfe36e2396": "Ybodychange",
    "871cb56152e6039ff56c6fabfcd45451029471c3": "Ybodychange",
    "a9dc5cd7069f721e8c55794b877026ba02537167": "Ybodychange",
    "a97a1e73177974cff8afafad6ca43a96563f3c61": "Ybodychange",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": "Ybodychange",
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": "Ybodychange",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": "Ybodychange",
    "9a4030e0e84a688c12daa21fe9a165808c3eca70": "Ybodychange",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": "Ybodychange",
    "837e17b2eac1471d93e2eff395272063b265fee7": "Ybodychange",
    "239b2742d0e80d13c970fd062af4930e672fe903": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b3119b9ab60a19d624db476c4e1c53410870c7a6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14648. Implement DeadNodeDetector basic model. Contributed by Lisheng Sun.\n",
      "commitDate": "15/11/19 7:32 PM",
      "commitName": "b3119b9ab60a19d624db476c4e1c53410870c7a6",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "06/11/19 5:58 AM",
      "commitNameOld": "c36014165c212b26d75268ee3659aa2cadcff349",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 9.57,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,36 @@\n   public synchronized void close() throws IOException {\n-    if (!closed.compareAndSet(false, true)) {\n-      DFSClient.LOG.debug(\"DFSInputStream has been closed already\");\n-      return;\n-    }\n-    dfsClient.checkOpen();\n+    try {\n+      if (!closed.compareAndSet(false, true)) {\n+        DFSClient.LOG.debug(\"DFSInputStream has been closed already\");\n+        return;\n+      }\n+      dfsClient.checkOpen();\n \n-    if ((extendedReadBuffers !\u003d null) \u0026\u0026 (!extendedReadBuffers.isEmpty())) {\n-      final StringBuilder builder \u003d new StringBuilder();\n-      extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n-        private String prefix \u003d \"\";\n-        @Override\n-        public void accept(ByteBuffer k, Object v) {\n-          builder.append(prefix).append(k);\n-          prefix \u003d \", \";\n-        }\n-      });\n-      DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n-          \"unreleased ByteBuffers allocated by read().  \" +\n-          \"Please release \" + builder.toString() + \".\");\n+      if ((extendedReadBuffers !\u003d null) \u0026\u0026 (!extendedReadBuffers.isEmpty())) {\n+        final StringBuilder builder \u003d new StringBuilder();\n+        extendedReadBuffers\n+            .visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n+              private String prefix \u003d \"\";\n+\n+              @Override\n+              public void accept(ByteBuffer k, Object v) {\n+                builder.append(prefix).append(k);\n+                prefix \u003d \", \";\n+              }\n+            });\n+        DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \"\n+            + \"unreleased ByteBuffers allocated by read().  \"\n+            + \"Please release \" + builder.toString() + \".\");\n+      }\n+      closeCurrentBlockReaders();\n+      super.close();\n+    } finally {\n+      /**\n+       * If dfsInputStream is closed and datanode is in\n+       * DeadNodeDetector#dfsInputStreamNodes, we need remove the datanode from\n+       * the DeadNodeDetector#dfsInputStreamNodes. Since user should not use\n+       * this dfsInputStream anymore.\n+       */\n+      dfsClient.removeNodeFromDeadNodeDetector(this, locatedBlocks);\n     }\n-    closeCurrentBlockReaders();\n-    super.close();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    try {\n      if (!closed.compareAndSet(false, true)) {\n        DFSClient.LOG.debug(\"DFSInputStream has been closed already\");\n        return;\n      }\n      dfsClient.checkOpen();\n\n      if ((extendedReadBuffers !\u003d null) \u0026\u0026 (!extendedReadBuffers.isEmpty())) {\n        final StringBuilder builder \u003d new StringBuilder();\n        extendedReadBuffers\n            .visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n              private String prefix \u003d \"\";\n\n              @Override\n              public void accept(ByteBuffer k, Object v) {\n                builder.append(prefix).append(k);\n                prefix \u003d \", \";\n              }\n            });\n        DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \"\n            + \"unreleased ByteBuffers allocated by read().  \"\n            + \"Please release \" + builder.toString() + \".\");\n      }\n      closeCurrentBlockReaders();\n      super.close();\n    } finally {\n      /**\n       * If dfsInputStream is closed and datanode is in\n       * DeadNodeDetector#dfsInputStreamNodes, we need remove the datanode from\n       * the DeadNodeDetector#dfsInputStreamNodes. Since user should not use\n       * this dfsInputStream anymore.\n       */\n      dfsClient.removeNodeFromDeadNodeDetector(this, locatedBlocks);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (!closed.compareAndSet(false, true)) {\n      DFSClient.LOG.debug(\"DFSInputStream has been closed already\");\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if ((extendedReadBuffers !\u003d null) \u0026\u0026 (!extendedReadBuffers.isEmpty())) {\n      final StringBuilder builder \u003d new StringBuilder();\n      extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n        private String prefix \u003d \"\";\n        @Override\n        public void accept(ByteBuffer k, Object v) {\n          builder.append(prefix).append(k);\n          prefix \u003d \", \";\n        }\n      });\n      DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n          \"unreleased ByteBuffers allocated by read().  \" +\n          \"Please release \" + builder.toString() + \".\");\n    }\n    closeCurrentBlockReaders();\n    super.close();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "bff5999d07e9416a22846c849487e509ede55040": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
      "commitDate": "02/07/15 3:41 AM",
      "commitName": "bff5999d07e9416a22846c849487e509ede55040",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "04/06/15 10:51 AM",
      "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 27.7,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public synchronized void close() throws IOException {\n     if (!closed.compareAndSet(false, true)) {\n       DFSClient.LOG.debug(\"DFSInputStream has been closed already\");\n       return;\n     }\n     dfsClient.checkOpen();\n \n     if ((extendedReadBuffers !\u003d null) \u0026\u0026 (!extendedReadBuffers.isEmpty())) {\n       final StringBuilder builder \u003d new StringBuilder();\n       extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n         private String prefix \u003d \"\";\n         @Override\n         public void accept(ByteBuffer k, Object v) {\n           builder.append(prefix).append(k);\n           prefix \u003d \", \";\n         }\n       });\n       DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n           \"unreleased ByteBuffers allocated by read().  \" +\n           \"Please release \" + builder.toString() + \".\");\n     }\n-    closeCurrentBlockReader();\n+    closeCurrentBlockReaders();\n     super.close();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (!closed.compareAndSet(false, true)) {\n      DFSClient.LOG.debug(\"DFSInputStream has been closed already\");\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if ((extendedReadBuffers !\u003d null) \u0026\u0026 (!extendedReadBuffers.isEmpty())) {\n      final StringBuilder builder \u003d new StringBuilder();\n      extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n        private String prefix \u003d \"\";\n        @Override\n        public void accept(ByteBuffer k, Object v) {\n          builder.append(prefix).append(k);\n          prefix \u003d \", \";\n        }\n      });\n      DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n          \"unreleased ByteBuffers allocated by read().  \" +\n          \"Please release \" + builder.toString() + \".\");\n    }\n    closeCurrentBlockReaders();\n    super.close();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "30acb7372ab97adf9bc86ead529c96cfe36e2396": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8099. Change \"DFSInputStream has been closed already\" message to debug log level (Charles Lamb via Colin P. McCabe)\n",
      "commitDate": "09/04/15 11:22 AM",
      "commitName": "30acb7372ab97adf9bc86ead529c96cfe36e2396",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "08/04/15 3:41 PM",
      "commitNameOld": "a42bb1cd915abe5dc33eda3c01e8c74c64f35748",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.82,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public synchronized void close() throws IOException {\n     if (!closed.compareAndSet(false, true)) {\n-      DFSClient.LOG.warn(\"DFSInputStream has been closed already\");\n+      DFSClient.LOG.debug(\"DFSInputStream has been closed already\");\n       return;\n     }\n     dfsClient.checkOpen();\n \n     if ((extendedReadBuffers !\u003d null) \u0026\u0026 (!extendedReadBuffers.isEmpty())) {\n       final StringBuilder builder \u003d new StringBuilder();\n       extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n         private String prefix \u003d \"\";\n         @Override\n         public void accept(ByteBuffer k, Object v) {\n           builder.append(prefix).append(k);\n           prefix \u003d \", \";\n         }\n       });\n       DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n           \"unreleased ByteBuffers allocated by read().  \" +\n           \"Please release \" + builder.toString() + \".\");\n     }\n     closeCurrentBlockReader();\n     super.close();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (!closed.compareAndSet(false, true)) {\n      DFSClient.LOG.debug(\"DFSInputStream has been closed already\");\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if ((extendedReadBuffers !\u003d null) \u0026\u0026 (!extendedReadBuffers.isEmpty())) {\n      final StringBuilder builder \u003d new StringBuilder();\n      extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n        private String prefix \u003d \"\";\n        @Override\n        public void accept(ByteBuffer k, Object v) {\n          builder.append(prefix).append(k);\n          prefix \u003d \", \";\n        }\n      });\n      DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n          \"unreleased ByteBuffers allocated by read().  \" +\n          \"Please release \" + builder.toString() + \".\");\n    }\n    closeCurrentBlockReader();\n    super.close();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "871cb56152e6039ff56c6fabfcd45451029471c3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7790. Do not create optional fields in DFSInputStream unless they are needed (cmccabe)\n",
      "commitDate": "12/02/15 5:48 PM",
      "commitName": "871cb56152e6039ff56c6fabfcd45451029471c3",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "12/02/15 10:40 AM",
      "commitNameOld": "6b39ad0865cb2a7960dd59d68178f0bf28865ce2",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.3,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public synchronized void close() throws IOException {\n     if (!closed.compareAndSet(false, true)) {\n       DFSClient.LOG.warn(\"DFSInputStream has been closed already\");\n       return;\n     }\n     dfsClient.checkOpen();\n \n-    if (!extendedReadBuffers.isEmpty()) {\n+    if ((extendedReadBuffers !\u003d null) \u0026\u0026 (!extendedReadBuffers.isEmpty())) {\n       final StringBuilder builder \u003d new StringBuilder();\n       extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n         private String prefix \u003d \"\";\n         @Override\n         public void accept(ByteBuffer k, Object v) {\n           builder.append(prefix).append(k);\n           prefix \u003d \", \";\n         }\n       });\n       DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n           \"unreleased ByteBuffers allocated by read().  \" +\n           \"Please release \" + builder.toString() + \".\");\n     }\n     closeCurrentBlockReader();\n     super.close();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (!closed.compareAndSet(false, true)) {\n      DFSClient.LOG.warn(\"DFSInputStream has been closed already\");\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if ((extendedReadBuffers !\u003d null) \u0026\u0026 (!extendedReadBuffers.isEmpty())) {\n      final StringBuilder builder \u003d new StringBuilder();\n      extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n        private String prefix \u003d \"\";\n        @Override\n        public void accept(ByteBuffer k, Object v) {\n          builder.append(prefix).append(k);\n          prefix \u003d \", \";\n        }\n      });\n      DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n          \"unreleased ByteBuffers allocated by read().  \" +\n          \"Please release \" + builder.toString() + \".\");\n    }\n    closeCurrentBlockReader();\n    super.close();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "a9dc5cd7069f721e8c55794b877026ba02537167": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7744. Fix potential NPE in DFSInputStream after setDropBehind or setReadahead is called (cmccabe)\n",
      "commitDate": "09/02/15 8:16 PM",
      "commitName": "a9dc5cd7069f721e8c55794b877026ba02537167",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "05/02/15 7:56 AM",
      "commitNameOld": "45ea53f9388e6bff1ac0aa3989a1dad56a611fd3",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 4.51,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,24 @@\n   public synchronized void close() throws IOException {\n     if (!closed.compareAndSet(false, true)) {\n       DFSClient.LOG.warn(\"DFSInputStream has been closed already\");\n       return;\n     }\n     dfsClient.checkOpen();\n \n     if (!extendedReadBuffers.isEmpty()) {\n       final StringBuilder builder \u003d new StringBuilder();\n       extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n         private String prefix \u003d \"\";\n         @Override\n         public void accept(ByteBuffer k, Object v) {\n           builder.append(prefix).append(k);\n           prefix \u003d \", \";\n         }\n       });\n       DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n           \"unreleased ByteBuffers allocated by read().  \" +\n           \"Please release \" + builder.toString() + \".\");\n     }\n-    if (blockReader !\u003d null) {\n-      blockReader.close();\n-      blockReader \u003d null;\n-    }\n+    closeCurrentBlockReader();\n     super.close();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (!closed.compareAndSet(false, true)) {\n      DFSClient.LOG.warn(\"DFSInputStream has been closed already\");\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (!extendedReadBuffers.isEmpty()) {\n      final StringBuilder builder \u003d new StringBuilder();\n      extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n        private String prefix \u003d \"\";\n        @Override\n        public void accept(ByteBuffer k, Object v) {\n          builder.append(prefix).append(k);\n          prefix \u003d \", \";\n        }\n      });\n      DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n          \"unreleased ByteBuffers allocated by read().  \" +\n          \"Please release \" + builder.toString() + \".\");\n    }\n    closeCurrentBlockReader();\n    super.close();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "a97a1e73177974cff8afafad6ca43a96563f3c61": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7494. Checking of closed in DFSInputStream#pread() should be protected by synchronization (Ted Yu via Colin P. McCabe)\n",
      "commitDate": "16/12/14 11:07 AM",
      "commitName": "a97a1e73177974cff8afafad6ca43a96563f3c61",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "02/12/14 8:57 PM",
      "commitNameOld": "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
      "commitAuthorOld": "stack",
      "daysBetweenCommits": 13.59,
      "commitsBetweenForRepo": 103,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   public synchronized void close() throws IOException {\n-    if (closed) {\n+    if (!closed.compareAndSet(false, true)) {\n+      DFSClient.LOG.warn(\"DFSInputStream has been closed already\");\n       return;\n     }\n     dfsClient.checkOpen();\n \n     if (!extendedReadBuffers.isEmpty()) {\n       final StringBuilder builder \u003d new StringBuilder();\n       extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n         private String prefix \u003d \"\";\n         @Override\n         public void accept(ByteBuffer k, Object v) {\n           builder.append(prefix).append(k);\n           prefix \u003d \", \";\n         }\n       });\n       DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n           \"unreleased ByteBuffers allocated by read().  \" +\n           \"Please release \" + builder.toString() + \".\");\n     }\n     if (blockReader !\u003d null) {\n       blockReader.close();\n       blockReader \u003d null;\n     }\n     super.close();\n-    closed \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (!closed.compareAndSet(false, true)) {\n      DFSClient.LOG.warn(\"DFSInputStream has been closed already\");\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (!extendedReadBuffers.isEmpty()) {\n      final StringBuilder builder \u003d new StringBuilder();\n      extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n        private String prefix \u003d \"\";\n        @Override\n        public void accept(ByteBuffer k, Object v) {\n          builder.append(prefix).append(k);\n          prefix \u003d \", \";\n        }\n      });\n      DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n          \"unreleased ByteBuffers allocated by read().  \" +\n          \"Please release \" + builder.toString() + \".\");\n    }\n    if (blockReader !\u003d null) {\n      blockReader.close();\n      blockReader \u003d null;\n    }\n    super.close();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 11:08 AM",
      "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "06/02/14 7:45 AM",
      "commitNameOld": "ab96a0838dafbfea77382135914feadbfd03cf53",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 6.14,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,27 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       return;\n     }\n     dfsClient.checkOpen();\n \n     if (!extendedReadBuffers.isEmpty()) {\n       final StringBuilder builder \u003d new StringBuilder();\n       extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n         private String prefix \u003d \"\";\n         @Override\n         public void accept(ByteBuffer k, Object v) {\n           builder.append(prefix).append(k);\n           prefix \u003d \", \";\n         }\n       });\n       DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n           \"unreleased ByteBuffers allocated by read().  \" +\n           \"Please release \" + builder.toString() + \".\");\n     }\n     if (blockReader !\u003d null) {\n       blockReader.close();\n       blockReader \u003d null;\n     }\n     super.close();\n-    fileInputStreamCache.close();\n     closed \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (!extendedReadBuffers.isEmpty()) {\n      final StringBuilder builder \u003d new StringBuilder();\n      extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n        private String prefix \u003d \"\";\n        @Override\n        public void accept(ByteBuffer k, Object v) {\n          builder.append(prefix).append(k);\n          prefix \u003d \", \";\n        }\n      });\n      DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n          \"unreleased ByteBuffers allocated by read().  \" +\n          \"Please release \" + builder.toString() + \".\");\n    }\n    if (blockReader !\u003d null) {\n      blockReader.close();\n      blockReader \u003d null;\n    }\n    super.close();\n    closed \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5260. Merge zero-copy memory-mapped HDFS client reads to trunk and branch-2. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1527113 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 3:51 PM",
      "commitName": "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "22/07/13 11:15 AM",
      "commitNameOld": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 67.19,
      "commitsBetweenForRepo": 393,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,28 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       return;\n     }\n     dfsClient.checkOpen();\n \n+    if (!extendedReadBuffers.isEmpty()) {\n+      final StringBuilder builder \u003d new StringBuilder();\n+      extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n+        private String prefix \u003d \"\";\n+        @Override\n+        public void accept(ByteBuffer k, Object v) {\n+          builder.append(prefix).append(k);\n+          prefix \u003d \", \";\n+        }\n+      });\n+      DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n+          \"unreleased ByteBuffers allocated by read().  \" +\n+          \"Please release \" + builder.toString() + \".\");\n+    }\n     if (blockReader !\u003d null) {\n       blockReader.close();\n       blockReader \u003d null;\n     }\n     super.close();\n     fileInputStreamCache.close();\n     closed \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (!extendedReadBuffers.isEmpty()) {\n      final StringBuilder builder \u003d new StringBuilder();\n      extendedReadBuffers.visitAll(new IdentityHashStore.Visitor\u003cByteBuffer, Object\u003e() {\n        private String prefix \u003d \"\";\n        @Override\n        public void accept(ByteBuffer k, Object v) {\n          builder.append(prefix).append(k);\n          prefix \u003d \", \";\n        }\n      });\n      DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" +\n          \"unreleased ByteBuffers allocated by read().  \" +\n          \"Please release \" + builder.toString() + \".\");\n    }\n    if (blockReader !\u003d null) {\n      blockReader.close();\n      blockReader \u003d null;\n    }\n    super.close();\n    fileInputStreamCache.close();\n    closed \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4661. A few little code cleanups of some HDFS-347-related code. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1480839 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/13 5:03 PM",
      "commitName": "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "29/03/13 2:33 PM",
      "commitNameOld": "bbb24fbf5d220fbe137d43651ba3802a9806b1a3",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 41.1,
      "commitsBetweenForRepo": 266,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       return;\n     }\n     dfsClient.checkOpen();\n \n     if (blockReader !\u003d null) {\n-      blockReader.close(peerCache, fileInputStreamCache);\n+      blockReader.close();\n       blockReader \u003d null;\n     }\n     super.close();\n     fileInputStreamCache.close();\n     closed \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (blockReader !\u003d null) {\n      blockReader.close();\n      blockReader \u003d null;\n    }\n    super.close();\n    fileInputStreamCache.close();\n    closed \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "9a4030e0e84a688c12daa21fe9a165808c3eca70": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4356. BlockReaderLocal should use passed file descriptors rather than paths. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1432335 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/01/13 3:52 PM",
      "commitName": "9a4030e0e84a688c12daa21fe9a165808c3eca70",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "09/01/13 1:34 PM",
      "commitNameOld": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 2.1,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,14 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       return;\n     }\n     dfsClient.checkOpen();\n \n     if (blockReader !\u003d null) {\n-      blockReader.close(peerCache);\n+      blockReader.close(peerCache, fileInputStreamCache);\n       blockReader \u003d null;\n     }\n     super.close();\n+    fileInputStreamCache.close();\n     closed \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (blockReader !\u003d null) {\n      blockReader.close(peerCache, fileInputStreamCache);\n      blockReader \u003d null;\n    }\n    super.close();\n    fileInputStreamCache.close();\n    closed \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:34 PM",
      "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "08/01/13 6:41 PM",
      "commitNameOld": "fab2cbc2c1fa7b592e27a186411dcc4a67ea2bc2",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       return;\n     }\n     dfsClient.checkOpen();\n \n     if (blockReader !\u003d null) {\n-      closeBlockReader(blockReader);\n+      blockReader.close(peerCache);\n       blockReader \u003d null;\n     }\n     super.close();\n     closed \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (blockReader !\u003d null) {\n      blockReader.close(peerCache);\n      blockReader \u003d null;\n    }\n    super.close();\n    closed \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "837e17b2eac1471d93e2eff395272063b265fee7": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 6:39 PM",
      "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/01/13 12:44 PM",
      "commitNameOld": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.25,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       return;\n     }\n     dfsClient.checkOpen();\n \n     if (blockReader !\u003d null) {\n-      blockReader.close(peerCache);\n+      closeBlockReader(blockReader);\n       blockReader \u003d null;\n     }\n     super.close();\n     closed \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (blockReader !\u003d null) {\n      closeBlockReader(blockReader);\n      blockReader \u003d null;\n    }\n    super.close();\n    closed \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "239b2742d0e80d13c970fd062af4930e672fe903": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 12:44 PM",
      "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "03/01/13 10:59 PM",
      "commitNameOld": "32052a1e3a8007b5348dc42415861aeb859ebc5a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.57,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       return;\n     }\n     dfsClient.checkOpen();\n \n     if (blockReader !\u003d null) {\n-      closeBlockReader(blockReader);\n+      blockReader.close(peerCache);\n       blockReader \u003d null;\n     }\n     super.close();\n     closed \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (blockReader !\u003d null) {\n      blockReader.close(peerCache);\n      blockReader \u003d null;\n    }\n    super.close();\n    closed \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (blockReader !\u003d null) {\n      closeBlockReader(blockReader);\n      blockReader \u003d null;\n    }\n    super.close();\n    closed \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (blockReader !\u003d null) {\n      closeBlockReader(blockReader);\n      blockReader \u003d null;\n    }\n    super.close();\n    closed \u003d true;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,13 @@\n+  public synchronized void close() throws IOException {\n+    if (closed) {\n+      return;\n+    }\n+    dfsClient.checkOpen();\n+\n+    if (blockReader !\u003d null) {\n+      closeBlockReader(blockReader);\n+      blockReader \u003d null;\n+    }\n+    super.close();\n+    closed \u003d true;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    dfsClient.checkOpen();\n\n    if (blockReader !\u003d null) {\n      closeBlockReader(blockReader);\n      blockReader \u003d null;\n    }\n    super.close();\n    closed \u003d true;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}