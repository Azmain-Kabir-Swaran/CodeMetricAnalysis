{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSEditLog.java",
  "functionName": "getEditLogManifest",
  "functionId": "getEditLogManifest___fromTxId-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
  "functionStartLine": 1331,
  "functionEndLine": 1334,
  "numCommitsSeen": 158,
  "timeTaken": 7075,
  "changeHistory": [
    "512a18a8d92305a34f3037064ceabdc5aff1f8bf",
    "8c62c46046656c01b327c378e89d57b4bf37e16e",
    "1ae5b5e338ef383c5642e2f04b927871c7b184f6",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "eb6e44b1ba58ed971360a39ea5d5ce02ae65aa0f",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "512a18a8d92305a34f3037064ceabdc5aff1f8bf": "Ybodychange",
    "8c62c46046656c01b327c378e89d57b4bf37e16e": "Ybodychange",
    "1ae5b5e338ef383c5642e2f04b927871c7b184f6": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "eb6e44b1ba58ed971360a39ea5d5ce02ae65aa0f": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Ymodifierchange,Ybodychange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "512a18a8d92305a34f3037064ceabdc5aff1f8bf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5074. Allow starting up from an fsimage checkpoint in the middle of a segment. Contributed by Todd Lipcon.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1550016 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/12/13 5:32 PM",
      "commitName": "512a18a8d92305a34f3037064ceabdc5aff1f8bf",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "27/11/13 11:20 PM",
      "commitNameOld": "9da451cac57f3cd64c2c047675e5b60ca88ecf83",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 12.76,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,4 +1,4 @@\n   public synchronized RemoteEditLogManifest getEditLogManifest(long fromTxId)\n       throws IOException {\n-    return journalSet.getEditLogManifest(fromTxId, true);\n+    return journalSet.getEditLogManifest(fromTxId);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized RemoteEditLogManifest getEditLogManifest(long fromTxId)\n      throws IOException {\n    return journalSet.getEditLogManifest(fromTxId);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {}
    },
    "8c62c46046656c01b327c378e89d57b4bf37e16e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4298. StorageRetentionManager spews warnings when used with QJM. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1485371 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/05/13 12:37 PM",
      "commitName": "8c62c46046656c01b327c378e89d57b4bf37e16e",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "23/04/13 7:21 PM",
      "commitNameOld": "5f1e3b561a24c05537561a058a44eaa2d3408d67",
      "commitAuthorOld": "",
      "daysBetweenCommits": 28.72,
      "commitsBetweenForRepo": 174,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,4 +1,4 @@\n   public synchronized RemoteEditLogManifest getEditLogManifest(long fromTxId)\n       throws IOException {\n-    return journalSet.getEditLogManifest(fromTxId);\n+    return journalSet.getEditLogManifest(fromTxId, true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized RemoteEditLogManifest getEditLogManifest(long fromTxId)\n      throws IOException {\n    return journalSet.getEditLogManifest(fromTxId, true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {}
    },
    "1ae5b5e338ef383c5642e2f04b927871c7b184f6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2158. Add JournalSet to manage the set of journals.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1177473 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/09/11 6:14 PM",
      "commitName": "1ae5b5e338ef383c5642e2f04b927871c7b184f6",
      "commitAuthor": "Jitendra Nath Pandey",
      "commitDateOld": "06/09/11 1:27 PM",
      "commitNameOld": "bdc3720d5b67a1c8fc2dfb29be16e4155c0e7f15",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 23.2,
      "commitsBetweenForRepo": 169,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,4 @@\n-  public synchronized RemoteEditLogManifest getEditLogManifest(\n-      long fromTxId) throws IOException {\n-    // Collect RemoteEditLogs available from each FileJournalManager\n-    List\u003cRemoteEditLog\u003e allLogs \u003d Lists.newArrayList();\n-    for (JournalAndStream j : journals) {\n-      if (j.getManager() instanceof FileJournalManager) {\n-        FileJournalManager fjm \u003d (FileJournalManager)j.getManager();\n-        try {\n-          allLogs.addAll(fjm.getRemoteEditLogs(fromTxId));\n-        } catch (Throwable t) {\n-          LOG.warn(\"Cannot list edit logs in \" + fjm, t);\n-        }\n-      }\n-    }\n-    \n-    // Group logs by their starting txid\n-    ImmutableListMultimap\u003cLong, RemoteEditLog\u003e logsByStartTxId \u003d\n-      Multimaps.index(allLogs, RemoteEditLog.GET_START_TXID);\n-    long curStartTxId \u003d fromTxId;\n-\n-    List\u003cRemoteEditLog\u003e logs \u003d Lists.newArrayList();\n-    while (true) {\n-      ImmutableList\u003cRemoteEditLog\u003e logGroup \u003d logsByStartTxId.get(curStartTxId);\n-      if (logGroup.isEmpty()) {\n-        // we have a gap in logs - for example because we recovered some old\n-        // storage directory with ancient logs. Clear out any logs we\u0027ve\n-        // accumulated so far, and then skip to the next segment of logs\n-        // after the gap.\n-        SortedSet\u003cLong\u003e startTxIds \u003d Sets.newTreeSet(logsByStartTxId.keySet());\n-        startTxIds \u003d startTxIds.tailSet(curStartTxId);\n-        if (startTxIds.isEmpty()) {\n-          break;\n-        } else {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Found gap in logs at \" + curStartTxId + \": \" +\n-                \"not returning previous logs in manifest.\");\n-          }\n-          logs.clear();\n-          curStartTxId \u003d startTxIds.first();\n-          continue;\n-        }\n-      }\n-\n-      // Find the one that extends the farthest forward\n-      RemoteEditLog bestLog \u003d Collections.max(logGroup);\n-      logs.add(bestLog);\n-      // And then start looking from after that point\n-      curStartTxId \u003d bestLog.getEndTxId() + 1;\n-    }\n-    RemoteEditLogManifest ret \u003d new RemoteEditLogManifest(logs);\n-    \n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Generated manifest for logs since \" + fromTxId + \":\"\n-          + ret);      \n-    }\n-    return ret;\n+  public synchronized RemoteEditLogManifest getEditLogManifest(long fromTxId)\n+      throws IOException {\n+    return journalSet.getEditLogManifest(fromTxId);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized RemoteEditLogManifest getEditLogManifest(long fromTxId)\n      throws IOException {\n    return journalSet.getEditLogManifest(fromTxId);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized RemoteEditLogManifest getEditLogManifest(\n      long fromTxId) throws IOException {\n    // Collect RemoteEditLogs available from each FileJournalManager\n    List\u003cRemoteEditLog\u003e allLogs \u003d Lists.newArrayList();\n    for (JournalAndStream j : journals) {\n      if (j.getManager() instanceof FileJournalManager) {\n        FileJournalManager fjm \u003d (FileJournalManager)j.getManager();\n        try {\n          allLogs.addAll(fjm.getRemoteEditLogs(fromTxId));\n        } catch (Throwable t) {\n          LOG.warn(\"Cannot list edit logs in \" + fjm, t);\n        }\n      }\n    }\n    \n    // Group logs by their starting txid\n    ImmutableListMultimap\u003cLong, RemoteEditLog\u003e logsByStartTxId \u003d\n      Multimaps.index(allLogs, RemoteEditLog.GET_START_TXID);\n    long curStartTxId \u003d fromTxId;\n\n    List\u003cRemoteEditLog\u003e logs \u003d Lists.newArrayList();\n    while (true) {\n      ImmutableList\u003cRemoteEditLog\u003e logGroup \u003d logsByStartTxId.get(curStartTxId);\n      if (logGroup.isEmpty()) {\n        // we have a gap in logs - for example because we recovered some old\n        // storage directory with ancient logs. Clear out any logs we\u0027ve\n        // accumulated so far, and then skip to the next segment of logs\n        // after the gap.\n        SortedSet\u003cLong\u003e startTxIds \u003d Sets.newTreeSet(logsByStartTxId.keySet());\n        startTxIds \u003d startTxIds.tailSet(curStartTxId);\n        if (startTxIds.isEmpty()) {\n          break;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Found gap in logs at \" + curStartTxId + \": \" +\n                \"not returning previous logs in manifest.\");\n          }\n          logs.clear();\n          curStartTxId \u003d startTxIds.first();\n          continue;\n        }\n      }\n\n      // Find the one that extends the farthest forward\n      RemoteEditLog bestLog \u003d Collections.max(logGroup);\n      logs.add(bestLog);\n      // And then start looking from after that point\n      curStartTxId \u003d bestLog.getEndTxId() + 1;\n    }\n    RemoteEditLogManifest ret \u003d new RemoteEditLogManifest(logs);\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Generated manifest for logs since \" + fromTxId + \":\"\n          + ret);      \n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized RemoteEditLogManifest getEditLogManifest(\n      long fromTxId) throws IOException {\n    // Collect RemoteEditLogs available from each FileJournalManager\n    List\u003cRemoteEditLog\u003e allLogs \u003d Lists.newArrayList();\n    for (JournalAndStream j : journals) {\n      if (j.getManager() instanceof FileJournalManager) {\n        FileJournalManager fjm \u003d (FileJournalManager)j.getManager();\n        try {\n          allLogs.addAll(fjm.getRemoteEditLogs(fromTxId));\n        } catch (Throwable t) {\n          LOG.warn(\"Cannot list edit logs in \" + fjm, t);\n        }\n      }\n    }\n    \n    // Group logs by their starting txid\n    ImmutableListMultimap\u003cLong, RemoteEditLog\u003e logsByStartTxId \u003d\n      Multimaps.index(allLogs, RemoteEditLog.GET_START_TXID);\n    long curStartTxId \u003d fromTxId;\n\n    List\u003cRemoteEditLog\u003e logs \u003d Lists.newArrayList();\n    while (true) {\n      ImmutableList\u003cRemoteEditLog\u003e logGroup \u003d logsByStartTxId.get(curStartTxId);\n      if (logGroup.isEmpty()) {\n        // we have a gap in logs - for example because we recovered some old\n        // storage directory with ancient logs. Clear out any logs we\u0027ve\n        // accumulated so far, and then skip to the next segment of logs\n        // after the gap.\n        SortedSet\u003cLong\u003e startTxIds \u003d Sets.newTreeSet(logsByStartTxId.keySet());\n        startTxIds \u003d startTxIds.tailSet(curStartTxId);\n        if (startTxIds.isEmpty()) {\n          break;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Found gap in logs at \" + curStartTxId + \": \" +\n                \"not returning previous logs in manifest.\");\n          }\n          logs.clear();\n          curStartTxId \u003d startTxIds.first();\n          continue;\n        }\n      }\n\n      // Find the one that extends the farthest forward\n      RemoteEditLog bestLog \u003d Collections.max(logGroup);\n      logs.add(bestLog);\n      // And then start looking from after that point\n      curStartTxId \u003d bestLog.getEndTxId() + 1;\n    }\n    RemoteEditLogManifest ret \u003d new RemoteEditLogManifest(logs);\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Generated manifest for logs since \" + fromTxId + \":\"\n          + ret);      \n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java"
      }
    },
    "eb6e44b1ba58ed971360a39ea5d5ce02ae65aa0f": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-2227. getRemoteEditLogManifest should pull its information from FileJournalManager during checkpoint process. Contributed by Ivan Kelly and Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1155977 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/11 5:01 PM",
      "commitName": "eb6e44b1ba58ed971360a39ea5d5ce02ae65aa0f",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2227. getRemoteEditLogManifest should pull its information from FileJournalManager during checkpoint process. Contributed by Ivan Kelly and Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1155977 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/08/11 5:01 PM",
          "commitName": "eb6e44b1ba58ed971360a39ea5d5ce02ae65aa0f",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "04/08/11 2:56 PM",
          "commitNameOld": "23762da4fa17ce6ea7b70722147977123a28a7e6",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 5.09,
          "commitsBetweenForRepo": 14,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,11 +1,57 @@\n-  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n-      throws IOException {\n-    FSImageTransactionalStorageInspector inspector \u003d\n-        new FSImageTransactionalStorageInspector();\n-\n-    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n-      inspector.inspectDirectory(sd);\n+  public synchronized RemoteEditLogManifest getEditLogManifest(\n+      long fromTxId) throws IOException {\n+    // Collect RemoteEditLogs available from each FileJournalManager\n+    List\u003cRemoteEditLog\u003e allLogs \u003d Lists.newArrayList();\n+    for (JournalAndStream j : journals) {\n+      if (j.getManager() instanceof FileJournalManager) {\n+        FileJournalManager fjm \u003d (FileJournalManager)j.getManager();\n+        try {\n+          allLogs.addAll(fjm.getRemoteEditLogs(fromTxId));\n+        } catch (Throwable t) {\n+          LOG.warn(\"Cannot list edit logs in \" + fjm, t);\n+        }\n+      }\n     }\n     \n-    return inspector.getEditLogManifest(sinceTxId);\n+    // Group logs by their starting txid\n+    ImmutableListMultimap\u003cLong, RemoteEditLog\u003e logsByStartTxId \u003d\n+      Multimaps.index(allLogs, RemoteEditLog.GET_START_TXID);\n+    long curStartTxId \u003d fromTxId;\n+\n+    List\u003cRemoteEditLog\u003e logs \u003d Lists.newArrayList();\n+    while (true) {\n+      ImmutableList\u003cRemoteEditLog\u003e logGroup \u003d logsByStartTxId.get(curStartTxId);\n+      if (logGroup.isEmpty()) {\n+        // we have a gap in logs - for example because we recovered some old\n+        // storage directory with ancient logs. Clear out any logs we\u0027ve\n+        // accumulated so far, and then skip to the next segment of logs\n+        // after the gap.\n+        SortedSet\u003cLong\u003e startTxIds \u003d Sets.newTreeSet(logsByStartTxId.keySet());\n+        startTxIds \u003d startTxIds.tailSet(curStartTxId);\n+        if (startTxIds.isEmpty()) {\n+          break;\n+        } else {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Found gap in logs at \" + curStartTxId + \": \" +\n+                \"not returning previous logs in manifest.\");\n+          }\n+          logs.clear();\n+          curStartTxId \u003d startTxIds.first();\n+          continue;\n+        }\n+      }\n+\n+      // Find the one that extends the farthest forward\n+      RemoteEditLog bestLog \u003d Collections.max(logGroup);\n+      logs.add(bestLog);\n+      // And then start looking from after that point\n+      curStartTxId \u003d bestLog.getEndTxId() + 1;\n+    }\n+    RemoteEditLogManifest ret \u003d new RemoteEditLogManifest(logs);\n+    \n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Generated manifest for logs since \" + fromTxId + \":\"\n+          + ret);      \n+    }\n+    return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized RemoteEditLogManifest getEditLogManifest(\n      long fromTxId) throws IOException {\n    // Collect RemoteEditLogs available from each FileJournalManager\n    List\u003cRemoteEditLog\u003e allLogs \u003d Lists.newArrayList();\n    for (JournalAndStream j : journals) {\n      if (j.getManager() instanceof FileJournalManager) {\n        FileJournalManager fjm \u003d (FileJournalManager)j.getManager();\n        try {\n          allLogs.addAll(fjm.getRemoteEditLogs(fromTxId));\n        } catch (Throwable t) {\n          LOG.warn(\"Cannot list edit logs in \" + fjm, t);\n        }\n      }\n    }\n    \n    // Group logs by their starting txid\n    ImmutableListMultimap\u003cLong, RemoteEditLog\u003e logsByStartTxId \u003d\n      Multimaps.index(allLogs, RemoteEditLog.GET_START_TXID);\n    long curStartTxId \u003d fromTxId;\n\n    List\u003cRemoteEditLog\u003e logs \u003d Lists.newArrayList();\n    while (true) {\n      ImmutableList\u003cRemoteEditLog\u003e logGroup \u003d logsByStartTxId.get(curStartTxId);\n      if (logGroup.isEmpty()) {\n        // we have a gap in logs - for example because we recovered some old\n        // storage directory with ancient logs. Clear out any logs we\u0027ve\n        // accumulated so far, and then skip to the next segment of logs\n        // after the gap.\n        SortedSet\u003cLong\u003e startTxIds \u003d Sets.newTreeSet(logsByStartTxId.keySet());\n        startTxIds \u003d startTxIds.tailSet(curStartTxId);\n        if (startTxIds.isEmpty()) {\n          break;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Found gap in logs at \" + curStartTxId + \": \" +\n                \"not returning previous logs in manifest.\");\n          }\n          logs.clear();\n          curStartTxId \u003d startTxIds.first();\n          continue;\n        }\n      }\n\n      // Find the one that extends the farthest forward\n      RemoteEditLog bestLog \u003d Collections.max(logGroup);\n      logs.add(bestLog);\n      // And then start looking from after that point\n      curStartTxId \u003d bestLog.getEndTxId() + 1;\n    }\n    RemoteEditLogManifest ret \u003d new RemoteEditLogManifest(logs);\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Generated manifest for logs since \" + fromTxId + \":\"\n          + ret);      \n    }\n    return ret;\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {
            "oldValue": "[sinceTxId-long]",
            "newValue": "[fromTxId-long]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2227. getRemoteEditLogManifest should pull its information from FileJournalManager during checkpoint process. Contributed by Ivan Kelly and Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1155977 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/08/11 5:01 PM",
          "commitName": "eb6e44b1ba58ed971360a39ea5d5ce02ae65aa0f",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "04/08/11 2:56 PM",
          "commitNameOld": "23762da4fa17ce6ea7b70722147977123a28a7e6",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 5.09,
          "commitsBetweenForRepo": 14,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,11 +1,57 @@\n-  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n-      throws IOException {\n-    FSImageTransactionalStorageInspector inspector \u003d\n-        new FSImageTransactionalStorageInspector();\n-\n-    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n-      inspector.inspectDirectory(sd);\n+  public synchronized RemoteEditLogManifest getEditLogManifest(\n+      long fromTxId) throws IOException {\n+    // Collect RemoteEditLogs available from each FileJournalManager\n+    List\u003cRemoteEditLog\u003e allLogs \u003d Lists.newArrayList();\n+    for (JournalAndStream j : journals) {\n+      if (j.getManager() instanceof FileJournalManager) {\n+        FileJournalManager fjm \u003d (FileJournalManager)j.getManager();\n+        try {\n+          allLogs.addAll(fjm.getRemoteEditLogs(fromTxId));\n+        } catch (Throwable t) {\n+          LOG.warn(\"Cannot list edit logs in \" + fjm, t);\n+        }\n+      }\n     }\n     \n-    return inspector.getEditLogManifest(sinceTxId);\n+    // Group logs by their starting txid\n+    ImmutableListMultimap\u003cLong, RemoteEditLog\u003e logsByStartTxId \u003d\n+      Multimaps.index(allLogs, RemoteEditLog.GET_START_TXID);\n+    long curStartTxId \u003d fromTxId;\n+\n+    List\u003cRemoteEditLog\u003e logs \u003d Lists.newArrayList();\n+    while (true) {\n+      ImmutableList\u003cRemoteEditLog\u003e logGroup \u003d logsByStartTxId.get(curStartTxId);\n+      if (logGroup.isEmpty()) {\n+        // we have a gap in logs - for example because we recovered some old\n+        // storage directory with ancient logs. Clear out any logs we\u0027ve\n+        // accumulated so far, and then skip to the next segment of logs\n+        // after the gap.\n+        SortedSet\u003cLong\u003e startTxIds \u003d Sets.newTreeSet(logsByStartTxId.keySet());\n+        startTxIds \u003d startTxIds.tailSet(curStartTxId);\n+        if (startTxIds.isEmpty()) {\n+          break;\n+        } else {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Found gap in logs at \" + curStartTxId + \": \" +\n+                \"not returning previous logs in manifest.\");\n+          }\n+          logs.clear();\n+          curStartTxId \u003d startTxIds.first();\n+          continue;\n+        }\n+      }\n+\n+      // Find the one that extends the farthest forward\n+      RemoteEditLog bestLog \u003d Collections.max(logGroup);\n+      logs.add(bestLog);\n+      // And then start looking from after that point\n+      curStartTxId \u003d bestLog.getEndTxId() + 1;\n+    }\n+    RemoteEditLogManifest ret \u003d new RemoteEditLogManifest(logs);\n+    \n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Generated manifest for logs since \" + fromTxId + \":\"\n+          + ret);      \n+    }\n+    return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized RemoteEditLogManifest getEditLogManifest(\n      long fromTxId) throws IOException {\n    // Collect RemoteEditLogs available from each FileJournalManager\n    List\u003cRemoteEditLog\u003e allLogs \u003d Lists.newArrayList();\n    for (JournalAndStream j : journals) {\n      if (j.getManager() instanceof FileJournalManager) {\n        FileJournalManager fjm \u003d (FileJournalManager)j.getManager();\n        try {\n          allLogs.addAll(fjm.getRemoteEditLogs(fromTxId));\n        } catch (Throwable t) {\n          LOG.warn(\"Cannot list edit logs in \" + fjm, t);\n        }\n      }\n    }\n    \n    // Group logs by their starting txid\n    ImmutableListMultimap\u003cLong, RemoteEditLog\u003e logsByStartTxId \u003d\n      Multimaps.index(allLogs, RemoteEditLog.GET_START_TXID);\n    long curStartTxId \u003d fromTxId;\n\n    List\u003cRemoteEditLog\u003e logs \u003d Lists.newArrayList();\n    while (true) {\n      ImmutableList\u003cRemoteEditLog\u003e logGroup \u003d logsByStartTxId.get(curStartTxId);\n      if (logGroup.isEmpty()) {\n        // we have a gap in logs - for example because we recovered some old\n        // storage directory with ancient logs. Clear out any logs we\u0027ve\n        // accumulated so far, and then skip to the next segment of logs\n        // after the gap.\n        SortedSet\u003cLong\u003e startTxIds \u003d Sets.newTreeSet(logsByStartTxId.keySet());\n        startTxIds \u003d startTxIds.tailSet(curStartTxId);\n        if (startTxIds.isEmpty()) {\n          break;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Found gap in logs at \" + curStartTxId + \": \" +\n                \"not returning previous logs in manifest.\");\n          }\n          logs.clear();\n          curStartTxId \u003d startTxIds.first();\n          continue;\n        }\n      }\n\n      // Find the one that extends the farthest forward\n      RemoteEditLog bestLog \u003d Collections.max(logGroup);\n      logs.add(bestLog);\n      // And then start looking from after that point\n      curStartTxId \u003d bestLog.getEndTxId() + 1;\n    }\n    RemoteEditLogManifest ret \u003d new RemoteEditLogManifest(logs);\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Generated manifest for logs since \" + fromTxId + \":\"\n          + ret);      \n    }\n    return ret;\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[public, synchronized]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2227. getRemoteEditLogManifest should pull its information from FileJournalManager during checkpoint process. Contributed by Ivan Kelly and Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1155977 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/08/11 5:01 PM",
          "commitName": "eb6e44b1ba58ed971360a39ea5d5ce02ae65aa0f",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "04/08/11 2:56 PM",
          "commitNameOld": "23762da4fa17ce6ea7b70722147977123a28a7e6",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 5.09,
          "commitsBetweenForRepo": 14,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,11 +1,57 @@\n-  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n-      throws IOException {\n-    FSImageTransactionalStorageInspector inspector \u003d\n-        new FSImageTransactionalStorageInspector();\n-\n-    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n-      inspector.inspectDirectory(sd);\n+  public synchronized RemoteEditLogManifest getEditLogManifest(\n+      long fromTxId) throws IOException {\n+    // Collect RemoteEditLogs available from each FileJournalManager\n+    List\u003cRemoteEditLog\u003e allLogs \u003d Lists.newArrayList();\n+    for (JournalAndStream j : journals) {\n+      if (j.getManager() instanceof FileJournalManager) {\n+        FileJournalManager fjm \u003d (FileJournalManager)j.getManager();\n+        try {\n+          allLogs.addAll(fjm.getRemoteEditLogs(fromTxId));\n+        } catch (Throwable t) {\n+          LOG.warn(\"Cannot list edit logs in \" + fjm, t);\n+        }\n+      }\n     }\n     \n-    return inspector.getEditLogManifest(sinceTxId);\n+    // Group logs by their starting txid\n+    ImmutableListMultimap\u003cLong, RemoteEditLog\u003e logsByStartTxId \u003d\n+      Multimaps.index(allLogs, RemoteEditLog.GET_START_TXID);\n+    long curStartTxId \u003d fromTxId;\n+\n+    List\u003cRemoteEditLog\u003e logs \u003d Lists.newArrayList();\n+    while (true) {\n+      ImmutableList\u003cRemoteEditLog\u003e logGroup \u003d logsByStartTxId.get(curStartTxId);\n+      if (logGroup.isEmpty()) {\n+        // we have a gap in logs - for example because we recovered some old\n+        // storage directory with ancient logs. Clear out any logs we\u0027ve\n+        // accumulated so far, and then skip to the next segment of logs\n+        // after the gap.\n+        SortedSet\u003cLong\u003e startTxIds \u003d Sets.newTreeSet(logsByStartTxId.keySet());\n+        startTxIds \u003d startTxIds.tailSet(curStartTxId);\n+        if (startTxIds.isEmpty()) {\n+          break;\n+        } else {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Found gap in logs at \" + curStartTxId + \": \" +\n+                \"not returning previous logs in manifest.\");\n+          }\n+          logs.clear();\n+          curStartTxId \u003d startTxIds.first();\n+          continue;\n+        }\n+      }\n+\n+      // Find the one that extends the farthest forward\n+      RemoteEditLog bestLog \u003d Collections.max(logGroup);\n+      logs.add(bestLog);\n+      // And then start looking from after that point\n+      curStartTxId \u003d bestLog.getEndTxId() + 1;\n+    }\n+    RemoteEditLogManifest ret \u003d new RemoteEditLogManifest(logs);\n+    \n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Generated manifest for logs since \" + fromTxId + \":\"\n+          + ret);      \n+    }\n+    return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized RemoteEditLogManifest getEditLogManifest(\n      long fromTxId) throws IOException {\n    // Collect RemoteEditLogs available from each FileJournalManager\n    List\u003cRemoteEditLog\u003e allLogs \u003d Lists.newArrayList();\n    for (JournalAndStream j : journals) {\n      if (j.getManager() instanceof FileJournalManager) {\n        FileJournalManager fjm \u003d (FileJournalManager)j.getManager();\n        try {\n          allLogs.addAll(fjm.getRemoteEditLogs(fromTxId));\n        } catch (Throwable t) {\n          LOG.warn(\"Cannot list edit logs in \" + fjm, t);\n        }\n      }\n    }\n    \n    // Group logs by their starting txid\n    ImmutableListMultimap\u003cLong, RemoteEditLog\u003e logsByStartTxId \u003d\n      Multimaps.index(allLogs, RemoteEditLog.GET_START_TXID);\n    long curStartTxId \u003d fromTxId;\n\n    List\u003cRemoteEditLog\u003e logs \u003d Lists.newArrayList();\n    while (true) {\n      ImmutableList\u003cRemoteEditLog\u003e logGroup \u003d logsByStartTxId.get(curStartTxId);\n      if (logGroup.isEmpty()) {\n        // we have a gap in logs - for example because we recovered some old\n        // storage directory with ancient logs. Clear out any logs we\u0027ve\n        // accumulated so far, and then skip to the next segment of logs\n        // after the gap.\n        SortedSet\u003cLong\u003e startTxIds \u003d Sets.newTreeSet(logsByStartTxId.keySet());\n        startTxIds \u003d startTxIds.tailSet(curStartTxId);\n        if (startTxIds.isEmpty()) {\n          break;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Found gap in logs at \" + curStartTxId + \": \" +\n                \"not returning previous logs in manifest.\");\n          }\n          logs.clear();\n          curStartTxId \u003d startTxIds.first();\n          continue;\n        }\n      }\n\n      // Find the one that extends the farthest forward\n      RemoteEditLog bestLog \u003d Collections.max(logGroup);\n      logs.add(bestLog);\n      // And then start looking from after that point\n      curStartTxId \u003d bestLog.getEndTxId() + 1;\n    }\n    RemoteEditLogManifest ret \u003d new RemoteEditLogManifest(logs);\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Generated manifest for logs since \" + fromTxId + \":\"\n          + ret);      \n    }\n    return ret;\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {}
        }
      ]
    },
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": {
      "type": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/07/11 9:28 AM",
      "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "29/07/11 9:28 AM",
          "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "26/07/11 1:46 PM",
          "commitNameOld": "438c32aaf9fb0c63f55044cf5ef1b2e0adcf7fea",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 2.82,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,11 @@\n-  synchronized long getEditLogSize() throws IOException {\n-    assert getNumEditsDirs() \u003c\u003d getNumEditStreams() : \n-        \"Number of edits directories should not exceed the number of streams.\";\n-    long size \u003d 0;\n-    ArrayList\u003cEditLogOutputStream\u003e al \u003d null;\n-    for (int idx \u003d 0; idx \u003c getNumEditStreams(); idx++) {\n-      EditLogOutputStream es \u003d editStreams.get(idx);\n-      try {\n-        long curSize \u003d es.length();\n-        assert (size \u003d\u003d 0 || size \u003d\u003d curSize || curSize \u003d\u003d0) :\n-          \"Wrong streams size\";\n-        size \u003d Math.max(size, curSize);\n-      } catch (IOException e) {\n-        LOG.error(\"getEditLogSize: editstream.length failed. removing editlog (\" +\n-            idx + \") \" + es.getName());\n-        if(al\u003d\u003dnull) al \u003d new ArrayList\u003cEditLogOutputStream\u003e(1);\n-        al.add(es);\n-      }\n+  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n+      throws IOException {\n+    FSImageTransactionalStorageInspector inspector \u003d\n+        new FSImageTransactionalStorageInspector();\n+\n+    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n+      inspector.inspectDirectory(sd);\n     }\n-    if(al!\u003dnull) disableAndReportErrorOnStreams(al);\n-    return size;\n+    \n+    return inspector.getEditLogManifest(sinceTxId);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n      throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n\n    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n      inspector.inspectDirectory(sd);\n    }\n    \n    return inspector.getEditLogManifest(sinceTxId);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {
            "oldValue": "getEditLogSize",
            "newValue": "getEditLogManifest"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "29/07/11 9:28 AM",
          "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "26/07/11 1:46 PM",
          "commitNameOld": "438c32aaf9fb0c63f55044cf5ef1b2e0adcf7fea",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 2.82,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,11 @@\n-  synchronized long getEditLogSize() throws IOException {\n-    assert getNumEditsDirs() \u003c\u003d getNumEditStreams() : \n-        \"Number of edits directories should not exceed the number of streams.\";\n-    long size \u003d 0;\n-    ArrayList\u003cEditLogOutputStream\u003e al \u003d null;\n-    for (int idx \u003d 0; idx \u003c getNumEditStreams(); idx++) {\n-      EditLogOutputStream es \u003d editStreams.get(idx);\n-      try {\n-        long curSize \u003d es.length();\n-        assert (size \u003d\u003d 0 || size \u003d\u003d curSize || curSize \u003d\u003d0) :\n-          \"Wrong streams size\";\n-        size \u003d Math.max(size, curSize);\n-      } catch (IOException e) {\n-        LOG.error(\"getEditLogSize: editstream.length failed. removing editlog (\" +\n-            idx + \") \" + es.getName());\n-        if(al\u003d\u003dnull) al \u003d new ArrayList\u003cEditLogOutputStream\u003e(1);\n-        al.add(es);\n-      }\n+  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n+      throws IOException {\n+    FSImageTransactionalStorageInspector inspector \u003d\n+        new FSImageTransactionalStorageInspector();\n+\n+    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n+      inspector.inspectDirectory(sd);\n     }\n-    if(al!\u003dnull) disableAndReportErrorOnStreams(al);\n-    return size;\n+    \n+    return inspector.getEditLogManifest(sinceTxId);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n      throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n\n    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n      inspector.inspectDirectory(sd);\n    }\n    \n    return inspector.getEditLogManifest(sinceTxId);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[sinceTxId-long]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "29/07/11 9:28 AM",
          "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "26/07/11 1:46 PM",
          "commitNameOld": "438c32aaf9fb0c63f55044cf5ef1b2e0adcf7fea",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 2.82,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,11 @@\n-  synchronized long getEditLogSize() throws IOException {\n-    assert getNumEditsDirs() \u003c\u003d getNumEditStreams() : \n-        \"Number of edits directories should not exceed the number of streams.\";\n-    long size \u003d 0;\n-    ArrayList\u003cEditLogOutputStream\u003e al \u003d null;\n-    for (int idx \u003d 0; idx \u003c getNumEditStreams(); idx++) {\n-      EditLogOutputStream es \u003d editStreams.get(idx);\n-      try {\n-        long curSize \u003d es.length();\n-        assert (size \u003d\u003d 0 || size \u003d\u003d curSize || curSize \u003d\u003d0) :\n-          \"Wrong streams size\";\n-        size \u003d Math.max(size, curSize);\n-      } catch (IOException e) {\n-        LOG.error(\"getEditLogSize: editstream.length failed. removing editlog (\" +\n-            idx + \") \" + es.getName());\n-        if(al\u003d\u003dnull) al \u003d new ArrayList\u003cEditLogOutputStream\u003e(1);\n-        al.add(es);\n-      }\n+  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n+      throws IOException {\n+    FSImageTransactionalStorageInspector inspector \u003d\n+        new FSImageTransactionalStorageInspector();\n+\n+    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n+      inspector.inspectDirectory(sd);\n     }\n-    if(al!\u003dnull) disableAndReportErrorOnStreams(al);\n-    return size;\n+    \n+    return inspector.getEditLogManifest(sinceTxId);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n      throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n\n    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n      inspector.inspectDirectory(sd);\n    }\n    \n    return inspector.getEditLogManifest(sinceTxId);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {
            "oldValue": "long",
            "newValue": "RemoteEditLogManifest"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "29/07/11 9:28 AM",
          "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "26/07/11 1:46 PM",
          "commitNameOld": "438c32aaf9fb0c63f55044cf5ef1b2e0adcf7fea",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 2.82,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,11 @@\n-  synchronized long getEditLogSize() throws IOException {\n-    assert getNumEditsDirs() \u003c\u003d getNumEditStreams() : \n-        \"Number of edits directories should not exceed the number of streams.\";\n-    long size \u003d 0;\n-    ArrayList\u003cEditLogOutputStream\u003e al \u003d null;\n-    for (int idx \u003d 0; idx \u003c getNumEditStreams(); idx++) {\n-      EditLogOutputStream es \u003d editStreams.get(idx);\n-      try {\n-        long curSize \u003d es.length();\n-        assert (size \u003d\u003d 0 || size \u003d\u003d curSize || curSize \u003d\u003d0) :\n-          \"Wrong streams size\";\n-        size \u003d Math.max(size, curSize);\n-      } catch (IOException e) {\n-        LOG.error(\"getEditLogSize: editstream.length failed. removing editlog (\" +\n-            idx + \") \" + es.getName());\n-        if(al\u003d\u003dnull) al \u003d new ArrayList\u003cEditLogOutputStream\u003e(1);\n-        al.add(es);\n-      }\n+  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n+      throws IOException {\n+    FSImageTransactionalStorageInspector inspector \u003d\n+        new FSImageTransactionalStorageInspector();\n+\n+    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n+      inspector.inspectDirectory(sd);\n     }\n-    if(al!\u003dnull) disableAndReportErrorOnStreams(al);\n-    return size;\n+    \n+    return inspector.getEditLogManifest(sinceTxId);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n      throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n\n    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n      inspector.inspectDirectory(sd);\n    }\n    \n    return inspector.getEditLogManifest(sinceTxId);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {
            "oldValue": "[synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "29/07/11 9:28 AM",
          "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "26/07/11 1:46 PM",
          "commitNameOld": "438c32aaf9fb0c63f55044cf5ef1b2e0adcf7fea",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 2.82,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,11 @@\n-  synchronized long getEditLogSize() throws IOException {\n-    assert getNumEditsDirs() \u003c\u003d getNumEditStreams() : \n-        \"Number of edits directories should not exceed the number of streams.\";\n-    long size \u003d 0;\n-    ArrayList\u003cEditLogOutputStream\u003e al \u003d null;\n-    for (int idx \u003d 0; idx \u003c getNumEditStreams(); idx++) {\n-      EditLogOutputStream es \u003d editStreams.get(idx);\n-      try {\n-        long curSize \u003d es.length();\n-        assert (size \u003d\u003d 0 || size \u003d\u003d curSize || curSize \u003d\u003d0) :\n-          \"Wrong streams size\";\n-        size \u003d Math.max(size, curSize);\n-      } catch (IOException e) {\n-        LOG.error(\"getEditLogSize: editstream.length failed. removing editlog (\" +\n-            idx + \") \" + es.getName());\n-        if(al\u003d\u003dnull) al \u003d new ArrayList\u003cEditLogOutputStream\u003e(1);\n-        al.add(es);\n-      }\n+  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n+      throws IOException {\n+    FSImageTransactionalStorageInspector inspector \u003d\n+        new FSImageTransactionalStorageInspector();\n+\n+    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n+      inspector.inspectDirectory(sd);\n     }\n-    if(al!\u003dnull) disableAndReportErrorOnStreams(al);\n-    return size;\n+    \n+    return inspector.getEditLogManifest(sinceTxId);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n      throws IOException {\n    FSImageTransactionalStorageInspector inspector \u003d\n        new FSImageTransactionalStorageInspector();\n\n    for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {\n      inspector.inspectDirectory(sd);\n    }\n    \n    return inspector.getEditLogManifest(sinceTxId);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {}
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,22 @@\n+  synchronized long getEditLogSize() throws IOException {\n+    assert getNumEditsDirs() \u003c\u003d getNumEditStreams() : \n+        \"Number of edits directories should not exceed the number of streams.\";\n+    long size \u003d 0;\n+    ArrayList\u003cEditLogOutputStream\u003e al \u003d null;\n+    for (int idx \u003d 0; idx \u003c getNumEditStreams(); idx++) {\n+      EditLogOutputStream es \u003d editStreams.get(idx);\n+      try {\n+        long curSize \u003d es.length();\n+        assert (size \u003d\u003d 0 || size \u003d\u003d curSize || curSize \u003d\u003d0) :\n+          \"Wrong streams size\";\n+        size \u003d Math.max(size, curSize);\n+      } catch (IOException e) {\n+        LOG.error(\"getEditLogSize: editstream.length failed. removing editlog (\" +\n+            idx + \") \" + es.getName());\n+        if(al\u003d\u003dnull) al \u003d new ArrayList\u003cEditLogOutputStream\u003e(1);\n+        al.add(es);\n+      }\n+    }\n+    if(al!\u003dnull) disableAndReportErrorOnStreams(al);\n+    return size;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized long getEditLogSize() throws IOException {\n    assert getNumEditsDirs() \u003c\u003d getNumEditStreams() : \n        \"Number of edits directories should not exceed the number of streams.\";\n    long size \u003d 0;\n    ArrayList\u003cEditLogOutputStream\u003e al \u003d null;\n    for (int idx \u003d 0; idx \u003c getNumEditStreams(); idx++) {\n      EditLogOutputStream es \u003d editStreams.get(idx);\n      try {\n        long curSize \u003d es.length();\n        assert (size \u003d\u003d 0 || size \u003d\u003d curSize || curSize \u003d\u003d0) :\n          \"Wrong streams size\";\n        size \u003d Math.max(size, curSize);\n      } catch (IOException e) {\n        LOG.error(\"getEditLogSize: editstream.length failed. removing editlog (\" +\n            idx + \") \" + es.getName());\n        if(al\u003d\u003dnull) al \u003d new ArrayList\u003cEditLogOutputStream\u003e(1);\n        al.add(es);\n      }\n    }\n    if(al!\u003dnull) disableAndReportErrorOnStreams(al);\n    return size;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java"
    }
  }
}