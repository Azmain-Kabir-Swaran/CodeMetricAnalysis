{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSOutputStream.java",
  "functionName": "completeFile",
  "functionId": "completeFile___last-ExtendedBlock",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
  "functionStartLine": 948,
  "functionEndLine": 985,
  "numCommitsSeen": 146,
  "timeTaken": 9440,
  "changeHistory": [
    "3c1a1ceea9e35ac53376276139416b728ed57f10",
    "10185d9a77ce07080588f3c77399a07cd7ccf427",
    "2c155afe2736a5571bbb3bdfb2fe6f9709227229",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "15612313f578a5115f8d03885e9b0c8c376ed56e",
    "57d8f829d930091d82ec50b2ff7d327d0301e9d6",
    "ceea91c9cd8b2a18be13217894ccf1c17198de18",
    "92cbba386ff6e8daafc813a07aa30dbbe9825b1d",
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
    "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
    "735046ebecd9e803398be56fbf79dbde5226b4c1",
    "4a5ba3b7bd2360fd9605863630b477d362874e1e",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "3c1a1ceea9e35ac53376276139416b728ed57f10": "Ybodychange",
    "10185d9a77ce07080588f3c77399a07cd7ccf427": "Ybodychange",
    "2c155afe2736a5571bbb3bdfb2fe6f9709227229": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a": "Ymodifierchange",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "15612313f578a5115f8d03885e9b0c8c376ed56e": "Ybodychange",
    "57d8f829d930091d82ec50b2ff7d327d0301e9d6": "Ybodychange",
    "ceea91c9cd8b2a18be13217894ccf1c17198de18": "Ybodychange",
    "92cbba386ff6e8daafc813a07aa30dbbe9825b1d": "Ybodychange",
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6": "Ybodychange",
    "cea7bbc630deede93dbe6a1bbda56ad49de4f3de": "Ybodychange",
    "735046ebecd9e803398be56fbf79dbde5226b4c1": "Ybodychange",
    "4a5ba3b7bd2360fd9605863630b477d362874e1e": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3c1a1ceea9e35ac53376276139416b728ed57f10": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14487. Missing Space in Client Error Message (Contributed by Shweta Yakkali via Daniel Templeton)\n\nChange-Id: I0f8ce74a35ab24fe94fd0e57d8247bb3fa575e6f\n",
      "commitDate": "18/06/19 10:21 AM",
      "commitName": "3c1a1ceea9e35ac53376276139416b728ed57f10",
      "commitAuthor": "Shweta Yakkali",
      "commitDateOld": "05/03/19 5:56 AM",
      "commitNameOld": "f940ab242da80a22bae95509d5c282d7e2f7ecdb",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 105.14,
      "commitsBetweenForRepo": 710,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   protected void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.monotonicNow();\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     long sleeptime \u003d conf.getBlockWriteLocateFollowingInitialDelayMs();\n     long maxSleepTime \u003d conf.getBlockWriteLocateFollowingMaxDelayMs();\n     boolean fileComplete \u003d false;\n     int retries \u003d conf.getNumBlockWriteLocateFollowingRetry();\n     while (!fileComplete) {\n       fileComplete \u003d\n           dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n       if (!fileComplete) {\n         final int hdfsTimeout \u003d conf.getHdfsTimeout();\n         if (!dfsClient.clientRunning\n             || (hdfsTimeout \u003e 0\n                 \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n           String msg \u003d \"Unable to close file because dfsclient \" +\n               \" was unable to contact the HDFS servers. clientRunning \" +\n               dfsClient.clientRunning + \" hdfsTimeout \" + hdfsTimeout;\n           DFSClient.LOG.info(msg);\n           throw new IOException(msg);\n         }\n         try {\n           if (retries \u003d\u003d 0) {\n-            throw new IOException(\"Unable to close file because the last block\"\n+            throw new IOException(\"Unable to close file because the last block \"\n                 + last + \" does not have enough number of replicas.\");\n           }\n           retries--;\n           Thread.sleep(sleeptime);\n           sleeptime \u003d calculateDelayForNextRetry(sleeptime, maxSleepTime);\n           if (Time.monotonicNow() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.monotonicNow();\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    long sleeptime \u003d conf.getBlockWriteLocateFollowingInitialDelayMs();\n    long maxSleepTime \u003d conf.getBlockWriteLocateFollowingMaxDelayMs();\n    boolean fileComplete \u003d false;\n    int retries \u003d conf.getNumBlockWriteLocateFollowingRetry();\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d conf.getHdfsTimeout();\n        if (!dfsClient.clientRunning\n            || (hdfsTimeout \u003e 0\n                \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n          String msg \u003d \"Unable to close file because dfsclient \" +\n              \" was unable to contact the HDFS servers. clientRunning \" +\n              dfsClient.clientRunning + \" hdfsTimeout \" + hdfsTimeout;\n          DFSClient.LOG.info(msg);\n          throw new IOException(msg);\n        }\n        try {\n          if (retries \u003d\u003d 0) {\n            throw new IOException(\"Unable to close file because the last block \"\n                + last + \" does not have enough number of replicas.\");\n          }\n          retries--;\n          Thread.sleep(sleeptime);\n          sleeptime \u003d calculateDelayForNextRetry(sleeptime, maxSleepTime);\n          if (Time.monotonicNow() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "10185d9a77ce07080588f3c77399a07cd7ccf427": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13882. Set a maximum delay for retrying locateFollowingBlock. Contributed by Kitti Nanasi.\n\nSigned-off-by: Xiao Chen \u003cxiao@apache.org\u003e\n",
      "commitDate": "10/10/18 8:55 AM",
      "commitName": "10185d9a77ce07080588f3c77399a07cd7ccf427",
      "commitAuthor": "Kitti Nanasi",
      "commitDateOld": "24/07/18 4:05 PM",
      "commitNameOld": "849c45db187224095b13fe297a4d7377fbb9d2cd",
      "commitAuthorOld": "Daniel Templeton",
      "daysBetweenCommits": 77.7,
      "commitsBetweenForRepo": 692,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,38 @@\n   protected void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.monotonicNow();\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     long sleeptime \u003d conf.getBlockWriteLocateFollowingInitialDelayMs();\n+    long maxSleepTime \u003d conf.getBlockWriteLocateFollowingMaxDelayMs();\n     boolean fileComplete \u003d false;\n     int retries \u003d conf.getNumBlockWriteLocateFollowingRetry();\n     while (!fileComplete) {\n       fileComplete \u003d\n           dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n       if (!fileComplete) {\n         final int hdfsTimeout \u003d conf.getHdfsTimeout();\n         if (!dfsClient.clientRunning\n             || (hdfsTimeout \u003e 0\n                 \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n           String msg \u003d \"Unable to close file because dfsclient \" +\n               \" was unable to contact the HDFS servers. clientRunning \" +\n               dfsClient.clientRunning + \" hdfsTimeout \" + hdfsTimeout;\n           DFSClient.LOG.info(msg);\n           throw new IOException(msg);\n         }\n         try {\n           if (retries \u003d\u003d 0) {\n             throw new IOException(\"Unable to close file because the last block\"\n                 + last + \" does not have enough number of replicas.\");\n           }\n           retries--;\n           Thread.sleep(sleeptime);\n-          sleeptime *\u003d 2;\n+          sleeptime \u003d calculateDelayForNextRetry(sleeptime, maxSleepTime);\n           if (Time.monotonicNow() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.monotonicNow();\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    long sleeptime \u003d conf.getBlockWriteLocateFollowingInitialDelayMs();\n    long maxSleepTime \u003d conf.getBlockWriteLocateFollowingMaxDelayMs();\n    boolean fileComplete \u003d false;\n    int retries \u003d conf.getNumBlockWriteLocateFollowingRetry();\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d conf.getHdfsTimeout();\n        if (!dfsClient.clientRunning\n            || (hdfsTimeout \u003e 0\n                \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n          String msg \u003d \"Unable to close file because dfsclient \" +\n              \" was unable to contact the HDFS servers. clientRunning \" +\n              dfsClient.clientRunning + \" hdfsTimeout \" + hdfsTimeout;\n          DFSClient.LOG.info(msg);\n          throw new IOException(msg);\n        }\n        try {\n          if (retries \u003d\u003d 0) {\n            throw new IOException(\"Unable to close file because the last block\"\n                + last + \" does not have enough number of replicas.\");\n          }\n          retries--;\n          Thread.sleep(sleeptime);\n          sleeptime \u003d calculateDelayForNextRetry(sleeptime, maxSleepTime);\n          if (Time.monotonicNow() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "2c155afe2736a5571bbb3bdfb2fe6f9709227229": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10292. Add block id when client got Unable to close file exception. Contributed by Brahma Reddy Battula.\n",
      "commitDate": "14/04/16 12:25 PM",
      "commitName": "2c155afe2736a5571bbb3bdfb2fe6f9709227229",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "07/03/16 5:43 PM",
      "commitNameOld": "352d299cf8ebe330d24117df98d1e6a64ae38c26",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 37.74,
      "commitsBetweenForRepo": 227,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n   protected void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.monotonicNow();\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     long sleeptime \u003d conf.getBlockWriteLocateFollowingInitialDelayMs();\n     boolean fileComplete \u003d false;\n     int retries \u003d conf.getNumBlockWriteLocateFollowingRetry();\n     while (!fileComplete) {\n       fileComplete \u003d\n           dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n       if (!fileComplete) {\n         final int hdfsTimeout \u003d conf.getHdfsTimeout();\n         if (!dfsClient.clientRunning\n             || (hdfsTimeout \u003e 0\n                 \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n           String msg \u003d \"Unable to close file because dfsclient \" +\n               \" was unable to contact the HDFS servers. clientRunning \" +\n               dfsClient.clientRunning + \" hdfsTimeout \" + hdfsTimeout;\n           DFSClient.LOG.info(msg);\n           throw new IOException(msg);\n         }\n         try {\n           if (retries \u003d\u003d 0) {\n             throw new IOException(\"Unable to close file because the last block\"\n-                + \" does not have enough number of replicas.\");\n+                + last + \" does not have enough number of replicas.\");\n           }\n           retries--;\n           Thread.sleep(sleeptime);\n           sleeptime *\u003d 2;\n           if (Time.monotonicNow() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.monotonicNow();\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    long sleeptime \u003d conf.getBlockWriteLocateFollowingInitialDelayMs();\n    boolean fileComplete \u003d false;\n    int retries \u003d conf.getNumBlockWriteLocateFollowingRetry();\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d conf.getHdfsTimeout();\n        if (!dfsClient.clientRunning\n            || (hdfsTimeout \u003e 0\n                \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n          String msg \u003d \"Unable to close file because dfsclient \" +\n              \" was unable to contact the HDFS servers. clientRunning \" +\n              dfsClient.clientRunning + \" hdfsTimeout \" + hdfsTimeout;\n          DFSClient.LOG.info(msg);\n          throw new IOException(msg);\n        }\n        try {\n          if (retries \u003d\u003d 0) {\n            throw new IOException(\"Unable to close file because the last block\"\n                + last + \" does not have enough number of replicas.\");\n          }\n          retries--;\n          Thread.sleep(sleeptime);\n          sleeptime *\u003d 2;\n          if (Time.monotonicNow() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,37 @@\n   protected void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.monotonicNow();\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     long sleeptime \u003d conf.getBlockWriteLocateFollowingInitialDelayMs();\n     boolean fileComplete \u003d false;\n     int retries \u003d conf.getNumBlockWriteLocateFollowingRetry();\n     while (!fileComplete) {\n       fileComplete \u003d\n           dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n       if (!fileComplete) {\n         final int hdfsTimeout \u003d conf.getHdfsTimeout();\n         if (!dfsClient.clientRunning\n             || (hdfsTimeout \u003e 0\n                 \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n-            String msg \u003d \"Unable to close file because dfsclient \" +\n-                          \" was unable to contact the HDFS servers.\" +\n-                          \" clientRunning \" + dfsClient.clientRunning +\n-                          \" hdfsTimeout \" + hdfsTimeout;\n-            DFSClient.LOG.info(msg);\n-            throw new IOException(msg);\n+          String msg \u003d \"Unable to close file because dfsclient \" +\n+              \" was unable to contact the HDFS servers. clientRunning \" +\n+              dfsClient.clientRunning + \" hdfsTimeout \" + hdfsTimeout;\n+          DFSClient.LOG.info(msg);\n+          throw new IOException(msg);\n         }\n         try {\n           if (retries \u003d\u003d 0) {\n             throw new IOException(\"Unable to close file because the last block\"\n                 + \" does not have enough number of replicas.\");\n           }\n           retries--;\n           Thread.sleep(sleeptime);\n           sleeptime *\u003d 2;\n           if (Time.monotonicNow() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.monotonicNow();\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    long sleeptime \u003d conf.getBlockWriteLocateFollowingInitialDelayMs();\n    boolean fileComplete \u003d false;\n    int retries \u003d conf.getNumBlockWriteLocateFollowingRetry();\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d conf.getHdfsTimeout();\n        if (!dfsClient.clientRunning\n            || (hdfsTimeout \u003e 0\n                \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n          String msg \u003d \"Unable to close file because dfsclient \" +\n              \" was unable to contact the HDFS servers. clientRunning \" +\n              dfsClient.clientRunning + \" hdfsTimeout \" + hdfsTimeout;\n          DFSClient.LOG.info(msg);\n          throw new IOException(msg);\n        }\n        try {\n          if (retries \u003d\u003d 0) {\n            throw new IOException(\"Unable to close file because the last block\"\n                + \" does not have enough number of replicas.\");\n          }\n          retries--;\n          Thread.sleep(sleeptime);\n          sleeptime *\u003d 2;\n          if (Time.monotonicNow() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.monotonicNow();\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    long sleeptime \u003d conf.getBlockWriteLocateFollowingInitialDelayMs();\n    boolean fileComplete \u003d false;\n    int retries \u003d conf.getNumBlockWriteLocateFollowingRetry();\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d conf.getHdfsTimeout();\n        if (!dfsClient.clientRunning\n            || (hdfsTimeout \u003e 0\n                \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          if (retries \u003d\u003d 0) {\n            throw new IOException(\"Unable to close file because the last block\"\n                + \" does not have enough number of replicas.\");\n          }\n          retries--;\n          Thread.sleep(sleeptime);\n          sleeptime *\u003d 2;\n          if (Time.monotonicNow() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "02/04/15 10:59 AM",
      "commitNameOld": "9ed43f2189fb4674b7379e8e995d53d4970d5c3a",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 8.16,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   protected void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.monotonicNow();\n-    long sleeptime \u003d dfsClient.getConf().\n-        blockWriteLocateFollowingInitialDelayMs;\n+    final DfsClientConf conf \u003d dfsClient.getConf();\n+    long sleeptime \u003d conf.getBlockWriteLocateFollowingInitialDelayMs();\n     boolean fileComplete \u003d false;\n-    int retries \u003d dfsClient.getConf().nBlockWriteLocateFollowingRetry;\n+    int retries \u003d conf.getNumBlockWriteLocateFollowingRetry();\n     while (!fileComplete) {\n       fileComplete \u003d\n           dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n       if (!fileComplete) {\n-        final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n+        final int hdfsTimeout \u003d conf.getHdfsTimeout();\n         if (!dfsClient.clientRunning\n             || (hdfsTimeout \u003e 0\n                 \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n             String msg \u003d \"Unable to close file because dfsclient \" +\n                           \" was unable to contact the HDFS servers.\" +\n                           \" clientRunning \" + dfsClient.clientRunning +\n                           \" hdfsTimeout \" + hdfsTimeout;\n             DFSClient.LOG.info(msg);\n             throw new IOException(msg);\n         }\n         try {\n           if (retries \u003d\u003d 0) {\n             throw new IOException(\"Unable to close file because the last block\"\n                 + \" does not have enough number of replicas.\");\n           }\n           retries--;\n           Thread.sleep(sleeptime);\n           sleeptime *\u003d 2;\n           if (Time.monotonicNow() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.monotonicNow();\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    long sleeptime \u003d conf.getBlockWriteLocateFollowingInitialDelayMs();\n    boolean fileComplete \u003d false;\n    int retries \u003d conf.getNumBlockWriteLocateFollowingRetry();\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d conf.getHdfsTimeout();\n        if (!dfsClient.clientRunning\n            || (hdfsTimeout \u003e 0\n                \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          if (retries \u003d\u003d 0) {\n            throw new IOException(\"Unable to close file because the last block\"\n                + \" does not have enough number of replicas.\");\n          }\n          retries--;\n          Thread.sleep(sleeptime);\n          sleeptime *\u003d 2;\n          if (Time.monotonicNow() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-7888. Change DFSOutputStream and DataStreamer for convenience of subclassing. Contributed by Li Bo\n",
      "commitDate": "02/04/15 10:59 AM",
      "commitName": "9ed43f2189fb4674b7379e8e995d53d4970d5c3a",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "01/04/15 2:10 PM",
      "commitNameOld": "c94d594a57806dec515e2a2053a1221f8ce48cc4",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.87,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n-  private void completeFile(ExtendedBlock last) throws IOException {\n+  protected void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.monotonicNow();\n     long sleeptime \u003d dfsClient.getConf().\n         blockWriteLocateFollowingInitialDelayMs;\n     boolean fileComplete \u003d false;\n     int retries \u003d dfsClient.getConf().nBlockWriteLocateFollowingRetry;\n     while (!fileComplete) {\n       fileComplete \u003d\n           dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n       if (!fileComplete) {\n         final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n         if (!dfsClient.clientRunning\n             || (hdfsTimeout \u003e 0\n                 \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n             String msg \u003d \"Unable to close file because dfsclient \" +\n                           \" was unable to contact the HDFS servers.\" +\n                           \" clientRunning \" + dfsClient.clientRunning +\n                           \" hdfsTimeout \" + hdfsTimeout;\n             DFSClient.LOG.info(msg);\n             throw new IOException(msg);\n         }\n         try {\n           if (retries \u003d\u003d 0) {\n             throw new IOException(\"Unable to close file because the last block\"\n                 + \" does not have enough number of replicas.\");\n           }\n           retries--;\n           Thread.sleep(sleeptime);\n           sleeptime *\u003d 2;\n           if (Time.monotonicNow() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.monotonicNow();\n    long sleeptime \u003d dfsClient.getConf().\n        blockWriteLocateFollowingInitialDelayMs;\n    boolean fileComplete \u003d false;\n    int retries \u003d dfsClient.getConf().nBlockWriteLocateFollowingRetry;\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n        if (!dfsClient.clientRunning\n            || (hdfsTimeout \u003e 0\n                \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          if (retries \u003d\u003d 0) {\n            throw new IOException(\"Unable to close file because the last block\"\n                + \" does not have enough number of replicas.\");\n          }\n          retries--;\n          Thread.sleep(sleeptime);\n          sleeptime *\u003d 2;\n          if (Time.monotonicNow() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[protected]"
      }
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "20/03/15 9:12 AM",
      "commitNameOld": "15612313f578a5115f8d03885e9b0c8c376ed56e",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 0.12,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,38 @@\n   private void completeFile(ExtendedBlock last) throws IOException {\n-    long localstart \u003d Time.now();\n+    long localstart \u003d Time.monotonicNow();\n     long sleeptime \u003d dfsClient.getConf().\n         blockWriteLocateFollowingInitialDelayMs;\n     boolean fileComplete \u003d false;\n     int retries \u003d dfsClient.getConf().nBlockWriteLocateFollowingRetry;\n     while (!fileComplete) {\n       fileComplete \u003d\n           dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n       if (!fileComplete) {\n         final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n-        if (!dfsClient.clientRunning ||\n-              (hdfsTimeout \u003e 0 \u0026\u0026 localstart + hdfsTimeout \u003c Time.now())) {\n+        if (!dfsClient.clientRunning\n+            || (hdfsTimeout \u003e 0 \n+                \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n             String msg \u003d \"Unable to close file because dfsclient \" +\n                           \" was unable to contact the HDFS servers.\" +\n                           \" clientRunning \" + dfsClient.clientRunning +\n                           \" hdfsTimeout \" + hdfsTimeout;\n             DFSClient.LOG.info(msg);\n             throw new IOException(msg);\n         }\n         try {\n           if (retries \u003d\u003d 0) {\n             throw new IOException(\"Unable to close file because the last block\"\n                 + \" does not have enough number of replicas.\");\n           }\n           retries--;\n           Thread.sleep(sleeptime);\n           sleeptime *\u003d 2;\n-          if (Time.now() - localstart \u003e 5000) {\n+          if (Time.monotonicNow() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.monotonicNow();\n    long sleeptime \u003d dfsClient.getConf().\n        blockWriteLocateFollowingInitialDelayMs;\n    boolean fileComplete \u003d false;\n    int retries \u003d dfsClient.getConf().nBlockWriteLocateFollowingRetry;\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n        if (!dfsClient.clientRunning\n            || (hdfsTimeout \u003e 0 \n                \u0026\u0026 localstart + hdfsTimeout \u003c Time.monotonicNow())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          if (retries \u003d\u003d 0) {\n            throw new IOException(\"Unable to close file because the last block\"\n                + \" does not have enough number of replicas.\");\n          }\n          retries--;\n          Thread.sleep(sleeptime);\n          sleeptime *\u003d 2;\n          if (Time.monotonicNow() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "15612313f578a5115f8d03885e9b0c8c376ed56e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7835. make initial sleeptime in locateFollowingBlock configurable for DFSClient. Contributed by Zhihai Xu.\n",
      "commitDate": "20/03/15 9:12 AM",
      "commitName": "15612313f578a5115f8d03885e9b0c8c376ed56e",
      "commitAuthor": "Yongjun Zhang",
      "commitDateOld": "18/03/15 6:14 PM",
      "commitNameOld": "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.62,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,37 @@\n   private void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.now();\n-    long localTimeout \u003d 400;\n+    long sleeptime \u003d dfsClient.getConf().\n+        blockWriteLocateFollowingInitialDelayMs;\n     boolean fileComplete \u003d false;\n     int retries \u003d dfsClient.getConf().nBlockWriteLocateFollowingRetry;\n     while (!fileComplete) {\n       fileComplete \u003d\n           dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n       if (!fileComplete) {\n         final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n         if (!dfsClient.clientRunning ||\n               (hdfsTimeout \u003e 0 \u0026\u0026 localstart + hdfsTimeout \u003c Time.now())) {\n             String msg \u003d \"Unable to close file because dfsclient \" +\n                           \" was unable to contact the HDFS servers.\" +\n                           \" clientRunning \" + dfsClient.clientRunning +\n                           \" hdfsTimeout \" + hdfsTimeout;\n             DFSClient.LOG.info(msg);\n             throw new IOException(msg);\n         }\n         try {\n           if (retries \u003d\u003d 0) {\n             throw new IOException(\"Unable to close file because the last block\"\n                 + \" does not have enough number of replicas.\");\n           }\n           retries--;\n-          Thread.sleep(localTimeout);\n-          localTimeout *\u003d 2;\n+          Thread.sleep(sleeptime);\n+          sleeptime *\u003d 2;\n           if (Time.now() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.now();\n    long sleeptime \u003d dfsClient.getConf().\n        blockWriteLocateFollowingInitialDelayMs;\n    boolean fileComplete \u003d false;\n    int retries \u003d dfsClient.getConf().nBlockWriteLocateFollowingRetry;\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n        if (!dfsClient.clientRunning ||\n              (hdfsTimeout \u003e 0 \u0026\u0026 localstart + hdfsTimeout \u003c Time.now())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          if (retries \u003d\u003d 0) {\n            throw new IOException(\"Unable to close file because the last block\"\n                + \" does not have enough number of replicas.\");\n          }\n          retries--;\n          Thread.sleep(sleeptime);\n          sleeptime *\u003d 2;\n          if (Time.now() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "57d8f829d930091d82ec50b2ff7d327d0301e9d6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6755. There is an unnecessary sleep in the code path where DFSOutputStream#close gives up its attempt to contact the namenode (mitdesai21 via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1613522 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/07/14 3:12 PM",
      "commitName": "57d8f829d930091d82ec50b2ff7d327d0301e9d6",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "22/07/14 12:41 AM",
      "commitNameOld": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.61,
      "commitsBetweenForRepo": 42,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   private void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.now();\n     long localTimeout \u003d 400;\n     boolean fileComplete \u003d false;\n     int retries \u003d dfsClient.getConf().nBlockWriteLocateFollowingRetry;\n     while (!fileComplete) {\n       fileComplete \u003d\n           dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n       if (!fileComplete) {\n         final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n         if (!dfsClient.clientRunning ||\n               (hdfsTimeout \u003e 0 \u0026\u0026 localstart + hdfsTimeout \u003c Time.now())) {\n             String msg \u003d \"Unable to close file because dfsclient \" +\n                           \" was unable to contact the HDFS servers.\" +\n                           \" clientRunning \" + dfsClient.clientRunning +\n                           \" hdfsTimeout \" + hdfsTimeout;\n             DFSClient.LOG.info(msg);\n             throw new IOException(msg);\n         }\n         try {\n-          Thread.sleep(localTimeout);\n           if (retries \u003d\u003d 0) {\n             throw new IOException(\"Unable to close file because the last block\"\n                 + \" does not have enough number of replicas.\");\n           }\n           retries--;\n+          Thread.sleep(localTimeout);\n           localTimeout *\u003d 2;\n           if (Time.now() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.now();\n    long localTimeout \u003d 400;\n    boolean fileComplete \u003d false;\n    int retries \u003d dfsClient.getConf().nBlockWriteLocateFollowingRetry;\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n        if (!dfsClient.clientRunning ||\n              (hdfsTimeout \u003e 0 \u0026\u0026 localstart + hdfsTimeout \u003c Time.now())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          if (retries \u003d\u003d 0) {\n            throw new IOException(\"Unable to close file because the last block\"\n                + \" does not have enough number of replicas.\");\n          }\n          retries--;\n          Thread.sleep(localTimeout);\n          localTimeout *\u003d 2;\n          if (Time.now() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "ceea91c9cd8b2a18be13217894ccf1c17198de18": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5438. Flaws in block report processing can cause data loss. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1542054 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/11/13 12:11 PM",
      "commitName": "ceea91c9cd8b2a18be13217894ccf1c17198de18",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "25/10/13 11:43 AM",
      "commitNameOld": "5829029154b8e8e02bc6aeb45435046ca080bbe9",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 20.06,
      "commitsBetweenForRepo": 102,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,36 @@\n   private void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.now();\n+    long localTimeout \u003d 400;\n     boolean fileComplete \u003d false;\n+    int retries \u003d dfsClient.getConf().nBlockWriteLocateFollowingRetry;\n     while (!fileComplete) {\n       fileComplete \u003d\n           dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n       if (!fileComplete) {\n         final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n         if (!dfsClient.clientRunning ||\n               (hdfsTimeout \u003e 0 \u0026\u0026 localstart + hdfsTimeout \u003c Time.now())) {\n             String msg \u003d \"Unable to close file because dfsclient \" +\n                           \" was unable to contact the HDFS servers.\" +\n                           \" clientRunning \" + dfsClient.clientRunning +\n                           \" hdfsTimeout \" + hdfsTimeout;\n             DFSClient.LOG.info(msg);\n             throw new IOException(msg);\n         }\n         try {\n-          Thread.sleep(400);\n+          Thread.sleep(localTimeout);\n+          if (retries \u003d\u003d 0) {\n+            throw new IOException(\"Unable to close file because the last block\"\n+                + \" does not have enough number of replicas.\");\n+          }\n+          retries--;\n+          localTimeout *\u003d 2;\n           if (Time.now() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.now();\n    long localTimeout \u003d 400;\n    boolean fileComplete \u003d false;\n    int retries \u003d dfsClient.getConf().nBlockWriteLocateFollowingRetry;\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n        if (!dfsClient.clientRunning ||\n              (hdfsTimeout \u003e 0 \u0026\u0026 localstart + hdfsTimeout \u003c Time.now())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          Thread.sleep(localTimeout);\n          if (retries \u003d\u003d 0) {\n            throw new IOException(\"Unable to close file because the last block\"\n                + \" does not have enough number of replicas.\");\n          }\n          retries--;\n          localTimeout *\u003d 2;\n          if (Time.now() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "92cbba386ff6e8daafc813a07aa30dbbe9825b1d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4883. complete() should verify fileId. Contributed by Tao Luo.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1495302 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/06/13 12:28 AM",
      "commitName": "92cbba386ff6e8daafc813a07aa30dbbe9825b1d",
      "commitAuthor": "Konstantin Shvachko",
      "commitDateOld": "19/06/13 9:43 PM",
      "commitNameOld": "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.11,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,28 @@\n   private void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.now();\n     boolean fileComplete \u003d false;\n     while (!fileComplete) {\n-      fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n+      fileComplete \u003d\n+          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n       if (!fileComplete) {\n         final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n         if (!dfsClient.clientRunning ||\n               (hdfsTimeout \u003e 0 \u0026\u0026 localstart + hdfsTimeout \u003c Time.now())) {\n             String msg \u003d \"Unable to close file because dfsclient \" +\n                           \" was unable to contact the HDFS servers.\" +\n                           \" clientRunning \" + dfsClient.clientRunning +\n                           \" hdfsTimeout \" + hdfsTimeout;\n             DFSClient.LOG.info(msg);\n             throw new IOException(msg);\n         }\n         try {\n           Thread.sleep(400);\n           if (Time.now() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.now();\n    boolean fileComplete \u003d false;\n    while (!fileComplete) {\n      fileComplete \u003d\n          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n        if (!dfsClient.clientRunning ||\n              (hdfsTimeout \u003e 0 \u0026\u0026 localstart + hdfsTimeout \u003c Time.now())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          Thread.sleep(400);\n          if (Time.now() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4914. Use DFSClient.Conf instead of Configuration.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1494854 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/06/13 9:43 PM",
      "commitName": "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "18/06/13 2:05 PM",
      "commitNameOld": "1c309f763be3dd2e3d7d1616d2c960ff80cf9b03",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 1.32,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   private void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.now();\n     boolean fileComplete \u003d false;\n     while (!fileComplete) {\n       fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n       if (!fileComplete) {\n+        final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n         if (!dfsClient.clientRunning ||\n-              (dfsClient.hdfsTimeout \u003e 0 \u0026\u0026\n-               localstart + dfsClient.hdfsTimeout \u003c Time.now())) {\n+              (hdfsTimeout \u003e 0 \u0026\u0026 localstart + hdfsTimeout \u003c Time.now())) {\n             String msg \u003d \"Unable to close file because dfsclient \" +\n                           \" was unable to contact the HDFS servers.\" +\n                           \" clientRunning \" + dfsClient.clientRunning +\n-                          \" hdfsTimeout \" + dfsClient.hdfsTimeout;\n+                          \" hdfsTimeout \" + hdfsTimeout;\n             DFSClient.LOG.info(msg);\n             throw new IOException(msg);\n         }\n         try {\n           Thread.sleep(400);\n           if (Time.now() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.now();\n    boolean fileComplete \u003d false;\n    while (!fileComplete) {\n      fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n      if (!fileComplete) {\n        final int hdfsTimeout \u003d dfsClient.getHdfsTimeout();\n        if (!dfsClient.clientRunning ||\n              (hdfsTimeout \u003e 0 \u0026\u0026 localstart + hdfsTimeout \u003c Time.now())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          Thread.sleep(400);\n          if (Time.now() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cea7bbc630deede93dbe6a1bbda56ad49de4f3de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4122. Cleanup HDFS logs and reduce the size of logged messages. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1403120 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/10/12 4:10 PM",
      "commitName": "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "25/08/12 9:00 PM",
      "commitNameOld": "735046ebecd9e803398be56fbf79dbde5226b4c1",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 63.8,
      "commitsBetweenForRepo": 381,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   private void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.now();\n     boolean fileComplete \u003d false;\n     while (!fileComplete) {\n       fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n       if (!fileComplete) {\n         if (!dfsClient.clientRunning ||\n               (dfsClient.hdfsTimeout \u003e 0 \u0026\u0026\n                localstart + dfsClient.hdfsTimeout \u003c Time.now())) {\n             String msg \u003d \"Unable to close file because dfsclient \" +\n                           \" was unable to contact the HDFS servers.\" +\n                           \" clientRunning \" + dfsClient.clientRunning +\n                           \" hdfsTimeout \" + dfsClient.hdfsTimeout;\n             DFSClient.LOG.info(msg);\n             throw new IOException(msg);\n         }\n         try {\n           Thread.sleep(400);\n           if (Time.now() - localstart \u003e 5000) {\n-            DFSClient.LOG.info(\"Could not complete file \" + src + \" retrying...\");\n+            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n           DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.now();\n    boolean fileComplete \u003d false;\n    while (!fileComplete) {\n      fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n      if (!fileComplete) {\n        if (!dfsClient.clientRunning ||\n              (dfsClient.hdfsTimeout \u003e 0 \u0026\u0026\n               localstart + dfsClient.hdfsTimeout \u003c Time.now())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + dfsClient.hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          Thread.sleep(400);\n          if (Time.now() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "735046ebecd9e803398be56fbf79dbde5226b4c1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3851. DFSOutputStream class code cleanup. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1377372 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/08/12 9:00 PM",
      "commitName": "735046ebecd9e803398be56fbf79dbde5226b4c1",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/08/12 1:59 PM",
      "commitNameOld": "f98d8eb291be364102b5c3011ce72e8f43eab389",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 11.29,
      "commitsBetweenForRepo": 87,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,27 @@\n   private void completeFile(ExtendedBlock last) throws IOException {\n     long localstart \u003d Time.now();\n     boolean fileComplete \u003d false;\n     while (!fileComplete) {\n       fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n       if (!fileComplete) {\n         if (!dfsClient.clientRunning ||\n               (dfsClient.hdfsTimeout \u003e 0 \u0026\u0026\n                localstart + dfsClient.hdfsTimeout \u003c Time.now())) {\n             String msg \u003d \"Unable to close file because dfsclient \" +\n                           \" was unable to contact the HDFS servers.\" +\n                           \" clientRunning \" + dfsClient.clientRunning +\n                           \" hdfsTimeout \" + dfsClient.hdfsTimeout;\n             DFSClient.LOG.info(msg);\n             throw new IOException(msg);\n         }\n         try {\n           Thread.sleep(400);\n           if (Time.now() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete file \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n+          DFSClient.LOG.warn(\"Caught exception \", ie);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.now();\n    boolean fileComplete \u003d false;\n    while (!fileComplete) {\n      fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n      if (!fileComplete) {\n        if (!dfsClient.clientRunning ||\n              (dfsClient.hdfsTimeout \u003e 0 \u0026\u0026\n               localstart + dfsClient.hdfsTimeout \u003c Time.now())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + dfsClient.hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          Thread.sleep(400);\n          if (Time.now() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete file \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n          DFSClient.LOG.warn(\"Caught exception \", ie);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "4a5ba3b7bd2360fd9605863630b477d362874e1e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3641. Move server Util time methods to common and use now instead of System#currentTimeMillis. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1360858 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/07/12 12:01 PM",
      "commitName": "4a5ba3b7bd2360fd9605863630b477d362874e1e",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "30/05/12 12:10 PM",
      "commitNameOld": "83cf475050dba27e72b4e399491638c670621175",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 42.99,
      "commitsBetweenForRepo": 208,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   private void completeFile(ExtendedBlock last) throws IOException {\n-    long localstart \u003d System.currentTimeMillis();\n+    long localstart \u003d Time.now();\n     boolean fileComplete \u003d false;\n     while (!fileComplete) {\n       fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n       if (!fileComplete) {\n         if (!dfsClient.clientRunning ||\n               (dfsClient.hdfsTimeout \u003e 0 \u0026\u0026\n-               localstart + dfsClient.hdfsTimeout \u003c System.currentTimeMillis())) {\n+               localstart + dfsClient.hdfsTimeout \u003c Time.now())) {\n             String msg \u003d \"Unable to close file because dfsclient \" +\n                           \" was unable to contact the HDFS servers.\" +\n                           \" clientRunning \" + dfsClient.clientRunning +\n                           \" hdfsTimeout \" + dfsClient.hdfsTimeout;\n             DFSClient.LOG.info(msg);\n             throw new IOException(msg);\n         }\n         try {\n           Thread.sleep(400);\n-          if (System.currentTimeMillis() - localstart \u003e 5000) {\n+          if (Time.now() - localstart \u003e 5000) {\n             DFSClient.LOG.info(\"Could not complete file \" + src + \" retrying...\");\n           }\n         } catch (InterruptedException ie) {\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d Time.now();\n    boolean fileComplete \u003d false;\n    while (!fileComplete) {\n      fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n      if (!fileComplete) {\n        if (!dfsClient.clientRunning ||\n              (dfsClient.hdfsTimeout \u003e 0 \u0026\u0026\n               localstart + dfsClient.hdfsTimeout \u003c Time.now())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + dfsClient.hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          Thread.sleep(400);\n          if (Time.now() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete file \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d System.currentTimeMillis();\n    boolean fileComplete \u003d false;\n    while (!fileComplete) {\n      fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n      if (!fileComplete) {\n        if (!dfsClient.clientRunning ||\n              (dfsClient.hdfsTimeout \u003e 0 \u0026\u0026\n               localstart + dfsClient.hdfsTimeout \u003c System.currentTimeMillis())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + dfsClient.hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          Thread.sleep(400);\n          if (System.currentTimeMillis() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete file \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d System.currentTimeMillis();\n    boolean fileComplete \u003d false;\n    while (!fileComplete) {\n      fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n      if (!fileComplete) {\n        if (!dfsClient.clientRunning ||\n              (dfsClient.hdfsTimeout \u003e 0 \u0026\u0026\n               localstart + dfsClient.hdfsTimeout \u003c System.currentTimeMillis())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + dfsClient.hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          Thread.sleep(400);\n          if (System.currentTimeMillis() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete file \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,26 @@\n+  private void completeFile(ExtendedBlock last) throws IOException {\n+    long localstart \u003d System.currentTimeMillis();\n+    boolean fileComplete \u003d false;\n+    while (!fileComplete) {\n+      fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n+      if (!fileComplete) {\n+        if (!dfsClient.clientRunning ||\n+              (dfsClient.hdfsTimeout \u003e 0 \u0026\u0026\n+               localstart + dfsClient.hdfsTimeout \u003c System.currentTimeMillis())) {\n+            String msg \u003d \"Unable to close file because dfsclient \" +\n+                          \" was unable to contact the HDFS servers.\" +\n+                          \" clientRunning \" + dfsClient.clientRunning +\n+                          \" hdfsTimeout \" + dfsClient.hdfsTimeout;\n+            DFSClient.LOG.info(msg);\n+            throw new IOException(msg);\n+        }\n+        try {\n+          Thread.sleep(400);\n+          if (System.currentTimeMillis() - localstart \u003e 5000) {\n+            DFSClient.LOG.info(\"Could not complete file \" + src + \" retrying...\");\n+          }\n+        } catch (InterruptedException ie) {\n+        }\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void completeFile(ExtendedBlock last) throws IOException {\n    long localstart \u003d System.currentTimeMillis();\n    boolean fileComplete \u003d false;\n    while (!fileComplete) {\n      fileComplete \u003d dfsClient.namenode.complete(src, dfsClient.clientName, last);\n      if (!fileComplete) {\n        if (!dfsClient.clientRunning ||\n              (dfsClient.hdfsTimeout \u003e 0 \u0026\u0026\n               localstart + dfsClient.hdfsTimeout \u003c System.currentTimeMillis())) {\n            String msg \u003d \"Unable to close file because dfsclient \" +\n                          \" was unable to contact the HDFS servers.\" +\n                          \" clientRunning \" + dfsClient.clientRunning +\n                          \" hdfsTimeout \" + dfsClient.hdfsTimeout;\n            DFSClient.LOG.info(msg);\n            throw new IOException(msg);\n        }\n        try {\n          Thread.sleep(400);\n          if (System.currentTimeMillis() - localstart \u003e 5000) {\n            DFSClient.LOG.info(\"Could not complete file \" + src + \" retrying...\");\n          }\n        } catch (InterruptedException ie) {\n        }\n      }\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}