{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "findNewDatanode",
  "functionId": "findNewDatanode___original-DatanodeInfo[](modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 1297,
  "functionEndLine": 1320,
  "numCommitsSeen": 156,
  "timeTaken": 8387,
  "changeHistory": [
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "8f378733423a5244461df79a92c00239514b8b93",
    "7fc50e2525b8b8fe36d92e283a68eeeb09c63d21",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
    "9639f37ee21427303e877e8aeb486e0d71982e0f",
    "ed678e52ce2c46e092ae4a99afd2f0901d7cf12f",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "8f378733423a5244461df79a92c00239514b8b93": "Ybodychange",
    "7fc50e2525b8b8fe36d92e283a68eeeb09c63d21": "Ybodychange",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": "Ymovefromfile",
    "9639f37ee21427303e877e8aeb486e0d71982e0f": "Ybodychange",
    "ed678e52ce2c46e092ae4a99afd2f0901d7cf12f": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,24 @@\n   private int findNewDatanode(final DatanodeInfo[] original\n   ) throws IOException {\n     if (nodes.length !\u003d original.length + 1) {\n       throw new IOException(\n-          new StringBuilder()\n-              .append(\"Failed to replace a bad datanode on the existing pipeline \")\n-              .append(\"due to no more good datanodes being available to try. \")\n-              .append(\"(Nodes: current\u003d\").append(Arrays.asList(nodes))\n-              .append(\", original\u003d\").append(Arrays.asList(original)).append(\"). \")\n-              .append(\"The current failed datanode replacement policy is \")\n-              .append(dfsClient.dtpReplaceDatanodeOnFailure).append(\", and \")\n-              .append(\"a client may configure this via \u0027\")\n-              .append(BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY)\n-              .append(\"\u0027 in its configuration.\")\n-              .toString());\n+          \"Failed to replace a bad datanode on the existing pipeline \"\n+              + \"due to no more good datanodes being available to try. \"\n+              + \"(Nodes: current\u003d\" + Arrays.asList(nodes)\n+              + \", original\u003d\" + Arrays.asList(original) + \"). \"\n+              + \"The current failed datanode replacement policy is \"\n+              + dfsClient.dtpReplaceDatanodeOnFailure\n+              + \", and a client may configure this via \u0027\"\n+              + BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY\n+              + \"\u0027 in its configuration.\");\n     }\n     for(int i \u003d 0; i \u003c nodes.length; i++) {\n       int j \u003d 0;\n       for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n       if (j \u003d\u003d original.length) {\n         return i;\n       }\n     }\n     throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n         + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private int findNewDatanode(final DatanodeInfo[] original\n  ) throws IOException {\n    if (nodes.length !\u003d original.length + 1) {\n      throw new IOException(\n          \"Failed to replace a bad datanode on the existing pipeline \"\n              + \"due to no more good datanodes being available to try. \"\n              + \"(Nodes: current\u003d\" + Arrays.asList(nodes)\n              + \", original\u003d\" + Arrays.asList(original) + \"). \"\n              + \"The current failed datanode replacement policy is \"\n              + dfsClient.dtpReplaceDatanodeOnFailure\n              + \", and a client may configure this via \u0027\"\n              + BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY\n              + \"\u0027 in its configuration.\");\n    }\n    for(int i \u003d 0; i \u003c nodes.length; i++) {\n      int j \u003d 0;\n      for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n      if (j \u003d\u003d original.length) {\n        return i;\n      }\n    }\n    throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n        + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private int findNewDatanode(final DatanodeInfo[] original\n  ) throws IOException {\n    if (nodes.length !\u003d original.length + 1) {\n      throw new IOException(\n          new StringBuilder()\n              .append(\"Failed to replace a bad datanode on the existing pipeline \")\n              .append(\"due to no more good datanodes being available to try. \")\n              .append(\"(Nodes: current\u003d\").append(Arrays.asList(nodes))\n              .append(\", original\u003d\").append(Arrays.asList(original)).append(\"). \")\n              .append(\"The current failed datanode replacement policy is \")\n              .append(dfsClient.dtpReplaceDatanodeOnFailure).append(\", and \")\n              .append(\"a client may configure this via \u0027\")\n              .append(BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY)\n              .append(\"\u0027 in its configuration.\")\n              .toString());\n    }\n    for(int i \u003d 0; i \u003c nodes.length; i++) {\n      int j \u003d 0;\n      for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n      if (j \u003d\u003d original.length) {\n        return i;\n      }\n    }\n    throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n        + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
      }
    },
    "8f378733423a5244461df79a92c00239514b8b93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8397. Refactor the error handling code in DataStreamer. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "15/05/15 4:14 PM",
      "commitName": "8f378733423a5244461df79a92c00239514b8b93",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "08/05/15 12:11 AM",
      "commitNameOld": "730f9930a48259f34e48404aee51e8d641cc3d36",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 7.67,
      "commitsBetweenForRepo": 95,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   private int findNewDatanode(final DatanodeInfo[] original\n   ) throws IOException {\n     if (nodes.length !\u003d original.length + 1) {\n       throw new IOException(\n           new StringBuilder()\n               .append(\"Failed to replace a bad datanode on the existing pipeline \")\n               .append(\"due to no more good datanodes being available to try. \")\n               .append(\"(Nodes: current\u003d\").append(Arrays.asList(nodes))\n               .append(\", original\u003d\").append(Arrays.asList(original)).append(\"). \")\n               .append(\"The current failed datanode replacement policy is \")\n               .append(dfsClient.dtpReplaceDatanodeOnFailure).append(\", and \")\n               .append(\"a client may configure this via \u0027\")\n-              .append(HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY)\n+              .append(BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY)\n               .append(\"\u0027 in its configuration.\")\n               .toString());\n     }\n     for(int i \u003d 0; i \u003c nodes.length; i++) {\n       int j \u003d 0;\n       for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n       if (j \u003d\u003d original.length) {\n         return i;\n       }\n     }\n     throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n         + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private int findNewDatanode(final DatanodeInfo[] original\n  ) throws IOException {\n    if (nodes.length !\u003d original.length + 1) {\n      throw new IOException(\n          new StringBuilder()\n              .append(\"Failed to replace a bad datanode on the existing pipeline \")\n              .append(\"due to no more good datanodes being available to try. \")\n              .append(\"(Nodes: current\u003d\").append(Arrays.asList(nodes))\n              .append(\", original\u003d\").append(Arrays.asList(original)).append(\"). \")\n              .append(\"The current failed datanode replacement policy is \")\n              .append(dfsClient.dtpReplaceDatanodeOnFailure).append(\", and \")\n              .append(\"a client may configure this via \u0027\")\n              .append(BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY)\n              .append(\"\u0027 in its configuration.\")\n              .toString());\n    }\n    for(int i \u003d 0; i \u003c nodes.length; i++) {\n      int j \u003d 0;\n      for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n      if (j \u003d\u003d original.length) {\n        return i;\n      }\n    }\n    throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n        + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "7fc50e2525b8b8fe36d92e283a68eeeb09c63d21": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8083. Move dfs.client.write.* conf from DFSConfigKeys to HdfsClientConfigKeys.Write.\n",
      "commitDate": "13/04/15 11:43 AM",
      "commitName": "7fc50e2525b8b8fe36d92e283a68eeeb09c63d21",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "10/04/15 2:48 PM",
      "commitNameOld": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 2.87,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   private int findNewDatanode(final DatanodeInfo[] original\n   ) throws IOException {\n     if (nodes.length !\u003d original.length + 1) {\n       throw new IOException(\n           new StringBuilder()\n               .append(\"Failed to replace a bad datanode on the existing pipeline \")\n               .append(\"due to no more good datanodes being available to try. \")\n               .append(\"(Nodes: current\u003d\").append(Arrays.asList(nodes))\n               .append(\", original\u003d\").append(Arrays.asList(original)).append(\"). \")\n               .append(\"The current failed datanode replacement policy is \")\n               .append(dfsClient.dtpReplaceDatanodeOnFailure).append(\", and \")\n               .append(\"a client may configure this via \u0027\")\n-              .append(DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY)\n+              .append(HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY)\n               .append(\"\u0027 in its configuration.\")\n               .toString());\n     }\n     for(int i \u003d 0; i \u003c nodes.length; i++) {\n       int j \u003d 0;\n       for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n       if (j \u003d\u003d original.length) {\n         return i;\n       }\n     }\n     throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n         + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private int findNewDatanode(final DatanodeInfo[] original\n  ) throws IOException {\n    if (nodes.length !\u003d original.length + 1) {\n      throw new IOException(\n          new StringBuilder()\n              .append(\"Failed to replace a bad datanode on the existing pipeline \")\n              .append(\"due to no more good datanodes being available to try. \")\n              .append(\"(Nodes: current\u003d\").append(Arrays.asList(nodes))\n              .append(\", original\u003d\").append(Arrays.asList(original)).append(\"). \")\n              .append(\"The current failed datanode replacement policy is \")\n              .append(dfsClient.dtpReplaceDatanodeOnFailure).append(\", and \")\n              .append(\"a client may configure this via \u0027\")\n              .append(HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY)\n              .append(\"\u0027 in its configuration.\")\n              .toString());\n    }\n    for(int i \u003d 0; i \u003c nodes.length; i++) {\n      int j \u003d 0;\n      for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n      if (j \u003d\u003d original.length) {\n        return i;\n      }\n    }\n    throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n        + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "24/03/15 11:06 AM",
      "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/03/15 10:49 AM",
      "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n-    private int findNewDatanode(final DatanodeInfo[] original\n-        ) throws IOException {\n-      if (nodes.length !\u003d original.length + 1) {\n-        throw new IOException(\n-            new StringBuilder()\n-            .append(\"Failed to replace a bad datanode on the existing pipeline \")\n-            .append(\"due to no more good datanodes being available to try. \")\n-            .append(\"(Nodes: current\u003d\").append(Arrays.asList(nodes))\n-            .append(\", original\u003d\").append(Arrays.asList(original)).append(\"). \")\n-            .append(\"The current failed datanode replacement policy is \")\n-            .append(dfsClient.dtpReplaceDatanodeOnFailure).append(\", and \")\n-            .append(\"a client may configure this via \u0027\")\n-            .append(DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY)\n-            .append(\"\u0027 in its configuration.\")\n-            .toString());\n+  private int findNewDatanode(final DatanodeInfo[] original\n+  ) throws IOException {\n+    if (nodes.length !\u003d original.length + 1) {\n+      throw new IOException(\n+          new StringBuilder()\n+              .append(\"Failed to replace a bad datanode on the existing pipeline \")\n+              .append(\"due to no more good datanodes being available to try. \")\n+              .append(\"(Nodes: current\u003d\").append(Arrays.asList(nodes))\n+              .append(\", original\u003d\").append(Arrays.asList(original)).append(\"). \")\n+              .append(\"The current failed datanode replacement policy is \")\n+              .append(dfsClient.dtpReplaceDatanodeOnFailure).append(\", and \")\n+              .append(\"a client may configure this via \u0027\")\n+              .append(DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY)\n+              .append(\"\u0027 in its configuration.\")\n+              .toString());\n+    }\n+    for(int i \u003d 0; i \u003c nodes.length; i++) {\n+      int j \u003d 0;\n+      for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n+      if (j \u003d\u003d original.length) {\n+        return i;\n       }\n-      for(int i \u003d 0; i \u003c nodes.length; i++) {\n-        int j \u003d 0;\n-        for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n-        if (j \u003d\u003d original.length) {\n-          return i;\n-        }\n-      }\n-      throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n-          + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n-    }\n\\ No newline at end of file\n+    }\n+    throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n+        + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private int findNewDatanode(final DatanodeInfo[] original\n  ) throws IOException {\n    if (nodes.length !\u003d original.length + 1) {\n      throw new IOException(\n          new StringBuilder()\n              .append(\"Failed to replace a bad datanode on the existing pipeline \")\n              .append(\"due to no more good datanodes being available to try. \")\n              .append(\"(Nodes: current\u003d\").append(Arrays.asList(nodes))\n              .append(\", original\u003d\").append(Arrays.asList(original)).append(\"). \")\n              .append(\"The current failed datanode replacement policy is \")\n              .append(dfsClient.dtpReplaceDatanodeOnFailure).append(\", and \")\n              .append(\"a client may configure this via \u0027\")\n              .append(DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY)\n              .append(\"\u0027 in its configuration.\")\n              .toString());\n    }\n    for(int i \u003d 0; i \u003c nodes.length; i++) {\n      int j \u003d 0;\n      for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n      if (j \u003d\u003d original.length) {\n        return i;\n      }\n    }\n    throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n        + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
        "oldMethodName": "findNewDatanode",
        "newMethodName": "findNewDatanode"
      }
    },
    "9639f37ee21427303e877e8aeb486e0d71982e0f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4259. Improve pipeline DN replacement failure message. Contributed by Harsh J. (harsh)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1439126 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/01/13 9:42 AM",
      "commitName": "9639f37ee21427303e877e8aeb486e0d71982e0f",
      "commitAuthor": "Harsh J",
      "commitDateOld": "09/01/13 1:20 PM",
      "commitNameOld": "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 17.85,
      "commitsBetweenForRepo": 92,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,26 @@\n     private int findNewDatanode(final DatanodeInfo[] original\n         ) throws IOException {\n       if (nodes.length !\u003d original.length + 1) {\n-        throw new IOException(\"Failed to add a datanode.  \"\n-            + \"User may turn off this feature by setting \"\n-            + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY\n-            + \" in configuration, where the current policy is \"\n-            + dfsClient.dtpReplaceDatanodeOnFailure\n-            + \".  (Nodes: current\u003d\" + Arrays.asList(nodes)\n-            + \", original\u003d\" + Arrays.asList(original) + \")\");\n+        throw new IOException(\n+            new StringBuilder()\n+            .append(\"Failed to replace a bad datanode on the existing pipeline \")\n+            .append(\"due to no more good datanodes being available to try. \")\n+            .append(\"(Nodes: current\u003d\").append(Arrays.asList(nodes))\n+            .append(\", original\u003d\").append(Arrays.asList(original)).append(\"). \")\n+            .append(\"The current failed datanode replacement policy is \")\n+            .append(dfsClient.dtpReplaceDatanodeOnFailure).append(\", and \")\n+            .append(\"a client may configure this via \u0027\")\n+            .append(DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY)\n+            .append(\"\u0027 in its configuration.\")\n+            .toString());\n       }\n       for(int i \u003d 0; i \u003c nodes.length; i++) {\n         int j \u003d 0;\n         for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n         if (j \u003d\u003d original.length) {\n           return i;\n         }\n       }\n       throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n           + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private int findNewDatanode(final DatanodeInfo[] original\n        ) throws IOException {\n      if (nodes.length !\u003d original.length + 1) {\n        throw new IOException(\n            new StringBuilder()\n            .append(\"Failed to replace a bad datanode on the existing pipeline \")\n            .append(\"due to no more good datanodes being available to try. \")\n            .append(\"(Nodes: current\u003d\").append(Arrays.asList(nodes))\n            .append(\", original\u003d\").append(Arrays.asList(original)).append(\"). \")\n            .append(\"The current failed datanode replacement policy is \")\n            .append(dfsClient.dtpReplaceDatanodeOnFailure).append(\", and \")\n            .append(\"a client may configure this via \u0027\")\n            .append(DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY)\n            .append(\"\u0027 in its configuration.\")\n            .toString());\n      }\n      for(int i \u003d 0; i \u003c nodes.length; i++) {\n        int j \u003d 0;\n        for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n        if (j \u003d\u003d original.length) {\n          return i;\n        }\n      }\n      throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n          + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "ed678e52ce2c46e092ae4a99afd2f0901d7cf12f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3179.  Improve the exception message thrown by DataStreamer when it failed to add a datanode.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1324892 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/04/12 10:49 AM",
      "commitName": "ed678e52ce2c46e092ae4a99afd2f0901d7cf12f",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "02/04/12 4:20 PM",
      "commitNameOld": "4f15b9dfed02845b07539f074ccee3074647dffd",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 8.77,
      "commitsBetweenForRepo": 95,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,21 @@\n     private int findNewDatanode(final DatanodeInfo[] original\n         ) throws IOException {\n       if (nodes.length !\u003d original.length + 1) {\n-        throw new IOException(\"Failed to add a datanode:\"\n-            + \" nodes.length !\u003d original.length + 1, nodes\u003d\"\n-            + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n+        throw new IOException(\"Failed to add a datanode.  \"\n+            + \"User may turn off this feature by setting \"\n+            + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY\n+            + \" in configuration, where the current policy is \"\n+            + dfsClient.dtpReplaceDatanodeOnFailure\n+            + \".  (Nodes: current\u003d\" + Arrays.asList(nodes)\n+            + \", original\u003d\" + Arrays.asList(original) + \")\");\n       }\n       for(int i \u003d 0; i \u003c nodes.length; i++) {\n         int j \u003d 0;\n         for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n         if (j \u003d\u003d original.length) {\n           return i;\n         }\n       }\n       throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n           + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private int findNewDatanode(final DatanodeInfo[] original\n        ) throws IOException {\n      if (nodes.length !\u003d original.length + 1) {\n        throw new IOException(\"Failed to add a datanode.  \"\n            + \"User may turn off this feature by setting \"\n            + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY\n            + \" in configuration, where the current policy is \"\n            + dfsClient.dtpReplaceDatanodeOnFailure\n            + \".  (Nodes: current\u003d\" + Arrays.asList(nodes)\n            + \", original\u003d\" + Arrays.asList(original) + \")\");\n      }\n      for(int i \u003d 0; i \u003c nodes.length; i++) {\n        int j \u003d 0;\n        for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n        if (j \u003d\u003d original.length) {\n          return i;\n        }\n      }\n      throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n          + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private int findNewDatanode(final DatanodeInfo[] original\n        ) throws IOException {\n      if (nodes.length !\u003d original.length + 1) {\n        throw new IOException(\"Failed to add a datanode:\"\n            + \" nodes.length !\u003d original.length + 1, nodes\u003d\"\n            + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n      }\n      for(int i \u003d 0; i \u003c nodes.length; i++) {\n        int j \u003d 0;\n        for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n        if (j \u003d\u003d original.length) {\n          return i;\n        }\n      }\n      throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n          + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private int findNewDatanode(final DatanodeInfo[] original\n        ) throws IOException {\n      if (nodes.length !\u003d original.length + 1) {\n        throw new IOException(\"Failed to add a datanode:\"\n            + \" nodes.length !\u003d original.length + 1, nodes\u003d\"\n            + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n      }\n      for(int i \u003d 0; i \u003c nodes.length; i++) {\n        int j \u003d 0;\n        for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n        if (j \u003d\u003d original.length) {\n          return i;\n        }\n      }\n      throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n          + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,17 @@\n+    private int findNewDatanode(final DatanodeInfo[] original\n+        ) throws IOException {\n+      if (nodes.length !\u003d original.length + 1) {\n+        throw new IOException(\"Failed to add a datanode:\"\n+            + \" nodes.length !\u003d original.length + 1, nodes\u003d\"\n+            + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n+      }\n+      for(int i \u003d 0; i \u003c nodes.length; i++) {\n+        int j \u003d 0;\n+        for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n+        if (j \u003d\u003d original.length) {\n+          return i;\n+        }\n+      }\n+      throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n+          + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private int findNewDatanode(final DatanodeInfo[] original\n        ) throws IOException {\n      if (nodes.length !\u003d original.length + 1) {\n        throw new IOException(\"Failed to add a datanode:\"\n            + \" nodes.length !\u003d original.length + 1, nodes\u003d\"\n            + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n      }\n      for(int i \u003d 0; i \u003c nodes.length; i++) {\n        int j \u003d 0;\n        for(; j \u003c original.length \u0026\u0026 !nodes[i].equals(original[j]); j++);\n        if (j \u003d\u003d original.length) {\n          return i;\n        }\n      }\n      throw new IOException(\"Failed: new datanode not found: nodes\u003d\"\n          + Arrays.asList(nodes) + \", original\u003d\" + Arrays.asList(original));\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}