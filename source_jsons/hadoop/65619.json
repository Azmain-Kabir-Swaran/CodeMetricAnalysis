{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LocalMetadataStore.java",
  "functionName": "prune",
  "functionId": "prune___pruneMode-PruneMode__cutoff-long__keyPrefix-String",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java",
  "functionStartLine": 386,
  "functionEndLine": 421,
  "numCommitsSeen": 35,
  "timeTaken": 5887,
  "changeHistory": [
    "49df83899543586bbcaf80f01399ade031cf68b0",
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
    "c687a6617d73293019d8d91ac48bbfd2ccca3b40",
    "69aac696d9d4e32a55ba9b6992f41a9ad13424f1",
    "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a",
    "621b43e254afaff708cd6fc4698b29628f6abc33"
  ],
  "changeHistoryShort": {
    "49df83899543586bbcaf80f01399ade031cf68b0": "Ymultichange(Yreturntypechange,Ybodychange)",
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c": "Ymultichange(Yparameterchange,Ybodychange)",
    "c687a6617d73293019d8d91ac48bbfd2ccca3b40": "Ymultichange(Yexceptionschange,Ybodychange)",
    "69aac696d9d4e32a55ba9b6992f41a9ad13424f1": "Ybodychange",
    "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a": "Ymultichange(Yparameterchange,Ybodychange)",
    "621b43e254afaff708cd6fc4698b29628f6abc33": "Yintroduced"
  },
  "changeHistoryDetails": {
    "49df83899543586bbcaf80f01399ade031cf68b0": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
      "commitDate": "10/01/20 3:11 AM",
      "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
          "commitDate": "10/01/20 3:11 AM",
          "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "26/11/19 7:36 AM",
          "commitNameOld": "ea25f4de236611d388e14a710ebe5d6872c421b6",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 44.82,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,36 @@\n-  public synchronized void prune(PruneMode pruneMode, long cutoff,\n+  public synchronized long prune(PruneMode pruneMode, long cutoff,\n       String keyPrefix) {\n     // prune files\n+    AtomicLong count \u003d new AtomicLong();\n     // filter path_metadata (files), filter expired, remove expired\n     localCache.asMap().entrySet().stream()\n         .filter(entry -\u003e entry.getValue().hasPathMeta())\n         .filter(entry -\u003e expired(pruneMode,\n             entry.getValue().getFileMeta(), cutoff, keyPrefix))\n-        .forEach(entry -\u003e localCache.invalidate(entry.getKey()));\n+        .forEach(entry -\u003e {\n+          localCache.invalidate(entry.getKey());\n+          count.incrementAndGet();\n+        });\n \n \n     // prune dirs\n     // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n     localCache.asMap().entrySet().stream()\n         .filter(entry -\u003e entry.getValue().hasDirMeta())\n         .forEach(entry -\u003e {\n           Path path \u003d entry.getKey();\n           DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n           Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n           Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n \n           for (PathMetadata child : oldChildren) {\n             if (!expired(pruneMode, child, cutoff, keyPrefix)) {\n               newChildren.add(child);\n+            } else {\n+              count.incrementAndGet();\n             }\n           }\n           removeAuthoritativeFromParent(path, oldChildren, newChildren);\n         });\n+    return count.get();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized long prune(PruneMode pruneMode, long cutoff,\n      String keyPrefix) {\n    // prune files\n    AtomicLong count \u003d new AtomicLong();\n    // filter path_metadata (files), filter expired, remove expired\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasPathMeta())\n        .filter(entry -\u003e expired(pruneMode,\n            entry.getValue().getFileMeta(), cutoff, keyPrefix))\n        .forEach(entry -\u003e {\n          localCache.invalidate(entry.getKey());\n          count.incrementAndGet();\n        });\n\n\n    // prune dirs\n    // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasDirMeta())\n        .forEach(entry -\u003e {\n          Path path \u003d entry.getKey();\n          DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n          Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n          Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n\n          for (PathMetadata child : oldChildren) {\n            if (!expired(pruneMode, child, cutoff, keyPrefix)) {\n              newChildren.add(child);\n            } else {\n              count.incrementAndGet();\n            }\n          }\n          removeAuthoritativeFromParent(path, oldChildren, newChildren);\n        });\n    return count.get();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "long"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-16697. Tune/audit S3A authoritative mode.\n\nContains:\n\nHADOOP-16474. S3Guard ProgressiveRenameTracker to mark destination\n              dirirectory as authoritative on success.\nHADOOP-16684. S3guard bucket info to list a bit more about\n              authoritative paths.\nHADOOP-16722. S3GuardTool to support FilterFileSystem.\n\nThis patch improves the marking of newly created/import directory\ntrees in S3Guard DynamoDB tables as authoritative.\n\nSpecific changes:\n\n * Renamed directories are marked as authoritative if the entire\n   operation succeeded (HADOOP-16474).\n * When updating parent table entries as part of any table write,\n   there\u0027s no overwriting of their authoritative flag.\n\ns3guard import changes:\n\n* new -verbose flag to print out what is going on.\n\n* The \"s3guard import\" command lets you declare that a directory tree\nis to be marked as authoritative\n\n  hadoop s3guard import -authoritative -verbose s3a://bucket/path\n\nWhen importing a listing and a file is found, the import tool queries\nthe metastore and only updates the entry if the file is different from\nbefore, where different \u003d\u003d new timestamp, etag, or length. S3Guard can get\ntimestamp differences due to clock skew in PUT operations.\n\nAs the recursive list performed by the import command doesn\u0027t retrieve the\nversionID, the existing entry may in fact be more complete.\nWhen updating an existing due to clock skew the existing version ID\nis propagated to the new entry (note: the etags must match; this is needed\nto deal with inconsistent listings).\n\nThere is a new s3guard command to audit a s3guard bucket/path\u0027s\nauthoritative state:\n\n  hadoop s3guard authoritative -check-config s3a://bucket/path\n\nThis is primarily for testing/auditing.\n\nThe s3guard bucket-info command also provides some more details on the\nauthoritative state of a store (HADOOP-16684).\n\nChange-Id: I58001341c04f6f3597fcb4fcb1581ccefeb77d91\n",
          "commitDate": "10/01/20 3:11 AM",
          "commitName": "49df83899543586bbcaf80f01399ade031cf68b0",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "26/11/19 7:36 AM",
          "commitNameOld": "ea25f4de236611d388e14a710ebe5d6872c421b6",
          "commitAuthorOld": "Gabor Bota",
          "daysBetweenCommits": 44.82,
          "commitsBetweenForRepo": 155,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,36 @@\n-  public synchronized void prune(PruneMode pruneMode, long cutoff,\n+  public synchronized long prune(PruneMode pruneMode, long cutoff,\n       String keyPrefix) {\n     // prune files\n+    AtomicLong count \u003d new AtomicLong();\n     // filter path_metadata (files), filter expired, remove expired\n     localCache.asMap().entrySet().stream()\n         .filter(entry -\u003e entry.getValue().hasPathMeta())\n         .filter(entry -\u003e expired(pruneMode,\n             entry.getValue().getFileMeta(), cutoff, keyPrefix))\n-        .forEach(entry -\u003e localCache.invalidate(entry.getKey()));\n+        .forEach(entry -\u003e {\n+          localCache.invalidate(entry.getKey());\n+          count.incrementAndGet();\n+        });\n \n \n     // prune dirs\n     // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n     localCache.asMap().entrySet().stream()\n         .filter(entry -\u003e entry.getValue().hasDirMeta())\n         .forEach(entry -\u003e {\n           Path path \u003d entry.getKey();\n           DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n           Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n           Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n \n           for (PathMetadata child : oldChildren) {\n             if (!expired(pruneMode, child, cutoff, keyPrefix)) {\n               newChildren.add(child);\n+            } else {\n+              count.incrementAndGet();\n             }\n           }\n           removeAuthoritativeFromParent(path, oldChildren, newChildren);\n         });\n+    return count.get();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized long prune(PruneMode pruneMode, long cutoff,\n      String keyPrefix) {\n    // prune files\n    AtomicLong count \u003d new AtomicLong();\n    // filter path_metadata (files), filter expired, remove expired\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasPathMeta())\n        .filter(entry -\u003e expired(pruneMode,\n            entry.getValue().getFileMeta(), cutoff, keyPrefix))\n        .forEach(entry -\u003e {\n          localCache.invalidate(entry.getKey());\n          count.incrementAndGet();\n        });\n\n\n    // prune dirs\n    // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasDirMeta())\n        .forEach(entry -\u003e {\n          Path path \u003d entry.getKey();\n          DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n          Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n          Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n\n          for (PathMetadata child : oldChildren) {\n            if (!expired(pruneMode, child, cutoff, keyPrefix)) {\n              newChildren.add(child);\n            } else {\n              count.incrementAndGet();\n            }\n          }\n          removeAuthoritativeFromParent(path, oldChildren, newChildren);\n        });\n    return count.get();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-16279. S3Guard: Implement time-based (TTL) expiry for entries (and tombstones).\n\nContributed by Gabor Bota.\n\nChange-Id: I73a2d2861901dedfe7a0e783b310fbb95e7c1af9\n",
      "commitDate": "16/06/19 9:05 AM",
      "commitName": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
      "commitAuthor": "Gabor Bota",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-16279. S3Guard: Implement time-based (TTL) expiry for entries (and tombstones).\n\nContributed by Gabor Bota.\n\nChange-Id: I73a2d2861901dedfe7a0e783b310fbb95e7c1af9\n",
          "commitDate": "16/06/19 9:05 AM",
          "commitName": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
          "commitAuthor": "Gabor Bota",
          "commitDateOld": "19/05/19 2:29 PM",
          "commitNameOld": "a36274d69947648dbe82721220cc5240ec5d396d",
          "commitAuthorOld": "Ben Roling",
          "daysBetweenCommits": 27.77,
          "commitsBetweenForRepo": 198,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,29 @@\n-  public synchronized void prune(long modTime, String keyPrefix) {\n+  public synchronized void prune(PruneMode pruneMode, long cutoff,\n+      String keyPrefix) {\n     // prune files\n     // filter path_metadata (files), filter expired, remove expired\n     localCache.asMap().entrySet().stream()\n         .filter(entry -\u003e entry.getValue().hasPathMeta())\n-        .filter(entry -\u003e expired(\n-            entry.getValue().getFileMeta().getFileStatus(), modTime, keyPrefix))\n+        .filter(entry -\u003e expired(pruneMode,\n+            entry.getValue().getFileMeta(), cutoff, keyPrefix))\n         .forEach(entry -\u003e localCache.invalidate(entry.getKey()));\n \n \n     // prune dirs\n     // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n     localCache.asMap().entrySet().stream()\n         .filter(entry -\u003e entry.getValue().hasDirMeta())\n         .forEach(entry -\u003e {\n           Path path \u003d entry.getKey();\n           DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n           Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n           Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n \n           for (PathMetadata child : oldChildren) {\n-            FileStatus status \u003d child.getFileStatus();\n-            if (!expired(status, modTime, keyPrefix)) {\n+            if (!expired(pruneMode, child, cutoff, keyPrefix)) {\n               newChildren.add(child);\n             }\n           }\n-          if (newChildren.size() !\u003d oldChildren.size()) {\n-            DirListingMetadata dlm \u003d\n-                new DirListingMetadata(path, newChildren, false);\n-            localCache.put(path, new LocalMetadataEntry(dlm));\n-            if (!path.isRoot()) {\n-              DirListingMetadata parent \u003d getDirListingMeta(path.getParent());\n-              if (parent !\u003d null) {\n-                parent.setAuthoritative(false);\n-              }\n-            }\n-          }\n+          removeAuthoritativeFromParent(path, oldChildren, newChildren);\n         });\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void prune(PruneMode pruneMode, long cutoff,\n      String keyPrefix) {\n    // prune files\n    // filter path_metadata (files), filter expired, remove expired\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasPathMeta())\n        .filter(entry -\u003e expired(pruneMode,\n            entry.getValue().getFileMeta(), cutoff, keyPrefix))\n        .forEach(entry -\u003e localCache.invalidate(entry.getKey()));\n\n\n    // prune dirs\n    // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasDirMeta())\n        .forEach(entry -\u003e {\n          Path path \u003d entry.getKey();\n          DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n          Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n          Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n\n          for (PathMetadata child : oldChildren) {\n            if (!expired(pruneMode, child, cutoff, keyPrefix)) {\n              newChildren.add(child);\n            }\n          }\n          removeAuthoritativeFromParent(path, oldChildren, newChildren);\n        });\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java",
          "extendedDetails": {
            "oldValue": "[modTime-long, keyPrefix-String]",
            "newValue": "[pruneMode-PruneMode, cutoff-long, keyPrefix-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-16279. S3Guard: Implement time-based (TTL) expiry for entries (and tombstones).\n\nContributed by Gabor Bota.\n\nChange-Id: I73a2d2861901dedfe7a0e783b310fbb95e7c1af9\n",
          "commitDate": "16/06/19 9:05 AM",
          "commitName": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
          "commitAuthor": "Gabor Bota",
          "commitDateOld": "19/05/19 2:29 PM",
          "commitNameOld": "a36274d69947648dbe82721220cc5240ec5d396d",
          "commitAuthorOld": "Ben Roling",
          "daysBetweenCommits": 27.77,
          "commitsBetweenForRepo": 198,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,29 @@\n-  public synchronized void prune(long modTime, String keyPrefix) {\n+  public synchronized void prune(PruneMode pruneMode, long cutoff,\n+      String keyPrefix) {\n     // prune files\n     // filter path_metadata (files), filter expired, remove expired\n     localCache.asMap().entrySet().stream()\n         .filter(entry -\u003e entry.getValue().hasPathMeta())\n-        .filter(entry -\u003e expired(\n-            entry.getValue().getFileMeta().getFileStatus(), modTime, keyPrefix))\n+        .filter(entry -\u003e expired(pruneMode,\n+            entry.getValue().getFileMeta(), cutoff, keyPrefix))\n         .forEach(entry -\u003e localCache.invalidate(entry.getKey()));\n \n \n     // prune dirs\n     // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n     localCache.asMap().entrySet().stream()\n         .filter(entry -\u003e entry.getValue().hasDirMeta())\n         .forEach(entry -\u003e {\n           Path path \u003d entry.getKey();\n           DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n           Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n           Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n \n           for (PathMetadata child : oldChildren) {\n-            FileStatus status \u003d child.getFileStatus();\n-            if (!expired(status, modTime, keyPrefix)) {\n+            if (!expired(pruneMode, child, cutoff, keyPrefix)) {\n               newChildren.add(child);\n             }\n           }\n-          if (newChildren.size() !\u003d oldChildren.size()) {\n-            DirListingMetadata dlm \u003d\n-                new DirListingMetadata(path, newChildren, false);\n-            localCache.put(path, new LocalMetadataEntry(dlm));\n-            if (!path.isRoot()) {\n-              DirListingMetadata parent \u003d getDirListingMeta(path.getParent());\n-              if (parent !\u003d null) {\n-                parent.setAuthoritative(false);\n-              }\n-            }\n-          }\n+          removeAuthoritativeFromParent(path, oldChildren, newChildren);\n         });\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void prune(PruneMode pruneMode, long cutoff,\n      String keyPrefix) {\n    // prune files\n    // filter path_metadata (files), filter expired, remove expired\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasPathMeta())\n        .filter(entry -\u003e expired(pruneMode,\n            entry.getValue().getFileMeta(), cutoff, keyPrefix))\n        .forEach(entry -\u003e localCache.invalidate(entry.getKey()));\n\n\n    // prune dirs\n    // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasDirMeta())\n        .forEach(entry -\u003e {\n          Path path \u003d entry.getKey();\n          DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n          Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n          Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n\n          for (PathMetadata child : oldChildren) {\n            if (!expired(pruneMode, child, cutoff, keyPrefix)) {\n              newChildren.add(child);\n            }\n          }\n          removeAuthoritativeFromParent(path, oldChildren, newChildren);\n        });\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "c687a6617d73293019d8d91ac48bbfd2ccca3b40": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HADOOP-15423. Merge fileCache and dirCache into ine single cache in LocalMetadataStore. Contributed by Gabor Bota.\n",
      "commitDate": "25/06/18 1:59 PM",
      "commitName": "c687a6617d73293019d8d91ac48bbfd2ccca3b40",
      "commitAuthor": "Sean Mackrory",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HADOOP-15423. Merge fileCache and dirCache into ine single cache in LocalMetadataStore. Contributed by Gabor Bota.\n",
          "commitDate": "25/06/18 1:59 PM",
          "commitName": "c687a6617d73293019d8d91ac48bbfd2ccca3b40",
          "commitAuthor": "Sean Mackrory",
          "commitDateOld": "08/05/18 6:58 PM",
          "commitNameOld": "8981674bbcff0663af820f3e87a3eaea5789968a",
          "commitAuthorOld": "Aaron Fabbri",
          "daysBetweenCommits": 47.79,
          "commitsBetweenForRepo": 363,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,39 @@\n-  public synchronized void prune(long modTime, String keyPrefix)\n-      throws IOException {\n-    Iterator\u003cMap.Entry\u003cPath, PathMetadata\u003e\u003e files \u003d\n-        fileCache.asMap().entrySet().iterator();\n-    while (files.hasNext()) {\n-      Map.Entry\u003cPath, PathMetadata\u003e entry \u003d files.next();\n-      if (expired(entry.getValue().getFileStatus(), modTime, keyPrefix)) {\n-        files.remove();\n-      }\n-    }\n-    Iterator\u003cMap.Entry\u003cPath, DirListingMetadata\u003e\u003e dirs \u003d\n-        dirCache.asMap().entrySet().iterator();\n-    while (dirs.hasNext()) {\n-      Map.Entry\u003cPath, DirListingMetadata\u003e entry \u003d dirs.next();\n-      Path path \u003d entry.getKey();\n-      DirListingMetadata metadata \u003d entry.getValue();\n-      Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n-      Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n+  public synchronized void prune(long modTime, String keyPrefix) {\n+    // prune files\n+    // filter path_metadata (files), filter expired, remove expired\n+    localCache.asMap().entrySet().stream()\n+        .filter(entry -\u003e entry.getValue().hasPathMeta())\n+        .filter(entry -\u003e expired(\n+            entry.getValue().getFileMeta().getFileStatus(), modTime, keyPrefix))\n+        .forEach(entry -\u003e localCache.invalidate(entry.getKey()));\n \n-      for (PathMetadata child : oldChildren) {\n-        FileStatus status \u003d child.getFileStatus();\n-        if (!expired(status, modTime, keyPrefix)) {\n-          newChildren.add(child);\n-        }\n-      }\n-      if (newChildren.size() !\u003d oldChildren.size()) {\n-        dirCache.put(path, new DirListingMetadata(path, newChildren, false));\n-        if (!path.isRoot()) {\n-          DirListingMetadata parent \u003d null;\n-          parent \u003d dirCache.getIfPresent(path.getParent());\n-          if (parent !\u003d null) {\n-            parent.setAuthoritative(false);\n+\n+    // prune dirs\n+    // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n+    localCache.asMap().entrySet().stream()\n+        .filter(entry -\u003e entry.getValue().hasDirMeta())\n+        .forEach(entry -\u003e {\n+          Path path \u003d entry.getKey();\n+          DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n+          Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n+          Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n+\n+          for (PathMetadata child : oldChildren) {\n+            FileStatus status \u003d child.getFileStatus();\n+            if (!expired(status, modTime, keyPrefix)) {\n+              newChildren.add(child);\n+            }\n           }\n-        }\n-      }\n-    }\n+          if (newChildren.size() !\u003d oldChildren.size()) {\n+            DirListingMetadata dlm \u003d\n+                new DirListingMetadata(path, newChildren, false);\n+            localCache.put(path, new LocalMetadataEntry(dlm));\n+            if (!path.isRoot()) {\n+              DirListingMetadata parent \u003d getDirListingMeta(path.getParent());\n+              if (parent !\u003d null) {\n+                parent.setAuthoritative(false);\n+              }\n+            }\n+          }\n+        });\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void prune(long modTime, String keyPrefix) {\n    // prune files\n    // filter path_metadata (files), filter expired, remove expired\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasPathMeta())\n        .filter(entry -\u003e expired(\n            entry.getValue().getFileMeta().getFileStatus(), modTime, keyPrefix))\n        .forEach(entry -\u003e localCache.invalidate(entry.getKey()));\n\n\n    // prune dirs\n    // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasDirMeta())\n        .forEach(entry -\u003e {\n          Path path \u003d entry.getKey();\n          DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n          Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n          Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n\n          for (PathMetadata child : oldChildren) {\n            FileStatus status \u003d child.getFileStatus();\n            if (!expired(status, modTime, keyPrefix)) {\n              newChildren.add(child);\n            }\n          }\n          if (newChildren.size() !\u003d oldChildren.size()) {\n            DirListingMetadata dlm \u003d\n                new DirListingMetadata(path, newChildren, false);\n            localCache.put(path, new LocalMetadataEntry(dlm));\n            if (!path.isRoot()) {\n              DirListingMetadata parent \u003d getDirListingMeta(path.getParent());\n              if (parent !\u003d null) {\n                parent.setAuthoritative(false);\n              }\n            }\n          }\n        });\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java",
          "extendedDetails": {
            "oldValue": "[IOException]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-15423. Merge fileCache and dirCache into ine single cache in LocalMetadataStore. Contributed by Gabor Bota.\n",
          "commitDate": "25/06/18 1:59 PM",
          "commitName": "c687a6617d73293019d8d91ac48bbfd2ccca3b40",
          "commitAuthor": "Sean Mackrory",
          "commitDateOld": "08/05/18 6:58 PM",
          "commitNameOld": "8981674bbcff0663af820f3e87a3eaea5789968a",
          "commitAuthorOld": "Aaron Fabbri",
          "daysBetweenCommits": 47.79,
          "commitsBetweenForRepo": 363,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,39 @@\n-  public synchronized void prune(long modTime, String keyPrefix)\n-      throws IOException {\n-    Iterator\u003cMap.Entry\u003cPath, PathMetadata\u003e\u003e files \u003d\n-        fileCache.asMap().entrySet().iterator();\n-    while (files.hasNext()) {\n-      Map.Entry\u003cPath, PathMetadata\u003e entry \u003d files.next();\n-      if (expired(entry.getValue().getFileStatus(), modTime, keyPrefix)) {\n-        files.remove();\n-      }\n-    }\n-    Iterator\u003cMap.Entry\u003cPath, DirListingMetadata\u003e\u003e dirs \u003d\n-        dirCache.asMap().entrySet().iterator();\n-    while (dirs.hasNext()) {\n-      Map.Entry\u003cPath, DirListingMetadata\u003e entry \u003d dirs.next();\n-      Path path \u003d entry.getKey();\n-      DirListingMetadata metadata \u003d entry.getValue();\n-      Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n-      Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n+  public synchronized void prune(long modTime, String keyPrefix) {\n+    // prune files\n+    // filter path_metadata (files), filter expired, remove expired\n+    localCache.asMap().entrySet().stream()\n+        .filter(entry -\u003e entry.getValue().hasPathMeta())\n+        .filter(entry -\u003e expired(\n+            entry.getValue().getFileMeta().getFileStatus(), modTime, keyPrefix))\n+        .forEach(entry -\u003e localCache.invalidate(entry.getKey()));\n \n-      for (PathMetadata child : oldChildren) {\n-        FileStatus status \u003d child.getFileStatus();\n-        if (!expired(status, modTime, keyPrefix)) {\n-          newChildren.add(child);\n-        }\n-      }\n-      if (newChildren.size() !\u003d oldChildren.size()) {\n-        dirCache.put(path, new DirListingMetadata(path, newChildren, false));\n-        if (!path.isRoot()) {\n-          DirListingMetadata parent \u003d null;\n-          parent \u003d dirCache.getIfPresent(path.getParent());\n-          if (parent !\u003d null) {\n-            parent.setAuthoritative(false);\n+\n+    // prune dirs\n+    // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n+    localCache.asMap().entrySet().stream()\n+        .filter(entry -\u003e entry.getValue().hasDirMeta())\n+        .forEach(entry -\u003e {\n+          Path path \u003d entry.getKey();\n+          DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n+          Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n+          Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n+\n+          for (PathMetadata child : oldChildren) {\n+            FileStatus status \u003d child.getFileStatus();\n+            if (!expired(status, modTime, keyPrefix)) {\n+              newChildren.add(child);\n+            }\n           }\n-        }\n-      }\n-    }\n+          if (newChildren.size() !\u003d oldChildren.size()) {\n+            DirListingMetadata dlm \u003d\n+                new DirListingMetadata(path, newChildren, false);\n+            localCache.put(path, new LocalMetadataEntry(dlm));\n+            if (!path.isRoot()) {\n+              DirListingMetadata parent \u003d getDirListingMeta(path.getParent());\n+              if (parent !\u003d null) {\n+                parent.setAuthoritative(false);\n+              }\n+            }\n+          }\n+        });\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void prune(long modTime, String keyPrefix) {\n    // prune files\n    // filter path_metadata (files), filter expired, remove expired\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasPathMeta())\n        .filter(entry -\u003e expired(\n            entry.getValue().getFileMeta().getFileStatus(), modTime, keyPrefix))\n        .forEach(entry -\u003e localCache.invalidate(entry.getKey()));\n\n\n    // prune dirs\n    // filter DIR_LISTING_METADATA, remove expired, remove authoritative bit\n    localCache.asMap().entrySet().stream()\n        .filter(entry -\u003e entry.getValue().hasDirMeta())\n        .forEach(entry -\u003e {\n          Path path \u003d entry.getKey();\n          DirListingMetadata metadata \u003d entry.getValue().getDirListingMeta();\n          Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n          Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n\n          for (PathMetadata child : oldChildren) {\n            FileStatus status \u003d child.getFileStatus();\n            if (!expired(status, modTime, keyPrefix)) {\n              newChildren.add(child);\n            }\n          }\n          if (newChildren.size() !\u003d oldChildren.size()) {\n            DirListingMetadata dlm \u003d\n                new DirListingMetadata(path, newChildren, false);\n            localCache.put(path, new LocalMetadataEntry(dlm));\n            if (!path.isRoot()) {\n              DirListingMetadata parent \u003d getDirListingMeta(path.getParent());\n              if (parent !\u003d null) {\n                parent.setAuthoritative(false);\n              }\n            }\n          }\n        });\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "69aac696d9d4e32a55ba9b6992f41a9ad13424f1": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13649 s3guard: implement time-based (TTL) expiry for LocalMetadataStore (Gabor Bota)\n",
      "commitDate": "08/05/18 3:29 PM",
      "commitName": "69aac696d9d4e32a55ba9b6992f41a9ad13424f1",
      "commitAuthor": "Aaron Fabbri",
      "commitDateOld": "26/04/18 8:41 PM",
      "commitNameOld": "7d8bcf534acce52fb8cfb745a8671f9350d3b5be",
      "commitAuthorOld": "Aaron Fabbri",
      "daysBetweenCommits": 11.78,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,37 @@\n   public synchronized void prune(long modTime, String keyPrefix)\n       throws IOException {\n     Iterator\u003cMap.Entry\u003cPath, PathMetadata\u003e\u003e files \u003d\n-        fileHash.entrySet().iterator();\n+        fileCache.asMap().entrySet().iterator();\n     while (files.hasNext()) {\n       Map.Entry\u003cPath, PathMetadata\u003e entry \u003d files.next();\n       if (expired(entry.getValue().getFileStatus(), modTime, keyPrefix)) {\n         files.remove();\n       }\n     }\n     Iterator\u003cMap.Entry\u003cPath, DirListingMetadata\u003e\u003e dirs \u003d\n-        dirHash.entrySet().iterator();\n+        dirCache.asMap().entrySet().iterator();\n     while (dirs.hasNext()) {\n       Map.Entry\u003cPath, DirListingMetadata\u003e entry \u003d dirs.next();\n       Path path \u003d entry.getKey();\n       DirListingMetadata metadata \u003d entry.getValue();\n       Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n       Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n \n       for (PathMetadata child : oldChildren) {\n         FileStatus status \u003d child.getFileStatus();\n         if (!expired(status, modTime, keyPrefix)) {\n           newChildren.add(child);\n         }\n       }\n       if (newChildren.size() !\u003d oldChildren.size()) {\n-        dirHash.put(path, new DirListingMetadata(path, newChildren, false));\n+        dirCache.put(path, new DirListingMetadata(path, newChildren, false));\n         if (!path.isRoot()) {\n-          DirListingMetadata parent \u003d dirHash.get(path.getParent());\n+          DirListingMetadata parent \u003d null;\n+          parent \u003d dirCache.getIfPresent(path.getParent());\n           if (parent !\u003d null) {\n             parent.setAuthoritative(false);\n           }\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void prune(long modTime, String keyPrefix)\n      throws IOException {\n    Iterator\u003cMap.Entry\u003cPath, PathMetadata\u003e\u003e files \u003d\n        fileCache.asMap().entrySet().iterator();\n    while (files.hasNext()) {\n      Map.Entry\u003cPath, PathMetadata\u003e entry \u003d files.next();\n      if (expired(entry.getValue().getFileStatus(), modTime, keyPrefix)) {\n        files.remove();\n      }\n    }\n    Iterator\u003cMap.Entry\u003cPath, DirListingMetadata\u003e\u003e dirs \u003d\n        dirCache.asMap().entrySet().iterator();\n    while (dirs.hasNext()) {\n      Map.Entry\u003cPath, DirListingMetadata\u003e entry \u003d dirs.next();\n      Path path \u003d entry.getKey();\n      DirListingMetadata metadata \u003d entry.getValue();\n      Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n      Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n\n      for (PathMetadata child : oldChildren) {\n        FileStatus status \u003d child.getFileStatus();\n        if (!expired(status, modTime, keyPrefix)) {\n          newChildren.add(child);\n        }\n      }\n      if (newChildren.size() !\u003d oldChildren.size()) {\n        dirCache.put(path, new DirListingMetadata(path, newChildren, false));\n        if (!path.isRoot()) {\n          DirListingMetadata parent \u003d null;\n          parent \u003d dirCache.getIfPresent(path.getParent());\n          if (parent !\u003d null) {\n            parent.setAuthoritative(false);\n          }\n        }\n      }\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java",
      "extendedDetails": {}
    },
    "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-14759 S3GuardTool prune to prune specific bucket entries. Contributed by Gabor Bota.\n",
      "commitDate": "05/04/18 8:23 PM",
      "commitName": "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a",
      "commitAuthor": "Aaron Fabbri",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-14759 S3GuardTool prune to prune specific bucket entries. Contributed by Gabor Bota.\n",
          "commitDate": "05/04/18 8:23 PM",
          "commitName": "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a",
          "commitAuthor": "Aaron Fabbri",
          "commitDateOld": "25/09/17 3:59 PM",
          "commitNameOld": "47011d7dd300b0c74bb6cfe25b918c479d718f4f",
          "commitAuthorOld": "Aaron Fabbri",
          "daysBetweenCommits": 192.18,
          "commitsBetweenForRepo": 1418,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,35 +1,36 @@\n-  public synchronized void prune(long modTime) throws IOException {\n+  public synchronized void prune(long modTime, String keyPrefix)\n+      throws IOException {\n     Iterator\u003cMap.Entry\u003cPath, PathMetadata\u003e\u003e files \u003d\n         fileHash.entrySet().iterator();\n     while (files.hasNext()) {\n       Map.Entry\u003cPath, PathMetadata\u003e entry \u003d files.next();\n-      if (expired(entry.getValue().getFileStatus(), modTime)) {\n+      if (expired(entry.getValue().getFileStatus(), modTime, keyPrefix)) {\n         files.remove();\n       }\n     }\n     Iterator\u003cMap.Entry\u003cPath, DirListingMetadata\u003e\u003e dirs \u003d\n         dirHash.entrySet().iterator();\n     while (dirs.hasNext()) {\n       Map.Entry\u003cPath, DirListingMetadata\u003e entry \u003d dirs.next();\n       Path path \u003d entry.getKey();\n       DirListingMetadata metadata \u003d entry.getValue();\n       Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n       Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n \n       for (PathMetadata child : oldChildren) {\n         FileStatus status \u003d child.getFileStatus();\n-        if (!expired(status, modTime)) {\n+        if (!expired(status, modTime, keyPrefix)) {\n           newChildren.add(child);\n         }\n       }\n       if (newChildren.size() !\u003d oldChildren.size()) {\n         dirHash.put(path, new DirListingMetadata(path, newChildren, false));\n         if (!path.isRoot()) {\n           DirListingMetadata parent \u003d dirHash.get(path.getParent());\n           if (parent !\u003d null) {\n             parent.setAuthoritative(false);\n           }\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void prune(long modTime, String keyPrefix)\n      throws IOException {\n    Iterator\u003cMap.Entry\u003cPath, PathMetadata\u003e\u003e files \u003d\n        fileHash.entrySet().iterator();\n    while (files.hasNext()) {\n      Map.Entry\u003cPath, PathMetadata\u003e entry \u003d files.next();\n      if (expired(entry.getValue().getFileStatus(), modTime, keyPrefix)) {\n        files.remove();\n      }\n    }\n    Iterator\u003cMap.Entry\u003cPath, DirListingMetadata\u003e\u003e dirs \u003d\n        dirHash.entrySet().iterator();\n    while (dirs.hasNext()) {\n      Map.Entry\u003cPath, DirListingMetadata\u003e entry \u003d dirs.next();\n      Path path \u003d entry.getKey();\n      DirListingMetadata metadata \u003d entry.getValue();\n      Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n      Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n\n      for (PathMetadata child : oldChildren) {\n        FileStatus status \u003d child.getFileStatus();\n        if (!expired(status, modTime, keyPrefix)) {\n          newChildren.add(child);\n        }\n      }\n      if (newChildren.size() !\u003d oldChildren.size()) {\n        dirHash.put(path, new DirListingMetadata(path, newChildren, false));\n        if (!path.isRoot()) {\n          DirListingMetadata parent \u003d dirHash.get(path.getParent());\n          if (parent !\u003d null) {\n            parent.setAuthoritative(false);\n          }\n        }\n      }\n    }\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java",
          "extendedDetails": {
            "oldValue": "[modTime-long]",
            "newValue": "[modTime-long, keyPrefix-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-14759 S3GuardTool prune to prune specific bucket entries. Contributed by Gabor Bota.\n",
          "commitDate": "05/04/18 8:23 PM",
          "commitName": "ea3849f0ccd32b2f8acbc6107de3b9e91803ed4a",
          "commitAuthor": "Aaron Fabbri",
          "commitDateOld": "25/09/17 3:59 PM",
          "commitNameOld": "47011d7dd300b0c74bb6cfe25b918c479d718f4f",
          "commitAuthorOld": "Aaron Fabbri",
          "daysBetweenCommits": 192.18,
          "commitsBetweenForRepo": 1418,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,35 +1,36 @@\n-  public synchronized void prune(long modTime) throws IOException {\n+  public synchronized void prune(long modTime, String keyPrefix)\n+      throws IOException {\n     Iterator\u003cMap.Entry\u003cPath, PathMetadata\u003e\u003e files \u003d\n         fileHash.entrySet().iterator();\n     while (files.hasNext()) {\n       Map.Entry\u003cPath, PathMetadata\u003e entry \u003d files.next();\n-      if (expired(entry.getValue().getFileStatus(), modTime)) {\n+      if (expired(entry.getValue().getFileStatus(), modTime, keyPrefix)) {\n         files.remove();\n       }\n     }\n     Iterator\u003cMap.Entry\u003cPath, DirListingMetadata\u003e\u003e dirs \u003d\n         dirHash.entrySet().iterator();\n     while (dirs.hasNext()) {\n       Map.Entry\u003cPath, DirListingMetadata\u003e entry \u003d dirs.next();\n       Path path \u003d entry.getKey();\n       DirListingMetadata metadata \u003d entry.getValue();\n       Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n       Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n \n       for (PathMetadata child : oldChildren) {\n         FileStatus status \u003d child.getFileStatus();\n-        if (!expired(status, modTime)) {\n+        if (!expired(status, modTime, keyPrefix)) {\n           newChildren.add(child);\n         }\n       }\n       if (newChildren.size() !\u003d oldChildren.size()) {\n         dirHash.put(path, new DirListingMetadata(path, newChildren, false));\n         if (!path.isRoot()) {\n           DirListingMetadata parent \u003d dirHash.get(path.getParent());\n           if (parent !\u003d null) {\n             parent.setAuthoritative(false);\n           }\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void prune(long modTime, String keyPrefix)\n      throws IOException {\n    Iterator\u003cMap.Entry\u003cPath, PathMetadata\u003e\u003e files \u003d\n        fileHash.entrySet().iterator();\n    while (files.hasNext()) {\n      Map.Entry\u003cPath, PathMetadata\u003e entry \u003d files.next();\n      if (expired(entry.getValue().getFileStatus(), modTime, keyPrefix)) {\n        files.remove();\n      }\n    }\n    Iterator\u003cMap.Entry\u003cPath, DirListingMetadata\u003e\u003e dirs \u003d\n        dirHash.entrySet().iterator();\n    while (dirs.hasNext()) {\n      Map.Entry\u003cPath, DirListingMetadata\u003e entry \u003d dirs.next();\n      Path path \u003d entry.getKey();\n      DirListingMetadata metadata \u003d entry.getValue();\n      Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n      Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n\n      for (PathMetadata child : oldChildren) {\n        FileStatus status \u003d child.getFileStatus();\n        if (!expired(status, modTime, keyPrefix)) {\n          newChildren.add(child);\n        }\n      }\n      if (newChildren.size() !\u003d oldChildren.size()) {\n        dirHash.put(path, new DirListingMetadata(path, newChildren, false));\n        if (!path.isRoot()) {\n          DirListingMetadata parent \u003d dirHash.get(path.getParent());\n          if (parent !\u003d null) {\n            parent.setAuthoritative(false);\n          }\n        }\n      }\n    }\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "621b43e254afaff708cd6fc4698b29628f6abc33": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-13345 HS3Guard: Improved Consistency for S3A.\nContributed by: Chris Nauroth, Aaron Fabbri, Mingliang Liu, Lei (Eddy) Xu,\nSean Mackrory, Steve Loughran and others.\n",
      "commitDate": "01/09/17 6:13 AM",
      "commitName": "621b43e254afaff708cd6fc4698b29628f6abc33",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,35 @@\n+  public synchronized void prune(long modTime) throws IOException {\n+    Iterator\u003cMap.Entry\u003cPath, PathMetadata\u003e\u003e files \u003d\n+        fileHash.entrySet().iterator();\n+    while (files.hasNext()) {\n+      Map.Entry\u003cPath, PathMetadata\u003e entry \u003d files.next();\n+      if (expired(entry.getValue().getFileStatus(), modTime)) {\n+        files.remove();\n+      }\n+    }\n+    Iterator\u003cMap.Entry\u003cPath, DirListingMetadata\u003e\u003e dirs \u003d\n+        dirHash.entrySet().iterator();\n+    while (dirs.hasNext()) {\n+      Map.Entry\u003cPath, DirListingMetadata\u003e entry \u003d dirs.next();\n+      Path path \u003d entry.getKey();\n+      DirListingMetadata metadata \u003d entry.getValue();\n+      Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n+      Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n+\n+      for (PathMetadata child : oldChildren) {\n+        FileStatus status \u003d child.getFileStatus();\n+        if (!expired(status, modTime)) {\n+          newChildren.add(child);\n+        }\n+      }\n+      if (newChildren.size() !\u003d oldChildren.size()) {\n+        dirHash.put(path, new DirListingMetadata(path, newChildren, false));\n+        if (!path.isRoot()) {\n+          DirListingMetadata parent \u003d dirHash.get(path.getParent());\n+          if (parent !\u003d null) {\n+            parent.setAuthoritative(false);\n+          }\n+        }\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void prune(long modTime) throws IOException {\n    Iterator\u003cMap.Entry\u003cPath, PathMetadata\u003e\u003e files \u003d\n        fileHash.entrySet().iterator();\n    while (files.hasNext()) {\n      Map.Entry\u003cPath, PathMetadata\u003e entry \u003d files.next();\n      if (expired(entry.getValue().getFileStatus(), modTime)) {\n        files.remove();\n      }\n    }\n    Iterator\u003cMap.Entry\u003cPath, DirListingMetadata\u003e\u003e dirs \u003d\n        dirHash.entrySet().iterator();\n    while (dirs.hasNext()) {\n      Map.Entry\u003cPath, DirListingMetadata\u003e entry \u003d dirs.next();\n      Path path \u003d entry.getKey();\n      DirListingMetadata metadata \u003d entry.getValue();\n      Collection\u003cPathMetadata\u003e oldChildren \u003d metadata.getListing();\n      Collection\u003cPathMetadata\u003e newChildren \u003d new LinkedList\u003c\u003e();\n\n      for (PathMetadata child : oldChildren) {\n        FileStatus status \u003d child.getFileStatus();\n        if (!expired(status, modTime)) {\n          newChildren.add(child);\n        }\n      }\n      if (newChildren.size() !\u003d oldChildren.size()) {\n        dirHash.put(path, new DirListingMetadata(path, newChildren, false));\n        if (!path.isRoot()) {\n          DirListingMetadata parent \u003d dirHash.get(path.getParent());\n          if (parent !\u003d null) {\n            parent.setAuthoritative(false);\n          }\n        }\n      }\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java"
    }
  }
}