{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StreamJob.java",
  "functionName": "submitAndMonitorJob",
  "functionId": "submitAndMonitorJob",
  "sourceFilePath": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
  "functionStartLine": 1006,
  "functionEndLine": 1047,
  "numCommitsSeen": 22,
  "timeTaken": 5236,
  "changeHistory": [
    "d14e26b31fe46fb47a8e99a212c70016fd15a4d9",
    "9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
    "26447229ba2c3d43db978c1b3ce95613669182ee",
    "4cdcf7b867ea7fd13d974af875bf9322457cc0da",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "d14e26b31fe46fb47a8e99a212c70016fd15a4d9": "Ybodychange",
    "9f03a7c018bb2c497cd0ef758f1a3e08e8163d06": "Ybodychange",
    "26447229ba2c3d43db978c1b3ce95613669182ee": "Yfilerename",
    "4cdcf7b867ea7fd13d974af875bf9322457cc0da": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d14e26b31fe46fb47a8e99a212c70016fd15a4d9": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15477. Make unjar in RunJar overrideable\n\nSigned-off-by: Akira Ajisaka \u003caajisaka@apache.org\u003e\n",
      "commitDate": "28/05/18 1:29 AM",
      "commitName": "d14e26b31fe46fb47a8e99a212c70016fd15a4d9",
      "commitAuthor": "Johan Gustavsson",
      "commitDateOld": "24/02/18 2:41 PM",
      "commitNameOld": "2fa7963c3d8cdfc65f90efabc6fe51a160be5c78",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 92.41,
      "commitsBetweenForRepo": 1289,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n   public int submitAndMonitorJob() throws IOException {\n \n     if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n       // getAbs became required when shell and subvm have different working dirs...\n       File wd \u003d new File(\".\").getAbsoluteFile();\n-      RunJar.unJar(new File(jar_), wd);\n+      RunJar.unJar(new File(jar_), wd, MATCH_ANY);\n     }\n \n     // if jobConf_ changes must recreate a JobClient\n     jc_ \u003d new JobClient(jobConf_);\n     running_ \u003d null;\n     try {\n       running_ \u003d jc_.submitJob(jobConf_);\n       jobId_ \u003d running_.getID();\n       if (background_) {\n         LOG.info(\"Job is running in background.\");\n       } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n         LOG.error(\"Job not successful!\");\n         return 1;\n       }\n       LOG.info(\"Output directory: \" + output_);\n     } catch(FileNotFoundException fe) {\n       LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n       return 2;\n     } catch(InvalidJobConfException je) {\n       LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n       return 3;\n     } catch(FileAlreadyExistsException fae) {\n       LOG.error(\"Error launching job , Output path already exists : \"\n                 + fae.getMessage());\n       return 4;\n     } catch(IOException ioe) {\n       LOG.error(\"Error Launching job : \" + ioe.getMessage());\n       return 5;\n     } catch (InterruptedException ie) {\n       LOG.error(\"Error monitoring job : \" + ie.getMessage());\n       return 6;\n     } finally {\n       jc_.close();\n     }\n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int submitAndMonitorJob() throws IOException {\n\n    if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n      // getAbs became required when shell and subvm have different working dirs...\n      File wd \u003d new File(\".\").getAbsoluteFile();\n      RunJar.unJar(new File(jar_), wd, MATCH_ANY);\n    }\n\n    // if jobConf_ changes must recreate a JobClient\n    jc_ \u003d new JobClient(jobConf_);\n    running_ \u003d null;\n    try {\n      running_ \u003d jc_.submitJob(jobConf_);\n      jobId_ \u003d running_.getID();\n      if (background_) {\n        LOG.info(\"Job is running in background.\");\n      } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n        LOG.error(\"Job not successful!\");\n        return 1;\n      }\n      LOG.info(\"Output directory: \" + output_);\n    } catch(FileNotFoundException fe) {\n      LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n      return 2;\n    } catch(InvalidJobConfException je) {\n      LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n      return 3;\n    } catch(FileAlreadyExistsException fae) {\n      LOG.error(\"Error launching job , Output path already exists : \"\n                + fae.getMessage());\n      return 4;\n    } catch(IOException ioe) {\n      LOG.error(\"Error Launching job : \" + ioe.getMessage());\n      return 5;\n    } catch (InterruptedException ie) {\n      LOG.error(\"Error monitoring job : \" + ie.getMessage());\n      return 6;\n    } finally {\n      jc_.close();\n    }\n    return 0;\n  }",
      "path": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {}
    },
    "9f03a7c018bb2c497cd0ef758f1a3e08e8163d06": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10946. Fix a bunch of typos in log messages (Ray Chiang via aw)\n",
      "commitDate": "19/09/14 11:33 AM",
      "commitName": "9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
      "commitAuthor": "Allen Wittenauer",
      "commitDateOld": "29/04/13 4:00 PM",
      "commitNameOld": "8f7ce62085fca12bad3b675603e599b5a53b83e8",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 507.81,
      "commitsBetweenForRepo": 3550,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n   public int submitAndMonitorJob() throws IOException {\n \n     if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n       // getAbs became required when shell and subvm have different working dirs...\n       File wd \u003d new File(\".\").getAbsoluteFile();\n       RunJar.unJar(new File(jar_), wd);\n     }\n \n     // if jobConf_ changes must recreate a JobClient\n     jc_ \u003d new JobClient(jobConf_);\n     running_ \u003d null;\n     try {\n       running_ \u003d jc_.submitJob(jobConf_);\n       jobId_ \u003d running_.getID();\n       if (background_) {\n         LOG.info(\"Job is running in background.\");\n       } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n-        LOG.error(\"Job not Successful!\");\n+        LOG.error(\"Job not successful!\");\n         return 1;\n       }\n       LOG.info(\"Output directory: \" + output_);\n     } catch(FileNotFoundException fe) {\n       LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n       return 2;\n     } catch(InvalidJobConfException je) {\n       LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n       return 3;\n     } catch(FileAlreadyExistsException fae) {\n       LOG.error(\"Error launching job , Output path already exists : \"\n                 + fae.getMessage());\n       return 4;\n     } catch(IOException ioe) {\n       LOG.error(\"Error Launching job : \" + ioe.getMessage());\n       return 5;\n     } catch (InterruptedException ie) {\n       LOG.error(\"Error monitoring job : \" + ie.getMessage());\n       return 6;\n     } finally {\n       jc_.close();\n     }\n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int submitAndMonitorJob() throws IOException {\n\n    if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n      // getAbs became required when shell and subvm have different working dirs...\n      File wd \u003d new File(\".\").getAbsoluteFile();\n      RunJar.unJar(new File(jar_), wd);\n    }\n\n    // if jobConf_ changes must recreate a JobClient\n    jc_ \u003d new JobClient(jobConf_);\n    running_ \u003d null;\n    try {\n      running_ \u003d jc_.submitJob(jobConf_);\n      jobId_ \u003d running_.getID();\n      if (background_) {\n        LOG.info(\"Job is running in background.\");\n      } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n        LOG.error(\"Job not successful!\");\n        return 1;\n      }\n      LOG.info(\"Output directory: \" + output_);\n    } catch(FileNotFoundException fe) {\n      LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n      return 2;\n    } catch(InvalidJobConfException je) {\n      LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n      return 3;\n    } catch(FileAlreadyExistsException fae) {\n      LOG.error(\"Error launching job , Output path already exists : \"\n                + fae.getMessage());\n      return 4;\n    } catch(IOException ioe) {\n      LOG.error(\"Error Launching job : \" + ioe.getMessage());\n      return 5;\n    } catch (InterruptedException ie) {\n      LOG.error(\"Error monitoring job : \" + ie.getMessage());\n      return 6;\n    } finally {\n      jc_.close();\n    }\n    return 0;\n  }",
      "path": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {}
    },
    "26447229ba2c3d43db978c1b3ce95613669182ee": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7590. Mavenize streaming and MR examples. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203941 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 5:24 PM",
      "commitName": "26447229ba2c3d43db978c1b3ce95613669182ee",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "18/11/11 1:04 AM",
      "commitNameOld": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.68,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n   public int submitAndMonitorJob() throws IOException {\n \n     if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n       // getAbs became required when shell and subvm have different working dirs...\n       File wd \u003d new File(\".\").getAbsoluteFile();\n       RunJar.unJar(new File(jar_), wd);\n     }\n \n     // if jobConf_ changes must recreate a JobClient\n     jc_ \u003d new JobClient(jobConf_);\n     running_ \u003d null;\n     try {\n       running_ \u003d jc_.submitJob(jobConf_);\n       jobId_ \u003d running_.getID();\n       if (background_) {\n         LOG.info(\"Job is running in background.\");\n       } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n         LOG.error(\"Job not Successful!\");\n         return 1;\n       }\n       LOG.info(\"Output directory: \" + output_);\n     } catch(FileNotFoundException fe) {\n       LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n       return 2;\n     } catch(InvalidJobConfException je) {\n       LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n       return 3;\n     } catch(FileAlreadyExistsException fae) {\n-      LOG.error(\"Error launching job , Output path already exists : \" \n+      LOG.error(\"Error launching job , Output path already exists : \"\n                 + fae.getMessage());\n       return 4;\n     } catch(IOException ioe) {\n       LOG.error(\"Error Launching job : \" + ioe.getMessage());\n       return 5;\n     } catch (InterruptedException ie) {\n       LOG.error(\"Error monitoring job : \" + ie.getMessage());\n       return 6;\n     } finally {\n       jc_.close();\n     }\n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int submitAndMonitorJob() throws IOException {\n\n    if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n      // getAbs became required when shell and subvm have different working dirs...\n      File wd \u003d new File(\".\").getAbsoluteFile();\n      RunJar.unJar(new File(jar_), wd);\n    }\n\n    // if jobConf_ changes must recreate a JobClient\n    jc_ \u003d new JobClient(jobConf_);\n    running_ \u003d null;\n    try {\n      running_ \u003d jc_.submitJob(jobConf_);\n      jobId_ \u003d running_.getID();\n      if (background_) {\n        LOG.info(\"Job is running in background.\");\n      } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n        LOG.error(\"Job not Successful!\");\n        return 1;\n      }\n      LOG.info(\"Output directory: \" + output_);\n    } catch(FileNotFoundException fe) {\n      LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n      return 2;\n    } catch(InvalidJobConfException je) {\n      LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n      return 3;\n    } catch(FileAlreadyExistsException fae) {\n      LOG.error(\"Error launching job , Output path already exists : \"\n                + fae.getMessage());\n      return 4;\n    } catch(IOException ioe) {\n      LOG.error(\"Error Launching job : \" + ioe.getMessage());\n      return 5;\n    } catch (InterruptedException ie) {\n      LOG.error(\"Error monitoring job : \" + ie.getMessage());\n      return 6;\n    } finally {\n      jc_.close();\n    }\n    return 0;\n  }",
      "path": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "4cdcf7b867ea7fd13d974af875bf9322457cc0da": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3243. Invalid tracking URL for streaming jobs (Jonathan Eagles via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1201951 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/11/11 2:44 PM",
      "commitName": "4cdcf7b867ea7fd13d974af875bf9322457cc0da",
      "commitAuthor": "Mahadev Konar",
      "commitDateOld": "26/10/11 7:47 PM",
      "commitNameOld": "3e4efbb609ff25036aa6b04dc379b6db72e5a31f",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 18.83,
      "commitsBetweenForRepo": 140,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,42 @@\n   public int submitAndMonitorJob() throws IOException {\n \n     if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n       // getAbs became required when shell and subvm have different working dirs...\n       File wd \u003d new File(\".\").getAbsoluteFile();\n       RunJar.unJar(new File(jar_), wd);\n     }\n \n     // if jobConf_ changes must recreate a JobClient\n     jc_ \u003d new JobClient(jobConf_);\n     running_ \u003d null;\n     try {\n       running_ \u003d jc_.submitJob(jobConf_);\n       jobId_ \u003d running_.getID();\n-      jobInfo();\n       if (background_) {\n         LOG.info(\"Job is running in background.\");\n       } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n         LOG.error(\"Job not Successful!\");\n         return 1;\n       }\n       LOG.info(\"Output directory: \" + output_);\n     } catch(FileNotFoundException fe) {\n       LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n       return 2;\n     } catch(InvalidJobConfException je) {\n       LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n       return 3;\n     } catch(FileAlreadyExistsException fae) {\n       LOG.error(\"Error launching job , Output path already exists : \" \n                 + fae.getMessage());\n       return 4;\n     } catch(IOException ioe) {\n       LOG.error(\"Error Launching job : \" + ioe.getMessage());\n       return 5;\n     } catch (InterruptedException ie) {\n       LOG.error(\"Error monitoring job : \" + ie.getMessage());\n       return 6;\n     } finally {\n       jc_.close();\n     }\n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int submitAndMonitorJob() throws IOException {\n\n    if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n      // getAbs became required when shell and subvm have different working dirs...\n      File wd \u003d new File(\".\").getAbsoluteFile();\n      RunJar.unJar(new File(jar_), wd);\n    }\n\n    // if jobConf_ changes must recreate a JobClient\n    jc_ \u003d new JobClient(jobConf_);\n    running_ \u003d null;\n    try {\n      running_ \u003d jc_.submitJob(jobConf_);\n      jobId_ \u003d running_.getID();\n      if (background_) {\n        LOG.info(\"Job is running in background.\");\n      } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n        LOG.error(\"Job not Successful!\");\n        return 1;\n      }\n      LOG.info(\"Output directory: \" + output_);\n    } catch(FileNotFoundException fe) {\n      LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n      return 2;\n    } catch(InvalidJobConfException je) {\n      LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n      return 3;\n    } catch(FileAlreadyExistsException fae) {\n      LOG.error(\"Error launching job , Output path already exists : \" \n                + fae.getMessage());\n      return 4;\n    } catch(IOException ioe) {\n      LOG.error(\"Error Launching job : \" + ioe.getMessage());\n      return 5;\n    } catch (InterruptedException ie) {\n      LOG.error(\"Error monitoring job : \" + ie.getMessage());\n      return 6;\n    } finally {\n      jc_.close();\n    }\n    return 0;\n  }",
      "path": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public int submitAndMonitorJob() throws IOException {\n\n    if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n      // getAbs became required when shell and subvm have different working dirs...\n      File wd \u003d new File(\".\").getAbsoluteFile();\n      RunJar.unJar(new File(jar_), wd);\n    }\n\n    // if jobConf_ changes must recreate a JobClient\n    jc_ \u003d new JobClient(jobConf_);\n    running_ \u003d null;\n    try {\n      running_ \u003d jc_.submitJob(jobConf_);\n      jobId_ \u003d running_.getID();\n      jobInfo();\n      if (background_) {\n        LOG.info(\"Job is running in background.\");\n      } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n        LOG.error(\"Job not Successful!\");\n        return 1;\n      }\n      LOG.info(\"Output directory: \" + output_);\n    } catch(FileNotFoundException fe) {\n      LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n      return 2;\n    } catch(InvalidJobConfException je) {\n      LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n      return 3;\n    } catch(FileAlreadyExistsException fae) {\n      LOG.error(\"Error launching job , Output path already exists : \" \n                + fae.getMessage());\n      return 4;\n    } catch(IOException ioe) {\n      LOG.error(\"Error Launching job : \" + ioe.getMessage());\n      return 5;\n    } catch (InterruptedException ie) {\n      LOG.error(\"Error monitoring job : \" + ie.getMessage());\n      return 6;\n    } finally {\n      jc_.close();\n    }\n    return 0;\n  }",
      "path": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public int submitAndMonitorJob() throws IOException {\n\n    if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n      // getAbs became required when shell and subvm have different working dirs...\n      File wd \u003d new File(\".\").getAbsoluteFile();\n      RunJar.unJar(new File(jar_), wd);\n    }\n\n    // if jobConf_ changes must recreate a JobClient\n    jc_ \u003d new JobClient(jobConf_);\n    running_ \u003d null;\n    try {\n      running_ \u003d jc_.submitJob(jobConf_);\n      jobId_ \u003d running_.getID();\n      jobInfo();\n      if (background_) {\n        LOG.info(\"Job is running in background.\");\n      } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n        LOG.error(\"Job not Successful!\");\n        return 1;\n      }\n      LOG.info(\"Output directory: \" + output_);\n    } catch(FileNotFoundException fe) {\n      LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n      return 2;\n    } catch(InvalidJobConfException je) {\n      LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n      return 3;\n    } catch(FileAlreadyExistsException fae) {\n      LOG.error(\"Error launching job , Output path already exists : \" \n                + fae.getMessage());\n      return 4;\n    } catch(IOException ioe) {\n      LOG.error(\"Error Launching job : \" + ioe.getMessage());\n      return 5;\n    } catch (InterruptedException ie) {\n      LOG.error(\"Error monitoring job : \" + ie.getMessage());\n      return 6;\n    } finally {\n      jc_.close();\n    }\n    return 0;\n  }",
      "path": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,43 @@\n+  public int submitAndMonitorJob() throws IOException {\n+\n+    if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n+      // getAbs became required when shell and subvm have different working dirs...\n+      File wd \u003d new File(\".\").getAbsoluteFile();\n+      RunJar.unJar(new File(jar_), wd);\n+    }\n+\n+    // if jobConf_ changes must recreate a JobClient\n+    jc_ \u003d new JobClient(jobConf_);\n+    running_ \u003d null;\n+    try {\n+      running_ \u003d jc_.submitJob(jobConf_);\n+      jobId_ \u003d running_.getID();\n+      jobInfo();\n+      if (background_) {\n+        LOG.info(\"Job is running in background.\");\n+      } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n+        LOG.error(\"Job not Successful!\");\n+        return 1;\n+      }\n+      LOG.info(\"Output directory: \" + output_);\n+    } catch(FileNotFoundException fe) {\n+      LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n+      return 2;\n+    } catch(InvalidJobConfException je) {\n+      LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n+      return 3;\n+    } catch(FileAlreadyExistsException fae) {\n+      LOG.error(\"Error launching job , Output path already exists : \" \n+                + fae.getMessage());\n+      return 4;\n+    } catch(IOException ioe) {\n+      LOG.error(\"Error Launching job : \" + ioe.getMessage());\n+      return 5;\n+    } catch (InterruptedException ie) {\n+      LOG.error(\"Error monitoring job : \" + ie.getMessage());\n+      return 6;\n+    } finally {\n+      jc_.close();\n+    }\n+    return 0;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public int submitAndMonitorJob() throws IOException {\n\n    if (jar_ !\u003d null \u0026\u0026 isLocalHadoop()) {\n      // getAbs became required when shell and subvm have different working dirs...\n      File wd \u003d new File(\".\").getAbsoluteFile();\n      RunJar.unJar(new File(jar_), wd);\n    }\n\n    // if jobConf_ changes must recreate a JobClient\n    jc_ \u003d new JobClient(jobConf_);\n    running_ \u003d null;\n    try {\n      running_ \u003d jc_.submitJob(jobConf_);\n      jobId_ \u003d running_.getID();\n      jobInfo();\n      if (background_) {\n        LOG.info(\"Job is running in background.\");\n      } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n        LOG.error(\"Job not Successful!\");\n        return 1;\n      }\n      LOG.info(\"Output directory: \" + output_);\n    } catch(FileNotFoundException fe) {\n      LOG.error(\"Error launching job , bad input path : \" + fe.getMessage());\n      return 2;\n    } catch(InvalidJobConfException je) {\n      LOG.error(\"Error launching job , Invalid job conf : \" + je.getMessage());\n      return 3;\n    } catch(FileAlreadyExistsException fae) {\n      LOG.error(\"Error launching job , Output path already exists : \" \n                + fae.getMessage());\n      return 4;\n    } catch(IOException ioe) {\n      LOG.error(\"Error Launching job : \" + ioe.getMessage());\n      return 5;\n    } catch (InterruptedException ie) {\n      LOG.error(\"Error monitoring job : \" + ie.getMessage());\n      return 6;\n    } finally {\n      jc_.close();\n    }\n    return 0;\n  }",
      "path": "mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
    }
  }
}