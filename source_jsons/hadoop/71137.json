{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Client.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/Client.java",
  "functionStartLine": 518,
  "functionEndLine": 638,
  "numCommitsSeen": 7,
  "timeTaken": 561,
  "changeHistory": [
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91"
  ],
  "changeHistoryShort": {
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": "Yintroduced"
  },
  "changeHistoryDetails": {
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-12345 Add Dynamometer to hadoop-tools, a tool for scale testing the HDFS NameNode with real metadata and workloads. Contributed by Erik Krogen.\n",
      "commitDate": "25/06/19 8:07 AM",
      "commitName": "ab0b180ddb5d0775a2452d5eeb7badd252aadb91",
      "commitAuthor": "Erik Krogen",
      "diff": "@@ -0,0 +1,121 @@\n+  public boolean run() throws IOException, YarnException {\n+\n+    LOG.info(\"Running Client\");\n+    yarnClient.start();\n+\n+    YarnClusterMetrics clusterMetrics \u003d yarnClient.getYarnClusterMetrics();\n+    LOG.info(\"Got Cluster metric info from ASM, numNodeManagers\u003d{}\",\n+        clusterMetrics.getNumNodeManagers());\n+\n+    QueueInfo queueInfo \u003d yarnClient.getQueueInfo(this.amQueue);\n+    LOG.info(\"Queue info: queueName\u003d{}, queueCurrentCapacity\u003d{}, \"\n+        + \"queueMaxCapacity\u003d{}, queueApplicationCount\u003d{}, \"\n+        + \"queueChildQueueCount\u003d{}\", queueInfo.getQueueName(),\n+        queueInfo.getCurrentCapacity(), queueInfo.getMaximumCapacity(),\n+        queueInfo.getApplications().size(), queueInfo.getChildQueues().size());\n+\n+    // Get a new application id\n+    YarnClientApplication app \u003d yarnClient.createApplication();\n+    GetNewApplicationResponse appResponse \u003d app.getNewApplicationResponse();\n+    long maxMem \u003d appResponse.getMaximumResourceCapability().getMemorySize();\n+    LOG.info(\"Max mem capabililty of resources in this cluster \" + maxMem);\n+    int maxVCores \u003d appResponse.getMaximumResourceCapability()\n+        .getVirtualCores();\n+    LOG.info(\"Max virtual cores capabililty of resources in this cluster {}\",\n+        maxVCores);\n+    if (amMemory \u003e maxMem || amMemory \u003c 0 || amVCores \u003e maxVCores\n+        || amVCores \u003c 0) {\n+      throw new IllegalArgumentException(\"Invalid AM memory or vcores: memory\u003d\"\n+          + amMemory + \", vcores\u003d\" + amVCores);\n+    }\n+    amOptions.verify(maxMem, maxVCores);\n+\n+    // set the application name\n+    ApplicationSubmissionContext appContext \u003d\n+        app.getApplicationSubmissionContext();\n+    infraAppId \u003d appContext.getApplicationId();\n+    appContext.setApplicationName(appName);\n+\n+    // Set up the container launch context for the application master\n+    ContainerLaunchContext amContainer \u003d Records\n+        .newRecord(ContainerLaunchContext.class);\n+    Map\u003cApplicationAccessType, String\u003e acls \u003d new HashMap\u003c\u003e();\n+    acls.put(ApplicationAccessType.VIEW_APP, getConf().get(\n+        MRJobConfig.JOB_ACL_VIEW_JOB, MRJobConfig.DEFAULT_JOB_ACL_VIEW_JOB));\n+    amContainer.setApplicationACLs(acls);\n+\n+    FileSystem fs \u003d FileSystem.get(getConf());\n+    fs.mkdirs(getRemoteStoragePath(getConf(), infraAppId));\n+\n+    // Set the env variables to be setup in the env where the application master\n+    // will be run\n+    Map\u003cString, String\u003e env \u003d setupRemoteResourcesGetEnv();\n+\n+    amContainer.setEnvironment(env);\n+\n+    // All of the resources for both AM and NN/DNs have been put on remote\n+    // storage\n+    // Only the application master JAR is needed as a local resource for the AM\n+    // so\n+    // we explicitly add it here\n+    Map\u003cString, LocalResource\u003e localResources \u003d new HashMap\u003c\u003e();\n+    LocalResource scRsrc \u003d LocalResource.newInstance(\n+        org.apache.hadoop.yarn.api.records.URL\n+            .fromPath(DynoConstants.DYNO_DEPENDENCIES.getPath(env)),\n+        LocalResourceType.ARCHIVE, LocalResourceVisibility.APPLICATION,\n+        DynoConstants.DYNO_DEPENDENCIES.getLength(env),\n+        DynoConstants.DYNO_DEPENDENCIES.getTimestamp(env));\n+    localResources.put(DynoConstants.DYNO_DEPENDENCIES.getResourcePath(),\n+        scRsrc);\n+    // Set local resource info into app master container launch context\n+    amContainer.setLocalResources(localResources);\n+\n+    // Set the necessary command to execute the application master\n+    amContainer.setCommands(getAMCommand());\n+\n+    Resource capability \u003d Records.newRecord(Resource.class);\n+    capability.setMemorySize(amMemory);\n+    capability.setVirtualCores(amVCores);\n+    appContext.setResource(capability);\n+\n+    // Setup security tokens\n+    if (UserGroupInformation.isSecurityEnabled()) {\n+      ByteBuffer fsTokens;\n+      if (tokenFileLocation !\u003d null) {\n+        fsTokens \u003d ByteBuffer\n+            .wrap(Files.readAllBytes(Paths.get(tokenFileLocation)));\n+      } else {\n+        Credentials credentials \u003d new Credentials();\n+        String tokenRenewer \u003d getConf().get(YarnConfiguration.RM_PRINCIPAL);\n+        if (tokenRenewer \u003d\u003d null || tokenRenewer.length() \u003d\u003d 0) {\n+          throw new IOException(\"Can\u0027t get Master Kerberos principal for the \"\n+              + \"RM to use as renewer\");\n+        }\n+\n+        // For now, only getting tokens for the default file-system.\n+        final Token\u003c?\u003e[] tokens \u003d fs.addDelegationTokens(tokenRenewer,\n+            credentials);\n+        if (tokens !\u003d null) {\n+          for (Token\u003c?\u003e token : tokens) {\n+            LOG.info(\"Got dt for \" + fs.getUri() + \"; \" + token);\n+          }\n+        }\n+        DataOutputBuffer dob \u003d new DataOutputBuffer();\n+        credentials.writeTokenStorageToStream(dob);\n+        fsTokens \u003d ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n+      }\n+      amContainer.setTokens(fsTokens);\n+    }\n+\n+    appContext.setAMContainerSpec(amContainer);\n+\n+    // Set the queue to which this application is to be submitted in the RM\n+    appContext.setQueue(amQueue);\n+\n+    LOG.info(\"Submitting application to RM\");\n+    yarnClient.submitApplication(appContext);\n+\n+    // Monitor the application\n+    return monitorInfraApplication();\n+\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean run() throws IOException, YarnException {\n\n    LOG.info(\"Running Client\");\n    yarnClient.start();\n\n    YarnClusterMetrics clusterMetrics \u003d yarnClient.getYarnClusterMetrics();\n    LOG.info(\"Got Cluster metric info from ASM, numNodeManagers\u003d{}\",\n        clusterMetrics.getNumNodeManagers());\n\n    QueueInfo queueInfo \u003d yarnClient.getQueueInfo(this.amQueue);\n    LOG.info(\"Queue info: queueName\u003d{}, queueCurrentCapacity\u003d{}, \"\n        + \"queueMaxCapacity\u003d{}, queueApplicationCount\u003d{}, \"\n        + \"queueChildQueueCount\u003d{}\", queueInfo.getQueueName(),\n        queueInfo.getCurrentCapacity(), queueInfo.getMaximumCapacity(),\n        queueInfo.getApplications().size(), queueInfo.getChildQueues().size());\n\n    // Get a new application id\n    YarnClientApplication app \u003d yarnClient.createApplication();\n    GetNewApplicationResponse appResponse \u003d app.getNewApplicationResponse();\n    long maxMem \u003d appResponse.getMaximumResourceCapability().getMemorySize();\n    LOG.info(\"Max mem capabililty of resources in this cluster \" + maxMem);\n    int maxVCores \u003d appResponse.getMaximumResourceCapability()\n        .getVirtualCores();\n    LOG.info(\"Max virtual cores capabililty of resources in this cluster {}\",\n        maxVCores);\n    if (amMemory \u003e maxMem || amMemory \u003c 0 || amVCores \u003e maxVCores\n        || amVCores \u003c 0) {\n      throw new IllegalArgumentException(\"Invalid AM memory or vcores: memory\u003d\"\n          + amMemory + \", vcores\u003d\" + amVCores);\n    }\n    amOptions.verify(maxMem, maxVCores);\n\n    // set the application name\n    ApplicationSubmissionContext appContext \u003d\n        app.getApplicationSubmissionContext();\n    infraAppId \u003d appContext.getApplicationId();\n    appContext.setApplicationName(appName);\n\n    // Set up the container launch context for the application master\n    ContainerLaunchContext amContainer \u003d Records\n        .newRecord(ContainerLaunchContext.class);\n    Map\u003cApplicationAccessType, String\u003e acls \u003d new HashMap\u003c\u003e();\n    acls.put(ApplicationAccessType.VIEW_APP, getConf().get(\n        MRJobConfig.JOB_ACL_VIEW_JOB, MRJobConfig.DEFAULT_JOB_ACL_VIEW_JOB));\n    amContainer.setApplicationACLs(acls);\n\n    FileSystem fs \u003d FileSystem.get(getConf());\n    fs.mkdirs(getRemoteStoragePath(getConf(), infraAppId));\n\n    // Set the env variables to be setup in the env where the application master\n    // will be run\n    Map\u003cString, String\u003e env \u003d setupRemoteResourcesGetEnv();\n\n    amContainer.setEnvironment(env);\n\n    // All of the resources for both AM and NN/DNs have been put on remote\n    // storage\n    // Only the application master JAR is needed as a local resource for the AM\n    // so\n    // we explicitly add it here\n    Map\u003cString, LocalResource\u003e localResources \u003d new HashMap\u003c\u003e();\n    LocalResource scRsrc \u003d LocalResource.newInstance(\n        org.apache.hadoop.yarn.api.records.URL\n            .fromPath(DynoConstants.DYNO_DEPENDENCIES.getPath(env)),\n        LocalResourceType.ARCHIVE, LocalResourceVisibility.APPLICATION,\n        DynoConstants.DYNO_DEPENDENCIES.getLength(env),\n        DynoConstants.DYNO_DEPENDENCIES.getTimestamp(env));\n    localResources.put(DynoConstants.DYNO_DEPENDENCIES.getResourcePath(),\n        scRsrc);\n    // Set local resource info into app master container launch context\n    amContainer.setLocalResources(localResources);\n\n    // Set the necessary command to execute the application master\n    amContainer.setCommands(getAMCommand());\n\n    Resource capability \u003d Records.newRecord(Resource.class);\n    capability.setMemorySize(amMemory);\n    capability.setVirtualCores(amVCores);\n    appContext.setResource(capability);\n\n    // Setup security tokens\n    if (UserGroupInformation.isSecurityEnabled()) {\n      ByteBuffer fsTokens;\n      if (tokenFileLocation !\u003d null) {\n        fsTokens \u003d ByteBuffer\n            .wrap(Files.readAllBytes(Paths.get(tokenFileLocation)));\n      } else {\n        Credentials credentials \u003d new Credentials();\n        String tokenRenewer \u003d getConf().get(YarnConfiguration.RM_PRINCIPAL);\n        if (tokenRenewer \u003d\u003d null || tokenRenewer.length() \u003d\u003d 0) {\n          throw new IOException(\"Can\u0027t get Master Kerberos principal for the \"\n              + \"RM to use as renewer\");\n        }\n\n        // For now, only getting tokens for the default file-system.\n        final Token\u003c?\u003e[] tokens \u003d fs.addDelegationTokens(tokenRenewer,\n            credentials);\n        if (tokens !\u003d null) {\n          for (Token\u003c?\u003e token : tokens) {\n            LOG.info(\"Got dt for \" + fs.getUri() + \"; \" + token);\n          }\n        }\n        DataOutputBuffer dob \u003d new DataOutputBuffer();\n        credentials.writeTokenStorageToStream(dob);\n        fsTokens \u003d ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n      }\n      amContainer.setTokens(fsTokens);\n    }\n\n    appContext.setAMContainerSpec(amContainer);\n\n    // Set the queue to which this application is to be submitted in the RM\n    appContext.setQueue(amQueue);\n\n    LOG.info(\"Submitting application to RM\");\n    yarnClient.submitApplication(appContext);\n\n    // Monitor the application\n    return monitorInfraApplication();\n\n  }",
      "path": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/Client.java"
    }
  }
}