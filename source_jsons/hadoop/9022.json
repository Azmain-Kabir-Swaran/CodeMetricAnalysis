{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSNamesystem.java",
  "functionName": "getFilesBlockingDecom",
  "functionId": "getFilesBlockingDecom___prevId-long__path-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
  "functionStartLine": 1957,
  "functionEndLine": 1992,
  "numCommitsSeen": 1652,
  "timeTaken": 11124,
  "changeHistory": [
    "24080666e5e2214d4a362c889cd9aa617be5de81",
    "fba9d7cd746cd7b659d2fd9d2bfa23266be9009b",
    "bf5c94899537011465350d5d999fad9ffaeb605d",
    "42a1c98597e6dba2e371510a6b2b6b1fb94e4090"
  ],
  "changeHistoryShort": {
    "24080666e5e2214d4a362c889cd9aa617be5de81": "Ybodychange",
    "fba9d7cd746cd7b659d2fd9d2bfa23266be9009b": "Ybodychange",
    "bf5c94899537011465350d5d999fad9ffaeb605d": "Ymultichange(Yparameterchange,Ybodychange)",
    "42a1c98597e6dba2e371510a6b2b6b1fb94e4090": "Yintroduced"
  },
  "changeHistoryDetails": {
    "24080666e5e2214d4a362c889cd9aa617be5de81": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14908. LeaseManager should check parent-child relationship when filter open files. Contributed by Jinglun.\n",
      "commitDate": "16/12/19 6:41 PM",
      "commitName": "24080666e5e2214d4a362c889cd9aa617be5de81",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "29/11/19 10:25 AM",
      "commitNameOld": "6b2d6d4aafb110bef1b77d4ccbba4350e624b57d",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 17.34,
      "commitsBetweenForRepo": 63,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId,\n       String path) {\n     assert hasReadLock();\n     final List\u003cOpenFileEntry\u003e openFileEntries \u003d Lists.newArrayList();\n     LightWeightHashSet\u003cLong\u003e openFileIds \u003d new LightWeightHashSet\u003c\u003e();\n     for (DatanodeDescriptor dataNode :\n         blockManager.getDatanodeManager().getDatanodes()) {\n       for (long ucFileId : dataNode.getLeavingServiceStatus().getOpenFiles()) {\n         INode ucFile \u003d getFSDirectory().getInode(ucFileId);\n         if (ucFile \u003d\u003d null || ucFileId \u003c\u003d prevId ||\n             openFileIds.contains(ucFileId)) {\n           // probably got deleted or\n           // part of previous batch or\n           // already part of the current batch\n           continue;\n         }\n         Preconditions.checkState(ucFile instanceof INodeFile);\n         openFileIds.add(ucFileId);\n         INodeFile inodeFile \u003d ucFile.asFile();\n \n         String fullPathName \u003d inodeFile.getFullPathName();\n         if (org.apache.commons.lang3.StringUtils.isEmpty(path)\n-            || fullPathName.startsWith(path)) {\n+            || DFSUtil.isParentEntry(fullPathName, path)) {\n           openFileEntries.add(new OpenFileEntry(inodeFile.getId(),\n               inodeFile.getFullPathName(),\n               inodeFile.getFileUnderConstructionFeature().getClientName(),\n               inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n         }\n \n         if (openFileIds.size() \u003e\u003d this.maxListOpenFilesResponses) {\n           return new BatchedListEntries\u003c\u003e(openFileEntries, true);\n         }\n       }\n     }\n     return new BatchedListEntries\u003c\u003e(openFileEntries, false);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId,\n      String path) {\n    assert hasReadLock();\n    final List\u003cOpenFileEntry\u003e openFileEntries \u003d Lists.newArrayList();\n    LightWeightHashSet\u003cLong\u003e openFileIds \u003d new LightWeightHashSet\u003c\u003e();\n    for (DatanodeDescriptor dataNode :\n        blockManager.getDatanodeManager().getDatanodes()) {\n      for (long ucFileId : dataNode.getLeavingServiceStatus().getOpenFiles()) {\n        INode ucFile \u003d getFSDirectory().getInode(ucFileId);\n        if (ucFile \u003d\u003d null || ucFileId \u003c\u003d prevId ||\n            openFileIds.contains(ucFileId)) {\n          // probably got deleted or\n          // part of previous batch or\n          // already part of the current batch\n          continue;\n        }\n        Preconditions.checkState(ucFile instanceof INodeFile);\n        openFileIds.add(ucFileId);\n        INodeFile inodeFile \u003d ucFile.asFile();\n\n        String fullPathName \u003d inodeFile.getFullPathName();\n        if (org.apache.commons.lang3.StringUtils.isEmpty(path)\n            || DFSUtil.isParentEntry(fullPathName, path)) {\n          openFileEntries.add(new OpenFileEntry(inodeFile.getId(),\n              inodeFile.getFullPathName(),\n              inodeFile.getFileUnderConstructionFeature().getClientName(),\n              inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n        }\n\n        if (openFileIds.size() \u003e\u003d this.maxListOpenFilesResponses) {\n          return new BatchedListEntries\u003c\u003e(openFileEntries, true);\n        }\n      }\n    }\n    return new BatchedListEntries\u003c\u003e(openFileEntries, false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "fba9d7cd746cd7b659d2fd9d2bfa23266be9009b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13621. Upgrade commons-lang version to 3.7 in hadoop-hdfs-project. Contributed by Takanobu Asanuma.\n",
      "commitDate": "18/06/18 10:17 AM",
      "commitName": "fba9d7cd746cd7b659d2fd9d2bfa23266be9009b",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "08/06/18 3:14 PM",
      "commitNameOld": "cf4108313da83e28d07676078a33016ec8856ff6",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 9.79,
      "commitsBetweenForRepo": 58,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId,\n       String path) {\n     assert hasReadLock();\n     final List\u003cOpenFileEntry\u003e openFileEntries \u003d Lists.newArrayList();\n     LightWeightHashSet\u003cLong\u003e openFileIds \u003d new LightWeightHashSet\u003c\u003e();\n     for (DatanodeDescriptor dataNode :\n         blockManager.getDatanodeManager().getDatanodes()) {\n       for (long ucFileId : dataNode.getLeavingServiceStatus().getOpenFiles()) {\n         INode ucFile \u003d getFSDirectory().getInode(ucFileId);\n         if (ucFile \u003d\u003d null || ucFileId \u003c\u003d prevId ||\n             openFileIds.contains(ucFileId)) {\n           // probably got deleted or\n           // part of previous batch or\n           // already part of the current batch\n           continue;\n         }\n         Preconditions.checkState(ucFile instanceof INodeFile);\n         openFileIds.add(ucFileId);\n         INodeFile inodeFile \u003d ucFile.asFile();\n \n         String fullPathName \u003d inodeFile.getFullPathName();\n-        if (org.apache.commons.lang.StringUtils.isEmpty(path)\n+        if (org.apache.commons.lang3.StringUtils.isEmpty(path)\n             || fullPathName.startsWith(path)) {\n           openFileEntries.add(new OpenFileEntry(inodeFile.getId(),\n               inodeFile.getFullPathName(),\n               inodeFile.getFileUnderConstructionFeature().getClientName(),\n               inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n         }\n \n         if (openFileIds.size() \u003e\u003d this.maxListOpenFilesResponses) {\n           return new BatchedListEntries\u003c\u003e(openFileEntries, true);\n         }\n       }\n     }\n     return new BatchedListEntries\u003c\u003e(openFileEntries, false);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId,\n      String path) {\n    assert hasReadLock();\n    final List\u003cOpenFileEntry\u003e openFileEntries \u003d Lists.newArrayList();\n    LightWeightHashSet\u003cLong\u003e openFileIds \u003d new LightWeightHashSet\u003c\u003e();\n    for (DatanodeDescriptor dataNode :\n        blockManager.getDatanodeManager().getDatanodes()) {\n      for (long ucFileId : dataNode.getLeavingServiceStatus().getOpenFiles()) {\n        INode ucFile \u003d getFSDirectory().getInode(ucFileId);\n        if (ucFile \u003d\u003d null || ucFileId \u003c\u003d prevId ||\n            openFileIds.contains(ucFileId)) {\n          // probably got deleted or\n          // part of previous batch or\n          // already part of the current batch\n          continue;\n        }\n        Preconditions.checkState(ucFile instanceof INodeFile);\n        openFileIds.add(ucFileId);\n        INodeFile inodeFile \u003d ucFile.asFile();\n\n        String fullPathName \u003d inodeFile.getFullPathName();\n        if (org.apache.commons.lang3.StringUtils.isEmpty(path)\n            || fullPathName.startsWith(path)) {\n          openFileEntries.add(new OpenFileEntry(inodeFile.getId(),\n              inodeFile.getFullPathName(),\n              inodeFile.getFileUnderConstructionFeature().getClientName(),\n              inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n        }\n\n        if (openFileIds.size() \u003e\u003d this.maxListOpenFilesResponses) {\n          return new BatchedListEntries\u003c\u003e(openFileEntries, true);\n        }\n      }\n    }\n    return new BatchedListEntries\u003c\u003e(openFileEntries, false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "bf5c94899537011465350d5d999fad9ffaeb605d": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-11848. Enhance dfsadmin listOpenFiles command to list files under a given path. Contributed by Yiqun Lin.\n",
      "commitDate": "05/01/18 10:31 PM",
      "commitName": "bf5c94899537011465350d5d999fad9ffaeb605d",
      "commitAuthor": "Yiqun Lin",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-11848. Enhance dfsadmin listOpenFiles command to list files under a given path. Contributed by Yiqun Lin.\n",
          "commitDate": "05/01/18 10:31 PM",
          "commitName": "bf5c94899537011465350d5d999fad9ffaeb605d",
          "commitAuthor": "Yiqun Lin",
          "commitDateOld": "02/01/18 2:59 PM",
          "commitNameOld": "42a1c98597e6dba2e371510a6b2b6b1fb94e4090",
          "commitAuthorOld": "Manoj Govindassamy",
          "daysBetweenCommits": 3.31,
          "commitsBetweenForRepo": 26,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,36 @@\n-  public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId) {\n+  public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId,\n+      String path) {\n     assert hasReadLock();\n     final List\u003cOpenFileEntry\u003e openFileEntries \u003d Lists.newArrayList();\n     LightWeightHashSet\u003cLong\u003e openFileIds \u003d new LightWeightHashSet\u003c\u003e();\n     for (DatanodeDescriptor dataNode :\n         blockManager.getDatanodeManager().getDatanodes()) {\n       for (long ucFileId : dataNode.getLeavingServiceStatus().getOpenFiles()) {\n         INode ucFile \u003d getFSDirectory().getInode(ucFileId);\n         if (ucFile \u003d\u003d null || ucFileId \u003c\u003d prevId ||\n             openFileIds.contains(ucFileId)) {\n           // probably got deleted or\n           // part of previous batch or\n           // already part of the current batch\n           continue;\n         }\n         Preconditions.checkState(ucFile instanceof INodeFile);\n         openFileIds.add(ucFileId);\n         INodeFile inodeFile \u003d ucFile.asFile();\n-        openFileEntries.add(new OpenFileEntry(\n-            inodeFile.getId(), inodeFile.getFullPathName(),\n-            inodeFile.getFileUnderConstructionFeature().getClientName(),\n-            inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n+\n+        String fullPathName \u003d inodeFile.getFullPathName();\n+        if (org.apache.commons.lang.StringUtils.isEmpty(path)\n+            || fullPathName.startsWith(path)) {\n+          openFileEntries.add(new OpenFileEntry(inodeFile.getId(),\n+              inodeFile.getFullPathName(),\n+              inodeFile.getFileUnderConstructionFeature().getClientName(),\n+              inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n+        }\n+\n         if (openFileIds.size() \u003e\u003d this.maxListOpenFilesResponses) {\n           return new BatchedListEntries\u003c\u003e(openFileEntries, true);\n         }\n       }\n     }\n     return new BatchedListEntries\u003c\u003e(openFileEntries, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId,\n      String path) {\n    assert hasReadLock();\n    final List\u003cOpenFileEntry\u003e openFileEntries \u003d Lists.newArrayList();\n    LightWeightHashSet\u003cLong\u003e openFileIds \u003d new LightWeightHashSet\u003c\u003e();\n    for (DatanodeDescriptor dataNode :\n        blockManager.getDatanodeManager().getDatanodes()) {\n      for (long ucFileId : dataNode.getLeavingServiceStatus().getOpenFiles()) {\n        INode ucFile \u003d getFSDirectory().getInode(ucFileId);\n        if (ucFile \u003d\u003d null || ucFileId \u003c\u003d prevId ||\n            openFileIds.contains(ucFileId)) {\n          // probably got deleted or\n          // part of previous batch or\n          // already part of the current batch\n          continue;\n        }\n        Preconditions.checkState(ucFile instanceof INodeFile);\n        openFileIds.add(ucFileId);\n        INodeFile inodeFile \u003d ucFile.asFile();\n\n        String fullPathName \u003d inodeFile.getFullPathName();\n        if (org.apache.commons.lang.StringUtils.isEmpty(path)\n            || fullPathName.startsWith(path)) {\n          openFileEntries.add(new OpenFileEntry(inodeFile.getId(),\n              inodeFile.getFullPathName(),\n              inodeFile.getFileUnderConstructionFeature().getClientName(),\n              inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n        }\n\n        if (openFileIds.size() \u003e\u003d this.maxListOpenFilesResponses) {\n          return new BatchedListEntries\u003c\u003e(openFileEntries, true);\n        }\n      }\n    }\n    return new BatchedListEntries\u003c\u003e(openFileEntries, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[prevId-long]",
            "newValue": "[prevId-long, path-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-11848. Enhance dfsadmin listOpenFiles command to list files under a given path. Contributed by Yiqun Lin.\n",
          "commitDate": "05/01/18 10:31 PM",
          "commitName": "bf5c94899537011465350d5d999fad9ffaeb605d",
          "commitAuthor": "Yiqun Lin",
          "commitDateOld": "02/01/18 2:59 PM",
          "commitNameOld": "42a1c98597e6dba2e371510a6b2b6b1fb94e4090",
          "commitAuthorOld": "Manoj Govindassamy",
          "daysBetweenCommits": 3.31,
          "commitsBetweenForRepo": 26,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,36 @@\n-  public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId) {\n+  public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId,\n+      String path) {\n     assert hasReadLock();\n     final List\u003cOpenFileEntry\u003e openFileEntries \u003d Lists.newArrayList();\n     LightWeightHashSet\u003cLong\u003e openFileIds \u003d new LightWeightHashSet\u003c\u003e();\n     for (DatanodeDescriptor dataNode :\n         blockManager.getDatanodeManager().getDatanodes()) {\n       for (long ucFileId : dataNode.getLeavingServiceStatus().getOpenFiles()) {\n         INode ucFile \u003d getFSDirectory().getInode(ucFileId);\n         if (ucFile \u003d\u003d null || ucFileId \u003c\u003d prevId ||\n             openFileIds.contains(ucFileId)) {\n           // probably got deleted or\n           // part of previous batch or\n           // already part of the current batch\n           continue;\n         }\n         Preconditions.checkState(ucFile instanceof INodeFile);\n         openFileIds.add(ucFileId);\n         INodeFile inodeFile \u003d ucFile.asFile();\n-        openFileEntries.add(new OpenFileEntry(\n-            inodeFile.getId(), inodeFile.getFullPathName(),\n-            inodeFile.getFileUnderConstructionFeature().getClientName(),\n-            inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n+\n+        String fullPathName \u003d inodeFile.getFullPathName();\n+        if (org.apache.commons.lang.StringUtils.isEmpty(path)\n+            || fullPathName.startsWith(path)) {\n+          openFileEntries.add(new OpenFileEntry(inodeFile.getId(),\n+              inodeFile.getFullPathName(),\n+              inodeFile.getFileUnderConstructionFeature().getClientName(),\n+              inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n+        }\n+\n         if (openFileIds.size() \u003e\u003d this.maxListOpenFilesResponses) {\n           return new BatchedListEntries\u003c\u003e(openFileEntries, true);\n         }\n       }\n     }\n     return new BatchedListEntries\u003c\u003e(openFileEntries, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId,\n      String path) {\n    assert hasReadLock();\n    final List\u003cOpenFileEntry\u003e openFileEntries \u003d Lists.newArrayList();\n    LightWeightHashSet\u003cLong\u003e openFileIds \u003d new LightWeightHashSet\u003c\u003e();\n    for (DatanodeDescriptor dataNode :\n        blockManager.getDatanodeManager().getDatanodes()) {\n      for (long ucFileId : dataNode.getLeavingServiceStatus().getOpenFiles()) {\n        INode ucFile \u003d getFSDirectory().getInode(ucFileId);\n        if (ucFile \u003d\u003d null || ucFileId \u003c\u003d prevId ||\n            openFileIds.contains(ucFileId)) {\n          // probably got deleted or\n          // part of previous batch or\n          // already part of the current batch\n          continue;\n        }\n        Preconditions.checkState(ucFile instanceof INodeFile);\n        openFileIds.add(ucFileId);\n        INodeFile inodeFile \u003d ucFile.asFile();\n\n        String fullPathName \u003d inodeFile.getFullPathName();\n        if (org.apache.commons.lang.StringUtils.isEmpty(path)\n            || fullPathName.startsWith(path)) {\n          openFileEntries.add(new OpenFileEntry(inodeFile.getId(),\n              inodeFile.getFullPathName(),\n              inodeFile.getFileUnderConstructionFeature().getClientName(),\n              inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n        }\n\n        if (openFileIds.size() \u003e\u003d this.maxListOpenFilesResponses) {\n          return new BatchedListEntries\u003c\u003e(openFileEntries, true);\n        }\n      }\n    }\n    return new BatchedListEntries\u003c\u003e(openFileEntries, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "42a1c98597e6dba2e371510a6b2b6b1fb94e4090": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11847. Enhance dfsadmin listOpenFiles command to list files blocking datanode decommissioning.\n",
      "commitDate": "02/01/18 2:59 PM",
      "commitName": "42a1c98597e6dba2e371510a6b2b6b1fb94e4090",
      "commitAuthor": "Manoj Govindassamy",
      "diff": "@@ -0,0 +1,29 @@\n+  public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId) {\n+    assert hasReadLock();\n+    final List\u003cOpenFileEntry\u003e openFileEntries \u003d Lists.newArrayList();\n+    LightWeightHashSet\u003cLong\u003e openFileIds \u003d new LightWeightHashSet\u003c\u003e();\n+    for (DatanodeDescriptor dataNode :\n+        blockManager.getDatanodeManager().getDatanodes()) {\n+      for (long ucFileId : dataNode.getLeavingServiceStatus().getOpenFiles()) {\n+        INode ucFile \u003d getFSDirectory().getInode(ucFileId);\n+        if (ucFile \u003d\u003d null || ucFileId \u003c\u003d prevId ||\n+            openFileIds.contains(ucFileId)) {\n+          // probably got deleted or\n+          // part of previous batch or\n+          // already part of the current batch\n+          continue;\n+        }\n+        Preconditions.checkState(ucFile instanceof INodeFile);\n+        openFileIds.add(ucFileId);\n+        INodeFile inodeFile \u003d ucFile.asFile();\n+        openFileEntries.add(new OpenFileEntry(\n+            inodeFile.getId(), inodeFile.getFullPathName(),\n+            inodeFile.getFileUnderConstructionFeature().getClientName(),\n+            inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n+        if (openFileIds.size() \u003e\u003d this.maxListOpenFilesResponses) {\n+          return new BatchedListEntries\u003c\u003e(openFileEntries, true);\n+        }\n+      }\n+    }\n+    return new BatchedListEntries\u003c\u003e(openFileEntries, false);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public BatchedListEntries\u003cOpenFileEntry\u003e getFilesBlockingDecom(long prevId) {\n    assert hasReadLock();\n    final List\u003cOpenFileEntry\u003e openFileEntries \u003d Lists.newArrayList();\n    LightWeightHashSet\u003cLong\u003e openFileIds \u003d new LightWeightHashSet\u003c\u003e();\n    for (DatanodeDescriptor dataNode :\n        blockManager.getDatanodeManager().getDatanodes()) {\n      for (long ucFileId : dataNode.getLeavingServiceStatus().getOpenFiles()) {\n        INode ucFile \u003d getFSDirectory().getInode(ucFileId);\n        if (ucFile \u003d\u003d null || ucFileId \u003c\u003d prevId ||\n            openFileIds.contains(ucFileId)) {\n          // probably got deleted or\n          // part of previous batch or\n          // already part of the current batch\n          continue;\n        }\n        Preconditions.checkState(ucFile instanceof INodeFile);\n        openFileIds.add(ucFileId);\n        INodeFile inodeFile \u003d ucFile.asFile();\n        openFileEntries.add(new OpenFileEntry(\n            inodeFile.getId(), inodeFile.getFullPathName(),\n            inodeFile.getFileUnderConstructionFeature().getClientName(),\n            inodeFile.getFileUnderConstructionFeature().getClientMachine()));\n        if (openFileIds.size() \u003e\u003d this.maxListOpenFilesResponses) {\n          return new BatchedListEntries\u003c\u003e(openFileEntries, true);\n        }\n      }\n    }\n    return new BatchedListEntries\u003c\u003e(openFileEntries, false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}