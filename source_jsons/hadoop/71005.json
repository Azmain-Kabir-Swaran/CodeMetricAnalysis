{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ImageWriter.java",
  "functionName": "accept",
  "functionId": "accept___e-TreePath",
  "sourceFilePath": "hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageWriter.java",
  "functionStartLine": 207,
  "functionEndLine": 223,
  "numCommitsSeen": 13,
  "timeTaken": 781,
  "changeHistory": [
    "4531588a94dcd2b4141b12828cb60ca3b953a58c",
    "8da3a6e314609f9124bd9979cd09cddbc2a10d36"
  ],
  "changeHistoryShort": {
    "4531588a94dcd2b4141b12828cb60ca3b953a58c": "Ybodychange",
    "8da3a6e314609f9124bd9979cd09cddbc2a10d36": "Yintroduced"
  },
  "changeHistoryDetails": {
    "4531588a94dcd2b4141b12828cb60ca3b953a58c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11640. [READ] Datanodes should use a unique identifier when reading from external stores\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "4531588a94dcd2b4141b12828cb60ca3b953a58c",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "9c35be86e17021202823bfd3c2067ff3b312ce5c",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,17 @@\n   public void accept(TreePath e) throws IOException {\n     assert e.getParentId() \u003c curInode.get();\n     // allocate ID\n     long id \u003d curInode.getAndIncrement();\n     e.accept(id);\n     assert e.getId() \u003c curInode.get();\n-    INode n \u003d e.toINode(ugis, blockIds, blocks, blockPoolID);\n+    INode n \u003d e.toINode(ugis, blockIds, blocks);\n     writeInode(n);\n \n     if (e.getParentId() \u003e 0) {\n       // add DirEntry to map, which may page out entries\n       DirEntry.Builder de \u003d DirEntry.newBuilder()\n           .setParent(e.getParentId())\n           .addChildren(e.getId());\n       dircache.put(e.getParentId(), de);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void accept(TreePath e) throws IOException {\n    assert e.getParentId() \u003c curInode.get();\n    // allocate ID\n    long id \u003d curInode.getAndIncrement();\n    e.accept(id);\n    assert e.getId() \u003c curInode.get();\n    INode n \u003d e.toINode(ugis, blockIds, blocks);\n    writeInode(n);\n\n    if (e.getParentId() \u003e 0) {\n      // add DirEntry to map, which may page out entries\n      DirEntry.Builder de \u003d DirEntry.newBuilder()\n          .setParent(e.getParentId())\n          .addChildren(e.getId());\n      dircache.put(e.getParentId(), de);\n    }\n  }",
      "path": "hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageWriter.java",
      "extendedDetails": {}
    },
    "8da3a6e314609f9124bd9979cd09cddbc2a10d36": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-10706. [READ] Add tool generating FSImage from external store\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "8da3a6e314609f9124bd9979cd09cddbc2a10d36",
      "commitAuthor": "Virajith Jalaparti",
      "diff": "@@ -0,0 +1,17 @@\n+  public void accept(TreePath e) throws IOException {\n+    assert e.getParentId() \u003c curInode.get();\n+    // allocate ID\n+    long id \u003d curInode.getAndIncrement();\n+    e.accept(id);\n+    assert e.getId() \u003c curInode.get();\n+    INode n \u003d e.toINode(ugis, blockIds, blocks, blockPoolID);\n+    writeInode(n);\n+\n+    if (e.getParentId() \u003e 0) {\n+      // add DirEntry to map, which may page out entries\n+      DirEntry.Builder de \u003d DirEntry.newBuilder()\n+          .setParent(e.getParentId())\n+          .addChildren(e.getId());\n+      dircache.put(e.getParentId(), de);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void accept(TreePath e) throws IOException {\n    assert e.getParentId() \u003c curInode.get();\n    // allocate ID\n    long id \u003d curInode.getAndIncrement();\n    e.accept(id);\n    assert e.getId() \u003c curInode.get();\n    INode n \u003d e.toINode(ugis, blockIds, blocks, blockPoolID);\n    writeInode(n);\n\n    if (e.getParentId() \u003e 0) {\n      // add DirEntry to map, which may page out entries\n      DirEntry.Builder de \u003d DirEntry.newBuilder()\n          .setParent(e.getParentId())\n          .addChildren(e.getId());\n      dircache.put(e.getParentId(), de);\n    }\n  }",
      "path": "hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageWriter.java"
    }
  }
}