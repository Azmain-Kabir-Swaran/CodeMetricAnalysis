{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CapacityScheduler.java",
  "functionName": "allocateContainersToNode",
  "functionId": "allocateContainersToNode___candidates-CandidateNodeSet__FiCaSchedulerNode____withNodeHeartbeat-boolean",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
  "functionStartLine": 1753,
  "functionEndLine": 1791,
  "numCommitsSeen": 543,
  "timeTaken": 14770,
  "changeHistory": [
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8",
    "fc05b0e70e9bb556d6bdc00fa8735e18a6f90bc9",
    "9c3fc3ef2865164aa5f121793ac914cfeb21a181",
    "f24c842d52e166e8566337ef93c96438f1c870d8",
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
    "de3b4aac561258ad242a3c5ed1c919428893fd4c",
    "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f",
    "e0d131f055ee126052ad4d0f7b0d192e6c730188",
    "d62e121ffc0239e7feccc1e23ece92c5fac685f6",
    "ae14e5d07f1b6702a5160637438028bb03d9387e",
    "fa7a43529d529f0006c8033c2003f15b9b93f103",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
    "20d389ce61eaacb5ddfb329015f50e96ad894f8d",
    "f9692770a58af0ab082eb7f15da9cbdcd177605b",
    "7f46636495e23693d588b0915f464fa7afd9102e",
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
    "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
    "487374b7fe0c92fc7eb1406c568952722b5d5b15",
    "14dd647c556016d351f425ee956ccf800ccb9ce2",
    "054f28552687e9b9859c0126e16a2066e20ead3f",
    "9c22065109a77681bc2534063eabe8692fbcb3cd",
    "485c96e3cb9b0b05d6e490b4773506da83ebc61d",
    "82f3454f5ac1f1c457e668e2cee12b4dcc800ee1",
    "57cdf8626a32b8595a645b7551f46ab950db4789"
  ],
  "changeHistoryShort": {
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8": "Ybodychange",
    "fc05b0e70e9bb556d6bdc00fa8735e18a6f90bc9": "Ybodychange",
    "9c3fc3ef2865164aa5f121793ac914cfeb21a181": "Ybodychange",
    "f24c842d52e166e8566337ef93c96438f1c870d8": "Ybodychange",
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949": "Ymultichange(Yparameterchange,Ybodychange)",
    "de3b4aac561258ad242a3c5ed1c919428893fd4c": "Ymultichange(Yparameterchange,Yreturntypechange,Ymodifierchange,Ybodychange)",
    "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f": "Ymultichange(Ymodifierchange,Ybodychange)",
    "e0d131f055ee126052ad4d0f7b0d192e6c730188": "Ybodychange",
    "d62e121ffc0239e7feccc1e23ece92c5fac685f6": "Ymultichange(Ymodifierchange,Ybodychange)",
    "ae14e5d07f1b6702a5160637438028bb03d9387e": "Ybodychange",
    "fa7a43529d529f0006c8033c2003f15b9b93f103": "Ybodychange",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": "Ybodychange",
    "20d389ce61eaacb5ddfb329015f50e96ad894f8d": "Ybodychange",
    "f9692770a58af0ab082eb7f15da9cbdcd177605b": "Ybodychange",
    "7f46636495e23693d588b0915f464fa7afd9102e": "Ymodifierchange",
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b": "Ybodychange",
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5": "Ybodychange",
    "afa5d4715a3aea2a6e93380b014c7bb8f0880383": "Ybodychange",
    "487374b7fe0c92fc7eb1406c568952722b5d5b15": "Ybodychange",
    "14dd647c556016d351f425ee956ccf800ccb9ce2": "Ybodychange",
    "054f28552687e9b9859c0126e16a2066e20ead3f": "Ybodychange",
    "9c22065109a77681bc2534063eabe8692fbcb3cd": "Ybodychange",
    "485c96e3cb9b0b05d6e490b4773506da83ebc61d": "Ybodychange",
    "82f3454f5ac1f1c457e668e2cee12b4dcc800ee1": "Ybodychange",
    "57cdf8626a32b8595a645b7551f46ab950db4789": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9664. Improve response of scheduler/app activities for better understanding. Contributed by Tao Yang.\n",
      "commitDate": "29/08/19 3:14 AM",
      "commitName": "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "27/08/19 3:53 PM",
      "commitNameOld": "8ef46595da6aefe4458aa7181670c3d9b13e7ec6",
      "commitAuthorOld": "Jonathan Hung",
      "daysBetweenCommits": 1.47,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,39 @@\n   CSAssignment allocateContainersToNode(\n       CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       boolean withNodeHeartbeat) {\n     if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n         .isSchedulerReadyForAllocatingContainers()) {\n       return null;\n     }\n \n     long startTime \u003d System.nanoTime();\n \n     // Backward compatible way to make sure previous behavior which allocation\n     // driven by node heartbeat works.\n     FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     // We have two different logics to handle allocation on single node / multi\n     // nodes.\n     CSAssignment assignment;\n     if (!multiNodePlacementEnabled) {\n       ActivitiesLogger.NODE.startNodeUpdateRecording(activitiesManager,\n           node.getNodeID());\n       assignment \u003d allocateContainerOnSingleNode(candidates,\n           node, withNodeHeartbeat);\n       ActivitiesLogger.NODE.finishNodeUpdateRecording(activitiesManager,\n-          node.getNodeID());\n+          node.getNodeID(), candidates.getPartition());\n     } else{\n       ActivitiesLogger.NODE.startNodeUpdateRecording(activitiesManager,\n           ActivitiesManager.EMPTY_NODE_ID);\n       assignment \u003d allocateContainersOnMultiNodes(candidates);\n       ActivitiesLogger.NODE.finishNodeUpdateRecording(activitiesManager,\n-          ActivitiesManager.EMPTY_NODE_ID);\n+          ActivitiesManager.EMPTY_NODE_ID, candidates.getPartition());\n     }\n \n     if (assignment !\u003d null \u0026\u0026 assignment.getAssignmentInformation() !\u003d null\n         \u0026\u0026 assignment.getAssignmentInformation().getNumAllocations() \u003e 0) {\n       long allocateTime \u003d System.nanoTime() - startTime;\n       CapacitySchedulerMetrics.getMetrics().addAllocate(allocateTime);\n     }\n     return assignment;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  CSAssignment allocateContainersToNode(\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      boolean withNodeHeartbeat) {\n    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n        .isSchedulerReadyForAllocatingContainers()) {\n      return null;\n    }\n\n    long startTime \u003d System.nanoTime();\n\n    // Backward compatible way to make sure previous behavior which allocation\n    // driven by node heartbeat works.\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    // We have two different logics to handle allocation on single node / multi\n    // nodes.\n    CSAssignment assignment;\n    if (!multiNodePlacementEnabled) {\n      ActivitiesLogger.NODE.startNodeUpdateRecording(activitiesManager,\n          node.getNodeID());\n      assignment \u003d allocateContainerOnSingleNode(candidates,\n          node, withNodeHeartbeat);\n      ActivitiesLogger.NODE.finishNodeUpdateRecording(activitiesManager,\n          node.getNodeID(), candidates.getPartition());\n    } else{\n      ActivitiesLogger.NODE.startNodeUpdateRecording(activitiesManager,\n          ActivitiesManager.EMPTY_NODE_ID);\n      assignment \u003d allocateContainersOnMultiNodes(candidates);\n      ActivitiesLogger.NODE.finishNodeUpdateRecording(activitiesManager,\n          ActivitiesManager.EMPTY_NODE_ID, candidates.getPartition());\n    }\n\n    if (assignment !\u003d null \u0026\u0026 assignment.getAssignmentInformation() !\u003d null\n        \u0026\u0026 assignment.getAssignmentInformation().getNumAllocations() \u003e 0) {\n      long allocateTime \u003d System.nanoTime() - startTime;\n      CapacitySchedulerMetrics.getMetrics().addAllocate(allocateTime);\n    }\n    return assignment;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "fc05b0e70e9bb556d6bdc00fa8735e18a6f90bc9": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9313. Support asynchronized scheduling mode and multi-node lookup mechanism for scheduler activities. Contributed by Tao Yang.\n",
      "commitDate": "07/04/19 10:40 PM",
      "commitName": "fc05b0e70e9bb556d6bdc00fa8735e18a6f90bc9",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "18/03/19 10:57 AM",
      "commitNameOld": "5f6e22516668ff94a76737ad5e2cdcb2ff9f6dfd",
      "commitAuthorOld": "Eric Yang",
      "daysBetweenCommits": 20.49,
      "commitsBetweenForRepo": 151,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,39 @@\n   CSAssignment allocateContainersToNode(\n       CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       boolean withNodeHeartbeat) {\n     if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n         .isSchedulerReadyForAllocatingContainers()) {\n       return null;\n     }\n \n     long startTime \u003d System.nanoTime();\n \n     // Backward compatible way to make sure previous behavior which allocation\n     // driven by node heartbeat works.\n     FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     // We have two different logics to handle allocation on single node / multi\n     // nodes.\n     CSAssignment assignment;\n     if (!multiNodePlacementEnabled) {\n+      ActivitiesLogger.NODE.startNodeUpdateRecording(activitiesManager,\n+          node.getNodeID());\n       assignment \u003d allocateContainerOnSingleNode(candidates,\n           node, withNodeHeartbeat);\n+      ActivitiesLogger.NODE.finishNodeUpdateRecording(activitiesManager,\n+          node.getNodeID());\n     } else{\n+      ActivitiesLogger.NODE.startNodeUpdateRecording(activitiesManager,\n+          ActivitiesManager.EMPTY_NODE_ID);\n       assignment \u003d allocateContainersOnMultiNodes(candidates);\n+      ActivitiesLogger.NODE.finishNodeUpdateRecording(activitiesManager,\n+          ActivitiesManager.EMPTY_NODE_ID);\n     }\n \n     if (assignment !\u003d null \u0026\u0026 assignment.getAssignmentInformation() !\u003d null\n         \u0026\u0026 assignment.getAssignmentInformation().getNumAllocations() \u003e 0) {\n       long allocateTime \u003d System.nanoTime() - startTime;\n       CapacitySchedulerMetrics.getMetrics().addAllocate(allocateTime);\n     }\n     return assignment;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  CSAssignment allocateContainersToNode(\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      boolean withNodeHeartbeat) {\n    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n        .isSchedulerReadyForAllocatingContainers()) {\n      return null;\n    }\n\n    long startTime \u003d System.nanoTime();\n\n    // Backward compatible way to make sure previous behavior which allocation\n    // driven by node heartbeat works.\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    // We have two different logics to handle allocation on single node / multi\n    // nodes.\n    CSAssignment assignment;\n    if (!multiNodePlacementEnabled) {\n      ActivitiesLogger.NODE.startNodeUpdateRecording(activitiesManager,\n          node.getNodeID());\n      assignment \u003d allocateContainerOnSingleNode(candidates,\n          node, withNodeHeartbeat);\n      ActivitiesLogger.NODE.finishNodeUpdateRecording(activitiesManager,\n          node.getNodeID());\n    } else{\n      ActivitiesLogger.NODE.startNodeUpdateRecording(activitiesManager,\n          ActivitiesManager.EMPTY_NODE_ID);\n      assignment \u003d allocateContainersOnMultiNodes(candidates);\n      ActivitiesLogger.NODE.finishNodeUpdateRecording(activitiesManager,\n          ActivitiesManager.EMPTY_NODE_ID);\n    }\n\n    if (assignment !\u003d null \u0026\u0026 assignment.getAssignmentInformation() !\u003d null\n        \u0026\u0026 assignment.getAssignmentInformation().getNumAllocations() \u003e 0) {\n      long allocateTime \u003d System.nanoTime() - startTime;\n      CapacitySchedulerMetrics.getMetrics().addAllocate(allocateTime);\n    }\n    return assignment;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "9c3fc3ef2865164aa5f121793ac914cfeb21a181": {
      "type": "Ybodychange",
      "commitMessage": "YARN-7494. Add muti-node lookup mechanism and pluggable nodes sorting policies to optimize placement decision. Contributed by Sunil Govindan.\n",
      "commitDate": "21/08/18 7:42 AM",
      "commitName": "9c3fc3ef2865164aa5f121793ac914cfeb21a181",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "31/07/18 8:03 PM",
      "commitNameOld": "6310c0d17d6422a595f856a55b4f1fb82be43739",
      "commitAuthorOld": "Sunil G",
      "daysBetweenCommits": 20.49,
      "commitsBetweenForRepo": 195,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   CSAssignment allocateContainersToNode(\n       CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       boolean withNodeHeartbeat) {\n     if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n         .isSchedulerReadyForAllocatingContainers()) {\n       return null;\n     }\n \n     long startTime \u003d System.nanoTime();\n \n     // Backward compatible way to make sure previous behavior which allocation\n     // driven by node heartbeat works.\n     FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     // We have two different logics to handle allocation on single node / multi\n     // nodes.\n     CSAssignment assignment;\n-    if (null !\u003d node) {\n+    if (!multiNodePlacementEnabled) {\n       assignment \u003d allocateContainerOnSingleNode(candidates,\n           node, withNodeHeartbeat);\n     } else{\n       assignment \u003d allocateContainersOnMultiNodes(candidates);\n     }\n \n     if (assignment !\u003d null \u0026\u0026 assignment.getAssignmentInformation() !\u003d null\n         \u0026\u0026 assignment.getAssignmentInformation().getNumAllocations() \u003e 0) {\n       long allocateTime \u003d System.nanoTime() - startTime;\n       CapacitySchedulerMetrics.getMetrics().addAllocate(allocateTime);\n     }\n     return assignment;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  CSAssignment allocateContainersToNode(\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      boolean withNodeHeartbeat) {\n    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n        .isSchedulerReadyForAllocatingContainers()) {\n      return null;\n    }\n\n    long startTime \u003d System.nanoTime();\n\n    // Backward compatible way to make sure previous behavior which allocation\n    // driven by node heartbeat works.\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    // We have two different logics to handle allocation on single node / multi\n    // nodes.\n    CSAssignment assignment;\n    if (!multiNodePlacementEnabled) {\n      assignment \u003d allocateContainerOnSingleNode(candidates,\n          node, withNodeHeartbeat);\n    } else{\n      assignment \u003d allocateContainersOnMultiNodes(candidates);\n    }\n\n    if (assignment !\u003d null \u0026\u0026 assignment.getAssignmentInformation() !\u003d null\n        \u0026\u0026 assignment.getAssignmentInformation().getNumAllocations() \u003e 0) {\n      long allocateTime \u003d System.nanoTime() - startTime;\n      CapacitySchedulerMetrics.getMetrics().addAllocate(allocateTime);\n    }\n    return assignment;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "f24c842d52e166e8566337ef93c96438f1c870d8": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8213. Add Capacity Scheduler performance metrics. (Weiwei Yang via wangda)\n\nChange-Id: Ieea6f3eeb83c90cd74233fea896f0fcd0f325d5f\n",
      "commitDate": "25/05/18 9:53 PM",
      "commitName": "f24c842d52e166e8566337ef93c96438f1c870d8",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "10/05/18 7:47 PM",
      "commitNameOld": "9db9cd95bd0348070a286e69e7965c03c9bd39d6",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 15.09,
      "commitsBetweenForRepo": 138,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,31 @@\n   CSAssignment allocateContainersToNode(\n       CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       boolean withNodeHeartbeat) {\n     if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n         .isSchedulerReadyForAllocatingContainers()) {\n       return null;\n     }\n \n+    long startTime \u003d System.nanoTime();\n+\n     // Backward compatible way to make sure previous behavior which allocation\n     // driven by node heartbeat works.\n     FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     // We have two different logics to handle allocation on single node / multi\n     // nodes.\n+    CSAssignment assignment;\n     if (null !\u003d node) {\n-      return allocateContainerOnSingleNode(candidates, node, withNodeHeartbeat);\n+      assignment \u003d allocateContainerOnSingleNode(candidates,\n+          node, withNodeHeartbeat);\n     } else{\n-      return allocateContainersOnMultiNodes(candidates);\n+      assignment \u003d allocateContainersOnMultiNodes(candidates);\n     }\n+\n+    if (assignment !\u003d null \u0026\u0026 assignment.getAssignmentInformation() !\u003d null\n+        \u0026\u0026 assignment.getAssignmentInformation().getNumAllocations() \u003e 0) {\n+      long allocateTime \u003d System.nanoTime() - startTime;\n+      CapacitySchedulerMetrics.getMetrics().addAllocate(allocateTime);\n+    }\n+    return assignment;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  CSAssignment allocateContainersToNode(\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      boolean withNodeHeartbeat) {\n    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n        .isSchedulerReadyForAllocatingContainers()) {\n      return null;\n    }\n\n    long startTime \u003d System.nanoTime();\n\n    // Backward compatible way to make sure previous behavior which allocation\n    // driven by node heartbeat works.\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    // We have two different logics to handle allocation on single node / multi\n    // nodes.\n    CSAssignment assignment;\n    if (null !\u003d node) {\n      assignment \u003d allocateContainerOnSingleNode(candidates,\n          node, withNodeHeartbeat);\n    } else{\n      assignment \u003d allocateContainersOnMultiNodes(candidates);\n    }\n\n    if (assignment !\u003d null \u0026\u0026 assignment.getAssignmentInformation() !\u003d null\n        \u0026\u0026 assignment.getAssignmentInformation().getNumAllocations() \u003e 0) {\n      long allocateTime \u003d System.nanoTime() - startTime;\n      CapacitySchedulerMetrics.getMetrics().addAllocate(allocateTime);\n    }\n    return assignment;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-7437. Rename PlacementSet and SchedulingPlacementSet. (Wangda Tan via kkaranasos)\n",
      "commitDate": "09/11/17 1:01 PM",
      "commitName": "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
      "commitAuthor": "Konstantinos Karanasos",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-7437. Rename PlacementSet and SchedulingPlacementSet. (Wangda Tan via kkaranasos)\n",
          "commitDate": "09/11/17 1:01 PM",
          "commitName": "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
          "commitAuthor": "Konstantinos Karanasos",
          "commitDateOld": "09/11/17 10:49 AM",
          "commitNameOld": "a1382a18dff8a70aa25240d6fbba6e22832a7679",
          "commitAuthorOld": "Haibo Chen",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,20 @@\n-  CSAssignment allocateContainersToNode(PlacementSet\u003cFiCaSchedulerNode\u003e ps,\n+  CSAssignment allocateContainersToNode(\n+      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       boolean withNodeHeartbeat) {\n     if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n         .isSchedulerReadyForAllocatingContainers()) {\n       return null;\n     }\n \n     // Backward compatible way to make sure previous behavior which allocation\n     // driven by node heartbeat works.\n-    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n+    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     // We have two different logics to handle allocation on single node / multi\n     // nodes.\n     if (null !\u003d node) {\n-      return allocateContainerOnSingleNode(ps, node, withNodeHeartbeat);\n-    } else {\n-      return allocateContainersOnMultiNodes(ps);\n+      return allocateContainerOnSingleNode(candidates, node, withNodeHeartbeat);\n+    } else{\n+      return allocateContainersOnMultiNodes(candidates);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  CSAssignment allocateContainersToNode(\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      boolean withNodeHeartbeat) {\n    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n        .isSchedulerReadyForAllocatingContainers()) {\n      return null;\n    }\n\n    // Backward compatible way to make sure previous behavior which allocation\n    // driven by node heartbeat works.\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    // We have two different logics to handle allocation on single node / multi\n    // nodes.\n    if (null !\u003d node) {\n      return allocateContainerOnSingleNode(candidates, node, withNodeHeartbeat);\n    } else{\n      return allocateContainersOnMultiNodes(candidates);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
          "extendedDetails": {
            "oldValue": "[ps-PlacementSet\u003cFiCaSchedulerNode\u003e, withNodeHeartbeat-boolean]",
            "newValue": "[candidates-CandidateNodeSet\u003cFiCaSchedulerNode\u003e, withNodeHeartbeat-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-7437. Rename PlacementSet and SchedulingPlacementSet. (Wangda Tan via kkaranasos)\n",
          "commitDate": "09/11/17 1:01 PM",
          "commitName": "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
          "commitAuthor": "Konstantinos Karanasos",
          "commitDateOld": "09/11/17 10:49 AM",
          "commitNameOld": "a1382a18dff8a70aa25240d6fbba6e22832a7679",
          "commitAuthorOld": "Haibo Chen",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,20 @@\n-  CSAssignment allocateContainersToNode(PlacementSet\u003cFiCaSchedulerNode\u003e ps,\n+  CSAssignment allocateContainersToNode(\n+      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n       boolean withNodeHeartbeat) {\n     if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n         .isSchedulerReadyForAllocatingContainers()) {\n       return null;\n     }\n \n     // Backward compatible way to make sure previous behavior which allocation\n     // driven by node heartbeat works.\n-    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n+    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n \n     // We have two different logics to handle allocation on single node / multi\n     // nodes.\n     if (null !\u003d node) {\n-      return allocateContainerOnSingleNode(ps, node, withNodeHeartbeat);\n-    } else {\n-      return allocateContainersOnMultiNodes(ps);\n+      return allocateContainerOnSingleNode(candidates, node, withNodeHeartbeat);\n+    } else{\n+      return allocateContainersOnMultiNodes(candidates);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  CSAssignment allocateContainersToNode(\n      CandidateNodeSet\u003cFiCaSchedulerNode\u003e candidates,\n      boolean withNodeHeartbeat) {\n    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n        .isSchedulerReadyForAllocatingContainers()) {\n      return null;\n    }\n\n    // Backward compatible way to make sure previous behavior which allocation\n    // driven by node heartbeat works.\n    FiCaSchedulerNode node \u003d CandidateNodeSetUtils.getSingleNode(candidates);\n\n    // We have two different logics to handle allocation on single node / multi\n    // nodes.\n    if (null !\u003d node) {\n      return allocateContainerOnSingleNode(candidates, node, withNodeHeartbeat);\n    } else{\n      return allocateContainersOnMultiNodes(candidates);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
          "extendedDetails": {}
        }
      ]
    },
    "de3b4aac561258ad242a3c5ed1c919428893fd4c": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ymodifierchange,Ybodychange)",
      "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
      "commitDate": "07/11/16 10:14 AM",
      "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
          "commitDate": "07/11/16 10:14 AM",
          "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
          "commitAuthor": "Jian He",
          "commitDateOld": "01/11/16 3:02 AM",
          "commitNameOld": "7d2d8d25ba0cb10a3c6192d4123f27ede5ef2ba6",
          "commitAuthorOld": "Varun Saxena",
          "daysBetweenCommits": 6.34,
          "commitsBetweenForRepo": 78,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,130 +1,19 @@\n-  public void allocateContainersToNode(FiCaSchedulerNode node) {\n-    try {\n-      writeLock.lock();\n-      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n-          .isSchedulerReadyForAllocatingContainers()) {\n-        return;\n-      }\n+  CSAssignment allocateContainersToNode(PlacementSet\u003cFiCaSchedulerNode\u003e ps,\n+      boolean withNodeHeartbeat) {\n+    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n+        .isSchedulerReadyForAllocatingContainers()) {\n+      return null;\n+    }\n \n-      if (!nodeTracker.exists(node.getNodeID())) {\n-        LOG.info(\"Skipping scheduling as the node \" + node.getNodeID()\n-            + \" has been removed\");\n-        return;\n-      }\n+    // Backward compatible way to make sure previous behavior which allocation\n+    // driven by node heartbeat works.\n+    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n \n-      // reset allocation and reservation stats before we start doing any work\n-      updateSchedulerHealth(lastNodeUpdateTime, node,\n-          new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n-\n-      CSAssignment assignment;\n-\n-      // Assign new containers...\n-      // 1. Check for reserved applications\n-      // 2. Schedule if there are no reservations\n-\n-      RMContainer reservedContainer \u003d node.getReservedContainer();\n-      if (reservedContainer !\u003d null) {\n-\n-        FiCaSchedulerApp reservedApplication \u003d getCurrentAttemptForContainer(\n-            reservedContainer.getContainerId());\n-\n-        // Try to fulfill the reservation\n-        LOG.info(\"Trying to fulfill reservation for application \"\n-            + reservedApplication.getApplicationId() + \" on node: \" + node\n-            .getNodeID());\n-\n-        LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n-        assignment \u003d queue.assignContainers(getClusterResource(), node,\n-            // TODO, now we only consider limits for parent for non-labeled\n-            // resources, should consider labeled resources as well.\n-            new ResourceLimits(labelManager\n-                .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n-                    getClusterResource())),\n-            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-        if (assignment.isFulfilledReservation()) {\n-          CSAssignment tmp \u003d new CSAssignment(\n-              reservedContainer.getReservedResource(), assignment.getType());\n-          Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-              reservedContainer.getReservedResource());\n-          tmp.getAssignmentInformation().addAllocationDetails(\n-              reservedContainer.getContainerId(), queue.getQueuePath());\n-          tmp.getAssignmentInformation().incrAllocations();\n-          updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n-          schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n-\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              queue.getParent().getQueueName(), queue.getQueueName(),\n-              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-              node, reservedContainer.getContainerId(),\n-              AllocationState.ALLOCATED_FROM_RESERVED);\n-        } else{\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              queue.getParent().getQueueName(), queue.getQueueName(),\n-              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-              node, reservedContainer.getContainerId(),\n-              AllocationState.SKIPPED);\n-        }\n-      }\n-\n-      // Try to schedule more if there are no reservations to fulfill\n-      if (node.getReservedContainer() \u003d\u003d null) {\n-        if (calculator.computeAvailableContainers(Resources\n-            .add(node.getUnallocatedResource(),\n-                node.getTotalKillableResources()), minimumAllocation) \u003e 0) {\n-\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Trying to schedule on node: \" + node.getNodeName()\n-                + \", available: \" + node.getUnallocatedResource());\n-          }\n-\n-          assignment \u003d root.assignContainers(getClusterResource(), node,\n-              new ResourceLimits(labelManager\n-                  .getResourceByLabel(node.getPartition(),\n-                      getClusterResource())),\n-              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-          if (Resources.greaterThan(calculator, getClusterResource(),\n-              assignment.getResource(), Resources.none())) {\n-            updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n-            return;\n-          }\n-\n-          // Only do non-exclusive allocation when node has node-labels.\n-          if (StringUtils.equals(node.getPartition(),\n-              RMNodeLabelsManager.NO_LABEL)) {\n-            return;\n-          }\n-\n-          // Only do non-exclusive allocation when the node-label supports that\n-          try {\n-            if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n-                node.getPartition())) {\n-              return;\n-            }\n-          } catch (IOException e) {\n-            LOG.warn(\n-                \"Exception when trying to get exclusivity of node label\u003d\" + node\n-                    .getPartition(), e);\n-            return;\n-          }\n-\n-          // Try to use NON_EXCLUSIVE\n-          assignment \u003d root.assignContainers(getClusterResource(), node,\n-              // TODO, now we only consider limits for parent for non-labeled\n-              // resources, should consider labeled resources as well.\n-              new ResourceLimits(labelManager\n-                  .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n-                      getClusterResource())),\n-              SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n-          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n-        }\n-      } else{\n-        LOG.info(\"Skipping scheduling since node \" + node.getNodeID()\n-            + \" is reserved by application \" + node.getReservedContainer()\n-            .getContainerId().getApplicationAttemptId());\n-      }\n-    } finally {\n-      writeLock.unlock();\n+    // We have two different logics to handle allocation on single node / multi\n+    // nodes.\n+    if (null !\u003d node) {\n+      return allocateContainerOnSingleNode(ps, node, withNodeHeartbeat);\n+    } else {\n+      return allocateContainersOnMultiNodes(ps);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  CSAssignment allocateContainersToNode(PlacementSet\u003cFiCaSchedulerNode\u003e ps,\n      boolean withNodeHeartbeat) {\n    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n        .isSchedulerReadyForAllocatingContainers()) {\n      return null;\n    }\n\n    // Backward compatible way to make sure previous behavior which allocation\n    // driven by node heartbeat works.\n    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n\n    // We have two different logics to handle allocation on single node / multi\n    // nodes.\n    if (null !\u003d node) {\n      return allocateContainerOnSingleNode(ps, node, withNodeHeartbeat);\n    } else {\n      return allocateContainersOnMultiNodes(ps);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
          "extendedDetails": {
            "oldValue": "[node-FiCaSchedulerNode]",
            "newValue": "[ps-PlacementSet\u003cFiCaSchedulerNode\u003e, withNodeHeartbeat-boolean]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
          "commitDate": "07/11/16 10:14 AM",
          "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
          "commitAuthor": "Jian He",
          "commitDateOld": "01/11/16 3:02 AM",
          "commitNameOld": "7d2d8d25ba0cb10a3c6192d4123f27ede5ef2ba6",
          "commitAuthorOld": "Varun Saxena",
          "daysBetweenCommits": 6.34,
          "commitsBetweenForRepo": 78,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,130 +1,19 @@\n-  public void allocateContainersToNode(FiCaSchedulerNode node) {\n-    try {\n-      writeLock.lock();\n-      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n-          .isSchedulerReadyForAllocatingContainers()) {\n-        return;\n-      }\n+  CSAssignment allocateContainersToNode(PlacementSet\u003cFiCaSchedulerNode\u003e ps,\n+      boolean withNodeHeartbeat) {\n+    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n+        .isSchedulerReadyForAllocatingContainers()) {\n+      return null;\n+    }\n \n-      if (!nodeTracker.exists(node.getNodeID())) {\n-        LOG.info(\"Skipping scheduling as the node \" + node.getNodeID()\n-            + \" has been removed\");\n-        return;\n-      }\n+    // Backward compatible way to make sure previous behavior which allocation\n+    // driven by node heartbeat works.\n+    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n \n-      // reset allocation and reservation stats before we start doing any work\n-      updateSchedulerHealth(lastNodeUpdateTime, node,\n-          new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n-\n-      CSAssignment assignment;\n-\n-      // Assign new containers...\n-      // 1. Check for reserved applications\n-      // 2. Schedule if there are no reservations\n-\n-      RMContainer reservedContainer \u003d node.getReservedContainer();\n-      if (reservedContainer !\u003d null) {\n-\n-        FiCaSchedulerApp reservedApplication \u003d getCurrentAttemptForContainer(\n-            reservedContainer.getContainerId());\n-\n-        // Try to fulfill the reservation\n-        LOG.info(\"Trying to fulfill reservation for application \"\n-            + reservedApplication.getApplicationId() + \" on node: \" + node\n-            .getNodeID());\n-\n-        LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n-        assignment \u003d queue.assignContainers(getClusterResource(), node,\n-            // TODO, now we only consider limits for parent for non-labeled\n-            // resources, should consider labeled resources as well.\n-            new ResourceLimits(labelManager\n-                .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n-                    getClusterResource())),\n-            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-        if (assignment.isFulfilledReservation()) {\n-          CSAssignment tmp \u003d new CSAssignment(\n-              reservedContainer.getReservedResource(), assignment.getType());\n-          Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-              reservedContainer.getReservedResource());\n-          tmp.getAssignmentInformation().addAllocationDetails(\n-              reservedContainer.getContainerId(), queue.getQueuePath());\n-          tmp.getAssignmentInformation().incrAllocations();\n-          updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n-          schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n-\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              queue.getParent().getQueueName(), queue.getQueueName(),\n-              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-              node, reservedContainer.getContainerId(),\n-              AllocationState.ALLOCATED_FROM_RESERVED);\n-        } else{\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              queue.getParent().getQueueName(), queue.getQueueName(),\n-              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-              node, reservedContainer.getContainerId(),\n-              AllocationState.SKIPPED);\n-        }\n-      }\n-\n-      // Try to schedule more if there are no reservations to fulfill\n-      if (node.getReservedContainer() \u003d\u003d null) {\n-        if (calculator.computeAvailableContainers(Resources\n-            .add(node.getUnallocatedResource(),\n-                node.getTotalKillableResources()), minimumAllocation) \u003e 0) {\n-\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Trying to schedule on node: \" + node.getNodeName()\n-                + \", available: \" + node.getUnallocatedResource());\n-          }\n-\n-          assignment \u003d root.assignContainers(getClusterResource(), node,\n-              new ResourceLimits(labelManager\n-                  .getResourceByLabel(node.getPartition(),\n-                      getClusterResource())),\n-              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-          if (Resources.greaterThan(calculator, getClusterResource(),\n-              assignment.getResource(), Resources.none())) {\n-            updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n-            return;\n-          }\n-\n-          // Only do non-exclusive allocation when node has node-labels.\n-          if (StringUtils.equals(node.getPartition(),\n-              RMNodeLabelsManager.NO_LABEL)) {\n-            return;\n-          }\n-\n-          // Only do non-exclusive allocation when the node-label supports that\n-          try {\n-            if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n-                node.getPartition())) {\n-              return;\n-            }\n-          } catch (IOException e) {\n-            LOG.warn(\n-                \"Exception when trying to get exclusivity of node label\u003d\" + node\n-                    .getPartition(), e);\n-            return;\n-          }\n-\n-          // Try to use NON_EXCLUSIVE\n-          assignment \u003d root.assignContainers(getClusterResource(), node,\n-              // TODO, now we only consider limits for parent for non-labeled\n-              // resources, should consider labeled resources as well.\n-              new ResourceLimits(labelManager\n-                  .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n-                      getClusterResource())),\n-              SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n-          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n-        }\n-      } else{\n-        LOG.info(\"Skipping scheduling since node \" + node.getNodeID()\n-            + \" is reserved by application \" + node.getReservedContainer()\n-            .getContainerId().getApplicationAttemptId());\n-      }\n-    } finally {\n-      writeLock.unlock();\n+    // We have two different logics to handle allocation on single node / multi\n+    // nodes.\n+    if (null !\u003d node) {\n+      return allocateContainerOnSingleNode(ps, node, withNodeHeartbeat);\n+    } else {\n+      return allocateContainersOnMultiNodes(ps);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  CSAssignment allocateContainersToNode(PlacementSet\u003cFiCaSchedulerNode\u003e ps,\n      boolean withNodeHeartbeat) {\n    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n        .isSchedulerReadyForAllocatingContainers()) {\n      return null;\n    }\n\n    // Backward compatible way to make sure previous behavior which allocation\n    // driven by node heartbeat works.\n    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n\n    // We have two different logics to handle allocation on single node / multi\n    // nodes.\n    if (null !\u003d node) {\n      return allocateContainerOnSingleNode(ps, node, withNodeHeartbeat);\n    } else {\n      return allocateContainersOnMultiNodes(ps);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "CSAssignment"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
          "commitDate": "07/11/16 10:14 AM",
          "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
          "commitAuthor": "Jian He",
          "commitDateOld": "01/11/16 3:02 AM",
          "commitNameOld": "7d2d8d25ba0cb10a3c6192d4123f27ede5ef2ba6",
          "commitAuthorOld": "Varun Saxena",
          "daysBetweenCommits": 6.34,
          "commitsBetweenForRepo": 78,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,130 +1,19 @@\n-  public void allocateContainersToNode(FiCaSchedulerNode node) {\n-    try {\n-      writeLock.lock();\n-      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n-          .isSchedulerReadyForAllocatingContainers()) {\n-        return;\n-      }\n+  CSAssignment allocateContainersToNode(PlacementSet\u003cFiCaSchedulerNode\u003e ps,\n+      boolean withNodeHeartbeat) {\n+    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n+        .isSchedulerReadyForAllocatingContainers()) {\n+      return null;\n+    }\n \n-      if (!nodeTracker.exists(node.getNodeID())) {\n-        LOG.info(\"Skipping scheduling as the node \" + node.getNodeID()\n-            + \" has been removed\");\n-        return;\n-      }\n+    // Backward compatible way to make sure previous behavior which allocation\n+    // driven by node heartbeat works.\n+    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n \n-      // reset allocation and reservation stats before we start doing any work\n-      updateSchedulerHealth(lastNodeUpdateTime, node,\n-          new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n-\n-      CSAssignment assignment;\n-\n-      // Assign new containers...\n-      // 1. Check for reserved applications\n-      // 2. Schedule if there are no reservations\n-\n-      RMContainer reservedContainer \u003d node.getReservedContainer();\n-      if (reservedContainer !\u003d null) {\n-\n-        FiCaSchedulerApp reservedApplication \u003d getCurrentAttemptForContainer(\n-            reservedContainer.getContainerId());\n-\n-        // Try to fulfill the reservation\n-        LOG.info(\"Trying to fulfill reservation for application \"\n-            + reservedApplication.getApplicationId() + \" on node: \" + node\n-            .getNodeID());\n-\n-        LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n-        assignment \u003d queue.assignContainers(getClusterResource(), node,\n-            // TODO, now we only consider limits for parent for non-labeled\n-            // resources, should consider labeled resources as well.\n-            new ResourceLimits(labelManager\n-                .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n-                    getClusterResource())),\n-            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-        if (assignment.isFulfilledReservation()) {\n-          CSAssignment tmp \u003d new CSAssignment(\n-              reservedContainer.getReservedResource(), assignment.getType());\n-          Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-              reservedContainer.getReservedResource());\n-          tmp.getAssignmentInformation().addAllocationDetails(\n-              reservedContainer.getContainerId(), queue.getQueuePath());\n-          tmp.getAssignmentInformation().incrAllocations();\n-          updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n-          schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n-\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              queue.getParent().getQueueName(), queue.getQueueName(),\n-              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-              node, reservedContainer.getContainerId(),\n-              AllocationState.ALLOCATED_FROM_RESERVED);\n-        } else{\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              queue.getParent().getQueueName(), queue.getQueueName(),\n-              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-              node, reservedContainer.getContainerId(),\n-              AllocationState.SKIPPED);\n-        }\n-      }\n-\n-      // Try to schedule more if there are no reservations to fulfill\n-      if (node.getReservedContainer() \u003d\u003d null) {\n-        if (calculator.computeAvailableContainers(Resources\n-            .add(node.getUnallocatedResource(),\n-                node.getTotalKillableResources()), minimumAllocation) \u003e 0) {\n-\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Trying to schedule on node: \" + node.getNodeName()\n-                + \", available: \" + node.getUnallocatedResource());\n-          }\n-\n-          assignment \u003d root.assignContainers(getClusterResource(), node,\n-              new ResourceLimits(labelManager\n-                  .getResourceByLabel(node.getPartition(),\n-                      getClusterResource())),\n-              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-          if (Resources.greaterThan(calculator, getClusterResource(),\n-              assignment.getResource(), Resources.none())) {\n-            updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n-            return;\n-          }\n-\n-          // Only do non-exclusive allocation when node has node-labels.\n-          if (StringUtils.equals(node.getPartition(),\n-              RMNodeLabelsManager.NO_LABEL)) {\n-            return;\n-          }\n-\n-          // Only do non-exclusive allocation when the node-label supports that\n-          try {\n-            if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n-                node.getPartition())) {\n-              return;\n-            }\n-          } catch (IOException e) {\n-            LOG.warn(\n-                \"Exception when trying to get exclusivity of node label\u003d\" + node\n-                    .getPartition(), e);\n-            return;\n-          }\n-\n-          // Try to use NON_EXCLUSIVE\n-          assignment \u003d root.assignContainers(getClusterResource(), node,\n-              // TODO, now we only consider limits for parent for non-labeled\n-              // resources, should consider labeled resources as well.\n-              new ResourceLimits(labelManager\n-                  .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n-                      getClusterResource())),\n-              SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n-          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n-        }\n-      } else{\n-        LOG.info(\"Skipping scheduling since node \" + node.getNodeID()\n-            + \" is reserved by application \" + node.getReservedContainer()\n-            .getContainerId().getApplicationAttemptId());\n-      }\n-    } finally {\n-      writeLock.unlock();\n+    // We have two different logics to handle allocation on single node / multi\n+    // nodes.\n+    if (null !\u003d node) {\n+      return allocateContainerOnSingleNode(ps, node, withNodeHeartbeat);\n+    } else {\n+      return allocateContainersOnMultiNodes(ps);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  CSAssignment allocateContainersToNode(PlacementSet\u003cFiCaSchedulerNode\u003e ps,\n      boolean withNodeHeartbeat) {\n    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n        .isSchedulerReadyForAllocatingContainers()) {\n      return null;\n    }\n\n    // Backward compatible way to make sure previous behavior which allocation\n    // driven by node heartbeat works.\n    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n\n    // We have two different logics to handle allocation on single node / multi\n    // nodes.\n    if (null !\u003d node) {\n      return allocateContainerOnSingleNode(ps, node, withNodeHeartbeat);\n    } else {\n      return allocateContainersOnMultiNodes(ps);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5716. Add global scheduler interface definition and update CapacityScheduler to use it. Contributed by Wangda Tan\n",
          "commitDate": "07/11/16 10:14 AM",
          "commitName": "de3b4aac561258ad242a3c5ed1c919428893fd4c",
          "commitAuthor": "Jian He",
          "commitDateOld": "01/11/16 3:02 AM",
          "commitNameOld": "7d2d8d25ba0cb10a3c6192d4123f27ede5ef2ba6",
          "commitAuthorOld": "Varun Saxena",
          "daysBetweenCommits": 6.34,
          "commitsBetweenForRepo": 78,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,130 +1,19 @@\n-  public void allocateContainersToNode(FiCaSchedulerNode node) {\n-    try {\n-      writeLock.lock();\n-      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n-          .isSchedulerReadyForAllocatingContainers()) {\n-        return;\n-      }\n+  CSAssignment allocateContainersToNode(PlacementSet\u003cFiCaSchedulerNode\u003e ps,\n+      boolean withNodeHeartbeat) {\n+    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n+        .isSchedulerReadyForAllocatingContainers()) {\n+      return null;\n+    }\n \n-      if (!nodeTracker.exists(node.getNodeID())) {\n-        LOG.info(\"Skipping scheduling as the node \" + node.getNodeID()\n-            + \" has been removed\");\n-        return;\n-      }\n+    // Backward compatible way to make sure previous behavior which allocation\n+    // driven by node heartbeat works.\n+    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n \n-      // reset allocation and reservation stats before we start doing any work\n-      updateSchedulerHealth(lastNodeUpdateTime, node,\n-          new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n-\n-      CSAssignment assignment;\n-\n-      // Assign new containers...\n-      // 1. Check for reserved applications\n-      // 2. Schedule if there are no reservations\n-\n-      RMContainer reservedContainer \u003d node.getReservedContainer();\n-      if (reservedContainer !\u003d null) {\n-\n-        FiCaSchedulerApp reservedApplication \u003d getCurrentAttemptForContainer(\n-            reservedContainer.getContainerId());\n-\n-        // Try to fulfill the reservation\n-        LOG.info(\"Trying to fulfill reservation for application \"\n-            + reservedApplication.getApplicationId() + \" on node: \" + node\n-            .getNodeID());\n-\n-        LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n-        assignment \u003d queue.assignContainers(getClusterResource(), node,\n-            // TODO, now we only consider limits for parent for non-labeled\n-            // resources, should consider labeled resources as well.\n-            new ResourceLimits(labelManager\n-                .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n-                    getClusterResource())),\n-            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-        if (assignment.isFulfilledReservation()) {\n-          CSAssignment tmp \u003d new CSAssignment(\n-              reservedContainer.getReservedResource(), assignment.getType());\n-          Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-              reservedContainer.getReservedResource());\n-          tmp.getAssignmentInformation().addAllocationDetails(\n-              reservedContainer.getContainerId(), queue.getQueuePath());\n-          tmp.getAssignmentInformation().incrAllocations();\n-          updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n-          schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n-\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              queue.getParent().getQueueName(), queue.getQueueName(),\n-              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-              node, reservedContainer.getContainerId(),\n-              AllocationState.ALLOCATED_FROM_RESERVED);\n-        } else{\n-          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-              queue.getParent().getQueueName(), queue.getQueueName(),\n-              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-              node, reservedContainer.getContainerId(),\n-              AllocationState.SKIPPED);\n-        }\n-      }\n-\n-      // Try to schedule more if there are no reservations to fulfill\n-      if (node.getReservedContainer() \u003d\u003d null) {\n-        if (calculator.computeAvailableContainers(Resources\n-            .add(node.getUnallocatedResource(),\n-                node.getTotalKillableResources()), minimumAllocation) \u003e 0) {\n-\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Trying to schedule on node: \" + node.getNodeName()\n-                + \", available: \" + node.getUnallocatedResource());\n-          }\n-\n-          assignment \u003d root.assignContainers(getClusterResource(), node,\n-              new ResourceLimits(labelManager\n-                  .getResourceByLabel(node.getPartition(),\n-                      getClusterResource())),\n-              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-          if (Resources.greaterThan(calculator, getClusterResource(),\n-              assignment.getResource(), Resources.none())) {\n-            updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n-            return;\n-          }\n-\n-          // Only do non-exclusive allocation when node has node-labels.\n-          if (StringUtils.equals(node.getPartition(),\n-              RMNodeLabelsManager.NO_LABEL)) {\n-            return;\n-          }\n-\n-          // Only do non-exclusive allocation when the node-label supports that\n-          try {\n-            if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n-                node.getPartition())) {\n-              return;\n-            }\n-          } catch (IOException e) {\n-            LOG.warn(\n-                \"Exception when trying to get exclusivity of node label\u003d\" + node\n-                    .getPartition(), e);\n-            return;\n-          }\n-\n-          // Try to use NON_EXCLUSIVE\n-          assignment \u003d root.assignContainers(getClusterResource(), node,\n-              // TODO, now we only consider limits for parent for non-labeled\n-              // resources, should consider labeled resources as well.\n-              new ResourceLimits(labelManager\n-                  .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n-                      getClusterResource())),\n-              SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n-          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n-        }\n-      } else{\n-        LOG.info(\"Skipping scheduling since node \" + node.getNodeID()\n-            + \" is reserved by application \" + node.getReservedContainer()\n-            .getContainerId().getApplicationAttemptId());\n-      }\n-    } finally {\n-      writeLock.unlock();\n+    // We have two different logics to handle allocation on single node / multi\n+    // nodes.\n+    if (null !\u003d node) {\n+      return allocateContainerOnSingleNode(ps, node, withNodeHeartbeat);\n+    } else {\n+      return allocateContainersOnMultiNodes(ps);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  CSAssignment allocateContainersToNode(PlacementSet\u003cFiCaSchedulerNode\u003e ps,\n      boolean withNodeHeartbeat) {\n    if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n        .isSchedulerReadyForAllocatingContainers()) {\n      return null;\n    }\n\n    // Backward compatible way to make sure previous behavior which allocation\n    // driven by node heartbeat works.\n    FiCaSchedulerNode node \u003d PlacementSetUtils.getSingleNode(ps);\n\n    // We have two different logics to handle allocation on single node / multi\n    // nodes.\n    if (null !\u003d node) {\n      return allocateContainerOnSingleNode(ps, node, withNodeHeartbeat);\n    } else {\n      return allocateContainersOnMultiNodes(ps);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
          "extendedDetails": {}
        }
      ]
    },
    "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "YARN-3139. Improve locks in AbstractYarnScheduler/CapacityScheduler/FairScheduler. Contributed by Wangda Tan\n",
      "commitDate": "04/10/16 5:23 PM",
      "commitName": "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-3139. Improve locks in AbstractYarnScheduler/CapacityScheduler/FairScheduler. Contributed by Wangda Tan\n",
          "commitDate": "04/10/16 5:23 PM",
          "commitName": "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f",
          "commitAuthor": "Jian He",
          "commitDateOld": "30/08/16 3:52 PM",
          "commitNameOld": "d6d9cff21b7b6141ed88359652cf22e8973c0661",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 35.06,
          "commitsBetweenForRepo": 195,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,130 +1,130 @@\n-  public synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n-    if (rmContext.isWorkPreservingRecoveryEnabled()\n-        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n-      return;\n-    }\n-\n-    if (!nodeTracker.exists(node.getNodeID())) {\n-      LOG.info(\"Skipping scheduling as the node \" + node.getNodeID() +\n-          \" has been removed\");\n-      return;\n-    }\n-\n-    // reset allocation and reservation stats before we start doing any work\n-    updateSchedulerHealth(lastNodeUpdateTime, node,\n-      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n-\n-    CSAssignment assignment;\n-\n-    // Assign new containers...\n-    // 1. Check for reserved applications\n-    // 2. Schedule if there are no reservations\n-\n-    RMContainer reservedContainer \u003d node.getReservedContainer();\n-    if (reservedContainer !\u003d null) {\n-\n-      FiCaSchedulerApp reservedApplication \u003d\n-          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n-\n-      // Try to fulfill the reservation\n-      LOG.info(\"Trying to fulfill reservation for application \"\n-          + reservedApplication.getApplicationId() + \" on node: \"\n-          + node.getNodeID());\n-\n-      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n-      assignment \u003d\n-          queue.assignContainers(\n-              getClusterResource(),\n-              node,\n-              // TODO, now we only consider limits for parent for non-labeled\n-              // resources, should consider labeled resources as well.\n-              new ResourceLimits(labelManager.getResourceByLabel(\n-                  RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n-              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-      if (assignment.isFulfilledReservation()) {\n-        CSAssignment tmp \u003d\n-            new CSAssignment(reservedContainer.getReservedResource(),\n-                assignment.getType());\n-        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-            reservedContainer.getReservedResource());\n-        tmp.getAssignmentInformation().addAllocationDetails(\n-            reservedContainer.getContainerId(), queue.getQueuePath());\n-        tmp.getAssignmentInformation().incrAllocations();\n-        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n-        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n-\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            queue.getParent().getQueueName(), queue.getQueueName(),\n-            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-        ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-            node, reservedContainer.getContainerId(),\n-            AllocationState.ALLOCATED_FROM_RESERVED);\n-      } else {\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            queue.getParent().getQueueName(), queue.getQueueName(),\n-            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-        ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-            node, reservedContainer.getContainerId(), AllocationState.SKIPPED);\n+  public void allocateContainersToNode(FiCaSchedulerNode node) {\n+    try {\n+      writeLock.lock();\n+      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n+          .isSchedulerReadyForAllocatingContainers()) {\n+        return;\n       }\n-    }\n \n-    // Try to schedule more if there are no reservations to fulfill\n-    if (node.getReservedContainer() \u003d\u003d null) {\n-      if (calculator.computeAvailableContainers(Resources\n-              .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n-          minimumAllocation) \u003e 0) {\n+      if (!nodeTracker.exists(node.getNodeID())) {\n+        LOG.info(\"Skipping scheduling as the node \" + node.getNodeID()\n+            + \" has been removed\");\n+        return;\n+      }\n \n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n-              \", available: \" + node.getUnallocatedResource());\n-        }\n+      // reset allocation and reservation stats before we start doing any work\n+      updateSchedulerHealth(lastNodeUpdateTime, node,\n+          new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n-        assignment \u003d root.assignContainers(\n-            getClusterResource(),\n-            node,\n-            new ResourceLimits(labelManager.getResourceByLabel(\n-                node.getPartition(), getClusterResource())),\n-            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-        if (Resources.greaterThan(calculator, getClusterResource(),\n-            assignment.getResource(), Resources.none())) {\n-          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n-          return;\n-        }\n-        \n-        // Only do non-exclusive allocation when node has node-labels.\n-        if (StringUtils.equals(node.getPartition(),\n-            RMNodeLabelsManager.NO_LABEL)) {\n-          return;\n-        }\n-        \n-        // Only do non-exclusive allocation when the node-label supports that\n-        try {\n-          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n-              node.getPartition())) {\n-            return;\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n-              + node.getPartition(), e);\n-          return;\n-        }\n-        \n-        // Try to use NON_EXCLUSIVE\n-        assignment \u003d root.assignContainers(\n-            getClusterResource(),\n-            node,\n+      CSAssignment assignment;\n+\n+      // Assign new containers...\n+      // 1. Check for reserved applications\n+      // 2. Schedule if there are no reservations\n+\n+      RMContainer reservedContainer \u003d node.getReservedContainer();\n+      if (reservedContainer !\u003d null) {\n+\n+        FiCaSchedulerApp reservedApplication \u003d getCurrentAttemptForContainer(\n+            reservedContainer.getContainerId());\n+\n+        // Try to fulfill the reservation\n+        LOG.info(\"Trying to fulfill reservation for application \"\n+            + reservedApplication.getApplicationId() + \" on node: \" + node\n+            .getNodeID());\n+\n+        LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n+        assignment \u003d queue.assignContainers(getClusterResource(), node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n-            new ResourceLimits(labelManager.getResourceByLabel(\n-                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n-            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n-        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n+            new ResourceLimits(labelManager\n+                .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n+                    getClusterResource())),\n+            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n+        if (assignment.isFulfilledReservation()) {\n+          CSAssignment tmp \u003d new CSAssignment(\n+              reservedContainer.getReservedResource(), assignment.getType());\n+          Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n+              reservedContainer.getReservedResource());\n+          tmp.getAssignmentInformation().addAllocationDetails(\n+              reservedContainer.getContainerId(), queue.getQueuePath());\n+          tmp.getAssignmentInformation().incrAllocations();\n+          updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n+          schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n+\n+          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+              queue.getParent().getQueueName(), queue.getQueueName(),\n+              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n+          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n+              node, reservedContainer.getContainerId(),\n+              AllocationState.ALLOCATED_FROM_RESERVED);\n+        } else{\n+          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+              queue.getParent().getQueueName(), queue.getQueueName(),\n+              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n+          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n+              node, reservedContainer.getContainerId(),\n+              AllocationState.SKIPPED);\n+        }\n       }\n-    } else {\n-      LOG.info(\"Skipping scheduling since node \"\n-          + node.getNodeID()\n-          + \" is reserved by application \"\n-          + node.getReservedContainer().getContainerId()\n-              .getApplicationAttemptId());\n+\n+      // Try to schedule more if there are no reservations to fulfill\n+      if (node.getReservedContainer() \u003d\u003d null) {\n+        if (calculator.computeAvailableContainers(Resources\n+            .add(node.getUnallocatedResource(),\n+                node.getTotalKillableResources()), minimumAllocation) \u003e 0) {\n+\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Trying to schedule on node: \" + node.getNodeName()\n+                + \", available: \" + node.getUnallocatedResource());\n+          }\n+\n+          assignment \u003d root.assignContainers(getClusterResource(), node,\n+              new ResourceLimits(labelManager\n+                  .getResourceByLabel(node.getPartition(),\n+                      getClusterResource())),\n+              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n+          if (Resources.greaterThan(calculator, getClusterResource(),\n+              assignment.getResource(), Resources.none())) {\n+            updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n+            return;\n+          }\n+\n+          // Only do non-exclusive allocation when node has node-labels.\n+          if (StringUtils.equals(node.getPartition(),\n+              RMNodeLabelsManager.NO_LABEL)) {\n+            return;\n+          }\n+\n+          // Only do non-exclusive allocation when the node-label supports that\n+          try {\n+            if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n+                node.getPartition())) {\n+              return;\n+            }\n+          } catch (IOException e) {\n+            LOG.warn(\n+                \"Exception when trying to get exclusivity of node label\u003d\" + node\n+                    .getPartition(), e);\n+            return;\n+          }\n+\n+          // Try to use NON_EXCLUSIVE\n+          assignment \u003d root.assignContainers(getClusterResource(), node,\n+              // TODO, now we only consider limits for parent for non-labeled\n+              // resources, should consider labeled resources as well.\n+              new ResourceLimits(labelManager\n+                  .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n+                      getClusterResource())),\n+              SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n+          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n+        }\n+      } else{\n+        LOG.info(\"Skipping scheduling since node \" + node.getNodeID()\n+            + \" is reserved by application \" + node.getReservedContainer()\n+            .getContainerId().getApplicationAttemptId());\n+      }\n+    } finally {\n+      writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void allocateContainersToNode(FiCaSchedulerNode node) {\n    try {\n      writeLock.lock();\n      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n          .isSchedulerReadyForAllocatingContainers()) {\n        return;\n      }\n\n      if (!nodeTracker.exists(node.getNodeID())) {\n        LOG.info(\"Skipping scheduling as the node \" + node.getNodeID()\n            + \" has been removed\");\n        return;\n      }\n\n      // reset allocation and reservation stats before we start doing any work\n      updateSchedulerHealth(lastNodeUpdateTime, node,\n          new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n      CSAssignment assignment;\n\n      // Assign new containers...\n      // 1. Check for reserved applications\n      // 2. Schedule if there are no reservations\n\n      RMContainer reservedContainer \u003d node.getReservedContainer();\n      if (reservedContainer !\u003d null) {\n\n        FiCaSchedulerApp reservedApplication \u003d getCurrentAttemptForContainer(\n            reservedContainer.getContainerId());\n\n        // Try to fulfill the reservation\n        LOG.info(\"Trying to fulfill reservation for application \"\n            + reservedApplication.getApplicationId() + \" on node: \" + node\n            .getNodeID());\n\n        LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n        assignment \u003d queue.assignContainers(getClusterResource(), node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager\n                .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n                    getClusterResource())),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (assignment.isFulfilledReservation()) {\n          CSAssignment tmp \u003d new CSAssignment(\n              reservedContainer.getReservedResource(), assignment.getType());\n          Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n              reservedContainer.getReservedResource());\n          tmp.getAssignmentInformation().addAllocationDetails(\n              reservedContainer.getContainerId(), queue.getQueuePath());\n          tmp.getAssignmentInformation().incrAllocations();\n          updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n          schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              queue.getParent().getQueueName(), queue.getQueueName(),\n              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n              node, reservedContainer.getContainerId(),\n              AllocationState.ALLOCATED_FROM_RESERVED);\n        } else{\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              queue.getParent().getQueueName(), queue.getQueueName(),\n              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n              node, reservedContainer.getContainerId(),\n              AllocationState.SKIPPED);\n        }\n      }\n\n      // Try to schedule more if there are no reservations to fulfill\n      if (node.getReservedContainer() \u003d\u003d null) {\n        if (calculator.computeAvailableContainers(Resources\n            .add(node.getUnallocatedResource(),\n                node.getTotalKillableResources()), minimumAllocation) \u003e 0) {\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Trying to schedule on node: \" + node.getNodeName()\n                + \", available: \" + node.getUnallocatedResource());\n          }\n\n          assignment \u003d root.assignContainers(getClusterResource(), node,\n              new ResourceLimits(labelManager\n                  .getResourceByLabel(node.getPartition(),\n                      getClusterResource())),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n          if (Resources.greaterThan(calculator, getClusterResource(),\n              assignment.getResource(), Resources.none())) {\n            updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n            return;\n          }\n\n          // Only do non-exclusive allocation when node has node-labels.\n          if (StringUtils.equals(node.getPartition(),\n              RMNodeLabelsManager.NO_LABEL)) {\n            return;\n          }\n\n          // Only do non-exclusive allocation when the node-label supports that\n          try {\n            if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n                node.getPartition())) {\n              return;\n            }\n          } catch (IOException e) {\n            LOG.warn(\n                \"Exception when trying to get exclusivity of node label\u003d\" + node\n                    .getPartition(), e);\n            return;\n          }\n\n          // Try to use NON_EXCLUSIVE\n          assignment \u003d root.assignContainers(getClusterResource(), node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager\n                  .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n                      getClusterResource())),\n              SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n        }\n      } else{\n        LOG.info(\"Skipping scheduling since node \" + node.getNodeID()\n            + \" is reserved by application \" + node.getReservedContainer()\n            .getContainerId().getApplicationAttemptId());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3139. Improve locks in AbstractYarnScheduler/CapacityScheduler/FairScheduler. Contributed by Wangda Tan\n",
          "commitDate": "04/10/16 5:23 PM",
          "commitName": "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f",
          "commitAuthor": "Jian He",
          "commitDateOld": "30/08/16 3:52 PM",
          "commitNameOld": "d6d9cff21b7b6141ed88359652cf22e8973c0661",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 35.06,
          "commitsBetweenForRepo": 195,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,130 +1,130 @@\n-  public synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n-    if (rmContext.isWorkPreservingRecoveryEnabled()\n-        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n-      return;\n-    }\n-\n-    if (!nodeTracker.exists(node.getNodeID())) {\n-      LOG.info(\"Skipping scheduling as the node \" + node.getNodeID() +\n-          \" has been removed\");\n-      return;\n-    }\n-\n-    // reset allocation and reservation stats before we start doing any work\n-    updateSchedulerHealth(lastNodeUpdateTime, node,\n-      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n-\n-    CSAssignment assignment;\n-\n-    // Assign new containers...\n-    // 1. Check for reserved applications\n-    // 2. Schedule if there are no reservations\n-\n-    RMContainer reservedContainer \u003d node.getReservedContainer();\n-    if (reservedContainer !\u003d null) {\n-\n-      FiCaSchedulerApp reservedApplication \u003d\n-          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n-\n-      // Try to fulfill the reservation\n-      LOG.info(\"Trying to fulfill reservation for application \"\n-          + reservedApplication.getApplicationId() + \" on node: \"\n-          + node.getNodeID());\n-\n-      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n-      assignment \u003d\n-          queue.assignContainers(\n-              getClusterResource(),\n-              node,\n-              // TODO, now we only consider limits for parent for non-labeled\n-              // resources, should consider labeled resources as well.\n-              new ResourceLimits(labelManager.getResourceByLabel(\n-                  RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n-              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-      if (assignment.isFulfilledReservation()) {\n-        CSAssignment tmp \u003d\n-            new CSAssignment(reservedContainer.getReservedResource(),\n-                assignment.getType());\n-        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-            reservedContainer.getReservedResource());\n-        tmp.getAssignmentInformation().addAllocationDetails(\n-            reservedContainer.getContainerId(), queue.getQueuePath());\n-        tmp.getAssignmentInformation().incrAllocations();\n-        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n-        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n-\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            queue.getParent().getQueueName(), queue.getQueueName(),\n-            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-        ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-            node, reservedContainer.getContainerId(),\n-            AllocationState.ALLOCATED_FROM_RESERVED);\n-      } else {\n-        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n-            queue.getParent().getQueueName(), queue.getQueueName(),\n-            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n-        ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n-            node, reservedContainer.getContainerId(), AllocationState.SKIPPED);\n+  public void allocateContainersToNode(FiCaSchedulerNode node) {\n+    try {\n+      writeLock.lock();\n+      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n+          .isSchedulerReadyForAllocatingContainers()) {\n+        return;\n       }\n-    }\n \n-    // Try to schedule more if there are no reservations to fulfill\n-    if (node.getReservedContainer() \u003d\u003d null) {\n-      if (calculator.computeAvailableContainers(Resources\n-              .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n-          minimumAllocation) \u003e 0) {\n+      if (!nodeTracker.exists(node.getNodeID())) {\n+        LOG.info(\"Skipping scheduling as the node \" + node.getNodeID()\n+            + \" has been removed\");\n+        return;\n+      }\n \n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n-              \", available: \" + node.getUnallocatedResource());\n-        }\n+      // reset allocation and reservation stats before we start doing any work\n+      updateSchedulerHealth(lastNodeUpdateTime, node,\n+          new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n-        assignment \u003d root.assignContainers(\n-            getClusterResource(),\n-            node,\n-            new ResourceLimits(labelManager.getResourceByLabel(\n-                node.getPartition(), getClusterResource())),\n-            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-        if (Resources.greaterThan(calculator, getClusterResource(),\n-            assignment.getResource(), Resources.none())) {\n-          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n-          return;\n-        }\n-        \n-        // Only do non-exclusive allocation when node has node-labels.\n-        if (StringUtils.equals(node.getPartition(),\n-            RMNodeLabelsManager.NO_LABEL)) {\n-          return;\n-        }\n-        \n-        // Only do non-exclusive allocation when the node-label supports that\n-        try {\n-          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n-              node.getPartition())) {\n-            return;\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n-              + node.getPartition(), e);\n-          return;\n-        }\n-        \n-        // Try to use NON_EXCLUSIVE\n-        assignment \u003d root.assignContainers(\n-            getClusterResource(),\n-            node,\n+      CSAssignment assignment;\n+\n+      // Assign new containers...\n+      // 1. Check for reserved applications\n+      // 2. Schedule if there are no reservations\n+\n+      RMContainer reservedContainer \u003d node.getReservedContainer();\n+      if (reservedContainer !\u003d null) {\n+\n+        FiCaSchedulerApp reservedApplication \u003d getCurrentAttemptForContainer(\n+            reservedContainer.getContainerId());\n+\n+        // Try to fulfill the reservation\n+        LOG.info(\"Trying to fulfill reservation for application \"\n+            + reservedApplication.getApplicationId() + \" on node: \" + node\n+            .getNodeID());\n+\n+        LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n+        assignment \u003d queue.assignContainers(getClusterResource(), node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n-            new ResourceLimits(labelManager.getResourceByLabel(\n-                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n-            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n-        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n+            new ResourceLimits(labelManager\n+                .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n+                    getClusterResource())),\n+            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n+        if (assignment.isFulfilledReservation()) {\n+          CSAssignment tmp \u003d new CSAssignment(\n+              reservedContainer.getReservedResource(), assignment.getType());\n+          Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n+              reservedContainer.getReservedResource());\n+          tmp.getAssignmentInformation().addAllocationDetails(\n+              reservedContainer.getContainerId(), queue.getQueuePath());\n+          tmp.getAssignmentInformation().incrAllocations();\n+          updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n+          schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n+\n+          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+              queue.getParent().getQueueName(), queue.getQueueName(),\n+              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n+          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n+              node, reservedContainer.getContainerId(),\n+              AllocationState.ALLOCATED_FROM_RESERVED);\n+        } else{\n+          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+              queue.getParent().getQueueName(), queue.getQueueName(),\n+              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n+          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n+              node, reservedContainer.getContainerId(),\n+              AllocationState.SKIPPED);\n+        }\n       }\n-    } else {\n-      LOG.info(\"Skipping scheduling since node \"\n-          + node.getNodeID()\n-          + \" is reserved by application \"\n-          + node.getReservedContainer().getContainerId()\n-              .getApplicationAttemptId());\n+\n+      // Try to schedule more if there are no reservations to fulfill\n+      if (node.getReservedContainer() \u003d\u003d null) {\n+        if (calculator.computeAvailableContainers(Resources\n+            .add(node.getUnallocatedResource(),\n+                node.getTotalKillableResources()), minimumAllocation) \u003e 0) {\n+\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Trying to schedule on node: \" + node.getNodeName()\n+                + \", available: \" + node.getUnallocatedResource());\n+          }\n+\n+          assignment \u003d root.assignContainers(getClusterResource(), node,\n+              new ResourceLimits(labelManager\n+                  .getResourceByLabel(node.getPartition(),\n+                      getClusterResource())),\n+              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n+          if (Resources.greaterThan(calculator, getClusterResource(),\n+              assignment.getResource(), Resources.none())) {\n+            updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n+            return;\n+          }\n+\n+          // Only do non-exclusive allocation when node has node-labels.\n+          if (StringUtils.equals(node.getPartition(),\n+              RMNodeLabelsManager.NO_LABEL)) {\n+            return;\n+          }\n+\n+          // Only do non-exclusive allocation when the node-label supports that\n+          try {\n+            if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n+                node.getPartition())) {\n+              return;\n+            }\n+          } catch (IOException e) {\n+            LOG.warn(\n+                \"Exception when trying to get exclusivity of node label\u003d\" + node\n+                    .getPartition(), e);\n+            return;\n+          }\n+\n+          // Try to use NON_EXCLUSIVE\n+          assignment \u003d root.assignContainers(getClusterResource(), node,\n+              // TODO, now we only consider limits for parent for non-labeled\n+              // resources, should consider labeled resources as well.\n+              new ResourceLimits(labelManager\n+                  .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n+                      getClusterResource())),\n+              SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n+          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n+        }\n+      } else{\n+        LOG.info(\"Skipping scheduling since node \" + node.getNodeID()\n+            + \" is reserved by application \" + node.getReservedContainer()\n+            .getContainerId().getApplicationAttemptId());\n+      }\n+    } finally {\n+      writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void allocateContainersToNode(FiCaSchedulerNode node) {\n    try {\n      writeLock.lock();\n      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n          .isSchedulerReadyForAllocatingContainers()) {\n        return;\n      }\n\n      if (!nodeTracker.exists(node.getNodeID())) {\n        LOG.info(\"Skipping scheduling as the node \" + node.getNodeID()\n            + \" has been removed\");\n        return;\n      }\n\n      // reset allocation and reservation stats before we start doing any work\n      updateSchedulerHealth(lastNodeUpdateTime, node,\n          new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n      CSAssignment assignment;\n\n      // Assign new containers...\n      // 1. Check for reserved applications\n      // 2. Schedule if there are no reservations\n\n      RMContainer reservedContainer \u003d node.getReservedContainer();\n      if (reservedContainer !\u003d null) {\n\n        FiCaSchedulerApp reservedApplication \u003d getCurrentAttemptForContainer(\n            reservedContainer.getContainerId());\n\n        // Try to fulfill the reservation\n        LOG.info(\"Trying to fulfill reservation for application \"\n            + reservedApplication.getApplicationId() + \" on node: \" + node\n            .getNodeID());\n\n        LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n        assignment \u003d queue.assignContainers(getClusterResource(), node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager\n                .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n                    getClusterResource())),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (assignment.isFulfilledReservation()) {\n          CSAssignment tmp \u003d new CSAssignment(\n              reservedContainer.getReservedResource(), assignment.getType());\n          Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n              reservedContainer.getReservedResource());\n          tmp.getAssignmentInformation().addAllocationDetails(\n              reservedContainer.getContainerId(), queue.getQueuePath());\n          tmp.getAssignmentInformation().incrAllocations();\n          updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n          schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              queue.getParent().getQueueName(), queue.getQueueName(),\n              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n              node, reservedContainer.getContainerId(),\n              AllocationState.ALLOCATED_FROM_RESERVED);\n        } else{\n          ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n              queue.getParent().getQueueName(), queue.getQueueName(),\n              ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n          ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n              node, reservedContainer.getContainerId(),\n              AllocationState.SKIPPED);\n        }\n      }\n\n      // Try to schedule more if there are no reservations to fulfill\n      if (node.getReservedContainer() \u003d\u003d null) {\n        if (calculator.computeAvailableContainers(Resources\n            .add(node.getUnallocatedResource(),\n                node.getTotalKillableResources()), minimumAllocation) \u003e 0) {\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Trying to schedule on node: \" + node.getNodeName()\n                + \", available: \" + node.getUnallocatedResource());\n          }\n\n          assignment \u003d root.assignContainers(getClusterResource(), node,\n              new ResourceLimits(labelManager\n                  .getResourceByLabel(node.getPartition(),\n                      getClusterResource())),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n          if (Resources.greaterThan(calculator, getClusterResource(),\n              assignment.getResource(), Resources.none())) {\n            updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n            return;\n          }\n\n          // Only do non-exclusive allocation when node has node-labels.\n          if (StringUtils.equals(node.getPartition(),\n              RMNodeLabelsManager.NO_LABEL)) {\n            return;\n          }\n\n          // Only do non-exclusive allocation when the node-label supports that\n          try {\n            if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n                node.getPartition())) {\n              return;\n            }\n          } catch (IOException e) {\n            LOG.warn(\n                \"Exception when trying to get exclusivity of node label\u003d\" + node\n                    .getPartition(), e);\n            return;\n          }\n\n          // Try to use NON_EXCLUSIVE\n          assignment \u003d root.assignContainers(getClusterResource(), node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager\n                  .getResourceByLabel(RMNodeLabelsManager.NO_LABEL,\n                      getClusterResource())),\n              SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n        }\n      } else{\n        LOG.info(\"Skipping scheduling since node \" + node.getNodeID()\n            + \" is reserved by application \" + node.getReservedContainer()\n            .getContainerId().getApplicationAttemptId());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
          "extendedDetails": {}
        }
      ]
    },
    "e0d131f055ee126052ad4d0f7b0d192e6c730188": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4091. Add REST API to retrieve scheduler activity. (Chen Ge and Sunil G via wangda)\n",
      "commitDate": "05/08/16 10:27 AM",
      "commitName": "e0d131f055ee126052ad4d0f7b0d192e6c730188",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "26/07/16 9:22 PM",
      "commitNameOld": "d62e121ffc0239e7feccc1e23ece92c5fac685f6",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 9.54,
      "commitsBetweenForRepo": 75,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,116 +1,130 @@\n   public synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n     if (!nodeTracker.exists(node.getNodeID())) {\n       LOG.info(\"Skipping scheduling as the node \" + node.getNodeID() +\n           \" has been removed\");\n       return;\n     }\n \n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n     CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n+\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \"\n           + reservedApplication.getApplicationId() + \" on node: \"\n           + node.getNodeID());\n \n       LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n       assignment \u003d\n           queue.assignContainers(\n               getClusterResource(),\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n                   RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n               SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n       if (assignment.isFulfilledReservation()) {\n         CSAssignment tmp \u003d\n             new CSAssignment(reservedContainer.getReservedResource(),\n                 assignment.getType());\n         Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n             reservedContainer.getReservedResource());\n         tmp.getAssignmentInformation().addAllocationDetails(\n             reservedContainer.getContainerId(), queue.getQueuePath());\n         tmp.getAssignmentInformation().incrAllocations();\n         updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n         schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n+\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            queue.getParent().getQueueName(), queue.getQueueName(),\n+            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n+        ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n+            node, reservedContainer.getContainerId(),\n+            AllocationState.ALLOCATED_FROM_RESERVED);\n+      } else {\n+        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n+            queue.getParent().getQueueName(), queue.getQueueName(),\n+            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n+        ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n+            node, reservedContainer.getContainerId(), AllocationState.SKIPPED);\n       }\n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (calculator.computeAvailableContainers(Resources\n               .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n           minimumAllocation) \u003e 0) {\n \n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getUnallocatedResource());\n         }\n \n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 node.getPartition(), getClusterResource())),\n             SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n         if (Resources.greaterThan(calculator, getClusterResource(),\n             assignment.getResource(), Resources.none())) {\n           updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n           return;\n         }\n         \n         // Only do non-exclusive allocation when node has node-labels.\n         if (StringUtils.equals(node.getPartition(),\n             RMNodeLabelsManager.NO_LABEL)) {\n           return;\n         }\n         \n         // Only do non-exclusive allocation when the node-label supports that\n         try {\n           if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n               node.getPartition())) {\n             return;\n           }\n         } catch (IOException e) {\n           LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n               + node.getPartition(), e);\n           return;\n         }\n         \n         // Try to use NON_EXCLUSIVE\n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n             SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n         updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \"\n           + node.getNodeID()\n           + \" is reserved by application \"\n           + node.getReservedContainer().getContainerId()\n               .getApplicationAttemptId());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    if (!nodeTracker.exists(node.getNodeID())) {\n      LOG.info(\"Skipping scheduling as the node \" + node.getNodeID() +\n          \" has been removed\");\n      return;\n    }\n\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \"\n          + reservedApplication.getApplicationId() + \" on node: \"\n          + node.getNodeID());\n\n      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n      assignment \u003d\n          queue.assignContainers(\n              getClusterResource(),\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n                assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n            reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n            reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            queue.getParent().getQueueName(), queue.getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n            node, reservedContainer.getContainerId(),\n            AllocationState.ALLOCATED_FROM_RESERVED);\n      } else {\n        ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node,\n            queue.getParent().getQueueName(), queue.getQueueName(),\n            ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);\n        ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,\n            node, reservedContainer.getContainerId(), AllocationState.SKIPPED);\n      }\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(Resources\n              .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n          minimumAllocation) \u003e 0) {\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getUnallocatedResource());\n        }\n\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            new ResourceLimits(labelManager.getResourceByLabel(\n                node.getPartition(), getClusterResource())),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (Resources.greaterThan(calculator, getClusterResource(),\n            assignment.getResource(), Resources.none())) {\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n          return;\n        }\n        \n        // Only do non-exclusive allocation when node has node-labels.\n        if (StringUtils.equals(node.getPartition(),\n            RMNodeLabelsManager.NO_LABEL)) {\n          return;\n        }\n        \n        // Only do non-exclusive allocation when the node-label supports that\n        try {\n          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n              node.getPartition())) {\n            return;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n              + node.getPartition(), e);\n          return;\n        }\n        \n        // Try to use NON_EXCLUSIVE\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \"\n          + node.getNodeID()\n          + \" is reserved by application \"\n          + node.getReservedContainer().getContainerId()\n              .getApplicationAttemptId());\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "d62e121ffc0239e7feccc1e23ece92c5fac685f6": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "YARN-5195. RM intermittently crashed with NPE while handling APP_ATTEMPT_REMOVED event when async-scheduling enabled in CapacityScheduler. (sandflee via wangda)\n",
      "commitDate": "26/07/16 9:22 PM",
      "commitName": "d62e121ffc0239e7feccc1e23ece92c5fac685f6",
      "commitAuthor": "Wangda Tan",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-5195. RM intermittently crashed with NPE while handling APP_ATTEMPT_REMOVED event when async-scheduling enabled in CapacityScheduler. (sandflee via wangda)\n",
          "commitDate": "26/07/16 9:22 PM",
          "commitName": "d62e121ffc0239e7feccc1e23ece92c5fac685f6",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "07/06/16 3:06 PM",
          "commitNameOld": "620325e81696fca140195b74929ed9eda2d5eb16",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 49.26,
          "commitsBetweenForRepo": 446,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,109 +1,116 @@\n-  protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n+  public synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n+\n+    if (!nodeTracker.exists(node.getNodeID())) {\n+      LOG.info(\"Skipping scheduling as the node \" + node.getNodeID() +\n+          \" has been removed\");\n+      return;\n+    }\n+\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n     CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \"\n           + reservedApplication.getApplicationId() + \" on node: \"\n           + node.getNodeID());\n \n       LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n       assignment \u003d\n           queue.assignContainers(\n               getClusterResource(),\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n                   RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n               SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n       if (assignment.isFulfilledReservation()) {\n         CSAssignment tmp \u003d\n             new CSAssignment(reservedContainer.getReservedResource(),\n                 assignment.getType());\n         Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n             reservedContainer.getReservedResource());\n         tmp.getAssignmentInformation().addAllocationDetails(\n             reservedContainer.getContainerId(), queue.getQueuePath());\n         tmp.getAssignmentInformation().incrAllocations();\n         updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n         schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n       }\n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (calculator.computeAvailableContainers(Resources\n               .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n           minimumAllocation) \u003e 0) {\n \n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getUnallocatedResource());\n         }\n \n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 node.getPartition(), getClusterResource())),\n             SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n         if (Resources.greaterThan(calculator, getClusterResource(),\n             assignment.getResource(), Resources.none())) {\n           updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n           return;\n         }\n         \n         // Only do non-exclusive allocation when node has node-labels.\n         if (StringUtils.equals(node.getPartition(),\n             RMNodeLabelsManager.NO_LABEL)) {\n           return;\n         }\n         \n         // Only do non-exclusive allocation when the node-label supports that\n         try {\n           if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n               node.getPartition())) {\n             return;\n           }\n         } catch (IOException e) {\n           LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n               + node.getPartition(), e);\n           return;\n         }\n         \n         // Try to use NON_EXCLUSIVE\n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n             SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n         updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \"\n           + node.getNodeID()\n           + \" is reserved by application \"\n           + node.getReservedContainer().getContainerId()\n               .getApplicationAttemptId());\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    if (!nodeTracker.exists(node.getNodeID())) {\n      LOG.info(\"Skipping scheduling as the node \" + node.getNodeID() +\n          \" has been removed\");\n      return;\n    }\n\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \"\n          + reservedApplication.getApplicationId() + \" on node: \"\n          + node.getNodeID());\n\n      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n      assignment \u003d\n          queue.assignContainers(\n              getClusterResource(),\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n                assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n            reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n            reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n      }\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(Resources\n              .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n          minimumAllocation) \u003e 0) {\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getUnallocatedResource());\n        }\n\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            new ResourceLimits(labelManager.getResourceByLabel(\n                node.getPartition(), getClusterResource())),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (Resources.greaterThan(calculator, getClusterResource(),\n            assignment.getResource(), Resources.none())) {\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n          return;\n        }\n        \n        // Only do non-exclusive allocation when node has node-labels.\n        if (StringUtils.equals(node.getPartition(),\n            RMNodeLabelsManager.NO_LABEL)) {\n          return;\n        }\n        \n        // Only do non-exclusive allocation when the node-label supports that\n        try {\n          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n              node.getPartition())) {\n            return;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n              + node.getPartition(), e);\n          return;\n        }\n        \n        // Try to use NON_EXCLUSIVE\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \"\n          + node.getNodeID()\n          + \" is reserved by application \"\n          + node.getReservedContainer().getContainerId()\n              .getApplicationAttemptId());\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
          "extendedDetails": {
            "oldValue": "[protected, synchronized]",
            "newValue": "[public, synchronized]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5195. RM intermittently crashed with NPE while handling APP_ATTEMPT_REMOVED event when async-scheduling enabled in CapacityScheduler. (sandflee via wangda)\n",
          "commitDate": "26/07/16 9:22 PM",
          "commitName": "d62e121ffc0239e7feccc1e23ece92c5fac685f6",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "07/06/16 3:06 PM",
          "commitNameOld": "620325e81696fca140195b74929ed9eda2d5eb16",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 49.26,
          "commitsBetweenForRepo": 446,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,109 +1,116 @@\n-  protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n+  public synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n+\n+    if (!nodeTracker.exists(node.getNodeID())) {\n+      LOG.info(\"Skipping scheduling as the node \" + node.getNodeID() +\n+          \" has been removed\");\n+      return;\n+    }\n+\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n     CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \"\n           + reservedApplication.getApplicationId() + \" on node: \"\n           + node.getNodeID());\n \n       LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n       assignment \u003d\n           queue.assignContainers(\n               getClusterResource(),\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n                   RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n               SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n       if (assignment.isFulfilledReservation()) {\n         CSAssignment tmp \u003d\n             new CSAssignment(reservedContainer.getReservedResource(),\n                 assignment.getType());\n         Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n             reservedContainer.getReservedResource());\n         tmp.getAssignmentInformation().addAllocationDetails(\n             reservedContainer.getContainerId(), queue.getQueuePath());\n         tmp.getAssignmentInformation().incrAllocations();\n         updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n         schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n       }\n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (calculator.computeAvailableContainers(Resources\n               .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n           minimumAllocation) \u003e 0) {\n \n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getUnallocatedResource());\n         }\n \n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 node.getPartition(), getClusterResource())),\n             SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n         if (Resources.greaterThan(calculator, getClusterResource(),\n             assignment.getResource(), Resources.none())) {\n           updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n           return;\n         }\n         \n         // Only do non-exclusive allocation when node has node-labels.\n         if (StringUtils.equals(node.getPartition(),\n             RMNodeLabelsManager.NO_LABEL)) {\n           return;\n         }\n         \n         // Only do non-exclusive allocation when the node-label supports that\n         try {\n           if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n               node.getPartition())) {\n             return;\n           }\n         } catch (IOException e) {\n           LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n               + node.getPartition(), e);\n           return;\n         }\n         \n         // Try to use NON_EXCLUSIVE\n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n             SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n         updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \"\n           + node.getNodeID()\n           + \" is reserved by application \"\n           + node.getReservedContainer().getContainerId()\n               .getApplicationAttemptId());\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    if (!nodeTracker.exists(node.getNodeID())) {\n      LOG.info(\"Skipping scheduling as the node \" + node.getNodeID() +\n          \" has been removed\");\n      return;\n    }\n\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \"\n          + reservedApplication.getApplicationId() + \" on node: \"\n          + node.getNodeID());\n\n      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n      assignment \u003d\n          queue.assignContainers(\n              getClusterResource(),\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n                assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n            reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n            reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n      }\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(Resources\n              .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n          minimumAllocation) \u003e 0) {\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getUnallocatedResource());\n        }\n\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            new ResourceLimits(labelManager.getResourceByLabel(\n                node.getPartition(), getClusterResource())),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (Resources.greaterThan(calculator, getClusterResource(),\n            assignment.getResource(), Resources.none())) {\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n          return;\n        }\n        \n        // Only do non-exclusive allocation when node has node-labels.\n        if (StringUtils.equals(node.getPartition(),\n            RMNodeLabelsManager.NO_LABEL)) {\n          return;\n        }\n        \n        // Only do non-exclusive allocation when the node-label supports that\n        try {\n          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n              node.getPartition())) {\n            return;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n              + node.getPartition(), e);\n          return;\n        }\n        \n        // Try to use NON_EXCLUSIVE\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \"\n          + node.getNodeID()\n          + \" is reserved by application \"\n          + node.getReservedContainer().getContainerId()\n              .getApplicationAttemptId());\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
          "extendedDetails": {}
        }
      ]
    },
    "ae14e5d07f1b6702a5160637438028bb03d9387e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4108. CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n\n(cherry picked from commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913)\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "ae14e5d07f1b6702a5160637438028bb03d9387e",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 5:02 PM",
      "commitNameOld": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,109 @@\n   protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n     CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \"\n           + reservedApplication.getApplicationId() + \" on node: \"\n           + node.getNodeID());\n \n       LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n       assignment \u003d\n           queue.assignContainers(\n               getClusterResource(),\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n                   RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n               SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n       if (assignment.isFulfilledReservation()) {\n         CSAssignment tmp \u003d\n             new CSAssignment(reservedContainer.getReservedResource(),\n                 assignment.getType());\n         Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n             reservedContainer.getReservedResource());\n         tmp.getAssignmentInformation().addAllocationDetails(\n             reservedContainer.getContainerId(), queue.getQueuePath());\n         tmp.getAssignmentInformation().incrAllocations();\n         updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n         schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n       }\n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n-      if (calculator.computeAvailableContainers(node.getUnallocatedResource(),\n-        minimumAllocation) \u003e 0) {\n+      if (calculator.computeAvailableContainers(Resources\n+              .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n+          minimumAllocation) \u003e 0) {\n+\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getUnallocatedResource());\n         }\n \n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n-            // TODO, now we only consider limits for parent for non-labeled\n-            // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n-                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n+                node.getPartition(), getClusterResource())),\n             SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n         if (Resources.greaterThan(calculator, getClusterResource(),\n             assignment.getResource(), Resources.none())) {\n           updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n           return;\n         }\n         \n         // Only do non-exclusive allocation when node has node-labels.\n         if (StringUtils.equals(node.getPartition(),\n             RMNodeLabelsManager.NO_LABEL)) {\n           return;\n         }\n         \n         // Only do non-exclusive allocation when the node-label supports that\n         try {\n           if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n               node.getPartition())) {\n             return;\n           }\n         } catch (IOException e) {\n           LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n               + node.getPartition(), e);\n           return;\n         }\n         \n         // Try to use NON_EXCLUSIVE\n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n             SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n         updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \"\n           + node.getNodeID()\n           + \" is reserved by application \"\n           + node.getReservedContainer().getContainerId()\n               .getApplicationAttemptId());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \"\n          + reservedApplication.getApplicationId() + \" on node: \"\n          + node.getNodeID());\n\n      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n      assignment \u003d\n          queue.assignContainers(\n              getClusterResource(),\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n                assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n            reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n            reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n      }\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(Resources\n              .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n          minimumAllocation) \u003e 0) {\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getUnallocatedResource());\n        }\n\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            new ResourceLimits(labelManager.getResourceByLabel(\n                node.getPartition(), getClusterResource())),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (Resources.greaterThan(calculator, getClusterResource(),\n            assignment.getResource(), Resources.none())) {\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n          return;\n        }\n        \n        // Only do non-exclusive allocation when node has node-labels.\n        if (StringUtils.equals(node.getPartition(),\n            RMNodeLabelsManager.NO_LABEL)) {\n          return;\n        }\n        \n        // Only do non-exclusive allocation when the node-label supports that\n        try {\n          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n              node.getPartition())) {\n            return;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n              + node.getPartition(), e);\n          return;\n        }\n        \n        // Try to use NON_EXCLUSIVE\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \"\n          + node.getNodeID()\n          + \" is reserved by application \"\n          + node.getReservedContainer().getContainerId()\n              .getApplicationAttemptId());\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "fa7a43529d529f0006c8033c2003f15b9b93f103": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\"\n\nThis reverts commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913.\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 4:59 PM",
      "commitNameOld": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,109 @@\n   protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n     CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \"\n           + reservedApplication.getApplicationId() + \" on node: \"\n           + node.getNodeID());\n \n       LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n       assignment \u003d\n           queue.assignContainers(\n               getClusterResource(),\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n                   RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n               SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n       if (assignment.isFulfilledReservation()) {\n         CSAssignment tmp \u003d\n             new CSAssignment(reservedContainer.getReservedResource(),\n                 assignment.getType());\n         Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n             reservedContainer.getReservedResource());\n         tmp.getAssignmentInformation().addAllocationDetails(\n             reservedContainer.getContainerId(), queue.getQueuePath());\n         tmp.getAssignmentInformation().incrAllocations();\n         updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n         schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n       }\n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n-      if (calculator.computeAvailableContainers(Resources\n-              .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n-          minimumAllocation) \u003e 0) {\n-\n+      if (calculator.computeAvailableContainers(node.getUnallocatedResource(),\n+        minimumAllocation) \u003e 0) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getUnallocatedResource());\n         }\n \n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n+            // TODO, now we only consider limits for parent for non-labeled\n+            // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n-                node.getPartition(), getClusterResource())),\n+                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n             SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n         if (Resources.greaterThan(calculator, getClusterResource(),\n             assignment.getResource(), Resources.none())) {\n           updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n           return;\n         }\n         \n         // Only do non-exclusive allocation when node has node-labels.\n         if (StringUtils.equals(node.getPartition(),\n             RMNodeLabelsManager.NO_LABEL)) {\n           return;\n         }\n         \n         // Only do non-exclusive allocation when the node-label supports that\n         try {\n           if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n               node.getPartition())) {\n             return;\n           }\n         } catch (IOException e) {\n           LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n               + node.getPartition(), e);\n           return;\n         }\n         \n         // Try to use NON_EXCLUSIVE\n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n             SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n         updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \"\n           + node.getNodeID()\n           + \" is reserved by application \"\n           + node.getReservedContainer().getContainerId()\n               .getApplicationAttemptId());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \"\n          + reservedApplication.getApplicationId() + \" on node: \"\n          + node.getNodeID());\n\n      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n      assignment \u003d\n          queue.assignContainers(\n              getClusterResource(),\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n                assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n            reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n            reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n      }\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(node.getUnallocatedResource(),\n        minimumAllocation) \u003e 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getUnallocatedResource());\n        }\n\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (Resources.greaterThan(calculator, getClusterResource(),\n            assignment.getResource(), Resources.none())) {\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n          return;\n        }\n        \n        // Only do non-exclusive allocation when node has node-labels.\n        if (StringUtils.equals(node.getPartition(),\n            RMNodeLabelsManager.NO_LABEL)) {\n          return;\n        }\n        \n        // Only do non-exclusive allocation when the node-label supports that\n        try {\n          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n              node.getPartition())) {\n            return;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n              + node.getPartition(), e);\n          return;\n        }\n        \n        // Try to use NON_EXCLUSIVE\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \"\n          + node.getNodeID()\n          + \" is reserved by application \"\n          + node.getReservedContainer().getContainerId()\n              .getApplicationAttemptId());\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": {
      "type": "Ybodychange",
      "commitMessage": "CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n",
      "commitDate": "16/03/16 4:59 PM",
      "commitName": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "14/03/16 2:19 PM",
      "commitNameOld": "20d389ce61eaacb5ddfb329015f50e96ad894f8d",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 2.11,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,109 @@\n   protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n     CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \"\n           + reservedApplication.getApplicationId() + \" on node: \"\n           + node.getNodeID());\n \n       LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n       assignment \u003d\n           queue.assignContainers(\n               getClusterResource(),\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n                   RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n               SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n       if (assignment.isFulfilledReservation()) {\n         CSAssignment tmp \u003d\n             new CSAssignment(reservedContainer.getReservedResource(),\n                 assignment.getType());\n         Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n             reservedContainer.getReservedResource());\n         tmp.getAssignmentInformation().addAllocationDetails(\n             reservedContainer.getContainerId(), queue.getQueuePath());\n         tmp.getAssignmentInformation().incrAllocations();\n         updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n         schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n       }\n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n-      if (calculator.computeAvailableContainers(node.getUnallocatedResource(),\n-        minimumAllocation) \u003e 0) {\n+      if (calculator.computeAvailableContainers(Resources\n+              .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n+          minimumAllocation) \u003e 0) {\n+\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getUnallocatedResource());\n         }\n \n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n-            // TODO, now we only consider limits for parent for non-labeled\n-            // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n-                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n+                node.getPartition(), getClusterResource())),\n             SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n         if (Resources.greaterThan(calculator, getClusterResource(),\n             assignment.getResource(), Resources.none())) {\n           updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n           return;\n         }\n         \n         // Only do non-exclusive allocation when node has node-labels.\n         if (StringUtils.equals(node.getPartition(),\n             RMNodeLabelsManager.NO_LABEL)) {\n           return;\n         }\n         \n         // Only do non-exclusive allocation when the node-label supports that\n         try {\n           if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n               node.getPartition())) {\n             return;\n           }\n         } catch (IOException e) {\n           LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n               + node.getPartition(), e);\n           return;\n         }\n         \n         // Try to use NON_EXCLUSIVE\n         assignment \u003d root.assignContainers(\n             getClusterResource(),\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n             SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n         updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \"\n           + node.getNodeID()\n           + \" is reserved by application \"\n           + node.getReservedContainer().getContainerId()\n               .getApplicationAttemptId());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \"\n          + reservedApplication.getApplicationId() + \" on node: \"\n          + node.getNodeID());\n\n      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n      assignment \u003d\n          queue.assignContainers(\n              getClusterResource(),\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n                assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n            reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n            reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n      }\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(Resources\n              .add(node.getUnallocatedResource(), node.getTotalKillableResources()),\n          minimumAllocation) \u003e 0) {\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getUnallocatedResource());\n        }\n\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            new ResourceLimits(labelManager.getResourceByLabel(\n                node.getPartition(), getClusterResource())),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (Resources.greaterThan(calculator, getClusterResource(),\n            assignment.getResource(), Resources.none())) {\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n          return;\n        }\n        \n        // Only do non-exclusive allocation when node has node-labels.\n        if (StringUtils.equals(node.getPartition(),\n            RMNodeLabelsManager.NO_LABEL)) {\n          return;\n        }\n        \n        // Only do non-exclusive allocation when the node-label supports that\n        try {\n          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n              node.getPartition())) {\n            return;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n              + node.getPartition(), e);\n          return;\n        }\n        \n        // Try to use NON_EXCLUSIVE\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \"\n          + node.getNodeID()\n          + \" is reserved by application \"\n          + node.getReservedContainer().getContainerId()\n              .getApplicationAttemptId());\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "20d389ce61eaacb5ddfb329015f50e96ad894f8d": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4719. Add a helper library to maintain node state and allows common queries. (kasha)\n",
      "commitDate": "14/03/16 2:19 PM",
      "commitName": "20d389ce61eaacb5ddfb329015f50e96ad894f8d",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "01/03/16 1:14 PM",
      "commitNameOld": "5c465df90414d43250d09084748ab2d41af44eea",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 13.0,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,109 @@\n   protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n     CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \"\n           + reservedApplication.getApplicationId() + \" on node: \"\n           + node.getNodeID());\n \n       LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n       assignment \u003d\n           queue.assignContainers(\n-              clusterResource,\n+              getClusterResource(),\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n-                  RMNodeLabelsManager.NO_LABEL, clusterResource)),\n+                  RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n               SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n       if (assignment.isFulfilledReservation()) {\n         CSAssignment tmp \u003d\n             new CSAssignment(reservedContainer.getReservedResource(),\n                 assignment.getType());\n         Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n             reservedContainer.getReservedResource());\n         tmp.getAssignmentInformation().addAllocationDetails(\n             reservedContainer.getContainerId(), queue.getQueuePath());\n         tmp.getAssignmentInformation().incrAllocations();\n         updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n         schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n       }\n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (calculator.computeAvailableContainers(node.getUnallocatedResource(),\n         minimumAllocation) \u003e 0) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getUnallocatedResource());\n         }\n \n         assignment \u003d root.assignContainers(\n-            clusterResource,\n+            getClusterResource(),\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n-                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n+                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n             SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n-        if (Resources.greaterThan(calculator, clusterResource,\n+        if (Resources.greaterThan(calculator, getClusterResource(),\n             assignment.getResource(), Resources.none())) {\n           updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n           return;\n         }\n         \n         // Only do non-exclusive allocation when node has node-labels.\n         if (StringUtils.equals(node.getPartition(),\n             RMNodeLabelsManager.NO_LABEL)) {\n           return;\n         }\n         \n         // Only do non-exclusive allocation when the node-label supports that\n         try {\n           if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n               node.getPartition())) {\n             return;\n           }\n         } catch (IOException e) {\n           LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n               + node.getPartition(), e);\n           return;\n         }\n         \n         // Try to use NON_EXCLUSIVE\n         assignment \u003d root.assignContainers(\n-            clusterResource,\n+            getClusterResource(),\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n-                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n+                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n             SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n         updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \"\n           + node.getNodeID()\n           + \" is reserved by application \"\n           + node.getReservedContainer().getContainerId()\n               .getApplicationAttemptId());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \"\n          + reservedApplication.getApplicationId() + \" on node: \"\n          + node.getNodeID());\n\n      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n      assignment \u003d\n          queue.assignContainers(\n              getClusterResource(),\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n                assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n            reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n            reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n      }\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(node.getUnallocatedResource(),\n        minimumAllocation) \u003e 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getUnallocatedResource());\n        }\n\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (Resources.greaterThan(calculator, getClusterResource(),\n            assignment.getResource(), Resources.none())) {\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n          return;\n        }\n        \n        // Only do non-exclusive allocation when node has node-labels.\n        if (StringUtils.equals(node.getPartition(),\n            RMNodeLabelsManager.NO_LABEL)) {\n          return;\n        }\n        \n        // Only do non-exclusive allocation when the node-label supports that\n        try {\n          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n              node.getPartition())) {\n            return;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n              + node.getPartition(), e);\n          return;\n        }\n        \n        // Try to use NON_EXCLUSIVE\n        assignment \u003d root.assignContainers(\n            getClusterResource(),\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, getClusterResource())),\n            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \"\n          + node.getNodeID()\n          + \" is reserved by application \"\n          + node.getReservedContainer().getContainerId()\n              .getApplicationAttemptId());\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "f9692770a58af0ab082eb7f15da9cbdcd177605b": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4718. Rename variables in SchedulerNode to reduce ambiguity post YARN-1011. (Inigo Goiri via kasha)\n",
      "commitDate": "28/02/16 9:35 AM",
      "commitName": "f9692770a58af0ab082eb7f15da9cbdcd177605b",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "23/02/16 3:30 AM",
      "commitNameOld": "9ed17f181d96b8719a0ef54a129081948781d57e",
      "commitAuthorOld": "Junping Du",
      "daysBetweenCommits": 5.25,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,109 @@\n   protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n     CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \"\n           + reservedApplication.getApplicationId() + \" on node: \"\n           + node.getNodeID());\n \n       LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n       assignment \u003d\n           queue.assignContainers(\n               clusterResource,\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n                   RMNodeLabelsManager.NO_LABEL, clusterResource)),\n               SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n       if (assignment.isFulfilledReservation()) {\n         CSAssignment tmp \u003d\n             new CSAssignment(reservedContainer.getReservedResource(),\n                 assignment.getType());\n         Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n             reservedContainer.getReservedResource());\n         tmp.getAssignmentInformation().addAllocationDetails(\n             reservedContainer.getContainerId(), queue.getQueuePath());\n         tmp.getAssignmentInformation().incrAllocations();\n         updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n         schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n       }\n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n-      if (calculator.computeAvailableContainers(node.getAvailableResource(),\n+      if (calculator.computeAvailableContainers(node.getUnallocatedResource(),\n         minimumAllocation) \u003e 0) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n-              \", available: \" + node.getAvailableResource());\n+              \", available: \" + node.getUnallocatedResource());\n         }\n \n         assignment \u003d root.assignContainers(\n             clusterResource,\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, clusterResource)),\n             SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n         if (Resources.greaterThan(calculator, clusterResource,\n             assignment.getResource(), Resources.none())) {\n           updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n           return;\n         }\n         \n         // Only do non-exclusive allocation when node has node-labels.\n         if (StringUtils.equals(node.getPartition(),\n             RMNodeLabelsManager.NO_LABEL)) {\n           return;\n         }\n         \n         // Only do non-exclusive allocation when the node-label supports that\n         try {\n           if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n               node.getPartition())) {\n             return;\n           }\n         } catch (IOException e) {\n           LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n               + node.getPartition(), e);\n           return;\n         }\n         \n         // Try to use NON_EXCLUSIVE\n         assignment \u003d root.assignContainers(\n             clusterResource,\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, clusterResource)),\n             SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n         updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \"\n           + node.getNodeID()\n           + \" is reserved by application \"\n           + node.getReservedContainer().getContainerId()\n               .getApplicationAttemptId());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \"\n          + reservedApplication.getApplicationId() + \" on node: \"\n          + node.getNodeID());\n\n      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n      assignment \u003d\n          queue.assignContainers(\n              clusterResource,\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, clusterResource)),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n                assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n            reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n            reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n      }\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(node.getUnallocatedResource(),\n        minimumAllocation) \u003e 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getUnallocatedResource());\n        }\n\n        assignment \u003d root.assignContainers(\n            clusterResource,\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (Resources.greaterThan(calculator, clusterResource,\n            assignment.getResource(), Resources.none())) {\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n          return;\n        }\n        \n        // Only do non-exclusive allocation when node has node-labels.\n        if (StringUtils.equals(node.getPartition(),\n            RMNodeLabelsManager.NO_LABEL)) {\n          return;\n        }\n        \n        // Only do non-exclusive allocation when the node-label supports that\n        try {\n          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n              node.getPartition())) {\n            return;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n              + node.getPartition(), e);\n          return;\n        }\n        \n        // Try to use NON_EXCLUSIVE\n        assignment \u003d root.assignContainers(\n            clusterResource,\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \"\n          + node.getNodeID()\n          + \" is reserved by application \"\n          + node.getReservedContainer().getContainerId()\n              .getApplicationAttemptId());\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "7f46636495e23693d588b0915f464fa7afd9102e": {
      "type": "Ymodifierchange",
      "commitMessage": "YARN-4519. Potential deadlock of CapacityScheduler between decrease container and assign containers. Contributed by Meng Ding\n",
      "commitDate": "28/01/16 2:51 PM",
      "commitName": "7f46636495e23693d588b0915f464fa7afd9102e",
      "commitAuthor": "Jian He",
      "commitDateOld": "18/01/16 9:04 PM",
      "commitNameOld": "edc43a9097530fd469dee47d4fefd091818331e5",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 9.74,
      "commitsBetweenForRepo": 75,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,109 @@\n-  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n+  protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n     CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \"\n           + reservedApplication.getApplicationId() + \" on node: \"\n           + node.getNodeID());\n \n       LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n       assignment \u003d\n           queue.assignContainers(\n               clusterResource,\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n                   RMNodeLabelsManager.NO_LABEL, clusterResource)),\n               SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n       if (assignment.isFulfilledReservation()) {\n         CSAssignment tmp \u003d\n             new CSAssignment(reservedContainer.getReservedResource(),\n                 assignment.getType());\n         Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n             reservedContainer.getReservedResource());\n         tmp.getAssignmentInformation().addAllocationDetails(\n             reservedContainer.getContainerId(), queue.getQueuePath());\n         tmp.getAssignmentInformation().incrAllocations();\n         updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n         schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n       }\n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (calculator.computeAvailableContainers(node.getAvailableResource(),\n         minimumAllocation) \u003e 0) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getAvailableResource());\n         }\n \n         assignment \u003d root.assignContainers(\n             clusterResource,\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, clusterResource)),\n             SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n         if (Resources.greaterThan(calculator, clusterResource,\n             assignment.getResource(), Resources.none())) {\n           updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n           return;\n         }\n         \n         // Only do non-exclusive allocation when node has node-labels.\n         if (StringUtils.equals(node.getPartition(),\n             RMNodeLabelsManager.NO_LABEL)) {\n           return;\n         }\n         \n         // Only do non-exclusive allocation when the node-label supports that\n         try {\n           if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n               node.getPartition())) {\n             return;\n           }\n         } catch (IOException e) {\n           LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n               + node.getPartition(), e);\n           return;\n         }\n         \n         // Try to use NON_EXCLUSIVE\n         assignment \u003d root.assignContainers(\n             clusterResource,\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, clusterResource)),\n             SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n         updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \"\n           + node.getNodeID()\n           + \" is reserved by application \"\n           + node.getReservedContainer().getContainerId()\n               .getApplicationAttemptId());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \"\n          + reservedApplication.getApplicationId() + \" on node: \"\n          + node.getNodeID());\n\n      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n      assignment \u003d\n          queue.assignContainers(\n              clusterResource,\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, clusterResource)),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n                assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n            reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n            reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n      }\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(node.getAvailableResource(),\n        minimumAllocation) \u003e 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getAvailableResource());\n        }\n\n        assignment \u003d root.assignContainers(\n            clusterResource,\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (Resources.greaterThan(calculator, clusterResource,\n            assignment.getResource(), Resources.none())) {\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n          return;\n        }\n        \n        // Only do non-exclusive allocation when node has node-labels.\n        if (StringUtils.equals(node.getPartition(),\n            RMNodeLabelsManager.NO_LABEL)) {\n          return;\n        }\n        \n        // Only do non-exclusive allocation when the node-label supports that\n        try {\n          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n              node.getPartition())) {\n            return;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n              + node.getPartition(), e);\n          return;\n        }\n        \n        // Try to use NON_EXCLUSIVE\n        assignment \u003d root.assignContainers(\n            clusterResource,\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \"\n          + node.getNodeID()\n          + \" is reserved by application \"\n          + node.getReservedContainer().getContainerId()\n              .getApplicationAttemptId());\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {
        "oldValue": "[private, synchronized]",
        "newValue": "[protected, synchronized]"
      }
    },
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
      "commitDate": "24/07/15 2:00 PM",
      "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
      "commitAuthor": "Jian He",
      "commitDateOld": "21/07/15 9:57 AM",
      "commitNameOld": "c39ca541f498712133890961598bbff50d89d68b",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 3.17,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,123 +1,109 @@\n   private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n     CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \"\n           + reservedApplication.getApplicationId() + \" on node: \"\n           + node.getNodeID());\n \n       LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n       assignment \u003d\n           queue.assignContainers(\n               clusterResource,\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n                   RMNodeLabelsManager.NO_LABEL, clusterResource)),\n               SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n       if (assignment.isFulfilledReservation()) {\n         CSAssignment tmp \u003d\n             new CSAssignment(reservedContainer.getReservedResource(),\n                 assignment.getType());\n         Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n             reservedContainer.getReservedResource());\n         tmp.getAssignmentInformation().addAllocationDetails(\n             reservedContainer.getContainerId(), queue.getQueuePath());\n         tmp.getAssignmentInformation().incrAllocations();\n         updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n         schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n       }\n-\n-      RMContainer excessReservation \u003d assignment.getExcessReservation();\n-      if (excessReservation !\u003d null) {\n-        Container container \u003d excessReservation.getContainer();\n-        queue.completedContainer(clusterResource, assignment.getApplication(),\n-            node, excessReservation, SchedulerUtils\n-                .createAbnormalContainerStatus(container.getId(),\n-                    SchedulerUtils.UNRESERVED_CONTAINER),\n-            RMContainerEventType.RELEASED, null, true);\n-      }\n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (calculator.computeAvailableContainers(node.getAvailableResource(),\n         minimumAllocation) \u003e 0) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getAvailableResource());\n         }\n \n         assignment \u003d root.assignContainers(\n             clusterResource,\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, clusterResource)),\n             SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n         if (Resources.greaterThan(calculator, clusterResource,\n             assignment.getResource(), Resources.none())) {\n           updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n           return;\n         }\n         \n         // Only do non-exclusive allocation when node has node-labels.\n         if (StringUtils.equals(node.getPartition(),\n             RMNodeLabelsManager.NO_LABEL)) {\n           return;\n         }\n         \n         // Only do non-exclusive allocation when the node-label supports that\n         try {\n           if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n               node.getPartition())) {\n             return;\n           }\n         } catch (IOException e) {\n           LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n               + node.getPartition(), e);\n           return;\n         }\n         \n         // Try to use NON_EXCLUSIVE\n         assignment \u003d root.assignContainers(\n             clusterResource,\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, clusterResource)),\n             SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n         updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n-        if (Resources.greaterThan(calculator, clusterResource,\n-            assignment.getResource(), Resources.none())) {\n-          return;\n-        }\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \"\n           + node.getNodeID()\n           + \" is reserved by application \"\n           + node.getReservedContainer().getContainerId()\n               .getApplicationAttemptId());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \"\n          + reservedApplication.getApplicationId() + \" on node: \"\n          + node.getNodeID());\n\n      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n      assignment \u003d\n          queue.assignContainers(\n              clusterResource,\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, clusterResource)),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n                assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n            reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n            reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n      }\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(node.getAvailableResource(),\n        minimumAllocation) \u003e 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getAvailableResource());\n        }\n\n        assignment \u003d root.assignContainers(\n            clusterResource,\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (Resources.greaterThan(calculator, clusterResource,\n            assignment.getResource(), Resources.none())) {\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n          return;\n        }\n        \n        // Only do non-exclusive allocation when node has node-labels.\n        if (StringUtils.equals(node.getPartition(),\n            RMNodeLabelsManager.NO_LABEL)) {\n          return;\n        }\n        \n        // Only do non-exclusive allocation when the node-label supports that\n        try {\n          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n              node.getPartition())) {\n            return;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n              + node.getPartition(), e);\n          return;\n        }\n        \n        // Try to use NON_EXCLUSIVE\n        assignment \u003d root.assignContainers(\n            clusterResource,\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \"\n          + node.getNodeID()\n          + \" is reserved by application \"\n          + node.getReservedContainer().getContainerId()\n              .getApplicationAttemptId());\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3361. CapacityScheduler side changes to support non-exclusive node labels. Contributed by Wangda Tan\n",
      "commitDate": "14/04/15 11:45 AM",
      "commitName": "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
      "commitAuthor": "Jian He",
      "commitDateOld": "09/04/15 11:38 PM",
      "commitNameOld": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
      "commitAuthorOld": "Xuan",
      "daysBetweenCommits": 4.51,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,123 @@\n   private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n \n     CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n-      \n+\n       // Try to fulfill the reservation\n-      LOG.info(\"Trying to fulfill reservation for application \" + \n-          reservedApplication.getApplicationId() + \" on node: \" + \n-          node.getNodeID());\n-      \n-      LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n-      assignment \u003d queue.assignContainers(\n+      LOG.info(\"Trying to fulfill reservation for application \"\n+          + reservedApplication.getApplicationId() + \" on node: \"\n+          + node.getNodeID());\n+\n+      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n+      assignment \u003d\n+          queue.assignContainers(\n               clusterResource,\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n-                  RMNodeLabelsManager.NO_LABEL, clusterResource)));\n+                  RMNodeLabelsManager.NO_LABEL, clusterResource)),\n+              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n       if (assignment.isFulfilledReservation()) {\n         CSAssignment tmp \u003d\n             new CSAssignment(reservedContainer.getReservedResource(),\n-              assignment.getType());\n+                assignment.getType());\n         Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-          reservedContainer.getReservedResource());\n+            reservedContainer.getReservedResource());\n         tmp.getAssignmentInformation().addAllocationDetails(\n-          reservedContainer.getContainerId(), queue.getQueuePath());\n+            reservedContainer.getContainerId(), queue.getQueuePath());\n         tmp.getAssignmentInformation().incrAllocations();\n         updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n         schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n       }\n \n       RMContainer excessReservation \u003d assignment.getExcessReservation();\n       if (excessReservation !\u003d null) {\n-      Container container \u003d excessReservation.getContainer();\n-      queue.completedContainer(\n-          clusterResource, assignment.getApplication(), node, \n-          excessReservation, \n-          SchedulerUtils.createAbnormalContainerStatus(\n-              container.getId(), \n-              SchedulerUtils.UNRESERVED_CONTAINER), \n-          RMContainerEventType.RELEASED, null, true);\n+        Container container \u003d excessReservation.getContainer();\n+        queue.completedContainer(clusterResource, assignment.getApplication(),\n+            node, excessReservation, SchedulerUtils\n+                .createAbnormalContainerStatus(container.getId(),\n+                    SchedulerUtils.UNRESERVED_CONTAINER),\n+            RMContainerEventType.RELEASED, null, true);\n       }\n-\n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (calculator.computeAvailableContainers(node.getAvailableResource(),\n         minimumAllocation) \u003e 0) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getAvailableResource());\n         }\n+\n         assignment \u003d root.assignContainers(\n             clusterResource,\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n-                RMNodeLabelsManager.NO_LABEL, clusterResource)));\n+                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n+            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n+        if (Resources.greaterThan(calculator, clusterResource,\n+            assignment.getResource(), Resources.none())) {\n+          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n+          return;\n+        }\n+        \n+        // Only do non-exclusive allocation when node has node-labels.\n+        if (StringUtils.equals(node.getPartition(),\n+            RMNodeLabelsManager.NO_LABEL)) {\n+          return;\n+        }\n+        \n+        // Only do non-exclusive allocation when the node-label supports that\n+        try {\n+          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n+              node.getPartition())) {\n+            return;\n+          }\n+        } catch (IOException e) {\n+          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n+              + node.getPartition(), e);\n+          return;\n+        }\n+        \n+        // Try to use NON_EXCLUSIVE\n+        assignment \u003d root.assignContainers(\n+            clusterResource,\n+            node,\n+            // TODO, now we only consider limits for parent for non-labeled\n+            // resources, should consider labeled resources as well.\n+            new ResourceLimits(labelManager.getResourceByLabel(\n+                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n+            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n         updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n+        if (Resources.greaterThan(calculator, clusterResource,\n+            assignment.getResource(), Resources.none())) {\n+          return;\n+        }\n       }\n     } else {\n-      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n-          \" is reserved by application \" + \n-          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n-          );\n+      LOG.info(\"Skipping scheduling since node \"\n+          + node.getNodeID()\n+          + \" is reserved by application \"\n+          + node.getReservedContainer().getContainerId()\n+              .getApplicationAttemptId());\n     }\n-  \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \"\n          + reservedApplication.getApplicationId() + \" on node: \"\n          + node.getNodeID());\n\n      LeafQueue queue \u003d ((LeafQueue) reservedApplication.getQueue());\n      assignment \u003d\n          queue.assignContainers(\n              clusterResource,\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, clusterResource)),\n              SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n                assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n            reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n            reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n      }\n\n      RMContainer excessReservation \u003d assignment.getExcessReservation();\n      if (excessReservation !\u003d null) {\n        Container container \u003d excessReservation.getContainer();\n        queue.completedContainer(clusterResource, assignment.getApplication(),\n            node, excessReservation, SchedulerUtils\n                .createAbnormalContainerStatus(container.getId(),\n                    SchedulerUtils.UNRESERVED_CONTAINER),\n            RMContainerEventType.RELEASED, null, true);\n      }\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(node.getAvailableResource(),\n        minimumAllocation) \u003e 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getAvailableResource());\n        }\n\n        assignment \u003d root.assignContainers(\n            clusterResource,\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);\n        if (Resources.greaterThan(calculator, clusterResource,\n            assignment.getResource(), Resources.none())) {\n          updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n          return;\n        }\n        \n        // Only do non-exclusive allocation when node has node-labels.\n        if (StringUtils.equals(node.getPartition(),\n            RMNodeLabelsManager.NO_LABEL)) {\n          return;\n        }\n        \n        // Only do non-exclusive allocation when the node-label supports that\n        try {\n          if (rmContext.getNodeLabelManager().isExclusiveNodeLabel(\n              node.getPartition())) {\n            return;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Exception when trying to get exclusivity of node label\u003d\"\n              + node.getPartition(), e);\n          return;\n        }\n        \n        // Try to use NON_EXCLUSIVE\n        assignment \u003d root.assignContainers(\n            clusterResource,\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)),\n            SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY);\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n        if (Resources.greaterThan(calculator, clusterResource,\n            assignment.getResource(), Resources.none())) {\n          return;\n        }\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \"\n          + node.getNodeID()\n          + \" is reserved by application \"\n          + node.getReservedContainer().getContainerId()\n              .getApplicationAttemptId());\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "afa5d4715a3aea2a6e93380b014c7bb8f0880383": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3293. Track and display capacity scheduler health metrics in web\nUI. Contributed by Varun Vasudev\n",
      "commitDate": "09/04/15 11:38 PM",
      "commitName": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
      "commitAuthor": "Xuan",
      "commitDateOld": "17/03/15 10:24 AM",
      "commitNameOld": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 23.55,
      "commitsBetweenForRepo": 232,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,85 @@\n   private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n+    // reset allocation and reservation stats before we start doing any work\n+    updateSchedulerHealth(lastNodeUpdateTime, node,\n+      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n+\n+    CSAssignment assignment;\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n       \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \" + \n           reservedApplication.getApplicationId() + \" on node: \" + \n           node.getNodeID());\n       \n       LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n-      CSAssignment assignment \u003d\n-          queue.assignContainers(\n+      assignment \u003d queue.assignContainers(\n               clusterResource,\n               node,\n               // TODO, now we only consider limits for parent for non-labeled\n               // resources, should consider labeled resources as well.\n               new ResourceLimits(labelManager.getResourceByLabel(\n                   RMNodeLabelsManager.NO_LABEL, clusterResource)));\n-      \n+      if (assignment.isFulfilledReservation()) {\n+        CSAssignment tmp \u003d\n+            new CSAssignment(reservedContainer.getReservedResource(),\n+              assignment.getType());\n+        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n+          reservedContainer.getReservedResource());\n+        tmp.getAssignmentInformation().addAllocationDetails(\n+          reservedContainer.getContainerId(), queue.getQueuePath());\n+        tmp.getAssignmentInformation().incrAllocations();\n+        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n+        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n+      }\n+\n       RMContainer excessReservation \u003d assignment.getExcessReservation();\n       if (excessReservation !\u003d null) {\n       Container container \u003d excessReservation.getContainer();\n       queue.completedContainer(\n           clusterResource, assignment.getApplication(), node, \n           excessReservation, \n           SchedulerUtils.createAbnormalContainerStatus(\n               container.getId(), \n               SchedulerUtils.UNRESERVED_CONTAINER), \n           RMContainerEventType.RELEASED, null, true);\n       }\n \n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (calculator.computeAvailableContainers(node.getAvailableResource(),\n         minimumAllocation) \u003e 0) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getAvailableResource());\n         }\n-        root.assignContainers(\n+        assignment \u003d root.assignContainers(\n             clusterResource,\n             node,\n             // TODO, now we only consider limits for parent for non-labeled\n             // resources, should consider labeled resources as well.\n             new ResourceLimits(labelManager.getResourceByLabel(\n                 RMNodeLabelsManager.NO_LABEL, clusterResource)));\n+        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n           \" is reserved by application \" + \n           node.getReservedContainer().getContainerId().getApplicationAttemptId()\n           );\n     }\n   \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n    // reset allocation and reservation stats before we start doing any work\n    updateSchedulerHealth(lastNodeUpdateTime, node,\n      new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));\n\n    CSAssignment assignment;\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + \n          node.getNodeID());\n      \n      LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n      assignment \u003d queue.assignContainers(\n              clusterResource,\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, clusterResource)));\n      if (assignment.isFulfilledReservation()) {\n        CSAssignment tmp \u003d\n            new CSAssignment(reservedContainer.getReservedResource(),\n              assignment.getType());\n        Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n          reservedContainer.getReservedResource());\n        tmp.getAssignmentInformation().addAllocationDetails(\n          reservedContainer.getContainerId(), queue.getQueuePath());\n        tmp.getAssignmentInformation().incrAllocations();\n        updateSchedulerHealth(lastNodeUpdateTime, node, tmp);\n        schedulerHealth.updateSchedulerFulfilledReservationCounts(1);\n      }\n\n      RMContainer excessReservation \u003d assignment.getExcessReservation();\n      if (excessReservation !\u003d null) {\n      Container container \u003d excessReservation.getContainer();\n      queue.completedContainer(\n          clusterResource, assignment.getApplication(), node, \n          excessReservation, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED, null, true);\n      }\n\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(node.getAvailableResource(),\n        minimumAllocation) \u003e 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getAvailableResource());\n        }\n        assignment \u003d root.assignContainers(\n            clusterResource,\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)));\n        updateSchedulerHealth(lastNodeUpdateTime, node, assignment);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "487374b7fe0c92fc7eb1406c568952722b5d5b15": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
      "commitDate": "17/03/15 10:24 AM",
      "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
      "commitAuthor": "Jian He",
      "commitDateOld": "17/03/15 3:27 AM",
      "commitNameOld": "7179f94f9d000fc52bd9ce5aa9741aba97ec3ee8",
      "commitAuthorOld": "Devaraj K",
      "daysBetweenCommits": 0.29,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,68 @@\n   private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n       \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \" + \n           reservedApplication.getApplicationId() + \" on node: \" + \n           node.getNodeID());\n       \n       LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n-      CSAssignment assignment \u003d queue.assignContainers(clusterResource, node,\n-          false, new ResourceLimits(\n-              clusterResource));\n+      CSAssignment assignment \u003d\n+          queue.assignContainers(\n+              clusterResource,\n+              node,\n+              // TODO, now we only consider limits for parent for non-labeled\n+              // resources, should consider labeled resources as well.\n+              new ResourceLimits(labelManager.getResourceByLabel(\n+                  RMNodeLabelsManager.NO_LABEL, clusterResource)));\n       \n       RMContainer excessReservation \u003d assignment.getExcessReservation();\n       if (excessReservation !\u003d null) {\n       Container container \u003d excessReservation.getContainer();\n       queue.completedContainer(\n           clusterResource, assignment.getApplication(), node, \n           excessReservation, \n           SchedulerUtils.createAbnormalContainerStatus(\n               container.getId(), \n               SchedulerUtils.UNRESERVED_CONTAINER), \n           RMContainerEventType.RELEASED, null, true);\n       }\n \n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (calculator.computeAvailableContainers(node.getAvailableResource(),\n         minimumAllocation) \u003e 0) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getAvailableResource());\n         }\n-        root.assignContainers(clusterResource, node, false, new ResourceLimits(\n-            clusterResource));\n+        root.assignContainers(\n+            clusterResource,\n+            node,\n+            // TODO, now we only consider limits for parent for non-labeled\n+            // resources, should consider labeled resources as well.\n+            new ResourceLimits(labelManager.getResourceByLabel(\n+                RMNodeLabelsManager.NO_LABEL, clusterResource)));\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n           \" is reserved by application \" + \n           node.getReservedContainer().getContainerId().getApplicationAttemptId()\n           );\n     }\n   \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + \n          node.getNodeID());\n      \n      LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n      CSAssignment assignment \u003d\n          queue.assignContainers(\n              clusterResource,\n              node,\n              // TODO, now we only consider limits for parent for non-labeled\n              // resources, should consider labeled resources as well.\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, clusterResource)));\n      \n      RMContainer excessReservation \u003d assignment.getExcessReservation();\n      if (excessReservation !\u003d null) {\n      Container container \u003d excessReservation.getContainer();\n      queue.completedContainer(\n          clusterResource, assignment.getApplication(), node, \n          excessReservation, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED, null, true);\n      }\n\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(node.getAvailableResource(),\n        minimumAllocation) \u003e 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getAvailableResource());\n        }\n        root.assignContainers(\n            clusterResource,\n            node,\n            // TODO, now we only consider limits for parent for non-labeled\n            // resources, should consider labeled resources as well.\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)));\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "14dd647c556016d351f425ee956ccf800ccb9ce2": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3265. Fixed a deadlock in CapacityScheduler by always passing a queue\u0027s available resource-limit from the parent queue. Contributed by Wangda Tan.\n",
      "commitDate": "02/03/15 5:52 PM",
      "commitName": "14dd647c556016d351f425ee956ccf800ccb9ce2",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "09/02/15 8:34 PM",
      "commitNameOld": "23bf6c72071782e3fd5a628e21495d6b974c7a9e",
      "commitAuthorOld": "Zhijie Shen",
      "daysBetweenCommits": 20.89,
      "commitsBetweenForRepo": 211,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,58 @@\n   private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n       \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \" + \n           reservedApplication.getApplicationId() + \" on node: \" + \n           node.getNodeID());\n       \n       LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n       CSAssignment assignment \u003d queue.assignContainers(clusterResource, node,\n-          false);\n+          false, new ResourceLimits(\n+              clusterResource));\n       \n       RMContainer excessReservation \u003d assignment.getExcessReservation();\n       if (excessReservation !\u003d null) {\n       Container container \u003d excessReservation.getContainer();\n       queue.completedContainer(\n           clusterResource, assignment.getApplication(), node, \n           excessReservation, \n           SchedulerUtils.createAbnormalContainerStatus(\n               container.getId(), \n               SchedulerUtils.UNRESERVED_CONTAINER), \n           RMContainerEventType.RELEASED, null, true);\n       }\n \n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (calculator.computeAvailableContainers(node.getAvailableResource(),\n         minimumAllocation) \u003e 0) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getAvailableResource());\n         }\n-        root.assignContainers(clusterResource, node, false);\n+        root.assignContainers(clusterResource, node, false, new ResourceLimits(\n+            clusterResource));\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n           \" is reserved by application \" + \n           node.getReservedContainer().getContainerId().getApplicationAttemptId()\n           );\n     }\n   \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + \n          node.getNodeID());\n      \n      LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n      CSAssignment assignment \u003d queue.assignContainers(clusterResource, node,\n          false, new ResourceLimits(\n              clusterResource));\n      \n      RMContainer excessReservation \u003d assignment.getExcessReservation();\n      if (excessReservation !\u003d null) {\n      Container container \u003d excessReservation.getContainer();\n      queue.completedContainer(\n          clusterResource, assignment.getApplication(), node, \n          excessReservation, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED, null, true);\n      }\n\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(node.getAvailableResource(),\n        minimumAllocation) \u003e 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getAvailableResource());\n        }\n        root.assignContainers(clusterResource, node, false, new ResourceLimits(\n            clusterResource));\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "054f28552687e9b9859c0126e16a2066e20ead3f": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2628. Capacity scheduler with DominantResourceCalculator carries out reservation even though slots are free. Contributed by Varun Vasudev\n",
      "commitDate": "02/10/14 3:13 PM",
      "commitName": "054f28552687e9b9859c0126e16a2066e20ead3f",
      "commitAuthor": "Jian He",
      "commitDateOld": "29/09/14 7:12 AM",
      "commitNameOld": "9c22065109a77681bc2534063eabe8692fbcb3cd",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 3.33,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n   private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n       \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \" + \n           reservedApplication.getApplicationId() + \" on node: \" + \n           node.getNodeID());\n       \n       LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n       CSAssignment assignment \u003d queue.assignContainers(clusterResource, node,\n           false);\n       \n       RMContainer excessReservation \u003d assignment.getExcessReservation();\n       if (excessReservation !\u003d null) {\n       Container container \u003d excessReservation.getContainer();\n       queue.completedContainer(\n           clusterResource, assignment.getApplication(), node, \n           excessReservation, \n           SchedulerUtils.createAbnormalContainerStatus(\n               container.getId(), \n               SchedulerUtils.UNRESERVED_CONTAINER), \n           RMContainerEventType.RELEASED, null, true);\n       }\n \n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n-      if (Resources.greaterThanOrEqual(calculator, getClusterResource(),\n-          node.getAvailableResource(), minimumAllocation)) {\n+      if (calculator.computeAvailableContainers(node.getAvailableResource(),\n+        minimumAllocation) \u003e 0) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getAvailableResource());\n         }\n         root.assignContainers(clusterResource, node, false);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n           \" is reserved by application \" + \n           node.getReservedContainer().getContainerId().getApplicationAttemptId()\n           );\n     }\n   \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + \n          node.getNodeID());\n      \n      LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n      CSAssignment assignment \u003d queue.assignContainers(clusterResource, node,\n          false);\n      \n      RMContainer excessReservation \u003d assignment.getExcessReservation();\n      if (excessReservation !\u003d null) {\n      Container container \u003d excessReservation.getContainer();\n      queue.completedContainer(\n          clusterResource, assignment.getApplication(), node, \n          excessReservation, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED, null, true);\n      }\n\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (calculator.computeAvailableContainers(node.getAvailableResource(),\n        minimumAllocation) \u003e 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getAvailableResource());\n        }\n        root.assignContainers(clusterResource, node, false);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "9c22065109a77681bc2534063eabe8692fbcb3cd": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1769. CapacityScheduler: Improve reservations. Contributed by Thomas Graves\n",
      "commitDate": "29/09/14 7:12 AM",
      "commitName": "9c22065109a77681bc2534063eabe8692fbcb3cd",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "18/09/14 11:03 AM",
      "commitNameOld": "485c96e3cb9b0b05d6e490b4773506da83ebc61d",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 10.84,
      "commitsBetweenForRepo": 109,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,56 @@\n   private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n       \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \" + \n           reservedApplication.getApplicationId() + \" on node: \" + \n           node.getNodeID());\n       \n       LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n-      CSAssignment assignment \u003d queue.assignContainers(clusterResource, node);\n+      CSAssignment assignment \u003d queue.assignContainers(clusterResource, node,\n+          false);\n       \n       RMContainer excessReservation \u003d assignment.getExcessReservation();\n       if (excessReservation !\u003d null) {\n       Container container \u003d excessReservation.getContainer();\n       queue.completedContainer(\n           clusterResource, assignment.getApplication(), node, \n           excessReservation, \n           SchedulerUtils.createAbnormalContainerStatus(\n               container.getId(), \n               SchedulerUtils.UNRESERVED_CONTAINER), \n-          RMContainerEventType.RELEASED, null);\n+          RMContainerEventType.RELEASED, null, true);\n       }\n \n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (Resources.greaterThanOrEqual(calculator, getClusterResource(),\n           node.getAvailableResource(), minimumAllocation)) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getAvailableResource());\n         }\n-        root.assignContainers(clusterResource, node);\n+        root.assignContainers(clusterResource, node, false);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n           \" is reserved by application \" + \n           node.getReservedContainer().getContainerId().getApplicationAttemptId()\n           );\n     }\n   \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + \n          node.getNodeID());\n      \n      LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n      CSAssignment assignment \u003d queue.assignContainers(clusterResource, node,\n          false);\n      \n      RMContainer excessReservation \u003d assignment.getExcessReservation();\n      if (excessReservation !\u003d null) {\n      Container container \u003d excessReservation.getContainer();\n      queue.completedContainer(\n          clusterResource, assignment.getApplication(), node, \n          excessReservation, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED, null, true);\n      }\n\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (Resources.greaterThanOrEqual(calculator, getClusterResource(),\n          node.getAvailableResource(), minimumAllocation)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getAvailableResource());\n        }\n        root.assignContainers(clusterResource, node, false);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "485c96e3cb9b0b05d6e490b4773506da83ebc61d": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2001. Added a time threshold for RM to wait before starting container allocations after restart/failover. Contributed by Jian He.\n",
      "commitDate": "18/09/14 11:03 AM",
      "commitName": "485c96e3cb9b0b05d6e490b4773506da83ebc61d",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "10/09/14 10:15 AM",
      "commitNameOld": "b67d5ba7842cc10695d987f217027848a5a8c3d8",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 8.03,
      "commitsBetweenForRepo": 79,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,55 @@\n   private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n+    if (rmContext.isWorkPreservingRecoveryEnabled()\n+        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n+      return;\n+    }\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n       \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \" + \n           reservedApplication.getApplicationId() + \" on node: \" + \n           node.getNodeID());\n       \n       LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n       CSAssignment assignment \u003d queue.assignContainers(clusterResource, node);\n       \n       RMContainer excessReservation \u003d assignment.getExcessReservation();\n       if (excessReservation !\u003d null) {\n       Container container \u003d excessReservation.getContainer();\n       queue.completedContainer(\n           clusterResource, assignment.getApplication(), node, \n           excessReservation, \n           SchedulerUtils.createAbnormalContainerStatus(\n               container.getId(), \n               SchedulerUtils.UNRESERVED_CONTAINER), \n           RMContainerEventType.RELEASED, null);\n       }\n \n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n       if (Resources.greaterThanOrEqual(calculator, getClusterResource(),\n           node.getAvailableResource(), minimumAllocation)) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getAvailableResource());\n         }\n         root.assignContainers(clusterResource, node);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n           \" is reserved by application \" + \n           node.getReservedContainer().getContainerId().getApplicationAttemptId()\n           );\n     }\n   \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + \n          node.getNodeID());\n      \n      LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n      CSAssignment assignment \u003d queue.assignContainers(clusterResource, node);\n      \n      RMContainer excessReservation \u003d assignment.getExcessReservation();\n      if (excessReservation !\u003d null) {\n      Container container \u003d excessReservation.getContainer();\n      queue.completedContainer(\n          clusterResource, assignment.getApplication(), node, \n          excessReservation, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED, null);\n      }\n\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (Resources.greaterThanOrEqual(calculator, getClusterResource(),\n          node.getAvailableResource(), minimumAllocation)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getAvailableResource());\n        }\n        root.assignContainers(clusterResource, node);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "82f3454f5ac1f1c457e668e2cee12b4dcc800ee1": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2017. Merged some of the common scheduler code. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596753 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/05/14 10:32 PM",
      "commitName": "82f3454f5ac1f1c457e668e2cee12b4dcc800ee1",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/03/14 7:53 PM",
      "commitNameOld": "57cdf8626a32b8595a645b7551f46ab950db4789",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 65.11,
      "commitsBetweenForRepo": 406,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     RMContainer reservedContainer \u003d node.getReservedContainer();\n     if (reservedContainer !\u003d null) {\n       FiCaSchedulerApp reservedApplication \u003d\n           getCurrentAttemptForContainer(reservedContainer.getContainerId());\n       \n       // Try to fulfill the reservation\n       LOG.info(\"Trying to fulfill reservation for application \" + \n           reservedApplication.getApplicationId() + \" on node: \" + \n           node.getNodeID());\n       \n       LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n       CSAssignment assignment \u003d queue.assignContainers(clusterResource, node);\n       \n       RMContainer excessReservation \u003d assignment.getExcessReservation();\n       if (excessReservation !\u003d null) {\n       Container container \u003d excessReservation.getContainer();\n       queue.completedContainer(\n           clusterResource, assignment.getApplication(), node, \n           excessReservation, \n           SchedulerUtils.createAbnormalContainerStatus(\n               container.getId(), \n               SchedulerUtils.UNRESERVED_CONTAINER), \n           RMContainerEventType.RELEASED, null);\n       }\n \n     }\n \n     // Try to schedule more if there are no reservations to fulfill\n     if (node.getReservedContainer() \u003d\u003d null) {\n-      if (Resources.greaterThanOrEqual(calculator, getClusterResources(),\n+      if (Resources.greaterThanOrEqual(calculator, getClusterResource(),\n           node.getAvailableResource(), minimumAllocation)) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n               \", available: \" + node.getAvailableResource());\n         }\n         root.assignContainers(clusterResource, node);\n       }\n     } else {\n       LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n           \" is reserved by application \" + \n           node.getReservedContainer().getContainerId().getApplicationAttemptId()\n           );\n     }\n   \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + \n          node.getNodeID());\n      \n      LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n      CSAssignment assignment \u003d queue.assignContainers(clusterResource, node);\n      \n      RMContainer excessReservation \u003d assignment.getExcessReservation();\n      if (excessReservation !\u003d null) {\n      Container container \u003d excessReservation.getContainer();\n      queue.completedContainer(\n          clusterResource, assignment.getApplication(), node, \n          excessReservation, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED, null);\n      }\n\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (Resources.greaterThanOrEqual(calculator, getClusterResource(),\n          node.getAvailableResource(), minimumAllocation)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getAvailableResource());\n        }\n        root.assignContainers(clusterResource, node);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
      "extendedDetails": {}
    },
    "57cdf8626a32b8595a645b7551f46ab950db4789": {
      "type": "Yintroduced",
      "commitMessage": "YARN-1512. Enhanced CapacityScheduler to be able to decouple scheduling from node-heartbeats. Contributed by Arun C Murthy.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1578722 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/03/14 7:53 PM",
      "commitName": "57cdf8626a32b8595a645b7551f46ab950db4789",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,51 @@\n+  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n+\n+    // Assign new containers...\n+    // 1. Check for reserved applications\n+    // 2. Schedule if there are no reservations\n+\n+    RMContainer reservedContainer \u003d node.getReservedContainer();\n+    if (reservedContainer !\u003d null) {\n+      FiCaSchedulerApp reservedApplication \u003d\n+          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n+      \n+      // Try to fulfill the reservation\n+      LOG.info(\"Trying to fulfill reservation for application \" + \n+          reservedApplication.getApplicationId() + \" on node: \" + \n+          node.getNodeID());\n+      \n+      LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n+      CSAssignment assignment \u003d queue.assignContainers(clusterResource, node);\n+      \n+      RMContainer excessReservation \u003d assignment.getExcessReservation();\n+      if (excessReservation !\u003d null) {\n+      Container container \u003d excessReservation.getContainer();\n+      queue.completedContainer(\n+          clusterResource, assignment.getApplication(), node, \n+          excessReservation, \n+          SchedulerUtils.createAbnormalContainerStatus(\n+              container.getId(), \n+              SchedulerUtils.UNRESERVED_CONTAINER), \n+          RMContainerEventType.RELEASED, null);\n+      }\n+\n+    }\n+\n+    // Try to schedule more if there are no reservations to fulfill\n+    if (node.getReservedContainer() \u003d\u003d null) {\n+      if (Resources.greaterThanOrEqual(calculator, getClusterResources(),\n+          node.getAvailableResource(), minimumAllocation)) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n+              \", available: \" + node.getAvailableResource());\n+        }\n+        root.assignContainers(clusterResource, node);\n+      }\n+    } else {\n+      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n+          \" is reserved by application \" + \n+          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n+          );\n+    }\n+  \n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer \u003d node.getReservedContainer();\n    if (reservedContainer !\u003d null) {\n      FiCaSchedulerApp reservedApplication \u003d\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + \n          node.getNodeID());\n      \n      LeafQueue queue \u003d ((LeafQueue)reservedApplication.getQueue());\n      CSAssignment assignment \u003d queue.assignContainers(clusterResource, node);\n      \n      RMContainer excessReservation \u003d assignment.getExcessReservation();\n      if (excessReservation !\u003d null) {\n      Container container \u003d excessReservation.getContainer();\n      queue.completedContainer(\n          clusterResource, assignment.getApplication(), node, \n          excessReservation, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED, null);\n      }\n\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() \u003d\u003d null) {\n      if (Resources.greaterThanOrEqual(calculator, getClusterResources(),\n          node.getAvailableResource(), minimumAllocation)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to schedule on node: \" + node.getNodeName() +\n              \", available: \" + node.getAvailableResource());\n        }\n        root.assignContainers(clusterResource, node);\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java"
    }
  }
}