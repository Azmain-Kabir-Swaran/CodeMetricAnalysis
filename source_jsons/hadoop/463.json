{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReaderLocal.java",
  "functionName": "fillDataBuf",
  "functionId": "fillDataBuf___canSkipChecksum-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java",
  "functionStartLine": 468,
  "functionEndLine": 487,
  "numCommitsSeen": 47,
  "timeTaken": 2965,
  "changeHistory": [
    "f308561f1d885491b88db73ac63003202056d661",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "e2c9b288b223b9fd82dc12018936e13128413492",
    "124e507674c0d396f8494585e64226957199097b"
  ],
  "changeHistoryShort": {
    "f308561f1d885491b88db73ac63003202056d661": "Yfilerename",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "e2c9b288b223b9fd82dc12018936e13128413492": "Yfilerename",
    "124e507674c0d396f8494585e64226957199097b": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f308561f1d885491b88db73ac63003202056d661": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
      "commitDate": "25/04/16 12:01 PM",
      "commitName": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "25/04/16 9:38 AM",
      "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n      throws IOException {\n    createDataBufIfNeeded();\n    final int slop \u003d (int)(dataPos % bytesPerChecksum);\n    final long oldDataPos \u003d dataPos;\n    dataBuf.limit(maxReadaheadLength);\n    if (canSkipChecksum) {\n      dataBuf.position(slop);\n      fillBuffer(dataBuf, true);\n    } else {\n      dataPos -\u003d slop;\n      dataBuf.position(0);\n      fillBuffer(dataBuf, false);\n    }\n    dataBuf.limit(dataBuf.position());\n    dataBuf.position(Math.min(dataBuf.position(), slop));\n    LOG.trace(\"loaded {} bytes into bounce buffer from offset {} of {}\",\n        dataBuf.remaining(), oldDataPos, block);\n    return dataBuf.limit() !\u003d maxReadaheadLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java"
      }
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n       throws IOException {\n     createDataBufIfNeeded();\n     final int slop \u003d (int)(dataPos % bytesPerChecksum);\n     final long oldDataPos \u003d dataPos;\n     dataBuf.limit(maxReadaheadLength);\n     if (canSkipChecksum) {\n       dataBuf.position(slop);\n-      fillBuffer(dataBuf, canSkipChecksum);\n+      fillBuffer(dataBuf, true);\n     } else {\n       dataPos -\u003d slop;\n       dataBuf.position(0);\n-      fillBuffer(dataBuf, canSkipChecksum);\n+      fillBuffer(dataBuf, false);\n     }\n     dataBuf.limit(dataBuf.position());\n     dataBuf.position(Math.min(dataBuf.position(), slop));\n     LOG.trace(\"loaded {} bytes into bounce buffer from offset {} of {}\",\n         dataBuf.remaining(), oldDataPos, block);\n     return dataBuf.limit() !\u003d maxReadaheadLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n      throws IOException {\n    createDataBufIfNeeded();\n    final int slop \u003d (int)(dataPos % bytesPerChecksum);\n    final long oldDataPos \u003d dataPos;\n    dataBuf.limit(maxReadaheadLength);\n    if (canSkipChecksum) {\n      dataBuf.position(slop);\n      fillBuffer(dataBuf, true);\n    } else {\n      dataPos -\u003d slop;\n      dataBuf.position(0);\n      fillBuffer(dataBuf, false);\n    }\n    dataBuf.limit(dataBuf.position());\n    dataBuf.position(Math.min(dataBuf.position(), slop));\n    LOG.trace(\"loaded {} bytes into bounce buffer from offset {} of {}\",\n        dataBuf.remaining(), oldDataPos, block);\n    return dataBuf.limit() !\u003d maxReadaheadLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,20 @@\n   private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n       throws IOException {\n     createDataBufIfNeeded();\n     final int slop \u003d (int)(dataPos % bytesPerChecksum);\n     final long oldDataPos \u003d dataPos;\n     dataBuf.limit(maxReadaheadLength);\n     if (canSkipChecksum) {\n       dataBuf.position(slop);\n       fillBuffer(dataBuf, canSkipChecksum);\n     } else {\n       dataPos -\u003d slop;\n       dataBuf.position(0);\n       fillBuffer(dataBuf, canSkipChecksum);\n     }\n     dataBuf.limit(dataBuf.position());\n     dataBuf.position(Math.min(dataBuf.position(), slop));\n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"loaded \" + dataBuf.remaining() + \" bytes into bounce \" +\n-          \"buffer from offset \" + oldDataPos + \" of \" + block);\n-    }\n+    LOG.trace(\"loaded {} bytes into bounce buffer from offset {} of {}\",\n+        dataBuf.remaining(), oldDataPos, block);\n     return dataBuf.limit() !\u003d maxReadaheadLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n      throws IOException {\n    createDataBufIfNeeded();\n    final int slop \u003d (int)(dataPos % bytesPerChecksum);\n    final long oldDataPos \u003d dataPos;\n    dataBuf.limit(maxReadaheadLength);\n    if (canSkipChecksum) {\n      dataBuf.position(slop);\n      fillBuffer(dataBuf, canSkipChecksum);\n    } else {\n      dataPos -\u003d slop;\n      dataBuf.position(0);\n      fillBuffer(dataBuf, canSkipChecksum);\n    }\n    dataBuf.limit(dataBuf.position());\n    dataBuf.position(Math.min(dataBuf.position(), slop));\n    LOG.trace(\"loaded {} bytes into bounce buffer from offset {} of {}\",\n        dataBuf.remaining(), oldDataPos, block);\n    return dataBuf.limit() !\u003d maxReadaheadLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,22 @@\n   private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n       throws IOException {\n     createDataBufIfNeeded();\n     final int slop \u003d (int)(dataPos % bytesPerChecksum);\n     final long oldDataPos \u003d dataPos;\n     dataBuf.limit(maxReadaheadLength);\n     if (canSkipChecksum) {\n       dataBuf.position(slop);\n       fillBuffer(dataBuf, canSkipChecksum);\n     } else {\n       dataPos -\u003d slop;\n       dataBuf.position(0);\n       fillBuffer(dataBuf, canSkipChecksum);\n     }\n     dataBuf.limit(dataBuf.position());\n     dataBuf.position(Math.min(dataBuf.position(), slop));\n-    LOG.trace(\"loaded {} bytes into bounce buffer from offset {} of {}\",\n-        dataBuf.remaining(), oldDataPos, block);\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"loaded \" + dataBuf.remaining() + \" bytes into bounce \" +\n+          \"buffer from offset \" + oldDataPos + \" of \" + block);\n+    }\n     return dataBuf.limit() !\u003d maxReadaheadLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n      throws IOException {\n    createDataBufIfNeeded();\n    final int slop \u003d (int)(dataPos % bytesPerChecksum);\n    final long oldDataPos \u003d dataPos;\n    dataBuf.limit(maxReadaheadLength);\n    if (canSkipChecksum) {\n      dataBuf.position(slop);\n      fillBuffer(dataBuf, canSkipChecksum);\n    } else {\n      dataPos -\u003d slop;\n      dataBuf.position(0);\n      fillBuffer(dataBuf, canSkipChecksum);\n    }\n    dataBuf.limit(dataBuf.position());\n    dataBuf.position(Math.min(dataBuf.position(), slop));\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"loaded \" + dataBuf.remaining() + \" bytes into bounce \" +\n          \"buffer from offset \" + oldDataPos + \" of \" + block);\n    }\n    return dataBuf.limit() !\u003d maxReadaheadLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,20 @@\n   private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n       throws IOException {\n     createDataBufIfNeeded();\n     final int slop \u003d (int)(dataPos % bytesPerChecksum);\n     final long oldDataPos \u003d dataPos;\n     dataBuf.limit(maxReadaheadLength);\n     if (canSkipChecksum) {\n       dataBuf.position(slop);\n       fillBuffer(dataBuf, canSkipChecksum);\n     } else {\n       dataPos -\u003d slop;\n       dataBuf.position(0);\n       fillBuffer(dataBuf, canSkipChecksum);\n     }\n     dataBuf.limit(dataBuf.position());\n     dataBuf.position(Math.min(dataBuf.position(), slop));\n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"loaded \" + dataBuf.remaining() + \" bytes into bounce \" +\n-          \"buffer from offset \" + oldDataPos + \" of \" + block);\n-    }\n+    LOG.trace(\"loaded {} bytes into bounce buffer from offset {} of {}\",\n+        dataBuf.remaining(), oldDataPos, block);\n     return dataBuf.limit() !\u003d maxReadaheadLength;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n      throws IOException {\n    createDataBufIfNeeded();\n    final int slop \u003d (int)(dataPos % bytesPerChecksum);\n    final long oldDataPos \u003d dataPos;\n    dataBuf.limit(maxReadaheadLength);\n    if (canSkipChecksum) {\n      dataBuf.position(slop);\n      fillBuffer(dataBuf, canSkipChecksum);\n    } else {\n      dataPos -\u003d slop;\n      dataBuf.position(0);\n      fillBuffer(dataBuf, canSkipChecksum);\n    }\n    dataBuf.limit(dataBuf.position());\n    dataBuf.position(Math.min(dataBuf.position(), slop));\n    LOG.trace(\"loaded {} bytes into bounce buffer from offset {} of {}\",\n        dataBuf.remaining(), oldDataPos, block);\n    return dataBuf.limit() !\u003d maxReadaheadLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "e2c9b288b223b9fd82dc12018936e13128413492": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "28/08/15 2:38 PM",
      "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/08/15 2:21 PM",
      "commitNameOld": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n      throws IOException {\n    createDataBufIfNeeded();\n    final int slop \u003d (int)(dataPos % bytesPerChecksum);\n    final long oldDataPos \u003d dataPos;\n    dataBuf.limit(maxReadaheadLength);\n    if (canSkipChecksum) {\n      dataBuf.position(slop);\n      fillBuffer(dataBuf, canSkipChecksum);\n    } else {\n      dataPos -\u003d slop;\n      dataBuf.position(0);\n      fillBuffer(dataBuf, canSkipChecksum);\n    }\n    dataBuf.limit(dataBuf.position());\n    dataBuf.position(Math.min(dataBuf.position(), slop));\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"loaded \" + dataBuf.remaining() + \" bytes into bounce \" +\n          \"buffer from offset \" + oldDataPos + \" of \" + block);\n    }\n    return dataBuf.limit() !\u003d maxReadaheadLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java"
      }
    },
    "124e507674c0d396f8494585e64226957199097b": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5634. Allow BlockReaderLocal to switch between checksumming and not (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551701 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 12:57 PM",
      "commitName": "124e507674c0d396f8494585e64226957199097b",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,22 @@\n+  private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n+      throws IOException {\n+    createDataBufIfNeeded();\n+    final int slop \u003d (int)(dataPos % bytesPerChecksum);\n+    final long oldDataPos \u003d dataPos;\n+    dataBuf.limit(maxReadaheadLength);\n+    if (canSkipChecksum) {\n+      dataBuf.position(slop);\n+      fillBuffer(dataBuf, canSkipChecksum);\n+    } else {\n+      dataPos -\u003d slop;\n+      dataBuf.position(0);\n+      fillBuffer(dataBuf, canSkipChecksum);\n+    }\n+    dataBuf.limit(dataBuf.position());\n+    dataBuf.position(Math.min(dataBuf.position(), slop));\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"loaded \" + dataBuf.remaining() + \" bytes into bounce \" +\n+          \"buffer from offset \" + oldDataPos + \" of \" + block);\n+    }\n+    return dataBuf.limit() !\u003d maxReadaheadLength;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized boolean fillDataBuf(boolean canSkipChecksum)\n      throws IOException {\n    createDataBufIfNeeded();\n    final int slop \u003d (int)(dataPos % bytesPerChecksum);\n    final long oldDataPos \u003d dataPos;\n    dataBuf.limit(maxReadaheadLength);\n    if (canSkipChecksum) {\n      dataBuf.position(slop);\n      fillBuffer(dataBuf, canSkipChecksum);\n    } else {\n      dataPos -\u003d slop;\n      dataBuf.position(0);\n      fillBuffer(dataBuf, canSkipChecksum);\n    }\n    dataBuf.limit(dataBuf.position());\n    dataBuf.position(Math.min(dataBuf.position(), slop));\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"loaded \" + dataBuf.remaining() + \" bytes into bounce \" +\n          \"buffer from offset \" + oldDataPos + \" of \" + block);\n    }\n    return dataBuf.limit() !\u003d maxReadaheadLength;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java"
    }
  }
}