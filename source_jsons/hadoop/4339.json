{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Receiver.java",
  "functionName": "opStripedBlockChecksum",
  "functionId": "opStripedBlockChecksum___dis-DataInputStream",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java",
  "functionStartLine": 313,
  "functionEndLine": 336,
  "numCommitsSeen": 30,
  "timeTaken": 2265,
  "changeHistory": [
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
    "e6cb07520f935efde3e881de8f84ee7f6e0a746f",
    "d749cf65e1ab0e0daf5be86931507183f189e855",
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720"
  ],
  "changeHistoryShort": {
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf": "Ybodychange",
    "e6cb07520f935efde3e881de8f84ee7f6e0a746f": "Ybodychange",
    "d749cf65e1ab0e0daf5be86931507183f189e855": "Ybodychange",
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7c9cdad6d04c98db5a83e2108219bf6e6c903daf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13056. Expose file-level composite CRCs in HDFS which are comparable across different instances/layouts. Contributed by Dennis Huo.\n",
      "commitDate": "10/04/18 9:31 PM",
      "commitName": "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "05/05/17 12:01 PM",
      "commitNameOld": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 340.4,
      "commitsBetweenForRepo": 2425,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n     OpBlockGroupChecksumProto proto \u003d\n         OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n     TraceScope traceScope \u003d continueTraceSpan(proto.getHeader(),\n         proto.getClass().getSimpleName());\n     StripedBlockInfo stripedBlockInfo \u003d new StripedBlockInfo(\n         PBHelperClient.convert(proto.getHeader().getBlock()),\n         PBHelperClient.convert(proto.getDatanodes()),\n         PBHelperClient.convertTokens(proto.getBlockTokensList()),\n         PBHelperClient.convertBlockIndices(proto.getBlockIndicesList()),\n         PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n     );\n \n     try {\n       blockGroupChecksum(stripedBlockInfo,\n           PBHelperClient.convert(proto.getHeader().getToken()),\n-          proto.getRequestedNumBytes());\n+          proto.getRequestedNumBytes(),\n+          PBHelperClient.convert(proto.getBlockChecksumOptions()));\n     } finally {\n       if (traceScope !\u003d null) {\n         traceScope.close();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n    OpBlockGroupChecksumProto proto \u003d\n        OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n    TraceScope traceScope \u003d continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    StripedBlockInfo stripedBlockInfo \u003d new StripedBlockInfo(\n        PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getDatanodes()),\n        PBHelperClient.convertTokens(proto.getBlockTokensList()),\n        PBHelperClient.convertBlockIndices(proto.getBlockIndicesList()),\n        PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n    );\n\n    try {\n      blockGroupChecksum(stripedBlockInfo,\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          proto.getRequestedNumBytes(),\n          PBHelperClient.convert(proto.getBlockChecksumOptions()));\n    } finally {\n      if (traceScope !\u003d null) {\n        traceScope.close();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java",
      "extendedDetails": {}
    },
    "e6cb07520f935efde3e881de8f84ee7f6e0a746f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10460. Recompute block checksum for a particular range less than file size on the fly by reconstructing missed block. Contributed by Rakesh R\n",
      "commitDate": "24/06/16 2:39 AM",
      "commitName": "e6cb07520f935efde3e881de8f84ee7f6e0a746f",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "01/06/16 9:56 PM",
      "commitNameOld": "d749cf65e1ab0e0daf5be86931507183f189e855",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 22.2,
      "commitsBetweenForRepo": 141,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,23 @@\n   private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n     OpBlockGroupChecksumProto proto \u003d\n         OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n     TraceScope traceScope \u003d continueTraceSpan(proto.getHeader(),\n         proto.getClass().getSimpleName());\n     StripedBlockInfo stripedBlockInfo \u003d new StripedBlockInfo(\n         PBHelperClient.convert(proto.getHeader().getBlock()),\n         PBHelperClient.convert(proto.getDatanodes()),\n         PBHelperClient.convertTokens(proto.getBlockTokensList()),\n         PBHelperClient.convertBlockIndices(proto.getBlockIndicesList()),\n         PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n     );\n \n     try {\n       blockGroupChecksum(stripedBlockInfo,\n-          PBHelperClient.convert(proto.getHeader().getToken()));\n+          PBHelperClient.convert(proto.getHeader().getToken()),\n+          proto.getRequestedNumBytes());\n     } finally {\n       if (traceScope !\u003d null) {\n         traceScope.close();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n    OpBlockGroupChecksumProto proto \u003d\n        OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n    TraceScope traceScope \u003d continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    StripedBlockInfo stripedBlockInfo \u003d new StripedBlockInfo(\n        PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getDatanodes()),\n        PBHelperClient.convertTokens(proto.getBlockTokensList()),\n        PBHelperClient.convertBlockIndices(proto.getBlockIndicesList()),\n        PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n    );\n\n    try {\n      blockGroupChecksum(stripedBlockInfo,\n          PBHelperClient.convert(proto.getHeader().getToken()),\n          proto.getRequestedNumBytes());\n    } finally {\n      if (traceScope !\u003d null) {\n        traceScope.close();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java",
      "extendedDetails": {}
    },
    "d749cf65e1ab0e0daf5be86931507183f189e855": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9833. Erasure coding: recomputing block checksum on the fly by reconstructing the missed/corrupt block data. Contributed by Rakesh R.\n",
      "commitDate": "01/06/16 9:56 PM",
      "commitName": "d749cf65e1ab0e0daf5be86931507183f189e855",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "26/03/16 7:58 PM",
      "commitNameOld": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 67.08,
      "commitsBetweenForRepo": 433,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,22 @@\n   private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n     OpBlockGroupChecksumProto proto \u003d\n         OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n     TraceScope traceScope \u003d continueTraceSpan(proto.getHeader(),\n         proto.getClass().getSimpleName());\n     StripedBlockInfo stripedBlockInfo \u003d new StripedBlockInfo(\n         PBHelperClient.convert(proto.getHeader().getBlock()),\n         PBHelperClient.convert(proto.getDatanodes()),\n         PBHelperClient.convertTokens(proto.getBlockTokensList()),\n+        PBHelperClient.convertBlockIndices(proto.getBlockIndicesList()),\n         PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n     );\n \n     try {\n       blockGroupChecksum(stripedBlockInfo,\n           PBHelperClient.convert(proto.getHeader().getToken()));\n     } finally {\n       if (traceScope !\u003d null) {\n         traceScope.close();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n    OpBlockGroupChecksumProto proto \u003d\n        OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n    TraceScope traceScope \u003d continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    StripedBlockInfo stripedBlockInfo \u003d new StripedBlockInfo(\n        PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getDatanodes()),\n        PBHelperClient.convertTokens(proto.getBlockTokensList()),\n        PBHelperClient.convertBlockIndices(proto.getBlockIndicesList()),\n        PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n    );\n\n    try {\n      blockGroupChecksum(stripedBlockInfo,\n          PBHelperClient.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope !\u003d null) {\n        traceScope.close();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java",
      "extendedDetails": {}
    },
    "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9694. Make existing DFSClient#getFileChecksum() work for striped blocks. Contributed by Kai Zheng\n",
      "commitDate": "26/03/16 7:58 PM",
      "commitName": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,21 @@\n+  private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n+    OpBlockGroupChecksumProto proto \u003d\n+        OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n+    TraceScope traceScope \u003d continueTraceSpan(proto.getHeader(),\n+        proto.getClass().getSimpleName());\n+    StripedBlockInfo stripedBlockInfo \u003d new StripedBlockInfo(\n+        PBHelperClient.convert(proto.getHeader().getBlock()),\n+        PBHelperClient.convert(proto.getDatanodes()),\n+        PBHelperClient.convertTokens(proto.getBlockTokensList()),\n+        PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n+    );\n+\n+    try {\n+      blockGroupChecksum(stripedBlockInfo,\n+          PBHelperClient.convert(proto.getHeader().getToken()));\n+    } finally {\n+      if (traceScope !\u003d null) {\n+        traceScope.close();\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void opStripedBlockChecksum(DataInputStream dis) throws IOException {\n    OpBlockGroupChecksumProto proto \u003d\n        OpBlockGroupChecksumProto.parseFrom(vintPrefixed(dis));\n    TraceScope traceScope \u003d continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    StripedBlockInfo stripedBlockInfo \u003d new StripedBlockInfo(\n        PBHelperClient.convert(proto.getHeader().getBlock()),\n        PBHelperClient.convert(proto.getDatanodes()),\n        PBHelperClient.convertTokens(proto.getBlockTokensList()),\n        PBHelperClient.convertErasureCodingPolicy(proto.getEcPolicy())\n    );\n\n    try {\n      blockGroupChecksum(stripedBlockInfo,\n          PBHelperClient.convert(proto.getHeader().getToken()));\n    } finally {\n      if (traceScope !\u003d null) {\n        traceScope.close();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java"
    }
  }
}