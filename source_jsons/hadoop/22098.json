{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "MergeManagerImpl.java",
  "functionName": "merge",
  "functionId": "merge___inputs-List__CompressAwarePath__",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
  "functionStartLine": 527,
  "functionEndLine": 590,
  "numCommitsSeen": 34,
  "timeTaken": 12485,
  "changeHistory": [
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
    "df68c56267ca7dfbfee4b241bc84325d1760d12d",
    "14089f1e57e078cf20caed9db6f86de60773d704",
    "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
    "da4cab10990b3a352fc2c699f3b41c994ac55e95",
    "539153a6798a667d39f20972c5ae0936060e2cc1",
    "73fd247c7649919350ecfd16806af57ffe554649",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": "Ybodychange",
    "df68c56267ca7dfbfee4b241bc84325d1760d12d": "Ybodychange",
    "14089f1e57e078cf20caed9db6f86de60773d704": "Ybodychange",
    "0f430e53fde884f24b473043f0a7e2bffa98ebd3": "Ymultichange(Yparameterchange,Ybodychange)",
    "da4cab10990b3a352fc2c699f3b41c994ac55e95": "Ymultichange(Yparameterchange,Ybodychange)",
    "539153a6798a667d39f20972c5ae0936060e2cc1": "Ymultichange(Yparameterchange,Ybodychange)",
    "73fd247c7649919350ecfd16806af57ffe554649": "Ymovefromfile",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymovefromfile",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5890. Support for encrypting Intermediate data and spills in local filesystem. (asuresh via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1609597 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/07/14 5:43 PM",
      "commitName": "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "06/01/14 10:35 AM",
      "commitNameOld": "76238b9722539b5fd4773129ecc31b11bd8255ef",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 185.26,
      "commitsBetweenForRepo": 1309,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,64 @@\n     public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n       // sanity check\n       if (inputs \u003d\u003d null || inputs.isEmpty()) {\n         LOG.info(\"No ondisk files to merge...\");\n         return;\n       }\n       \n       long approxOutputSize \u003d 0;\n       int bytesPerSum \u003d \n         jobConf.getInt(\"io.bytes.per.checksum\", 512);\n       \n       LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n                \" map outputs on disk. Triggering merge...\");\n       \n       // 1. Prepare the list of files to be merged. \n       for (CompressAwarePath file : inputs) {\n         approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n       }\n \n       // add the checksum length\n       approxOutputSize +\u003d \n         ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n \n       // 2. Start the on-disk merge process\n       Path outputPath \u003d \n         localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n             approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n-      Writer\u003cK,V\u003e writer \u003d \n-        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n-                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n-                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n-                        codec, null);\n+\n+      FSDataOutputStream out \u003d CryptoUtils.wrapIfNecessary(jobConf, rfs.create(outputPath));\n+      Writer\u003cK, V\u003e writer \u003d new Writer\u003cK, V\u003e(jobConf, out,\n+          (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n+          (Class\u003cV\u003e) jobConf.getMapOutputValueClass(), codec, null, true);\n+\n       RawKeyValueIterator iter  \u003d null;\n       CompressAwarePath compressAwarePath;\n       Path tmpDir \u003d new Path(reduceId.toString());\n       try {\n         iter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                             codec, inputs.toArray(new Path[inputs.size()]), \n                             true, ioSortFactor, tmpDir, \n                             (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                             reporter, spilledRecordsCounter, null, \n                             mergedMapOutputsCounter, null);\n \n         Merger.writeFile(iter, writer, reporter, jobConf);\n         writer.close();\n         compressAwarePath \u003d new CompressAwarePath(outputPath,\n             writer.getRawLength(), writer.getCompressedLength());\n       } catch (IOException e) {\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n       closeOnDiskFile(compressAwarePath);\n \n       LOG.info(reduceId +\n           \" Finished merging \" + inputs.size() + \n           \" map output files on disk of total-size \" + \n           approxOutputSize + \".\" + \n           \" Local output file is \" + outputPath + \" of size \" +\n           localFS.getFileStatus(outputPath).getLen());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (CompressAwarePath file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n\n      FSDataOutputStream out \u003d CryptoUtils.wrapIfNecessary(jobConf, rfs.create(outputPath));\n      Writer\u003cK, V\u003e writer \u003d new Writer\u003cK, V\u003e(jobConf, out,\n          (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n          (Class\u003cV\u003e) jobConf.getMapOutputValueClass(), codec, null, true);\n\n      RawKeyValueIterator iter  \u003d null;\n      CompressAwarePath compressAwarePath;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        writer.close();\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength(), writer.getCompressedLength());\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(compressAwarePath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "df68c56267ca7dfbfee4b241bc84325d1760d12d": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3685. Fix bugs in MergeManager to ensure compression codec is appropriately used and that on-disk segments are correctly sorted on file-size. Contributed by Anty Rao and Ravi Prakash.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1453365 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/03/13 7:02 AM",
      "commitName": "df68c56267ca7dfbfee4b241bc84325d1760d12d",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "27/02/13 2:40 AM",
      "commitNameOld": "14089f1e57e078cf20caed9db6f86de60773d704",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 7.18,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,63 @@\n     public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n       // sanity check\n       if (inputs \u003d\u003d null || inputs.isEmpty()) {\n         LOG.info(\"No ondisk files to merge...\");\n         return;\n       }\n       \n       long approxOutputSize \u003d 0;\n       int bytesPerSum \u003d \n         jobConf.getInt(\"io.bytes.per.checksum\", 512);\n       \n       LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n                \" map outputs on disk. Triggering merge...\");\n       \n       // 1. Prepare the list of files to be merged. \n       for (CompressAwarePath file : inputs) {\n         approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n       }\n \n       // add the checksum length\n       approxOutputSize +\u003d \n         ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n \n       // 2. Start the on-disk merge process\n       Path outputPath \u003d \n         localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n             approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n       RawKeyValueIterator iter  \u003d null;\n       CompressAwarePath compressAwarePath;\n       Path tmpDir \u003d new Path(reduceId.toString());\n       try {\n         iter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                             codec, inputs.toArray(new Path[inputs.size()]), \n                             true, ioSortFactor, tmpDir, \n                             (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                             reporter, spilledRecordsCounter, null, \n                             mergedMapOutputsCounter, null);\n \n         Merger.writeFile(iter, writer, reporter, jobConf);\n         writer.close();\n         compressAwarePath \u003d new CompressAwarePath(outputPath,\n-            writer.getRawLength());\n+            writer.getRawLength(), writer.getCompressedLength());\n       } catch (IOException e) {\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n       closeOnDiskFile(compressAwarePath);\n \n       LOG.info(reduceId +\n           \" Finished merging \" + inputs.size() + \n           \" map output files on disk of total-size \" + \n           approxOutputSize + \".\" + \n           \" Local output file is \" + outputPath + \" of size \" +\n           localFS.getFileStatus(outputPath).getLen());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (CompressAwarePath file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      CompressAwarePath compressAwarePath;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        writer.close();\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength(), writer.getCompressedLength());\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(compressAwarePath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "14089f1e57e078cf20caed9db6f86de60773d704": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5008. Merger progress miscounts with respect to EOF_MARKER. Contributed by Sandy Ryza.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1450723 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/02/13 2:40 AM",
      "commitName": "14089f1e57e078cf20caed9db6f86de60773d704",
      "commitAuthor": "Thomas White",
      "commitDateOld": "29/01/13 11:38 AM",
      "commitNameOld": "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 28.63,
      "commitsBetweenForRepo": 101,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,63 @@\n     public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n       // sanity check\n       if (inputs \u003d\u003d null || inputs.isEmpty()) {\n         LOG.info(\"No ondisk files to merge...\");\n         return;\n       }\n       \n       long approxOutputSize \u003d 0;\n       int bytesPerSum \u003d \n         jobConf.getInt(\"io.bytes.per.checksum\", 512);\n       \n       LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n                \" map outputs on disk. Triggering merge...\");\n       \n       // 1. Prepare the list of files to be merged. \n       for (CompressAwarePath file : inputs) {\n         approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n       }\n \n       // add the checksum length\n       approxOutputSize +\u003d \n         ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n \n       // 2. Start the on-disk merge process\n       Path outputPath \u003d \n         localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n             approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n       RawKeyValueIterator iter  \u003d null;\n       CompressAwarePath compressAwarePath;\n       Path tmpDir \u003d new Path(reduceId.toString());\n       try {\n         iter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                             codec, inputs.toArray(new Path[inputs.size()]), \n                             true, ioSortFactor, tmpDir, \n                             (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                             reporter, spilledRecordsCounter, null, \n                             mergedMapOutputsCounter, null);\n \n         Merger.writeFile(iter, writer, reporter, jobConf);\n+        writer.close();\n         compressAwarePath \u003d new CompressAwarePath(outputPath,\n             writer.getRawLength());\n-        writer.close();\n       } catch (IOException e) {\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n       closeOnDiskFile(compressAwarePath);\n \n       LOG.info(reduceId +\n           \" Finished merging \" + inputs.size() + \n           \" map output files on disk of total-size \" + \n           approxOutputSize + \".\" + \n           \" Local output file is \" + outputPath + \" of size \" +\n           localFS.getFileStatus(outputPath).getLen());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (CompressAwarePath file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      CompressAwarePath compressAwarePath;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        writer.close();\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength());\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(compressAwarePath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "0f430e53fde884f24b473043f0a7e2bffa98ebd3": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1440076 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/01/13 11:38 AM",
      "commitName": "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
      "commitAuthor": "Alejandro Abdelnur",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1440076 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "29/01/13 11:38 AM",
          "commitName": "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "28/01/13 10:58 AM",
          "commitNameOld": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 1.03,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,63 @@\n-    public void merge(List\u003cPath\u003e inputs) throws IOException {\n+    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n       // sanity check\n       if (inputs \u003d\u003d null || inputs.isEmpty()) {\n         LOG.info(\"No ondisk files to merge...\");\n         return;\n       }\n       \n       long approxOutputSize \u003d 0;\n       int bytesPerSum \u003d \n         jobConf.getInt(\"io.bytes.per.checksum\", 512);\n       \n       LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n                \" map outputs on disk. Triggering merge...\");\n       \n       // 1. Prepare the list of files to be merged. \n-      for (Path file : inputs) {\n+      for (CompressAwarePath file : inputs) {\n         approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n       }\n \n       // add the checksum length\n       approxOutputSize +\u003d \n         ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n \n       // 2. Start the on-disk merge process\n       Path outputPath \u003d \n         localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n             approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n       RawKeyValueIterator iter  \u003d null;\n+      CompressAwarePath compressAwarePath;\n       Path tmpDir \u003d new Path(reduceId.toString());\n       try {\n         iter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                             codec, inputs.toArray(new Path[inputs.size()]), \n                             true, ioSortFactor, tmpDir, \n                             (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                             reporter, spilledRecordsCounter, null, \n                             mergedMapOutputsCounter, null);\n \n         Merger.writeFile(iter, writer, reporter, jobConf);\n+        compressAwarePath \u003d new CompressAwarePath(outputPath,\n+            writer.getRawLength());\n         writer.close();\n       } catch (IOException e) {\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n-      closeOnDiskFile(outputPath);\n+      closeOnDiskFile(compressAwarePath);\n \n       LOG.info(reduceId +\n           \" Finished merging \" + inputs.size() + \n           \" map output files on disk of total-size \" + \n           approxOutputSize + \".\" + \n           \" Local output file is \" + outputPath + \" of size \" +\n           localFS.getFileStatus(outputPath).getLen());\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (CompressAwarePath file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      CompressAwarePath compressAwarePath;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength());\n        writer.close();\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(compressAwarePath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {
            "oldValue": "[inputs-List\u003cPath\u003e]",
            "newValue": "[inputs-List\u003cCompressAwarePath\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1440076 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "29/01/13 11:38 AM",
          "commitName": "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "28/01/13 10:58 AM",
          "commitNameOld": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 1.03,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,63 @@\n-    public void merge(List\u003cPath\u003e inputs) throws IOException {\n+    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n       // sanity check\n       if (inputs \u003d\u003d null || inputs.isEmpty()) {\n         LOG.info(\"No ondisk files to merge...\");\n         return;\n       }\n       \n       long approxOutputSize \u003d 0;\n       int bytesPerSum \u003d \n         jobConf.getInt(\"io.bytes.per.checksum\", 512);\n       \n       LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n                \" map outputs on disk. Triggering merge...\");\n       \n       // 1. Prepare the list of files to be merged. \n-      for (Path file : inputs) {\n+      for (CompressAwarePath file : inputs) {\n         approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n       }\n \n       // add the checksum length\n       approxOutputSize +\u003d \n         ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n \n       // 2. Start the on-disk merge process\n       Path outputPath \u003d \n         localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n             approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n       RawKeyValueIterator iter  \u003d null;\n+      CompressAwarePath compressAwarePath;\n       Path tmpDir \u003d new Path(reduceId.toString());\n       try {\n         iter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                             codec, inputs.toArray(new Path[inputs.size()]), \n                             true, ioSortFactor, tmpDir, \n                             (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                             reporter, spilledRecordsCounter, null, \n                             mergedMapOutputsCounter, null);\n \n         Merger.writeFile(iter, writer, reporter, jobConf);\n+        compressAwarePath \u003d new CompressAwarePath(outputPath,\n+            writer.getRawLength());\n         writer.close();\n       } catch (IOException e) {\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n-      closeOnDiskFile(outputPath);\n+      closeOnDiskFile(compressAwarePath);\n \n       LOG.info(reduceId +\n           \" Finished merging \" + inputs.size() + \n           \" map output files on disk of total-size \" + \n           approxOutputSize + \".\" + \n           \" Local output file is \" + outputPath + \" of size \" +\n           localFS.getFileStatus(outputPath).getLen());\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (CompressAwarePath file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      CompressAwarePath compressAwarePath;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength());\n        writer.close();\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(compressAwarePath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "da4cab10990b3a352fc2c699f3b41c994ac55e95": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "Revering MAPREDUCE-2264\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1439561 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/01/13 10:58 AM",
      "commitName": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
      "commitAuthor": "Alejandro Abdelnur",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "Revering MAPREDUCE-2264\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1439561 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/01/13 10:58 AM",
          "commitName": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "24/01/13 4:25 PM",
          "commitNameOld": "539153a6798a667d39f20972c5ae0936060e2cc1",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 3.77,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,60 @@\n-    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n+    public void merge(List\u003cPath\u003e inputs) throws IOException {\n       // sanity check\n       if (inputs \u003d\u003d null || inputs.isEmpty()) {\n         LOG.info(\"No ondisk files to merge...\");\n         return;\n       }\n       \n       long approxOutputSize \u003d 0;\n       int bytesPerSum \u003d \n         jobConf.getInt(\"io.bytes.per.checksum\", 512);\n       \n       LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n                \" map outputs on disk. Triggering merge...\");\n       \n       // 1. Prepare the list of files to be merged. \n-      for (CompressAwarePath file : inputs) {\n-        approxOutputSize +\u003d localFS.getFileStatus(file.getPath()).getLen();\n+      for (Path file : inputs) {\n+        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n       }\n \n       // add the checksum length\n       approxOutputSize +\u003d \n         ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n \n       // 2. Start the on-disk merge process\n       Path outputPath \u003d \n         localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n             approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n       RawKeyValueIterator iter  \u003d null;\n-      CompressAwarePath compressAwarePath;\n       Path tmpDir \u003d new Path(reduceId.toString());\n       try {\n         iter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                             codec, inputs.toArray(new Path[inputs.size()]), \n                             true, ioSortFactor, tmpDir, \n                             (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                             reporter, spilledRecordsCounter, null, \n                             mergedMapOutputsCounter, null);\n \n         Merger.writeFile(iter, writer, reporter, jobConf);\n-        compressAwarePath \u003d new CompressAwarePath(outputPath,\n-            writer.getRawLength());\n         writer.close();\n       } catch (IOException e) {\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n-      closeOnDiskFile(compressAwarePath);\n+      closeOnDiskFile(outputPath);\n \n       LOG.info(reduceId +\n           \" Finished merging \" + inputs.size() + \n           \" map output files on disk of total-size \" + \n           approxOutputSize + \".\" + \n           \" Local output file is \" + outputPath + \" of size \" +\n           localFS.getFileStatus(outputPath).getLen());\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void merge(List\u003cPath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (Path file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        writer.close();\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(outputPath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {
            "oldValue": "[inputs-List\u003cCompressAwarePath\u003e]",
            "newValue": "[inputs-List\u003cPath\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "Revering MAPREDUCE-2264\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1439561 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/01/13 10:58 AM",
          "commitName": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "24/01/13 4:25 PM",
          "commitNameOld": "539153a6798a667d39f20972c5ae0936060e2cc1",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 3.77,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,60 @@\n-    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n+    public void merge(List\u003cPath\u003e inputs) throws IOException {\n       // sanity check\n       if (inputs \u003d\u003d null || inputs.isEmpty()) {\n         LOG.info(\"No ondisk files to merge...\");\n         return;\n       }\n       \n       long approxOutputSize \u003d 0;\n       int bytesPerSum \u003d \n         jobConf.getInt(\"io.bytes.per.checksum\", 512);\n       \n       LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n                \" map outputs on disk. Triggering merge...\");\n       \n       // 1. Prepare the list of files to be merged. \n-      for (CompressAwarePath file : inputs) {\n-        approxOutputSize +\u003d localFS.getFileStatus(file.getPath()).getLen();\n+      for (Path file : inputs) {\n+        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n       }\n \n       // add the checksum length\n       approxOutputSize +\u003d \n         ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n \n       // 2. Start the on-disk merge process\n       Path outputPath \u003d \n         localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n             approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n       RawKeyValueIterator iter  \u003d null;\n-      CompressAwarePath compressAwarePath;\n       Path tmpDir \u003d new Path(reduceId.toString());\n       try {\n         iter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                             codec, inputs.toArray(new Path[inputs.size()]), \n                             true, ioSortFactor, tmpDir, \n                             (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                             reporter, spilledRecordsCounter, null, \n                             mergedMapOutputsCounter, null);\n \n         Merger.writeFile(iter, writer, reporter, jobConf);\n-        compressAwarePath \u003d new CompressAwarePath(outputPath,\n-            writer.getRawLength());\n         writer.close();\n       } catch (IOException e) {\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n-      closeOnDiskFile(compressAwarePath);\n+      closeOnDiskFile(outputPath);\n \n       LOG.info(reduceId +\n           \" Finished merging \" + inputs.size() + \n           \" map output files on disk of total-size \" + \n           approxOutputSize + \".\" + \n           \" Local output file is \" + outputPath + \" of size \" +\n           localFS.getFileStatus(outputPath).getLen());\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void merge(List\u003cPath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (Path file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        writer.close();\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(outputPath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "539153a6798a667d39f20972c5ae0936060e2cc1": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1438277 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/01/13 4:25 PM",
      "commitName": "539153a6798a667d39f20972c5ae0936060e2cc1",
      "commitAuthor": "Alejandro Abdelnur",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1438277 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "24/01/13 4:25 PM",
          "commitName": "539153a6798a667d39f20972c5ae0936060e2cc1",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "22/01/13 6:10 AM",
          "commitNameOld": "73fd247c7649919350ecfd16806af57ffe554649",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 2.43,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,63 @@\n-    public void merge(List\u003cPath\u003e inputs) throws IOException {\n+    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n       // sanity check\n       if (inputs \u003d\u003d null || inputs.isEmpty()) {\n         LOG.info(\"No ondisk files to merge...\");\n         return;\n       }\n       \n       long approxOutputSize \u003d 0;\n       int bytesPerSum \u003d \n         jobConf.getInt(\"io.bytes.per.checksum\", 512);\n       \n       LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n                \" map outputs on disk. Triggering merge...\");\n       \n       // 1. Prepare the list of files to be merged. \n-      for (Path file : inputs) {\n-        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n+      for (CompressAwarePath file : inputs) {\n+        approxOutputSize +\u003d localFS.getFileStatus(file.getPath()).getLen();\n       }\n \n       // add the checksum length\n       approxOutputSize +\u003d \n         ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n \n       // 2. Start the on-disk merge process\n       Path outputPath \u003d \n         localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n             approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n       RawKeyValueIterator iter  \u003d null;\n+      CompressAwarePath compressAwarePath;\n       Path tmpDir \u003d new Path(reduceId.toString());\n       try {\n         iter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                             codec, inputs.toArray(new Path[inputs.size()]), \n                             true, ioSortFactor, tmpDir, \n                             (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                             reporter, spilledRecordsCounter, null, \n                             mergedMapOutputsCounter, null);\n \n         Merger.writeFile(iter, writer, reporter, jobConf);\n+        compressAwarePath \u003d new CompressAwarePath(outputPath,\n+            writer.getRawLength());\n         writer.close();\n       } catch (IOException e) {\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n-      closeOnDiskFile(outputPath);\n+      closeOnDiskFile(compressAwarePath);\n \n       LOG.info(reduceId +\n           \" Finished merging \" + inputs.size() + \n           \" map output files on disk of total-size \" + \n           approxOutputSize + \".\" + \n           \" Local output file is \" + outputPath + \" of size \" +\n           localFS.getFileStatus(outputPath).getLen());\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (CompressAwarePath file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file.getPath()).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      CompressAwarePath compressAwarePath;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength());\n        writer.close();\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(compressAwarePath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {
            "oldValue": "[inputs-List\u003cPath\u003e]",
            "newValue": "[inputs-List\u003cCompressAwarePath\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1438277 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "24/01/13 4:25 PM",
          "commitName": "539153a6798a667d39f20972c5ae0936060e2cc1",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "22/01/13 6:10 AM",
          "commitNameOld": "73fd247c7649919350ecfd16806af57ffe554649",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 2.43,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,63 @@\n-    public void merge(List\u003cPath\u003e inputs) throws IOException {\n+    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n       // sanity check\n       if (inputs \u003d\u003d null || inputs.isEmpty()) {\n         LOG.info(\"No ondisk files to merge...\");\n         return;\n       }\n       \n       long approxOutputSize \u003d 0;\n       int bytesPerSum \u003d \n         jobConf.getInt(\"io.bytes.per.checksum\", 512);\n       \n       LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n                \" map outputs on disk. Triggering merge...\");\n       \n       // 1. Prepare the list of files to be merged. \n-      for (Path file : inputs) {\n-        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n+      for (CompressAwarePath file : inputs) {\n+        approxOutputSize +\u003d localFS.getFileStatus(file.getPath()).getLen();\n       }\n \n       // add the checksum length\n       approxOutputSize +\u003d \n         ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n \n       // 2. Start the on-disk merge process\n       Path outputPath \u003d \n         localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n             approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n       Writer\u003cK,V\u003e writer \u003d \n         new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                         (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                         (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                         codec, null);\n       RawKeyValueIterator iter  \u003d null;\n+      CompressAwarePath compressAwarePath;\n       Path tmpDir \u003d new Path(reduceId.toString());\n       try {\n         iter \u003d Merger.merge(jobConf, rfs,\n                             (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                             (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                             codec, inputs.toArray(new Path[inputs.size()]), \n                             true, ioSortFactor, tmpDir, \n                             (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                             reporter, spilledRecordsCounter, null, \n                             mergedMapOutputsCounter, null);\n \n         Merger.writeFile(iter, writer, reporter, jobConf);\n+        compressAwarePath \u003d new CompressAwarePath(outputPath,\n+            writer.getRawLength());\n         writer.close();\n       } catch (IOException e) {\n         localFS.delete(outputPath, true);\n         throw e;\n       }\n \n-      closeOnDiskFile(outputPath);\n+      closeOnDiskFile(compressAwarePath);\n \n       LOG.info(reduceId +\n           \" Finished merging \" + inputs.size() + \n           \" map output files on disk of total-size \" + \n           approxOutputSize + \".\" + \n           \" Local output file is \" + outputPath + \" of size \" +\n           localFS.getFileStatus(outputPath).getLen());\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void merge(List\u003cCompressAwarePath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (CompressAwarePath file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file.getPath()).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      CompressAwarePath compressAwarePath;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        compressAwarePath \u003d new CompressAwarePath(outputPath,\n            writer.getRawLength());\n        writer.close();\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(compressAwarePath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "73fd247c7649919350ecfd16806af57ffe554649": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-4808. Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations. (masokan via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1436936 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/01/13 6:10 AM",
      "commitName": "73fd247c7649919350ecfd16806af57ffe554649",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "21/01/13 6:59 PM",
      "commitNameOld": "cfae13306ac0fb3f3c139d5ac511bf78cede1b77",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.47,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void merge(List\u003cPath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (Path file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        writer.close();\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(outputPath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
        "oldMethodName": "merge",
        "newMethodName": "merge"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void merge(List\u003cPath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (Path file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        writer.close();\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(outputPath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void merge(List\u003cPath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (Path file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        writer.close();\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(outputPath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
        "oldMethodName": "merge",
        "newMethodName": "merge"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,60 @@\n+    public void merge(List\u003cPath\u003e inputs) throws IOException {\n+      // sanity check\n+      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n+        LOG.info(\"No ondisk files to merge...\");\n+        return;\n+      }\n+      \n+      long approxOutputSize \u003d 0;\n+      int bytesPerSum \u003d \n+        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n+      \n+      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n+               \" map outputs on disk. Triggering merge...\");\n+      \n+      // 1. Prepare the list of files to be merged. \n+      for (Path file : inputs) {\n+        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n+      }\n+\n+      // add the checksum length\n+      approxOutputSize +\u003d \n+        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n+\n+      // 2. Start the on-disk merge process\n+      Path outputPath \u003d \n+        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n+            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n+      Writer\u003cK,V\u003e writer \u003d \n+        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n+                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n+                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n+                        codec, null);\n+      RawKeyValueIterator iter  \u003d null;\n+      Path tmpDir \u003d new Path(reduceId.toString());\n+      try {\n+        iter \u003d Merger.merge(jobConf, rfs,\n+                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n+                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n+                            codec, inputs.toArray(new Path[inputs.size()]), \n+                            true, ioSortFactor, tmpDir, \n+                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n+                            reporter, spilledRecordsCounter, null, \n+                            mergedMapOutputsCounter, null);\n+\n+        Merger.writeFile(iter, writer, reporter, jobConf);\n+        writer.close();\n+      } catch (IOException e) {\n+        localFS.delete(outputPath, true);\n+        throw e;\n+      }\n+\n+      closeOnDiskFile(outputPath);\n+\n+      LOG.info(reduceId +\n+          \" Finished merging \" + inputs.size() + \n+          \" map output files on disk of total-size \" + \n+          approxOutputSize + \".\" + \n+          \" Local output file is \" + outputPath + \" of size \" +\n+          localFS.getFileStatus(outputPath).getLen());\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public void merge(List\u003cPath\u003e inputs) throws IOException {\n      // sanity check\n      if (inputs \u003d\u003d null || inputs.isEmpty()) {\n        LOG.info(\"No ondisk files to merge...\");\n        return;\n      }\n      \n      long approxOutputSize \u003d 0;\n      int bytesPerSum \u003d \n        jobConf.getInt(\"io.bytes.per.checksum\", 512);\n      \n      LOG.info(\"OnDiskMerger: We have  \" + inputs.size() + \n               \" map outputs on disk. Triggering merge...\");\n      \n      // 1. Prepare the list of files to be merged. \n      for (Path file : inputs) {\n        approxOutputSize +\u003d localFS.getFileStatus(file).getLen();\n      }\n\n      // add the checksum length\n      approxOutputSize +\u003d \n        ChecksumFileSystem.getChecksumLength(approxOutputSize, bytesPerSum);\n\n      // 2. Start the on-disk merge process\n      Path outputPath \u003d \n        localDirAllocator.getLocalPathForWrite(inputs.get(0).toString(), \n            approxOutputSize, jobConf).suffix(Task.MERGED_OUTPUT_PREFIX);\n      Writer\u003cK,V\u003e writer \u003d \n        new Writer\u003cK,V\u003e(jobConf, rfs, outputPath, \n                        (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(), \n                        (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                        codec, null);\n      RawKeyValueIterator iter  \u003d null;\n      Path tmpDir \u003d new Path(reduceId.toString());\n      try {\n        iter \u003d Merger.merge(jobConf, rfs,\n                            (Class\u003cK\u003e) jobConf.getMapOutputKeyClass(),\n                            (Class\u003cV\u003e) jobConf.getMapOutputValueClass(),\n                            codec, inputs.toArray(new Path[inputs.size()]), \n                            true, ioSortFactor, tmpDir, \n                            (RawComparator\u003cK\u003e) jobConf.getOutputKeyComparator(), \n                            reporter, spilledRecordsCounter, null, \n                            mergedMapOutputsCounter, null);\n\n        Merger.writeFile(iter, writer, reporter, jobConf);\n        writer.close();\n      } catch (IOException e) {\n        localFS.delete(outputPath, true);\n        throw e;\n      }\n\n      closeOnDiskFile(outputPath);\n\n      LOG.info(reduceId +\n          \" Finished merging \" + inputs.size() + \n          \" map output files on disk of total-size \" + \n          approxOutputSize + \".\" + \n          \" Local output file is \" + outputPath + \" of size \" +\n          localFS.getFileStatus(outputPath).getLen());\n    }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java"
    }
  }
}