{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "processExtraRedundancyBlocksOnInService",
  "functionId": "processExtraRedundancyBlocksOnInService___srcNode-DatanodeDescriptor(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 4477,
  "functionEndLine": 4521,
  "numCommitsSeen": 1188,
  "timeTaken": 23166,
  "changeHistory": [
    "be488b6070a124234c77f16193ee925d32ca9a20",
    "c36d69a7b30927eaea16335e06cfcc247accde35",
    "a7f085d6bf499edf23e650a4f7211c53a442da0e",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "f2ba7da4f0df6cf0fc245093aeb4500158e6ee0b",
    "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
    "715b9c649982bff91d1f9eae656ba3b82178e1a3",
    "745d04be59accf80feda0ad38efcc74ba362f2ca",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
    "6e3fcffe291faec40fa9214f4880a35a952836c4",
    "abf833a7b228fff2bca4f69cd9df99d532380038",
    "a38a37c63417a3b19dcdf98251af196c9d7b8c31",
    "6d5da9484185ca9f585195d6da069b9cd5be4044",
    "1be4ddef9ef08bfee9fdb8a6303e8a96a3e28a7d",
    "54b70db347c2ebf577919f2c42f171c6801e9ba1",
    "ad06a087131d69d173d8e03dce5c97650a530f2e",
    "7e8e983620f3ae3462d115972707c72b7d9cbabd",
    "f0f9a3631fe4950f5cf548f192226836925d0f05",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "7fac946ac983e31613fd62836c8ac9c4a579210a",
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
    "08928d067bb9e1d38b5e7db9e23fcf20fe161435"
  ],
  "changeHistoryShort": {
    "be488b6070a124234c77f16193ee925d32ca9a20": "Ybodychange",
    "c36d69a7b30927eaea16335e06cfcc247accde35": "Ybodychange",
    "a7f085d6bf499edf23e650a4f7211c53a442da0e": "Ybodychange",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ymultichange(Yrename,Ybodychange)",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": "Ybodychange",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ymultichange(Yrename,Ybodychange)",
    "f2ba7da4f0df6cf0fc245093aeb4500158e6ee0b": "Ybodychange",
    "47b92f2b6f2dafc129a41b247f35e77c8e47ffba": "Ybodychange",
    "715b9c649982bff91d1f9eae656ba3b82178e1a3": "Ybodychange",
    "745d04be59accf80feda0ad38efcc74ba362f2ca": "Ybodychange",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": "Ybodychange",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": "Ybodychange",
    "6e3fcffe291faec40fa9214f4880a35a952836c4": "Ybodychange",
    "abf833a7b228fff2bca4f69cd9df99d532380038": "Ybodychange",
    "a38a37c63417a3b19dcdf98251af196c9d7b8c31": "Ybodychange",
    "6d5da9484185ca9f585195d6da069b9cd5be4044": "Ybodychange",
    "1be4ddef9ef08bfee9fdb8a6303e8a96a3e28a7d": "Ybodychange",
    "54b70db347c2ebf577919f2c42f171c6801e9ba1": "Ybodychange",
    "ad06a087131d69d173d8e03dce5c97650a530f2e": "Ybodychange",
    "7e8e983620f3ae3462d115972707c72b7d9cbabd": "Ybodychange",
    "f0f9a3631fe4950f5cf548f192226836925d0f05": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "7fac946ac983e31613fd62836c8ac9c4a579210a": "Ymodifierchange",
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9": "Ymultichange(Ymodifierchange,Yparametermetachange)",
    "08928d067bb9e1d38b5e7db9e23fcf20fe161435": "Yintroduced"
  },
  "changeHistoryDetails": {
    "be488b6070a124234c77f16193ee925d32ca9a20": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10477. Stop decommission a rack of DataNodes caused NameNode fail over to standby. Contributed by  yunjiong zhao and Wei-Chiu Chuang.\n",
      "commitDate": "03/04/19 11:00 AM",
      "commitName": "be488b6070a124234c77f16193ee925d32ca9a20",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "15/03/19 11:06 AM",
      "commitNameOld": "ff06ef0631cb8a0f67bbc39b5b5a1b0a81ca3b3c",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 19.0,
      "commitsBetweenForRepo": 129,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,45 @@\n   void processExtraRedundancyBlocksOnInService(\n       final DatanodeDescriptor srcNode) {\n     if (!isPopulatingReplQueues()) {\n       return;\n     }\n-    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n+\n     int numExtraRedundancy \u003d 0;\n-    while(it.hasNext()) {\n-      final BlockInfo block \u003d it.next();\n-      if (block.isDeleted()) {\n-        //Orphan block, will be handled eventually, skip\n+    for (DatanodeStorageInfo datanodeStorageInfo : srcNode.getStorageInfos()) {\n+      // the namesystem lock is released between iterations. Make sure the\n+      // storage is not removed before continuing.\n+      if (srcNode.getStorageInfo(datanodeStorageInfo.getStorageID()) \u003d\u003d null) {\n         continue;\n       }\n-      int expectedReplication \u003d this.getExpectedRedundancyNum(block);\n-      NumberReplicas num \u003d countNodes(block);\n-      if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n-        // extra redundancy block\n-        processExtraRedundancyBlock(block, (short) expectedReplication, null,\n-            null);\n-        numExtraRedundancy++;\n+      final Iterator\u003cBlockInfo\u003e it \u003d datanodeStorageInfo.getBlockIterator();\n+      while(it.hasNext()) {\n+        final BlockInfo block \u003d it.next();\n+        if (block.isDeleted()) {\n+          //Orphan block, will be handled eventually, skip\n+          continue;\n+        }\n+        int expectedReplication \u003d this.getExpectedRedundancyNum(block);\n+        NumberReplicas num \u003d countNodes(block);\n+        if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n+          // extra redundancy block\n+          processExtraRedundancyBlock(block, (short) expectedReplication, null,\n+              null);\n+          numExtraRedundancy++;\n+        }\n+      }\n+      // When called by tests like TestDefaultBlockPlacementPolicy.\n+      // testPlacementWithLocalRackNodesDecommissioned, it is not protected by\n+      // lock, only when called by DatanodeManager.refreshNodes have writeLock\n+      if (namesystem.hasWriteLock()) {\n+        namesystem.writeUnlock();\n+        try {\n+          Thread.sleep(1);\n+        } catch (InterruptedException e) {\n+          Thread.currentThread().interrupt();\n+        }\n+        namesystem.writeLock();\n       }\n     }\n     LOG.info(\"Invalidated {} extra redundancy blocks on {} after \"\n              + \"it is in service\", numExtraRedundancy, srcNode);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processExtraRedundancyBlocksOnInService(\n      final DatanodeDescriptor srcNode) {\n    if (!isPopulatingReplQueues()) {\n      return;\n    }\n\n    int numExtraRedundancy \u003d 0;\n    for (DatanodeStorageInfo datanodeStorageInfo : srcNode.getStorageInfos()) {\n      // the namesystem lock is released between iterations. Make sure the\n      // storage is not removed before continuing.\n      if (srcNode.getStorageInfo(datanodeStorageInfo.getStorageID()) \u003d\u003d null) {\n        continue;\n      }\n      final Iterator\u003cBlockInfo\u003e it \u003d datanodeStorageInfo.getBlockIterator();\n      while(it.hasNext()) {\n        final BlockInfo block \u003d it.next();\n        if (block.isDeleted()) {\n          //Orphan block, will be handled eventually, skip\n          continue;\n        }\n        int expectedReplication \u003d this.getExpectedRedundancyNum(block);\n        NumberReplicas num \u003d countNodes(block);\n        if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n          // extra redundancy block\n          processExtraRedundancyBlock(block, (short) expectedReplication, null,\n              null);\n          numExtraRedundancy++;\n        }\n      }\n      // When called by tests like TestDefaultBlockPlacementPolicy.\n      // testPlacementWithLocalRackNodesDecommissioned, it is not protected by\n      // lock, only when called by DatanodeManager.refreshNodes have writeLock\n      if (namesystem.hasWriteLock()) {\n        namesystem.writeUnlock();\n        try {\n          Thread.sleep(1);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        namesystem.writeLock();\n      }\n    }\n    LOG.info(\"Invalidated {} extra redundancy blocks on {} after \"\n             + \"it is in service\", numExtraRedundancy, srcNode);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "c36d69a7b30927eaea16335e06cfcc247accde35": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13027. Handle possible NPEs due to deleted blocks in race condition. Contributed by Vinayakumar B.\n\n(cherry picked from commit 65977e5d8124be2bc208af25beed934933f170b3)\n",
      "commitDate": "30/08/18 9:45 AM",
      "commitName": "c36d69a7b30927eaea16335e06cfcc247accde35",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "16/08/18 4:29 PM",
      "commitNameOld": "1290e3c647092f0bfbb250731a6805aba1be8e4b",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 13.72,
      "commitsBetweenForRepo": 84,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,25 @@\n   void processExtraRedundancyBlocksOnInService(\n       final DatanodeDescriptor srcNode) {\n     if (!isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numExtraRedundancy \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n+      if (block.isDeleted()) {\n+        //Orphan block, will be handled eventually, skip\n+        continue;\n+      }\n       int expectedReplication \u003d this.getExpectedRedundancyNum(block);\n       NumberReplicas num \u003d countNodes(block);\n       if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n         // extra redundancy block\n         processExtraRedundancyBlock(block, (short) expectedReplication, null,\n             null);\n         numExtraRedundancy++;\n       }\n     }\n     LOG.info(\"Invalidated {} extra redundancy blocks on {} after \"\n              + \"it is in service\", numExtraRedundancy, srcNode);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processExtraRedundancyBlocksOnInService(\n      final DatanodeDescriptor srcNode) {\n    if (!isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numExtraRedundancy \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      if (block.isDeleted()) {\n        //Orphan block, will be handled eventually, skip\n        continue;\n      }\n      int expectedReplication \u003d this.getExpectedRedundancyNum(block);\n      NumberReplicas num \u003d countNodes(block);\n      if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n        // extra redundancy block\n        processExtraRedundancyBlock(block, (short) expectedReplication, null,\n            null);\n        numExtraRedundancy++;\n      }\n    }\n    LOG.info(\"Invalidated {} extra redundancy blocks on {} after \"\n             + \"it is in service\", numExtraRedundancy, srcNode);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "a7f085d6bf499edf23e650a4f7211c53a442da0e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11832. Switch leftover logs to slf4j format in BlockManager.java. Contributed by Hui Xu and Chen Liang.\n",
      "commitDate": "29/05/17 1:30 AM",
      "commitName": "a7f085d6bf499edf23e650a4f7211c53a442da0e",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "25/05/17 7:35 AM",
      "commitNameOld": "2e41f8803dd46d1bab16c1b206c71be72ea260a1",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 3.75,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   void processExtraRedundancyBlocksOnInService(\n       final DatanodeDescriptor srcNode) {\n     if (!isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numExtraRedundancy \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n       int expectedReplication \u003d this.getExpectedRedundancyNum(block);\n       NumberReplicas num \u003d countNodes(block);\n       if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n         // extra redundancy block\n         processExtraRedundancyBlock(block, (short) expectedReplication, null,\n             null);\n         numExtraRedundancy++;\n       }\n     }\n-    LOG.info(\"Invalidated \" + numExtraRedundancy\n-        + \" extra redundancy blocks on \" + srcNode + \" after it is in service\");\n+    LOG.info(\"Invalidated {} extra redundancy blocks on {} after \"\n+             + \"it is in service\", numExtraRedundancy, srcNode);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processExtraRedundancyBlocksOnInService(\n      final DatanodeDescriptor srcNode) {\n    if (!isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numExtraRedundancy \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      int expectedReplication \u003d this.getExpectedRedundancyNum(block);\n      NumberReplicas num \u003d countNodes(block);\n      if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n        // extra redundancy block\n        processExtraRedundancyBlock(block, (short) expectedReplication, null,\n            null);\n        numExtraRedundancy++;\n      }\n    }\n    LOG.info(\"Invalidated {} extra redundancy blocks on {} after \"\n             + \"it is in service\", numExtraRedundancy, srcNode);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ymultichange(Yrename,Ybodychange)",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
          "commitDate": "17/10/16 5:45 PM",
          "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
          "commitAuthor": "Ming Ma",
          "commitDateOld": "14/10/16 6:13 PM",
          "commitNameOld": "391ce535a739dc92cb90017d759217265a4fd969",
          "commitAuthorOld": "Vinitha Reddy Gankidi",
          "daysBetweenCommits": 2.98,
          "commitsBetweenForRepo": 11,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  void processExtraRedundancyBlocksOnReCommission(\n+  void processExtraRedundancyBlocksOnInService(\n       final DatanodeDescriptor srcNode) {\n     if (!isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numExtraRedundancy \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n-      int expectedReplication \u003d this.getRedundancy(block);\n+      int expectedReplication \u003d this.getExpectedRedundancyNum(block);\n       NumberReplicas num \u003d countNodes(block);\n       if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n         // extra redundancy block\n         processExtraRedundancyBlock(block, (short) expectedReplication, null,\n             null);\n         numExtraRedundancy++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numExtraRedundancy\n-        + \" extra redundancy blocks on \" + srcNode + \" during recommissioning\");\n+        + \" extra redundancy blocks on \" + srcNode + \" after it is in service\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void processExtraRedundancyBlocksOnInService(\n      final DatanodeDescriptor srcNode) {\n    if (!isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numExtraRedundancy \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      int expectedReplication \u003d this.getExpectedRedundancyNum(block);\n      NumberReplicas num \u003d countNodes(block);\n      if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n        // extra redundancy block\n        processExtraRedundancyBlock(block, (short) expectedReplication, null,\n            null);\n        numExtraRedundancy++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numExtraRedundancy\n        + \" extra redundancy blocks on \" + srcNode + \" after it is in service\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "processExtraRedundancyBlocksOnReCommission",
            "newValue": "processExtraRedundancyBlocksOnInService"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
          "commitDate": "17/10/16 5:45 PM",
          "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
          "commitAuthor": "Ming Ma",
          "commitDateOld": "14/10/16 6:13 PM",
          "commitNameOld": "391ce535a739dc92cb90017d759217265a4fd969",
          "commitAuthorOld": "Vinitha Reddy Gankidi",
          "daysBetweenCommits": 2.98,
          "commitsBetweenForRepo": 11,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  void processExtraRedundancyBlocksOnReCommission(\n+  void processExtraRedundancyBlocksOnInService(\n       final DatanodeDescriptor srcNode) {\n     if (!isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numExtraRedundancy \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n-      int expectedReplication \u003d this.getRedundancy(block);\n+      int expectedReplication \u003d this.getExpectedRedundancyNum(block);\n       NumberReplicas num \u003d countNodes(block);\n       if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n         // extra redundancy block\n         processExtraRedundancyBlock(block, (short) expectedReplication, null,\n             null);\n         numExtraRedundancy++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numExtraRedundancy\n-        + \" extra redundancy blocks on \" + srcNode + \" during recommissioning\");\n+        + \" extra redundancy blocks on \" + srcNode + \" after it is in service\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void processExtraRedundancyBlocksOnInService(\n      final DatanodeDescriptor srcNode) {\n    if (!isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numExtraRedundancy \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      int expectedReplication \u003d this.getExpectedRedundancyNum(block);\n      NumberReplicas num \u003d countNodes(block);\n      if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n        // extra redundancy block\n        processExtraRedundancyBlock(block, (short) expectedReplication, null,\n            null);\n        numExtraRedundancy++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numExtraRedundancy\n        + \" extra redundancy blocks on \" + srcNode + \" after it is in service\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10236. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-3]. Contributed by Rakesh R.\n",
      "commitDate": "26/05/16 4:50 PM",
      "commitName": "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "28/04/16 10:44 AM",
      "commitNameOld": "6243eabb48390fffada2418ade5adf9e0766afbe",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 28.25,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   void processExtraRedundancyBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numExtraRedundancy \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n-      int expectedReplication \u003d this.getReplication(block);\n+      int expectedReplication \u003d this.getRedundancy(block);\n       NumberReplicas num \u003d countNodes(block);\n       if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n         // extra redundancy block\n         processExtraRedundancyBlock(block, (short) expectedReplication, null,\n             null);\n         numExtraRedundancy++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numExtraRedundancy\n         + \" extra redundancy blocks on \" + srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processExtraRedundancyBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numExtraRedundancy \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      int expectedReplication \u003d this.getRedundancy(block);\n      NumberReplicas num \u003d countNodes(block);\n      if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n        // extra redundancy block\n        processExtraRedundancyBlock(block, (short) expectedReplication, null,\n            null);\n        numExtraRedundancy++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numExtraRedundancy\n        + \" extra redundancy blocks on \" + srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ymultichange(Yrename,Ybodychange)",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
          "commitDate": "16/03/16 4:53 PM",
          "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "10/03/16 7:03 PM",
          "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 5.87,
          "commitsBetweenForRepo": 26,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  void processOverReplicatedBlocksOnReCommission(\n+  void processExtraRedundancyBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n-    int numOverReplicated \u003d 0;\n+    int numExtraRedundancy \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n       int expectedReplication \u003d this.getReplication(block);\n       NumberReplicas num \u003d countNodes(block);\n-      if (shouldProcessOverReplicated(num, expectedReplication)) {\n-        // over-replicated block\n-        processOverReplicatedBlock(block, (short) expectedReplication, null,\n+      if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n+        // extra redundancy block\n+        processExtraRedundancyBlock(block, (short) expectedReplication, null,\n             null);\n-        numOverReplicated++;\n+        numExtraRedundancy++;\n       }\n     }\n-    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n-        srcNode + \" during recommissioning\");\n+    LOG.info(\"Invalidated \" + numExtraRedundancy\n+        + \" extra redundancy blocks on \" + srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void processExtraRedundancyBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numExtraRedundancy \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      int expectedReplication \u003d this.getReplication(block);\n      NumberReplicas num \u003d countNodes(block);\n      if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n        // extra redundancy block\n        processExtraRedundancyBlock(block, (short) expectedReplication, null,\n            null);\n        numExtraRedundancy++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numExtraRedundancy\n        + \" extra redundancy blocks on \" + srcNode + \" during recommissioning\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "processOverReplicatedBlocksOnReCommission",
            "newValue": "processExtraRedundancyBlocksOnReCommission"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
          "commitDate": "16/03/16 4:53 PM",
          "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "10/03/16 7:03 PM",
          "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 5.87,
          "commitsBetweenForRepo": 26,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  void processOverReplicatedBlocksOnReCommission(\n+  void processExtraRedundancyBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n-    int numOverReplicated \u003d 0;\n+    int numExtraRedundancy \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n       int expectedReplication \u003d this.getReplication(block);\n       NumberReplicas num \u003d countNodes(block);\n-      if (shouldProcessOverReplicated(num, expectedReplication)) {\n-        // over-replicated block\n-        processOverReplicatedBlock(block, (short) expectedReplication, null,\n+      if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n+        // extra redundancy block\n+        processExtraRedundancyBlock(block, (short) expectedReplication, null,\n             null);\n-        numOverReplicated++;\n+        numExtraRedundancy++;\n       }\n     }\n-    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n-        srcNode + \" during recommissioning\");\n+    LOG.info(\"Invalidated \" + numExtraRedundancy\n+        + \" extra redundancy blocks on \" + srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void processExtraRedundancyBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numExtraRedundancy \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      int expectedReplication \u003d this.getReplication(block);\n      NumberReplicas num \u003d countNodes(block);\n      if (shouldProcessExtraRedundancy(num, expectedReplication)) {\n        // extra redundancy block\n        processExtraRedundancyBlock(block, (short) expectedReplication, null,\n            null);\n        numExtraRedundancy++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numExtraRedundancy\n        + \" extra redundancy blocks on \" + srcNode + \" during recommissioning\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "f2ba7da4f0df6cf0fc245093aeb4500158e6ee0b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9876. shouldProcessOverReplicated should not count number of pending replicas. Contributed by Jing Zhao.\n",
      "commitDate": "01/03/16 6:41 PM",
      "commitName": "f2ba7da4f0df6cf0fc245093aeb4500158e6ee0b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "28/02/16 2:54 PM",
      "commitNameOld": "408f2c807bbaaaa37ce1b69a5dfa9d76ed427d6e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.16,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n       int expectedReplication \u003d this.getReplication(block);\n       NumberReplicas num \u003d countNodes(block);\n-      if (shouldProcessOverReplicated(num, 0, expectedReplication)) {\n+      if (shouldProcessOverReplicated(num, expectedReplication)) {\n         // over-replicated block\n         processOverReplicatedBlock(block, (short) expectedReplication, null,\n             null);\n         numOverReplicated++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n         srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      int expectedReplication \u003d this.getReplication(block);\n      NumberReplicas num \u003d countNodes(block);\n      if (shouldProcessOverReplicated(num, expectedReplication)) {\n        // over-replicated block\n        processOverReplicatedBlock(block, (short) expectedReplication, null,\n            null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "47b92f2b6f2dafc129a41b247f35e77c8e47ffba": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9837. BlockManager#countNodes should be able to detect duplicated internal blocks. Contributed by Jing Zhao.\n",
      "commitDate": "24/02/16 3:13 PM",
      "commitName": "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "20/02/16 11:19 PM",
      "commitNameOld": "d5abd293a890a8a1da48a166a291ae1c5644ad57",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 3.66,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,21 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n       int expectedReplication \u003d this.getReplication(block);\n       NumberReplicas num \u003d countNodes(block);\n-      int numCurrentReplica \u003d num.liveReplicas();\n-      if (numCurrentReplica \u003e expectedReplication) {\n-        // over-replicated block \n+      if (shouldProcessOverReplicated(num, 0, expectedReplication)) {\n+        // over-replicated block\n         processOverReplicatedBlock(block, (short) expectedReplication, null,\n             null);\n         numOverReplicated++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n         srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      int expectedReplication \u003d this.getReplication(block);\n      NumberReplicas num \u003d countNodes(block);\n      if (shouldProcessOverReplicated(num, 0, expectedReplication)) {\n        // over-replicated block\n        processOverReplicatedBlock(block, (short) expectedReplication, null,\n            null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "715b9c649982bff91d1f9eae656ba3b82178e1a3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8984. Move replication queues related methods in FSNamesystem to BlockManager. Contributed by Haohui Mai.\n",
      "commitDate": "04/09/15 11:45 AM",
      "commitName": "715b9c649982bff91d1f9eae656ba3b82178e1a3",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/09/15 1:46 PM",
      "commitNameOld": "afc88b396f06488c331564e0f6987013bb920d3e",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.92,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n-    if (!namesystem.isPopulatingReplQueues()) {\n+    if (!isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n       short expectedReplication \u003d block.getReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n         numOverReplicated++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n         srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      short expectedReplication \u003d block.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "745d04be59accf80feda0ad38efcc74ba362f2ca": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8823. Move replication factor into individual blocks. Contributed by Haohui Mai.\n",
      "commitDate": "22/08/15 12:09 AM",
      "commitName": "745d04be59accf80feda0ad38efcc74ba362f2ca",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/08/15 3:11 PM",
      "commitNameOld": "4e14f7982a6e57bf08deb3b266806c2b779a157d",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.37,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,21 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!namesystem.isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n-      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n-      short expectedReplication \u003d bc.getPreferredBlockReplication();\n+      short expectedReplication \u003d block.getReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n         numOverReplicated++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n         srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!namesystem.isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      short expectedReplication \u003d block.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\"\n\nThis reverts commit de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "31/07/15 4:15 PM",
      "commitNameOld": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 5.75,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!namesystem.isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n-      int expectedReplication \u003d this.getReplication(block);\n+      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n+      short expectedReplication \u003d bc.getPreferredBlockReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n-        processOverReplicatedBlock(block, (short) expectedReplication, null,\n-            null);\n+        processOverReplicatedBlock(block, expectedReplication, null, null);\n         numOverReplicated++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n         srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!namesystem.isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n      short expectedReplication \u003d bc.getPreferredBlockReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\n",
      "commitDate": "26/06/15 10:49 AM",
      "commitName": "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/06/15 2:42 PM",
      "commitNameOld": "afe9ea3c12e1f5a71922400eadb642960bc87ca1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!namesystem.isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n-      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n-      short expectedReplication \u003d bc.getPreferredBlockReplication();\n+      int expectedReplication \u003d this.getReplication(block);\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n-        processOverReplicatedBlock(block, expectedReplication, null, null);\n+        processOverReplicatedBlock(block, (short) expectedReplication, null,\n+            null);\n         numOverReplicated++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n         srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!namesystem.isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      int expectedReplication \u003d this.getReplication(block);\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, (short) expectedReplication, null,\n            null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "6e3fcffe291faec40fa9214f4880a35a952836c4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8608. Merge HDFS-7912 to trunk and branch-2 (track BlockInfo instead of Block in UnderReplicatedBlocks and PendingReplicationBlocks). Contributed by Zhe Zhang.\n",
      "commitDate": "17/06/15 8:05 AM",
      "commitName": "6e3fcffe291faec40fa9214f4880a35a952836c4",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "12/06/15 11:38 AM",
      "commitNameOld": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.85,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!namesystem.isPopulatingReplQueues()) {\n       return;\n     }\n-    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n+    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n-      final Block block \u003d it.next();\n+      final BlockInfo block \u003d it.next();\n       BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n       short expectedReplication \u003d bc.getPreferredBlockReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n         numOverReplicated++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n         srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!namesystem.isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n      short expectedReplication \u003d bc.getPreferredBlockReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "abf833a7b228fff2bca4f69cd9df99d532380038": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7907. Erasure Coding: track invalid, corrupt, and under-recovery striped blocks in NameNode. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:43 AM",
      "commitName": "abf833a7b228fff2bca4f69cd9df99d532380038",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:43 AM",
      "commitNameOld": "ea2e60fbcc79c65ec571224bd3f57c262a5d9114",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!namesystem.isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n       final BlockInfo block \u003d it.next();\n-      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n-      short expectedReplication \u003d bc.getPreferredBlockReplication();\n+      int expectedReplication \u003d this.getReplication(block);\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n-        processOverReplicatedBlock(block, expectedReplication, null, null);\n+        processOverReplicatedBlock(block, (short) expectedReplication, null,\n+            null);\n         numOverReplicated++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n         srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!namesystem.isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      int expectedReplication \u003d this.getReplication(block);\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, (short) expectedReplication, null,\n            null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "a38a37c63417a3b19dcdf98251af196c9d7b8c31": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7912. Erasure Coding: track BlockInfo instead of Block in UnderReplicatedBlocks and PendingReplicationBlocks. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:41 AM",
      "commitName": "a38a37c63417a3b19dcdf98251af196c9d7b8c31",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:32 AM",
      "commitNameOld": "f05c21285ef23b6a973d69f045b1cb46c5abc039",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!namesystem.isPopulatingReplQueues()) {\n       return;\n     }\n-    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n+    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n     int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n-      final Block block \u003d it.next();\n+      final BlockInfo block \u003d it.next();\n       BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n       short expectedReplication \u003d bc.getPreferredBlockReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n         numOverReplicated++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n         srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!namesystem.isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003cBlockInfo\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final BlockInfo block \u003d it.next();\n      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n      short expectedReplication \u003d bc.getPreferredBlockReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "6d5da9484185ca9f585195d6da069b9cd5be4044": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8255. Rename getBlockReplication to getPreferredBlockReplication. (Contributed by Zhe Zhang)\n",
      "commitDate": "12/05/15 6:29 AM",
      "commitName": "6d5da9484185ca9f585195d6da069b9cd5be4044",
      "commitAuthor": "yliu",
      "commitDateOld": "08/05/15 2:36 PM",
      "commitNameOld": "2d4ae3d18bc530fa9f81ee616db8af3395705fb9",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 3.66,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     if (!namesystem.isPopulatingReplQueues()) {\n       return;\n     }\n     final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n     int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n       final Block block \u003d it.next();\n       BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n-      short expectedReplication \u003d bc.getBlockReplication();\n+      short expectedReplication \u003d bc.getPreferredBlockReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n         numOverReplicated++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n         srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!namesystem.isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n      short expectedReplication \u003d bc.getPreferredBlockReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "1be4ddef9ef08bfee9fdb8a6303e8a96a3e28a7d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6178. Decommission on standby NN couldn\u0027t finish. Contributed by Ming Ma.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1589002 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/04/14 4:35 PM",
      "commitName": "1be4ddef9ef08bfee9fdb8a6303e8a96a3e28a7d",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/03/14 11:36 AM",
      "commitNameOld": "fe8c3dc2b80a2c127e7aed0d3beb41dcfd8f7eac",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 26.21,
      "commitsBetweenForRepo": 159,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,22 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n+    if (!namesystem.isPopulatingReplQueues()) {\n+      return;\n+    }\n     final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n     int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n       final Block block \u003d it.next();\n       BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n       short expectedReplication \u003d bc.getBlockReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n         numOverReplicated++;\n       }\n     }\n     LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n         srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    if (!namesystem.isPopulatingReplQueues()) {\n      return;\n    }\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n      short expectedReplication \u003d bc.getBlockReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "54b70db347c2ebf577919f2c42f171c6801e9ba1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4075. Reduce recommissioning overhead (Kihwal Lee via daryn)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1406278 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/11/12 11:27 AM",
      "commitName": "54b70db347c2ebf577919f2c42f171c6801e9ba1",
      "commitAuthor": "Daryn Sharp",
      "commitDateOld": "28/10/12 4:10 PM",
      "commitNameOld": "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 8.85,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,19 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n+    int numOverReplicated \u003d 0;\n     while(it.hasNext()) {\n       final Block block \u003d it.next();\n       BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n       short expectedReplication \u003d bc.getBlockReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n+        numOverReplicated++;\n       }\n     }\n+    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n+        srcNode + \" during recommissioning\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    int numOverReplicated \u003d 0;\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n      short expectedReplication \u003d bc.getBlockReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n        numOverReplicated++;\n      }\n    }\n    LOG.info(\"Invalidated \" + numOverReplicated + \" over-replicated blocks on \" +\n        srcNode + \" during recommissioning\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "ad06a087131d69d173d8e03dce5c97650a530f2e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4037. Rename the getReplication() method in BlockCollection to getBlockReplication(). \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1398288 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/10/12 6:48 AM",
      "commitName": "ad06a087131d69d173d8e03dce5c97650a530f2e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "11/09/12 9:10 PM",
      "commitNameOld": "414abe69183a39b38c8f8936785dce3e4774f4ca",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 33.4,
      "commitsBetweenForRepo": 170,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n     while(it.hasNext()) {\n       final Block block \u003d it.next();\n       BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n-      short expectedReplication \u003d bc.getReplication();\n+      short expectedReplication \u003d bc.getBlockReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n      short expectedReplication \u003d bc.getBlockReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "7e8e983620f3ae3462d115972707c72b7d9cbabd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3369. Rename {get|set|add}INode(..) methods in BlockManager and BlocksMap to {get|set|add}BlockCollection(..).  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1336909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/05/12 2:41 PM",
      "commitName": "7e8e983620f3ae3462d115972707c72b7d9cbabd",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "10/05/12 2:59 AM",
      "commitNameOld": "f1ff05bf47a7dfb670bc63e4e6e58d74f6b5b4a7",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.49,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n     while(it.hasNext()) {\n       final Block block \u003d it.next();\n-      BlockCollection fileINode \u003d blocksMap.getINode(block);\n-      short expectedReplication \u003d fileINode.getReplication();\n+      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n+      short expectedReplication \u003d bc.getReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n      short expectedReplication \u003d bc.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "f0f9a3631fe4950f5cf548f192226836925d0f05": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3363. Define BlockCollection and MutableBlockCollection interfaces so that INodeFile and INodeFileUnderConstruction do not have to be used in block management.  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1335304 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/05/12 5:06 PM",
      "commitName": "f0f9a3631fe4950f5cf548f192226836925d0f05",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/05/12 4:02 PM",
      "commitNameOld": "8620a99d1eea163b7505cde0a57e849b1b2a3a6f",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.04,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n     while(it.hasNext()) {\n       final Block block \u003d it.next();\n-      INodeFile fileINode \u003d blocksMap.getINode(block);\n+      BlockCollection fileINode \u003d blocksMap.getINode(block);\n       short expectedReplication \u003d fileINode.getReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      BlockCollection fileINode \u003d blocksMap.getINode(block);\n      short expectedReplication \u003d fileINode.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      INodeFile fileINode \u003d blocksMap.getINode(block);\n      short expectedReplication \u003d fileINode.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      INodeFile fileINode \u003d blocksMap.getINode(block);\n      short expectedReplication \u003d fileINode.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "7fac946ac983e31613fd62836c8ac9c4a579210a": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-2108. Move datanode heartbeat handling from namenode package to blockmanagement package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154042 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/08/11 3:55 PM",
      "commitName": "7fac946ac983e31613fd62836c8ac9c4a579210a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/08/11 6:57 AM",
      "commitNameOld": "d68e38b78d9687987c4de2046ce9aa0016685e98",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.37,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n-  private void processOverReplicatedBlocksOnReCommission(\n+  void processOverReplicatedBlocksOnReCommission(\n       final DatanodeDescriptor srcNode) {\n     final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n     while(it.hasNext()) {\n       final Block block \u003d it.next();\n       INodeFile fileINode \u003d blocksMap.getINode(block);\n       short expectedReplication \u003d fileINode.getReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      INodeFile fileINode \u003d blocksMap.getINode(block);\n      short expectedReplication \u003d fileINode.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[]"
      }
    },
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9": {
      "type": "Ymultichange(Ymodifierchange,Yparametermetachange)",
      "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/07/11 9:20 PM",
      "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/07/11 9:20 PM",
          "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "20/07/11 4:35 PM",
          "commitNameOld": "08928d067bb9e1d38b5e7db9e23fcf20fe161435",
          "commitAuthorOld": "Matthew Foley",
          "daysBetweenCommits": 1.2,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,15 @@\n-  public void processOverReplicatedBlocksOnReCommission(DatanodeDescriptor srcNode) {\n+  private void processOverReplicatedBlocksOnReCommission(\n+      final DatanodeDescriptor srcNode) {\n     final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n     while(it.hasNext()) {\n       final Block block \u003d it.next();\n       INodeFile fileINode \u003d blocksMap.getINode(block);\n       short expectedReplication \u003d fileINode.getReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      INodeFile fileINode \u003d blocksMap.getINode(block);\n      short expectedReplication \u003d fileINode.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/07/11 9:20 PM",
          "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "20/07/11 4:35 PM",
          "commitNameOld": "08928d067bb9e1d38b5e7db9e23fcf20fe161435",
          "commitAuthorOld": "Matthew Foley",
          "daysBetweenCommits": 1.2,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,15 @@\n-  public void processOverReplicatedBlocksOnReCommission(DatanodeDescriptor srcNode) {\n+  private void processOverReplicatedBlocksOnReCommission(\n+      final DatanodeDescriptor srcNode) {\n     final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n     while(it.hasNext()) {\n       final Block block \u003d it.next();\n       INodeFile fileINode \u003d blocksMap.getINode(block);\n       short expectedReplication \u003d fileINode.getReplication();\n       NumberReplicas num \u003d countNodes(block);\n       int numCurrentReplica \u003d num.liveReplicas();\n       if (numCurrentReplica \u003e expectedReplication) {\n         // over-replicated block \n         processOverReplicatedBlock(block, expectedReplication, null, null);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void processOverReplicatedBlocksOnReCommission(\n      final DatanodeDescriptor srcNode) {\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      INodeFile fileINode \u003d blocksMap.getINode(block);\n      short expectedReplication \u003d fileINode.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[srcNode-DatanodeDescriptor]",
            "newValue": "[srcNode-DatanodeDescriptor(modifiers-final)]"
          }
        }
      ]
    },
    "08928d067bb9e1d38b5e7db9e23fcf20fe161435": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2114. re-commission of a decommissioned node does not delete excess replicas. Contributed by John George.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1148981 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/07/11 4:35 PM",
      "commitName": "08928d067bb9e1d38b5e7db9e23fcf20fe161435",
      "commitAuthor": "Matthew Foley",
      "diff": "@@ -0,0 +1,14 @@\n+  public void processOverReplicatedBlocksOnReCommission(DatanodeDescriptor srcNode) {\n+    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n+    while(it.hasNext()) {\n+      final Block block \u003d it.next();\n+      INodeFile fileINode \u003d blocksMap.getINode(block);\n+      short expectedReplication \u003d fileINode.getReplication();\n+      NumberReplicas num \u003d countNodes(block);\n+      int numCurrentReplica \u003d num.liveReplicas();\n+      if (numCurrentReplica \u003e expectedReplication) {\n+        // over-replicated block \n+        processOverReplicatedBlock(block, expectedReplication, null, null);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void processOverReplicatedBlocksOnReCommission(DatanodeDescriptor srcNode) {\n    final Iterator\u003c? extends Block\u003e it \u003d srcNode.getBlockIterator();\n    while(it.hasNext()) {\n      final Block block \u003d it.next();\n      INodeFile fileINode \u003d blocksMap.getINode(block);\n      short expectedReplication \u003d fileINode.getReplication();\n      NumberReplicas num \u003d countNodes(block);\n      int numCurrentReplica \u003d num.liveReplicas();\n      if (numCurrentReplica \u003e expectedReplication) {\n        // over-replicated block \n        processOverReplicatedBlock(block, expectedReplication, null, null);\n      }\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    }
  }
}