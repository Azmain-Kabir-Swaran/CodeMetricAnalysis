{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "MRAppMaster.java",
  "functionName": "serviceInit",
  "functionId": "serviceInit___conf-Configuration(modifiers-final)",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
  "functionStartLine": 280,
  "functionEndLine": 495,
  "numCommitsSeen": 181,
  "timeTaken": 11574,
  "changeHistory": [
    "453d48bdfbb67ed3e66c33c4aef239c3d7bdd3bc",
    "dd43b895c2e50fa97cb7327be77509b87dad1823",
    "6502d59e73cd6f3f3a358fce58d398ca38a61fba",
    "10107243be66bae2212a2cd8575f9f5ade13fe9e",
    "444836b3dcd3ee28238af7b5e753d644e8095788",
    "9e62bcca4e2ee4aaa3844d1d975dc0adc93ab602",
    "64306aa1b5f280e5ffaf2186bef706acd93b1412",
    "9ca394d54dd24e67867c845a58150f6b51761512",
    "f0799c55360e1e77224955f331892390e4361729",
    "1c1ebc1553650ac8e4486faf21f0d95150f607ad",
    "740f4cb97a4d5ec498f6e91d91ee7e75ad1c52c2",
    "b64572b06b1282128180b9ebdd971f9b1e973e61",
    "0928502029ef141759008997335ea2cd836a7154",
    "a83fb61ac07c0468cbc7a38526e92683883dd932",
    "6a1c41111edcdc58c846fc50e53554fbba230171",
    "46315a2d914058969c7234272420c063ce268bf5",
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865",
    "bbc426f53ad9132104752074b78971a04d48634a",
    "64e4fb983e022d8d3375a3e1b8facbf95f7ba403",
    "78ab699fe93cafbaff8f496be53d26aff40a68b1",
    "402eb1851341fce72c8e46266a2578bb67b5b684",
    "25e96e455b3473387df865fbc1c3ad7ebf9ff1e4",
    "cfafd8c29dc3679e503c6155bcbf26f377b0ea8f",
    "5ee495e6f34faff231ad87ec890188eb63617393",
    "fcbad14a3da7fadbb601bf245552ecca2fbc5026",
    "b7ae5a6cb7b2d3e3112ac53007e984caeb07de58",
    "08da8ea5db5359fc04010be486b842a5d2e6b9c2",
    "408656614495674992349fbda3981559ada3de0b",
    "13e4562924a6cb3d16c262e0f595b2ffbf9e0546",
    "fa2529c9317b2f27dec16411b99f296904ea095d",
    "c9a7d3dbf902244902b636bf566154c09ecd1116",
    "61900651b1b85cf235e01142acf2a51727fc5537",
    "fafe8cd28e726566509c679e19d7da622f29f90d",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "7c8fcbecf14b2e24d54ccb276bb684fdbe62b669",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517"
  ],
  "changeHistoryShort": {
    "453d48bdfbb67ed3e66c33c4aef239c3d7bdd3bc": "Ybodychange",
    "dd43b895c2e50fa97cb7327be77509b87dad1823": "Ybodychange",
    "6502d59e73cd6f3f3a358fce58d398ca38a61fba": "Ybodychange",
    "10107243be66bae2212a2cd8575f9f5ade13fe9e": "Ybodychange",
    "444836b3dcd3ee28238af7b5e753d644e8095788": "Ybodychange",
    "9e62bcca4e2ee4aaa3844d1d975dc0adc93ab602": "Ybodychange",
    "64306aa1b5f280e5ffaf2186bef706acd93b1412": "Ybodychange",
    "9ca394d54dd24e67867c845a58150f6b51761512": "Ybodychange",
    "f0799c55360e1e77224955f331892390e4361729": "Ybodychange",
    "1c1ebc1553650ac8e4486faf21f0d95150f607ad": "Ybodychange",
    "740f4cb97a4d5ec498f6e91d91ee7e75ad1c52c2": "Ybodychange",
    "b64572b06b1282128180b9ebdd971f9b1e973e61": "Ybodychange",
    "0928502029ef141759008997335ea2cd836a7154": "Ymultichange(Yrename,Ymodifierchange,Yexceptionschange,Ybodychange)",
    "a83fb61ac07c0468cbc7a38526e92683883dd932": "Ybodychange",
    "6a1c41111edcdc58c846fc50e53554fbba230171": "Ybodychange",
    "46315a2d914058969c7234272420c063ce268bf5": "Ybodychange",
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865": "Ybodychange",
    "bbc426f53ad9132104752074b78971a04d48634a": "Ybodychange",
    "64e4fb983e022d8d3375a3e1b8facbf95f7ba403": "Ybodychange",
    "78ab699fe93cafbaff8f496be53d26aff40a68b1": "Ybodychange",
    "402eb1851341fce72c8e46266a2578bb67b5b684": "Ybodychange",
    "25e96e455b3473387df865fbc1c3ad7ebf9ff1e4": "Ybodychange",
    "cfafd8c29dc3679e503c6155bcbf26f377b0ea8f": "Ybodychange",
    "5ee495e6f34faff231ad87ec890188eb63617393": "Ybodychange",
    "fcbad14a3da7fadbb601bf245552ecca2fbc5026": "Ybodychange",
    "b7ae5a6cb7b2d3e3112ac53007e984caeb07de58": "Ybodychange",
    "08da8ea5db5359fc04010be486b842a5d2e6b9c2": "Ybodychange",
    "408656614495674992349fbda3981559ada3de0b": "Ybodychange",
    "13e4562924a6cb3d16c262e0f595b2ffbf9e0546": "Ybodychange",
    "fa2529c9317b2f27dec16411b99f296904ea095d": "Ybodychange",
    "c9a7d3dbf902244902b636bf566154c09ecd1116": "Ybodychange",
    "61900651b1b85cf235e01142acf2a51727fc5537": "Ybodychange",
    "fafe8cd28e726566509c679e19d7da622f29f90d": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "7c8fcbecf14b2e24d54ccb276bb684fdbe62b669": "Ybodychange",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yintroduced"
  },
  "changeHistoryDetails": {
    "453d48bdfbb67ed3e66c33c4aef239c3d7bdd3bc": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6971. Moving logging APIs over to slf4j in hadoop-mapreduce-client-app. Contributed by Jinjiang Ling.\n",
      "commitDate": "02/10/17 8:14 PM",
      "commitName": "453d48bdfbb67ed3e66c33c4aef239c3d7bdd3bc",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "08/08/17 12:46 PM",
      "commitNameOld": "735fce5bec17f4e1799daf922625c475cf588114",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 55.31,
      "commitsBetweenForRepo": 487,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,216 +1,216 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n     // create the job classloader if enabled\n     createJobClassLoader(conf);\n \n     initJobCredentialsAndUGI(conf);\n \n     dispatcher \u003d createDispatcher();\n     addIfService(dispatcher);\n     taskAttemptFinishingMonitor \u003d createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());\n     addIfService(taskAttemptFinishingMonitor);\n     context \u003d new RunningAppContext(conf, taskAttemptFinishingMonitor);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     committer \u003d createOutputCommitter(conf);\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n \n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n-        LOG.fatal(shutDownMessage);\n+        LOG.error(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d\n               \"Job commit succeeded in a prior MRAppMaster attempt \" +\n               \"before it crashed. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d\n               \"Job commit failed in a prior MRAppMaster attempt \" +\n               \"before it crashed. Not retrying.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           if (isCommitJobRepeatable()) {\n             // cleanup previous half done commits if committer supports\n             // repeatable job commit.\n             errorHappenedShutDown \u003d false;\n             cleanupInterruptedCommit(conf, fs, startCommitFile);\n           } else {\n             //The commit is still pending, commit error\n             shutDownMessage \u003d\n                 \"Job commit from a prior MRAppMaster attempt is \" +\n                 \"potentially in progress. Preventing multiple commit executions\";\n             forcedState \u003d JobStateInternal.ERROR;\n           }\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n \n     if (errorHappenedShutDown) {\n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n \n       if (copyHistory) {\n         // Now that there\u0027s a FINISHING state for application on RM to give AMs\n         // plenty of time to clean up after unregister it\u0027s safe to clean staging\n         // directory after unregistering with RM. So, we start the staging-dir\n         // cleaner BEFORE the ContainerAllocator so that on shut-down,\n         // ContainerAllocator unregisters first and then the staging-dir cleaner\n         // deletes staging directory.\n         addService(createStagingDirCleaningService());\n       }\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       // Init ClientService separately so that we stop it separately, since this\n       // service needs to wait some time before it stops so clients can know the\n       // final states\n       clientService.init(conf);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //policy handling preemption requests from RM\n       callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n         public Void call(Configuration conf) {\n           preemptionPolicy \u003d createPreemptionPolicy(conf);\n           preemptionPolicy.init(context);\n           return null;\n         }\n       });\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // Now that there\u0027s a FINISHING state for application on RM to give AMs\n       // plenty of time to clean up after unregister it\u0027s safe to clean staging\n       // directory after unregistering with RM. So, we start the staging-dir\n       // cleaner BEFORE the ContainerAllocator so that on shut-down,\n       // ContainerAllocator unregisters first and then the staging-dir cleaner\n       // deletes staging directory.\n       addService(createStagingDirCleaningService());\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    // create the job classloader if enabled\n    createJobClassLoader(conf);\n\n    initJobCredentialsAndUGI(conf);\n\n    dispatcher \u003d createDispatcher();\n    addIfService(dispatcher);\n    taskAttemptFinishingMonitor \u003d createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());\n    addIfService(taskAttemptFinishingMonitor);\n    context \u003d new RunningAppContext(conf, taskAttemptFinishingMonitor);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    committer \u003d createOutputCommitter(conf);\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.error(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d\n              \"Job commit succeeded in a prior MRAppMaster attempt \" +\n              \"before it crashed. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d\n              \"Job commit failed in a prior MRAppMaster attempt \" +\n              \"before it crashed. Not retrying.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          if (isCommitJobRepeatable()) {\n            // cleanup previous half done commits if committer supports\n            // repeatable job commit.\n            errorHappenedShutDown \u003d false;\n            cleanupInterruptedCommit(conf, fs, startCommitFile);\n          } else {\n            //The commit is still pending, commit error\n            shutDownMessage \u003d\n                \"Job commit from a prior MRAppMaster attempt is \" +\n                \"potentially in progress. Preventing multiple commit executions\";\n            forcedState \u003d JobStateInternal.ERROR;\n          }\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n\n    if (errorHappenedShutDown) {\n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n\n      if (copyHistory) {\n        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n        // directory after unregistering with RM. So, we start the staging-dir\n        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n        // ContainerAllocator unregisters first and then the staging-dir cleaner\n        // deletes staging directory.\n        addService(createStagingDirCleaningService());\n      }\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      // Init ClientService separately so that we stop it separately, since this\n      // service needs to wait some time before it stops so clients can know the\n      // final states\n      clientService.init(conf);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //policy handling preemption requests from RM\n      callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n        public Void call(Configuration conf) {\n          preemptionPolicy \u003d createPreemptionPolicy(conf);\n          preemptionPolicy.init(context);\n          return null;\n        }\n      });\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n      // directory after unregistering with RM. So, we start the staging-dir\n      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n      // ContainerAllocator unregisters first and then the staging-dir cleaner\n      // deletes staging directory.\n      addService(createStagingDirCleaningService());\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "dd43b895c2e50fa97cb7327be77509b87dad1823": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6202. Configuration item Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY is disregarded\n(Contributed by Yufei Gu via Daniel Templeton)\n",
      "commitDate": "19/04/17 11:44 AM",
      "commitName": "dd43b895c2e50fa97cb7327be77509b87dad1823",
      "commitAuthor": "Daniel Templeton",
      "commitDateOld": "16/02/17 11:41 AM",
      "commitNameOld": "4fa1afdb883dab8786d2fb5c72a195dd2e87d711",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 61.96,
      "commitsBetweenForRepo": 364,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,218 +1,216 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n     // create the job classloader if enabled\n     createJobClassLoader(conf);\n \n-    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n-\n     initJobCredentialsAndUGI(conf);\n \n     dispatcher \u003d createDispatcher();\n     addIfService(dispatcher);\n     taskAttemptFinishingMonitor \u003d createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());\n     addIfService(taskAttemptFinishingMonitor);\n     context \u003d new RunningAppContext(conf, taskAttemptFinishingMonitor);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     committer \u003d createOutputCommitter(conf);\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n \n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d\n               \"Job commit succeeded in a prior MRAppMaster attempt \" +\n               \"before it crashed. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d\n               \"Job commit failed in a prior MRAppMaster attempt \" +\n               \"before it crashed. Not retrying.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           if (isCommitJobRepeatable()) {\n             // cleanup previous half done commits if committer supports\n             // repeatable job commit.\n             errorHappenedShutDown \u003d false;\n             cleanupInterruptedCommit(conf, fs, startCommitFile);\n           } else {\n             //The commit is still pending, commit error\n             shutDownMessage \u003d\n                 \"Job commit from a prior MRAppMaster attempt is \" +\n                 \"potentially in progress. Preventing multiple commit executions\";\n             forcedState \u003d JobStateInternal.ERROR;\n           }\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n \n     if (errorHappenedShutDown) {\n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n \n       if (copyHistory) {\n         // Now that there\u0027s a FINISHING state for application on RM to give AMs\n         // plenty of time to clean up after unregister it\u0027s safe to clean staging\n         // directory after unregistering with RM. So, we start the staging-dir\n         // cleaner BEFORE the ContainerAllocator so that on shut-down,\n         // ContainerAllocator unregisters first and then the staging-dir cleaner\n         // deletes staging directory.\n         addService(createStagingDirCleaningService());\n       }\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       // Init ClientService separately so that we stop it separately, since this\n       // service needs to wait some time before it stops so clients can know the\n       // final states\n       clientService.init(conf);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //policy handling preemption requests from RM\n       callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n         public Void call(Configuration conf) {\n           preemptionPolicy \u003d createPreemptionPolicy(conf);\n           preemptionPolicy.init(context);\n           return null;\n         }\n       });\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // Now that there\u0027s a FINISHING state for application on RM to give AMs\n       // plenty of time to clean up after unregister it\u0027s safe to clean staging\n       // directory after unregistering with RM. So, we start the staging-dir\n       // cleaner BEFORE the ContainerAllocator so that on shut-down,\n       // ContainerAllocator unregisters first and then the staging-dir cleaner\n       // deletes staging directory.\n       addService(createStagingDirCleaningService());\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    // create the job classloader if enabled\n    createJobClassLoader(conf);\n\n    initJobCredentialsAndUGI(conf);\n\n    dispatcher \u003d createDispatcher();\n    addIfService(dispatcher);\n    taskAttemptFinishingMonitor \u003d createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());\n    addIfService(taskAttemptFinishingMonitor);\n    context \u003d new RunningAppContext(conf, taskAttemptFinishingMonitor);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    committer \u003d createOutputCommitter(conf);\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d\n              \"Job commit succeeded in a prior MRAppMaster attempt \" +\n              \"before it crashed. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d\n              \"Job commit failed in a prior MRAppMaster attempt \" +\n              \"before it crashed. Not retrying.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          if (isCommitJobRepeatable()) {\n            // cleanup previous half done commits if committer supports\n            // repeatable job commit.\n            errorHappenedShutDown \u003d false;\n            cleanupInterruptedCommit(conf, fs, startCommitFile);\n          } else {\n            //The commit is still pending, commit error\n            shutDownMessage \u003d\n                \"Job commit from a prior MRAppMaster attempt is \" +\n                \"potentially in progress. Preventing multiple commit executions\";\n            forcedState \u003d JobStateInternal.ERROR;\n          }\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n\n    if (errorHappenedShutDown) {\n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n\n      if (copyHistory) {\n        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n        // directory after unregistering with RM. So, we start the staging-dir\n        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n        // ContainerAllocator unregisters first and then the staging-dir cleaner\n        // deletes staging directory.\n        addService(createStagingDirCleaningService());\n      }\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      // Init ClientService separately so that we stop it separately, since this\n      // service needs to wait some time before it stops so clients can know the\n      // final states\n      clientService.init(conf);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //policy handling preemption requests from RM\n      callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n        public Void call(Configuration conf) {\n          preemptionPolicy \u003d createPreemptionPolicy(conf);\n          preemptionPolicy.init(context);\n          return null;\n        }\n      });\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n      // directory after unregistering with RM. So, we start the staging-dir\n      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n      // ContainerAllocator unregisters first and then the staging-dir cleaner\n      // deletes staging directory.\n      addService(createStagingDirCleaningService());\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "6502d59e73cd6f3f3a358fce58d398ca38a61fba": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5485. Allow repeating job commit by extending OutputCommitter API. Contributed by Junping Du\n",
      "commitDate": "16/11/15 5:06 PM",
      "commitName": "6502d59e73cd6f3f3a358fce58d398ca38a61fba",
      "commitAuthor": "Jian He",
      "commitDateOld": "30/07/15 11:07 PM",
      "commitNameOld": "93d50b782494af7eef980c4d596a59ff4e11646e",
      "commitAuthorOld": "Zhihai Xu",
      "daysBetweenCommits": 108.79,
      "commitsBetweenForRepo": 746,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,210 +1,218 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n     // create the job classloader if enabled\n     createJobClassLoader(conf);\n \n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     initJobCredentialsAndUGI(conf);\n \n     dispatcher \u003d createDispatcher();\n     addIfService(dispatcher);\n     taskAttemptFinishingMonitor \u003d createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());\n     addIfService(taskAttemptFinishingMonitor);\n     context \u003d new RunningAppContext(conf, taskAttemptFinishingMonitor);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n+    committer \u003d createOutputCommitter(conf);\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n+\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d\n               \"Job commit succeeded in a prior MRAppMaster attempt \" +\n               \"before it crashed. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d\n               \"Job commit failed in a prior MRAppMaster attempt \" +\n               \"before it crashed. Not retrying.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n-          //The commit is still pending, commit error\n-          shutDownMessage \u003d\n-              \"Job commit from a prior MRAppMaster attempt is \" +\n-              \"potentially in progress. Preventing multiple commit executions\";\n-          forcedState \u003d JobStateInternal.ERROR;\n+          if (isCommitJobRepeatable()) {\n+            // cleanup previous half done commits if committer supports\n+            // repeatable job commit.\n+            errorHappenedShutDown \u003d false;\n+            cleanupInterruptedCommit(conf, fs, startCommitFile);\n+          } else {\n+            //The commit is still pending, commit error\n+            shutDownMessage \u003d\n+                \"Job commit from a prior MRAppMaster attempt is \" +\n+                \"potentially in progress. Preventing multiple commit executions\";\n+            forcedState \u003d JobStateInternal.ERROR;\n+          }\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n-    \n+\n     if (errorHappenedShutDown) {\n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n \n       if (copyHistory) {\n         // Now that there\u0027s a FINISHING state for application on RM to give AMs\n         // plenty of time to clean up after unregister it\u0027s safe to clean staging\n         // directory after unregistering with RM. So, we start the staging-dir\n         // cleaner BEFORE the ContainerAllocator so that on shut-down,\n         // ContainerAllocator unregisters first and then the staging-dir cleaner\n         // deletes staging directory.\n         addService(createStagingDirCleaningService());\n       }\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n-      committer \u003d createOutputCommitter(conf);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       // Init ClientService separately so that we stop it separately, since this\n       // service needs to wait some time before it stops so clients can know the\n       // final states\n       clientService.init(conf);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //policy handling preemption requests from RM\n       callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n         public Void call(Configuration conf) {\n           preemptionPolicy \u003d createPreemptionPolicy(conf);\n           preemptionPolicy.init(context);\n           return null;\n         }\n       });\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // Now that there\u0027s a FINISHING state for application on RM to give AMs\n       // plenty of time to clean up after unregister it\u0027s safe to clean staging\n       // directory after unregistering with RM. So, we start the staging-dir\n       // cleaner BEFORE the ContainerAllocator so that on shut-down,\n       // ContainerAllocator unregisters first and then the staging-dir cleaner\n       // deletes staging directory.\n       addService(createStagingDirCleaningService());\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    // create the job classloader if enabled\n    createJobClassLoader(conf);\n\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    initJobCredentialsAndUGI(conf);\n\n    dispatcher \u003d createDispatcher();\n    addIfService(dispatcher);\n    taskAttemptFinishingMonitor \u003d createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());\n    addIfService(taskAttemptFinishingMonitor);\n    context \u003d new RunningAppContext(conf, taskAttemptFinishingMonitor);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    committer \u003d createOutputCommitter(conf);\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d\n              \"Job commit succeeded in a prior MRAppMaster attempt \" +\n              \"before it crashed. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d\n              \"Job commit failed in a prior MRAppMaster attempt \" +\n              \"before it crashed. Not retrying.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          if (isCommitJobRepeatable()) {\n            // cleanup previous half done commits if committer supports\n            // repeatable job commit.\n            errorHappenedShutDown \u003d false;\n            cleanupInterruptedCommit(conf, fs, startCommitFile);\n          } else {\n            //The commit is still pending, commit error\n            shutDownMessage \u003d\n                \"Job commit from a prior MRAppMaster attempt is \" +\n                \"potentially in progress. Preventing multiple commit executions\";\n            forcedState \u003d JobStateInternal.ERROR;\n          }\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n\n    if (errorHappenedShutDown) {\n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n\n      if (copyHistory) {\n        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n        // directory after unregistering with RM. So, we start the staging-dir\n        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n        // ContainerAllocator unregisters first and then the staging-dir cleaner\n        // deletes staging directory.\n        addService(createStagingDirCleaningService());\n      }\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      // Init ClientService separately so that we stop it separately, since this\n      // service needs to wait some time before it stops so clients can know the\n      // final states\n      clientService.init(conf);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //policy handling preemption requests from RM\n      callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n        public Void call(Configuration conf) {\n          preemptionPolicy \u003d createPreemptionPolicy(conf);\n          preemptionPolicy.init(context);\n          return null;\n        }\n      });\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n      // directory after unregistering with RM. So, we start the staging-dir\n      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n      // ContainerAllocator unregisters first and then the staging-dir cleaner\n      // deletes staging directory.\n      addService(createStagingDirCleaningService());\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "10107243be66bae2212a2cd8575f9f5ade13fe9e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6395. Improve the commit failure messages in MRAppMaster recovery. (Brahma Reddy Battula via gera)\n",
      "commitDate": "19/06/15 2:22 AM",
      "commitName": "10107243be66bae2212a2cd8575f9f5ade13fe9e",
      "commitAuthor": "Gera Shegalov",
      "commitDateOld": "14/05/15 4:07 PM",
      "commitNameOld": "6b710a42e00acca405e085724c89cda016cf7442",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 35.43,
      "commitsBetweenForRepo": 254,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,204 +1,210 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n     // create the job classloader if enabled\n     createJobClassLoader(conf);\n \n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     initJobCredentialsAndUGI(conf);\n \n     dispatcher \u003d createDispatcher();\n     addIfService(dispatcher);\n     taskAttemptFinishingMonitor \u003d createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());\n     addIfService(taskAttemptFinishingMonitor);\n     context \u003d new RunningAppContext(conf, taskAttemptFinishingMonitor);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n-          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n+          shutDownMessage \u003d\n+              \"Job commit succeeded in a prior MRAppMaster attempt \" +\n+              \"before it crashed. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n-          shutDownMessage \u003d \"We crashed after a commit failure.\";\n+          shutDownMessage \u003d\n+              \"Job commit failed in a prior MRAppMaster attempt \" +\n+              \"before it crashed. Not retrying.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n-          shutDownMessage \u003d \"We crashed durring a commit\";\n+          shutDownMessage \u003d\n+              \"Job commit from a prior MRAppMaster attempt is \" +\n+              \"potentially in progress. Preventing multiple commit executions\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n \n       if (copyHistory) {\n         // Now that there\u0027s a FINISHING state for application on RM to give AMs\n         // plenty of time to clean up after unregister it\u0027s safe to clean staging\n         // directory after unregistering with RM. So, we start the staging-dir\n         // cleaner BEFORE the ContainerAllocator so that on shut-down,\n         // ContainerAllocator unregisters first and then the staging-dir cleaner\n         // deletes staging directory.\n         addService(createStagingDirCleaningService());\n       }\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       // Init ClientService separately so that we stop it separately, since this\n       // service needs to wait some time before it stops so clients can know the\n       // final states\n       clientService.init(conf);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //policy handling preemption requests from RM\n       callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n         public Void call(Configuration conf) {\n           preemptionPolicy \u003d createPreemptionPolicy(conf);\n           preemptionPolicy.init(context);\n           return null;\n         }\n       });\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // Now that there\u0027s a FINISHING state for application on RM to give AMs\n       // plenty of time to clean up after unregister it\u0027s safe to clean staging\n       // directory after unregistering with RM. So, we start the staging-dir\n       // cleaner BEFORE the ContainerAllocator so that on shut-down,\n       // ContainerAllocator unregisters first and then the staging-dir cleaner\n       // deletes staging directory.\n       addService(createStagingDirCleaningService());\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    // create the job classloader if enabled\n    createJobClassLoader(conf);\n\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    initJobCredentialsAndUGI(conf);\n\n    dispatcher \u003d createDispatcher();\n    addIfService(dispatcher);\n    taskAttemptFinishingMonitor \u003d createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());\n    addIfService(taskAttemptFinishingMonitor);\n    context \u003d new RunningAppContext(conf, taskAttemptFinishingMonitor);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d\n              \"Job commit succeeded in a prior MRAppMaster attempt \" +\n              \"before it crashed. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d\n              \"Job commit failed in a prior MRAppMaster attempt \" +\n              \"before it crashed. Not retrying.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d\n              \"Job commit from a prior MRAppMaster attempt is \" +\n              \"potentially in progress. Preventing multiple commit executions\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n\n      if (copyHistory) {\n        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n        // directory after unregistering with RM. So, we start the staging-dir\n        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n        // ContainerAllocator unregisters first and then the staging-dir cleaner\n        // deletes staging directory.\n        addService(createStagingDirCleaningService());\n      }\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      // Init ClientService separately so that we stop it separately, since this\n      // service needs to wait some time before it stops so clients can know the\n      // final states\n      clientService.init(conf);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //policy handling preemption requests from RM\n      callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n        public Void call(Configuration conf) {\n          preemptionPolicy \u003d createPreemptionPolicy(conf);\n          preemptionPolicy.init(context);\n          return null;\n        }\n      });\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n      // directory after unregistering with RM. So, we start the staging-dir\n      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n      // ContainerAllocator unregisters first and then the staging-dir cleaner\n      // deletes staging directory.\n      addService(createStagingDirCleaningService());\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "444836b3dcd3ee28238af7b5e753d644e8095788": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5465. Tasks are often killed before they exit on their own. Contributed by Ming Ma\n",
      "commitDate": "11/05/15 3:37 PM",
      "commitName": "444836b3dcd3ee28238af7b5e753d644e8095788",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "07/05/15 3:05 PM",
      "commitNameOld": "f30065c8b6099372f57015b505434120fe83c2b0",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 4.02,
      "commitsBetweenForRepo": 96,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,206 +1,204 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n     // create the job classloader if enabled\n     createJobClassLoader(conf);\n \n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     initJobCredentialsAndUGI(conf);\n \n-    context \u003d new RunningAppContext(conf);\n+    dispatcher \u003d createDispatcher();\n+    addIfService(dispatcher);\n+    taskAttemptFinishingMonitor \u003d createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());\n+    addIfService(taskAttemptFinishingMonitor);\n+    context \u003d new RunningAppContext(conf, taskAttemptFinishingMonitor);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n-      dispatcher \u003d createDispatcher();\n-      addIfService(dispatcher);\n-      \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n \n       if (copyHistory) {\n         // Now that there\u0027s a FINISHING state for application on RM to give AMs\n         // plenty of time to clean up after unregister it\u0027s safe to clean staging\n         // directory after unregistering with RM. So, we start the staging-dir\n         // cleaner BEFORE the ContainerAllocator so that on shut-down,\n         // ContainerAllocator unregisters first and then the staging-dir cleaner\n         // deletes staging directory.\n         addService(createStagingDirCleaningService());\n       }\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n-      dispatcher \u003d createDispatcher();\n-      addIfService(dispatcher);\n-\n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       // Init ClientService separately so that we stop it separately, since this\n       // service needs to wait some time before it stops so clients can know the\n       // final states\n       clientService.init(conf);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //policy handling preemption requests from RM\n       callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n         public Void call(Configuration conf) {\n           preemptionPolicy \u003d createPreemptionPolicy(conf);\n           preemptionPolicy.init(context);\n           return null;\n         }\n       });\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // Now that there\u0027s a FINISHING state for application on RM to give AMs\n       // plenty of time to clean up after unregister it\u0027s safe to clean staging\n       // directory after unregistering with RM. So, we start the staging-dir\n       // cleaner BEFORE the ContainerAllocator so that on shut-down,\n       // ContainerAllocator unregisters first and then the staging-dir cleaner\n       // deletes staging directory.\n       addService(createStagingDirCleaningService());\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    // create the job classloader if enabled\n    createJobClassLoader(conf);\n\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    initJobCredentialsAndUGI(conf);\n\n    dispatcher \u003d createDispatcher();\n    addIfService(dispatcher);\n    taskAttemptFinishingMonitor \u003d createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());\n    addIfService(taskAttemptFinishingMonitor);\n    context \u003d new RunningAppContext(conf, taskAttemptFinishingMonitor);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n\n      if (copyHistory) {\n        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n        // directory after unregistering with RM. So, we start the staging-dir\n        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n        // ContainerAllocator unregisters first and then the staging-dir cleaner\n        // deletes staging directory.\n        addService(createStagingDirCleaningService());\n      }\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      // Init ClientService separately so that we stop it separately, since this\n      // service needs to wait some time before it stops so clients can know the\n      // final states\n      clientService.init(conf);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //policy handling preemption requests from RM\n      callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n        public Void call(Configuration conf) {\n          preemptionPolicy \u003d createPreemptionPolicy(conf);\n          preemptionPolicy.init(context);\n          return null;\n        }\n      });\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n      // directory after unregistering with RM. So, we start the staging-dir\n      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n      // ContainerAllocator unregisters first and then the staging-dir cleaner\n      // deletes staging directory.\n      addService(createStagingDirCleaningService());\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "9e62bcca4e2ee4aaa3844d1d975dc0adc93ab602": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5957. AM throws ClassNotFoundException with job classloader enabled if custom output format/committer is used. Contributed by Sangjin Lee\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612358 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/07/14 10:54 AM",
      "commitName": "9e62bcca4e2ee4aaa3844d1d975dc0adc93ab602",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "11/07/14 1:45 AM",
      "commitNameOld": "64306aa1b5f280e5ffaf2186bef706acd93b1412",
      "commitAuthorOld": "Zhijie Shen",
      "daysBetweenCommits": 10.38,
      "commitsBetweenForRepo": 64,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,198 +1,206 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n+    // create the job classloader if enabled\n+    createJobClassLoader(conf);\n+\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     initJobCredentialsAndUGI(conf);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n \n       if (copyHistory) {\n         // Now that there\u0027s a FINISHING state for application on RM to give AMs\n         // plenty of time to clean up after unregister it\u0027s safe to clean staging\n         // directory after unregistering with RM. So, we start the staging-dir\n         // cleaner BEFORE the ContainerAllocator so that on shut-down,\n         // ContainerAllocator unregisters first and then the staging-dir cleaner\n         // deletes staging directory.\n         addService(createStagingDirCleaningService());\n       }\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       // Init ClientService separately so that we stop it separately, since this\n       // service needs to wait some time before it stops so clients can know the\n       // final states\n       clientService.init(conf);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //policy handling preemption requests from RM\n-      preemptionPolicy \u003d createPreemptionPolicy(conf);\n-      preemptionPolicy.init(context);\n+      callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n+        public Void call(Configuration conf) {\n+          preemptionPolicy \u003d createPreemptionPolicy(conf);\n+          preemptionPolicy.init(context);\n+          return null;\n+        }\n+      });\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // Now that there\u0027s a FINISHING state for application on RM to give AMs\n       // plenty of time to clean up after unregister it\u0027s safe to clean staging\n       // directory after unregistering with RM. So, we start the staging-dir\n       // cleaner BEFORE the ContainerAllocator so that on shut-down,\n       // ContainerAllocator unregisters first and then the staging-dir cleaner\n       // deletes staging directory.\n       addService(createStagingDirCleaningService());\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    // create the job classloader if enabled\n    createJobClassLoader(conf);\n\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    initJobCredentialsAndUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n\n      if (copyHistory) {\n        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n        // directory after unregistering with RM. So, we start the staging-dir\n        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n        // ContainerAllocator unregisters first and then the staging-dir cleaner\n        // deletes staging directory.\n        addService(createStagingDirCleaningService());\n      }\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      // Init ClientService separately so that we stop it separately, since this\n      // service needs to wait some time before it stops so clients can know the\n      // final states\n      clientService.init(conf);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //policy handling preemption requests from RM\n      callWithJobClassLoader(conf, new Action\u003cVoid\u003e() {\n        public Void call(Configuration conf) {\n          preemptionPolicy \u003d createPreemptionPolicy(conf);\n          preemptionPolicy.init(context);\n          return null;\n        }\n      });\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n      // directory after unregistering with RM. So, we start the staging-dir\n      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n      // ContainerAllocator unregisters first and then the staging-dir cleaner\n      // deletes staging directory.\n      addService(createStagingDirCleaningService());\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "64306aa1b5f280e5ffaf2186bef706acd93b1412": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5956. Made MR AM not use maxAttempts to determine if the current attempt is the last retry. Contributed by Wangda Tan.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1609649 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/07/14 1:45 AM",
      "commitName": "64306aa1b5f280e5ffaf2186bef706acd93b1412",
      "commitAuthor": "Zhijie Shen",
      "commitDateOld": "28/04/14 8:18 AM",
      "commitNameOld": "bb7ce82816574f67aa1898f67e0e0cff54fa67be",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 73.73,
      "commitsBetweenForRepo": 441,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,204 +1,198 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     initJobCredentialsAndUGI(conf);\n \n     context \u003d new RunningAppContext(conf);\n \n-    ((RunningAppContext)context).computeIsLastAMRetry();\n-    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n-        \" for application: \" + appAttemptID.getApplicationId().getId() +\n-        \". Attempt num: \" + appAttemptID.getAttemptId() +\n-        \" is last retry: \" + isLastAMRetry);\n-\n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n \n       if (copyHistory) {\n         // Now that there\u0027s a FINISHING state for application on RM to give AMs\n         // plenty of time to clean up after unregister it\u0027s safe to clean staging\n         // directory after unregistering with RM. So, we start the staging-dir\n         // cleaner BEFORE the ContainerAllocator so that on shut-down,\n         // ContainerAllocator unregisters first and then the staging-dir cleaner\n         // deletes staging directory.\n         addService(createStagingDirCleaningService());\n       }\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       // Init ClientService separately so that we stop it separately, since this\n       // service needs to wait some time before it stops so clients can know the\n       // final states\n       clientService.init(conf);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //policy handling preemption requests from RM\n       preemptionPolicy \u003d createPreemptionPolicy(conf);\n       preemptionPolicy.init(context);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // Now that there\u0027s a FINISHING state for application on RM to give AMs\n       // plenty of time to clean up after unregister it\u0027s safe to clean staging\n       // directory after unregistering with RM. So, we start the staging-dir\n       // cleaner BEFORE the ContainerAllocator so that on shut-down,\n       // ContainerAllocator unregisters first and then the staging-dir cleaner\n       // deletes staging directory.\n       addService(createStagingDirCleaningService());\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    initJobCredentialsAndUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n\n      if (copyHistory) {\n        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n        // directory after unregistering with RM. So, we start the staging-dir\n        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n        // ContainerAllocator unregisters first and then the staging-dir cleaner\n        // deletes staging directory.\n        addService(createStagingDirCleaningService());\n      }\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      // Init ClientService separately so that we stop it separately, since this\n      // service needs to wait some time before it stops so clients can know the\n      // final states\n      clientService.init(conf);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //policy handling preemption requests from RM\n      preemptionPolicy \u003d createPreemptionPolicy(conf);\n      preemptionPolicy.init(context);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n      // directory after unregistering with RM. So, we start the staging-dir\n      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n      // ContainerAllocator unregisters first and then the staging-dir cleaner\n      // deletes staging directory.\n      addService(createStagingDirCleaningService());\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "9ca394d54dd24e67867c845a58150f6b51761512": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5189. Add policies and wiring to respond to preemption requests\nfrom YARN. Contributed by Carlo Curino.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551748 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 2:54 PM",
      "commitName": "9ca394d54dd24e67867c845a58150f6b51761512",
      "commitAuthor": "Christopher Douglas",
      "commitDateOld": "06/10/13 1:53 PM",
      "commitNameOld": "f0799c55360e1e77224955f331892390e4361729",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 72.08,
      "commitsBetweenForRepo": 459,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,200 +1,204 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     initJobCredentialsAndUGI(conf);\n \n     context \u003d new RunningAppContext(conf);\n \n     ((RunningAppContext)context).computeIsLastAMRetry();\n     LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n         \" for application: \" + appAttemptID.getApplicationId().getId() +\n         \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n \n       if (copyHistory) {\n         // Now that there\u0027s a FINISHING state for application on RM to give AMs\n         // plenty of time to clean up after unregister it\u0027s safe to clean staging\n         // directory after unregistering with RM. So, we start the staging-dir\n         // cleaner BEFORE the ContainerAllocator so that on shut-down,\n         // ContainerAllocator unregisters first and then the staging-dir cleaner\n         // deletes staging directory.\n         addService(createStagingDirCleaningService());\n       }\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       // Init ClientService separately so that we stop it separately, since this\n       // service needs to wait some time before it stops so clients can know the\n       // final states\n       clientService.init(conf);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n+      //policy handling preemption requests from RM\n+      preemptionPolicy \u003d createPreemptionPolicy(conf);\n+      preemptionPolicy.init(context);\n+\n       //service to handle requests to TaskUmbilicalProtocol\n-      taskAttemptListener \u003d createTaskAttemptListener(context);\n+      taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // Now that there\u0027s a FINISHING state for application on RM to give AMs\n       // plenty of time to clean up after unregister it\u0027s safe to clean staging\n       // directory after unregistering with RM. So, we start the staging-dir\n       // cleaner BEFORE the ContainerAllocator so that on shut-down,\n       // ContainerAllocator unregisters first and then the staging-dir cleaner\n       // deletes staging directory.\n       addService(createStagingDirCleaningService());\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    initJobCredentialsAndUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    ((RunningAppContext)context).computeIsLastAMRetry();\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n\n      if (copyHistory) {\n        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n        // directory after unregistering with RM. So, we start the staging-dir\n        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n        // ContainerAllocator unregisters first and then the staging-dir cleaner\n        // deletes staging directory.\n        addService(createStagingDirCleaningService());\n      }\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      // Init ClientService separately so that we stop it separately, since this\n      // service needs to wait some time before it stops so clients can know the\n      // final states\n      clientService.init(conf);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //policy handling preemption requests from RM\n      preemptionPolicy \u003d createPreemptionPolicy(conf);\n      preemptionPolicy.init(context);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context, preemptionPolicy);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n      // directory after unregistering with RM. So, we start the staging-dir\n      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n      // ContainerAllocator unregisters first and then the staging-dir cleaner\n      // deletes staging directory.\n      addService(createStagingDirCleaningService());\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "f0799c55360e1e77224955f331892390e4361729": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5562. Fixed MR App Master to perform pending tasks like staging-dir cleanup, sending job-end notification correctly when unregister with RM fails. Contributed by Zhijie Shen.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529682 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/10/13 1:53 PM",
      "commitName": "f0799c55360e1e77224955f331892390e4361729",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "06/10/13 11:43 AM",
      "commitNameOld": "21181b65531449e5fda321c11f0672c3067641aa",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 0.09,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,200 +1,200 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     initJobCredentialsAndUGI(conf);\n \n-    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n+    context \u003d new RunningAppContext(conf);\n+\n+    ((RunningAppContext)context).computeIsLastAMRetry();\n     LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n         \" for application: \" + appAttemptID.getApplicationId().getId() +\n         \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n \n-    context \u003d new RunningAppContext(conf);\n-\n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n \n       if (copyHistory) {\n         // Now that there\u0027s a FINISHING state for application on RM to give AMs\n         // plenty of time to clean up after unregister it\u0027s safe to clean staging\n         // directory after unregistering with RM. So, we start the staging-dir\n         // cleaner BEFORE the ContainerAllocator so that on shut-down,\n         // ContainerAllocator unregisters first and then the staging-dir cleaner\n         // deletes staging directory.\n         addService(createStagingDirCleaningService());\n       }\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       // Init ClientService separately so that we stop it separately, since this\n       // service needs to wait some time before it stops so clients can know the\n       // final states\n       clientService.init(conf);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // Now that there\u0027s a FINISHING state for application on RM to give AMs\n       // plenty of time to clean up after unregister it\u0027s safe to clean staging\n       // directory after unregistering with RM. So, we start the staging-dir\n       // cleaner BEFORE the ContainerAllocator so that on shut-down,\n       // ContainerAllocator unregisters first and then the staging-dir cleaner\n       // deletes staging directory.\n       addService(createStagingDirCleaningService());\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    initJobCredentialsAndUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    ((RunningAppContext)context).computeIsLastAMRetry();\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n\n      if (copyHistory) {\n        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n        // directory after unregistering with RM. So, we start the staging-dir\n        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n        // ContainerAllocator unregisters first and then the staging-dir cleaner\n        // deletes staging directory.\n        addService(createStagingDirCleaningService());\n      }\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      // Init ClientService separately so that we stop it separately, since this\n      // service needs to wait some time before it stops so clients can know the\n      // final states\n      clientService.init(conf);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n      // directory after unregistering with RM. So, we start the staging-dir\n      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n      // ContainerAllocator unregisters first and then the staging-dir cleaner\n      // deletes staging directory.\n      addService(createStagingDirCleaningService());\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "1c1ebc1553650ac8e4486faf21f0d95150f607ad": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5497. Changed MRAppMaster to sleep only after doing everything else but just before ClientService to avoid race conditions during RM restart. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1521699 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/09/13 5:38 PM",
      "commitName": "1c1ebc1553650ac8e4486faf21f0d95150f607ad",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "30/08/13 7:09 PM",
      "commitNameOld": "236b8530bd05015d3b8a8131b111454c54c9e55d",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 10.94,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,198 +1,200 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     initJobCredentialsAndUGI(conf);\n \n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n     LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n         \" for application: \" + appAttemptID.getApplicationId().getId() +\n         \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n \n       if (copyHistory) {\n         // Now that there\u0027s a FINISHING state for application on RM to give AMs\n         // plenty of time to clean up after unregister it\u0027s safe to clean staging\n         // directory after unregistering with RM. So, we start the staging-dir\n         // cleaner BEFORE the ContainerAllocator so that on shut-down,\n         // ContainerAllocator unregisters first and then the staging-dir cleaner\n         // deletes staging directory.\n         addService(createStagingDirCleaningService());\n       }\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n-      addIfService(clientService);\n+      // Init ClientService separately so that we stop it separately, since this\n+      // service needs to wait some time before it stops so clients can know the\n+      // final states\n+      clientService.init(conf);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // Now that there\u0027s a FINISHING state for application on RM to give AMs\n       // plenty of time to clean up after unregister it\u0027s safe to clean staging\n       // directory after unregistering with RM. So, we start the staging-dir\n       // cleaner BEFORE the ContainerAllocator so that on shut-down,\n       // ContainerAllocator unregisters first and then the staging-dir cleaner\n       // deletes staging directory.\n       addService(createStagingDirCleaningService());\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n-    \n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    initJobCredentialsAndUGI(conf);\n\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n\n      if (copyHistory) {\n        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n        // directory after unregistering with RM. So, we start the staging-dir\n        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n        // ContainerAllocator unregisters first and then the staging-dir cleaner\n        // deletes staging directory.\n        addService(createStagingDirCleaningService());\n      }\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      // Init ClientService separately so that we stop it separately, since this\n      // service needs to wait some time before it stops so clients can know the\n      // final states\n      clientService.init(conf);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n      // directory after unregistering with RM. So, we start the staging-dir\n      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n      // ContainerAllocator unregisters first and then the staging-dir cleaner\n      // deletes staging directory.\n      addService(createStagingDirCleaningService());\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "740f4cb97a4d5ec498f6e91d91ee7e75ad1c52c2": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5476. Changed MR AM recovery code to cleanup staging-directory only after unregistering from the RM. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1516660 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/08/13 4:17 PM",
      "commitName": "740f4cb97a4d5ec498f6e91d91ee7e75ad1c52c2",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "21/08/13 6:52 PM",
      "commitNameOld": "ded91b4cfa22c8d7c498ea21c8c1ac52fe9a9e29",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.89,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,191 +1,198 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     initJobCredentialsAndUGI(conf);\n \n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n     LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n         \" for application: \" + appAttemptID.getApplicationId().getId() +\n         \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n-      \n+\n+      if (copyHistory) {\n+        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n+        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n+        // directory after unregistering with RM. So, we start the staging-dir\n+        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n+        // ContainerAllocator unregisters first and then the staging-dir cleaner\n+        // deletes staging directory.\n+        addService(createStagingDirCleaningService());\n+      }\n+\n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n-        // Add the staging directory cleaner before the history server but after\n-        // the container allocator so the staging directory is cleaned after\n-        // the history has been flushed but before unregistering with the RM.\n-        addService(createStagingDirCleaningService());\n-\n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n-        \n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       addIfService(clientService);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n+      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n+      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n+      // directory after unregistering with RM. So, we start the staging-dir\n+      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n+      // ContainerAllocator unregisters first and then the staging-dir cleaner\n+      // deletes staging directory.\n+      addService(createStagingDirCleaningService());\n+\n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n-      // Add the staging directory cleaner before the history server but after\n-      // the container allocator so the staging directory is cleaned after\n-      // the history has been flushed but before unregistering with the RM.\n-      addService(createStagingDirCleaningService());\n-\n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     \n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    initJobCredentialsAndUGI(conf);\n\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n\n      if (copyHistory) {\n        // Now that there\u0027s a FINISHING state for application on RM to give AMs\n        // plenty of time to clean up after unregister it\u0027s safe to clean staging\n        // directory after unregistering with RM. So, we start the staging-dir\n        // cleaner BEFORE the ContainerAllocator so that on shut-down,\n        // ContainerAllocator unregisters first and then the staging-dir cleaner\n        // deletes staging directory.\n        addService(createStagingDirCleaningService());\n      }\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // Now that there\u0027s a FINISHING state for application on RM to give AMs\n      // plenty of time to clean up after unregister it\u0027s safe to clean staging\n      // directory after unregistering with RM. So, we start the staging-dir\n      // cleaner BEFORE the ContainerAllocator so that on shut-down,\n      // ContainerAllocator unregisters first and then the staging-dir cleaner\n      // deletes staging directory.\n      addService(createStagingDirCleaningService());\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "b64572b06b1282128180b9ebdd971f9b1e973e61": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5199. Removing ApplicationTokens file as it is no longer needed. Contributed by Daryn Sharp.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1492848 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/06/13 1:20 PM",
      "commitName": "b64572b06b1282128180b9ebdd971f9b1e973e61",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "13/06/13 8:54 AM",
      "commitNameOld": "0928502029ef141759008997335ea2cd836a7154",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 0.18,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,191 +1,191 @@\n   protected void serviceInit(final Configuration conf) throws Exception {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n-    downloadTokensAndSetupUGI(conf);\n+    initJobCredentialsAndUGI(conf);\n \n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n     LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n         \" for application: \" + appAttemptID.getApplicationId().getId() +\n         \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n       \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the staging directory cleaner before the history server but after\n         // the container allocator so the staging directory is cleaned after\n         // the history has been flushed but before unregistering with the RM.\n         addService(createStagingDirCleaningService());\n \n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n         \n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       addIfService(clientService);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the staging directory cleaner before the history server but after\n       // the container allocator so the staging directory is cleaned after\n       // the history has been flushed but before unregistering with the RM.\n       addService(createStagingDirCleaningService());\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     \n     super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    initJobCredentialsAndUGI(conf);\n\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n      \n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the staging directory cleaner before the history server but after\n        // the container allocator so the staging directory is cleaned after\n        // the history has been flushed but before unregistering with the RM.\n        addService(createStagingDirCleaningService());\n\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n        \n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the staging directory cleaner before the history server but after\n      // the container allocator so the staging directory is cleaned after\n      // the history has been flushed but before unregistering with the RM.\n      addService(createStagingDirCleaningService());\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.serviceInit(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "0928502029ef141759008997335ea2cd836a7154": {
      "type": "Ymultichange(Yrename,Ymodifierchange,Yexceptionschange,Ybodychange)",
      "commitMessage": "YARN-530. Defined Service model strictly, implemented AbstractService for robust subclassing and migrated yarn-common services. Contributed by Steve Loughran.\nYARN-117. Migrated rest of YARN to the new service model. Contributed by Steve Louhran.\nMAPREDUCE-5298. Moved MapReduce services to YARN-530 stricter lifecycle. Contributed by Steve Loughran.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1492718 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/06/13 8:54 AM",
      "commitName": "0928502029ef141759008997335ea2cd836a7154",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "YARN-530. Defined Service model strictly, implemented AbstractService for robust subclassing and migrated yarn-common services. Contributed by Steve Loughran.\nYARN-117. Migrated rest of YARN to the new service model. Contributed by Steve Louhran.\nMAPREDUCE-5298. Moved MapReduce services to YARN-530 stricter lifecycle. Contributed by Steve Loughran.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1492718 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/06/13 8:54 AM",
          "commitName": "0928502029ef141759008997335ea2cd836a7154",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "03/06/13 9:05 PM",
          "commitNameOld": "a83fb61ac07c0468cbc7a38526e92683883dd932",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 9.49,
          "commitsBetweenForRepo": 61,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,191 +1,191 @@\n-  public void init(final Configuration conf) {\n+  protected void serviceInit(final Configuration conf) throws Exception {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n \n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n     LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n         \" for application: \" + appAttemptID.getApplicationId().getId() +\n         \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n       \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the staging directory cleaner before the history server but after\n         // the container allocator so the staging directory is cleaned after\n         // the history has been flushed but before unregistering with the RM.\n         addService(createStagingDirCleaningService());\n \n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n         \n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       addIfService(clientService);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the staging directory cleaner before the history server but after\n       // the container allocator so the staging directory is cleaned after\n       // the history has been flushed but before unregistering with the RM.\n       addService(createStagingDirCleaningService());\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     \n-    super.init(conf);\n+    super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
          "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n      \n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the staging directory cleaner before the history server but after\n        // the container allocator so the staging directory is cleaned after\n        // the history has been flushed but before unregistering with the RM.\n        addService(createStagingDirCleaningService());\n\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n        \n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the staging directory cleaner before the history server but after\n      // the container allocator so the staging directory is cleaned after\n      // the history has been flushed but before unregistering with the RM.\n      addService(createStagingDirCleaningService());\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.serviceInit(conf);\n  } // end of init()",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
          "extendedDetails": {
            "oldValue": "init",
            "newValue": "serviceInit"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-530. Defined Service model strictly, implemented AbstractService for robust subclassing and migrated yarn-common services. Contributed by Steve Loughran.\nYARN-117. Migrated rest of YARN to the new service model. Contributed by Steve Louhran.\nMAPREDUCE-5298. Moved MapReduce services to YARN-530 stricter lifecycle. Contributed by Steve Loughran.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1492718 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/06/13 8:54 AM",
          "commitName": "0928502029ef141759008997335ea2cd836a7154",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "03/06/13 9:05 PM",
          "commitNameOld": "a83fb61ac07c0468cbc7a38526e92683883dd932",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 9.49,
          "commitsBetweenForRepo": 61,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,191 +1,191 @@\n-  public void init(final Configuration conf) {\n+  protected void serviceInit(final Configuration conf) throws Exception {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n \n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n     LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n         \" for application: \" + appAttemptID.getApplicationId().getId() +\n         \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n       \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the staging directory cleaner before the history server but after\n         // the container allocator so the staging directory is cleaned after\n         // the history has been flushed but before unregistering with the RM.\n         addService(createStagingDirCleaningService());\n \n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n         \n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       addIfService(clientService);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the staging directory cleaner before the history server but after\n       // the container allocator so the staging directory is cleaned after\n       // the history has been flushed but before unregistering with the RM.\n       addService(createStagingDirCleaningService());\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     \n-    super.init(conf);\n+    super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
          "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n      \n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the staging directory cleaner before the history server but after\n        // the container allocator so the staging directory is cleaned after\n        // the history has been flushed but before unregistering with the RM.\n        addService(createStagingDirCleaningService());\n\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n        \n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the staging directory cleaner before the history server but after\n      // the container allocator so the staging directory is cleaned after\n      // the history has been flushed but before unregistering with the RM.\n      addService(createStagingDirCleaningService());\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.serviceInit(conf);\n  } // end of init()",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[protected]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "YARN-530. Defined Service model strictly, implemented AbstractService for robust subclassing and migrated yarn-common services. Contributed by Steve Loughran.\nYARN-117. Migrated rest of YARN to the new service model. Contributed by Steve Louhran.\nMAPREDUCE-5298. Moved MapReduce services to YARN-530 stricter lifecycle. Contributed by Steve Loughran.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1492718 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/06/13 8:54 AM",
          "commitName": "0928502029ef141759008997335ea2cd836a7154",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "03/06/13 9:05 PM",
          "commitNameOld": "a83fb61ac07c0468cbc7a38526e92683883dd932",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 9.49,
          "commitsBetweenForRepo": 61,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,191 +1,191 @@\n-  public void init(final Configuration conf) {\n+  protected void serviceInit(final Configuration conf) throws Exception {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n \n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n     LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n         \" for application: \" + appAttemptID.getApplicationId().getId() +\n         \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n       \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the staging directory cleaner before the history server but after\n         // the container allocator so the staging directory is cleaned after\n         // the history has been flushed but before unregistering with the RM.\n         addService(createStagingDirCleaningService());\n \n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n         \n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       addIfService(clientService);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the staging directory cleaner before the history server but after\n       // the container allocator so the staging directory is cleaned after\n       // the history has been flushed but before unregistering with the RM.\n       addService(createStagingDirCleaningService());\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     \n-    super.init(conf);\n+    super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
          "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n      \n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the staging directory cleaner before the history server but after\n        // the container allocator so the staging directory is cleaned after\n        // the history has been flushed but before unregistering with the RM.\n        addService(createStagingDirCleaningService());\n\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n        \n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the staging directory cleaner before the history server but after\n      // the container allocator so the staging directory is cleaned after\n      // the history has been flushed but before unregistering with the RM.\n      addService(createStagingDirCleaningService());\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.serviceInit(conf);\n  } // end of init()",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[Exception]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-530. Defined Service model strictly, implemented AbstractService for robust subclassing and migrated yarn-common services. Contributed by Steve Loughran.\nYARN-117. Migrated rest of YARN to the new service model. Contributed by Steve Louhran.\nMAPREDUCE-5298. Moved MapReduce services to YARN-530 stricter lifecycle. Contributed by Steve Loughran.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1492718 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/06/13 8:54 AM",
          "commitName": "0928502029ef141759008997335ea2cd836a7154",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "03/06/13 9:05 PM",
          "commitNameOld": "a83fb61ac07c0468cbc7a38526e92683883dd932",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 9.49,
          "commitsBetweenForRepo": 61,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,191 +1,191 @@\n-  public void init(final Configuration conf) {\n+  protected void serviceInit(final Configuration conf) throws Exception {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n \n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n     LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n         \" for application: \" + appAttemptID.getApplicationId().getId() +\n         \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n       \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the staging directory cleaner before the history server but after\n         // the container allocator so the staging directory is cleaned after\n         // the history has been flushed but before unregistering with the RM.\n         addService(createStagingDirCleaningService());\n \n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n         \n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       addIfService(clientService);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the staging directory cleaner before the history server but after\n       // the container allocator so the staging directory is cleaned after\n       // the history has been flushed but before unregistering with the RM.\n       addService(createStagingDirCleaningService());\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     \n-    super.init(conf);\n+    super.serviceInit(conf);\n   } // end of init()\n\\ No newline at end of file\n",
          "actualSource": "  protected void serviceInit(final Configuration conf) throws Exception {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n      \n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the staging directory cleaner before the history server but after\n        // the container allocator so the staging directory is cleaned after\n        // the history has been flushed but before unregistering with the RM.\n        addService(createStagingDirCleaningService());\n\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n        \n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the staging directory cleaner before the history server but after\n      // the container allocator so the staging directory is cleaned after\n      // the history has been flushed but before unregistering with the RM.\n      addService(createStagingDirCleaningService());\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.serviceInit(conf);\n  } // end of init()",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
          "extendedDetails": {}
        }
      ]
    },
    "a83fb61ac07c0468cbc7a38526e92683883dd932": {
      "type": "Ybodychange",
      "commitMessage": "YARN-635. Renamed YarnRemoteException to YarnException. Contributed by Siddharth Seth.\nMAPREDUCE-5301. Updated MR code to work with YARN-635 changes of renaming YarnRemoteException to YarnException. Contributed by Siddharth Seth\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489283 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/06/13 9:05 PM",
      "commitName": "a83fb61ac07c0468cbc7a38526e92683883dd932",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "14/05/13 4:43 PM",
      "commitNameOld": "4d8e350750748b919ee2158690a44cd9fd80dcae",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 20.18,
      "commitsBetweenForRepo": 112,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,191 +1,191 @@\n   public void init(final Configuration conf) {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n \n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n     LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n         \" for application: \" + appAttemptID.getApplicationId().getId() +\n         \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n-      throw new YarnException(\"Error while initializing\", e);\n+      throw new YarnRuntimeException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n       \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the staging directory cleaner before the history server but after\n         // the container allocator so the staging directory is cleaned after\n         // the history has been flushed but before unregistering with the RM.\n         addService(createStagingDirCleaningService());\n \n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n         \n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n \n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       addIfService(clientService);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the staging directory cleaner before the history server but after\n       // the container allocator so the staging directory is cleaned after\n       // the history has been flushed but before unregistering with the RM.\n       addService(createStagingDirCleaningService());\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n      \n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the staging directory cleaner before the history server but after\n        // the container allocator so the staging directory is cleaned after\n        // the history has been flushed but before unregistering with the RM.\n        addService(createStagingDirCleaningService());\n\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n        \n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the staging directory cleaner before the history server but after\n      // the container allocator so the staging directory is cleaned after\n      // the history has been flushed but before unregistering with the RM.\n      addService(createStagingDirCleaningService());\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "6a1c41111edcdc58c846fc50e53554fbba230171": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5079. Changes job recovery to restore state directly from job history, instaed of simulating state machine events. Contributed by Jason Lowe and Robert Parker.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1466767 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/04/13 9:52 PM",
      "commitName": "6a1c41111edcdc58c846fc50e53554fbba230171",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "03/04/13 6:56 PM",
      "commitNameOld": "fc75d3f3dc2733d6c783eb4d4f1c5c6ae680f08e",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 7.12,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,216 +1,191 @@\n   public void init(final Configuration conf) {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n \n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n     LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n         \" for application: \" + appAttemptID.getApplicationId().getId() +\n         \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n             \" is last retry: \" + isLastAMRetry +\n             \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n       \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the staging directory cleaner before the history server but after\n         // the container allocator so the staging directory is cleaned after\n         // the history has been flushed but before unregistering with the RM.\n         addService(createStagingDirCleaningService());\n \n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n         \n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n-      boolean recoveryEnabled \u003d conf.getBoolean(\n-          MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n-      boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n \n-      // If a shuffle secret was not provided by the job client then this app\n-      // attempt will generate one.  However that disables recovery if there\n-      // are reducers as the shuffle secret would be app attempt specific.\n-      boolean shuffleKeyValidForRecovery \u003d (numReduceTasks \u003e 0 \u0026\u0026\n-          TokenCache.getShuffleSecretKey(fsTokens) !\u003d null);\n-\n-      if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n-          \u0026\u0026 shuffleKeyValidForRecovery \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n-        LOG.info(\"Recovery is enabled. \"\n-            + \"Will try to recover from previous life on best effort basis.\");\n-        recoveryServ \u003d createRecoveryService(context);\n-        addIfService(recoveryServ);\n-        dispatcher \u003d recoveryServ.getDispatcher();\n-        clock \u003d recoveryServ.getClock();\n-        inRecovery \u003d true;\n-      } else {\n-        LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n-            + recoveryEnabled + \" recoverySupportedByCommitter: \"\n-            + recoverySupportedByCommitter + \" shuffleKeyValidForRecovery: \"\n-            + shuffleKeyValidForRecovery + \" ApplicationAttemptID: \"\n-            + appAttemptID.getAttemptId());\n-        dispatcher \u003d createDispatcher();\n-        addIfService(dispatcher);\n-      }\n+      dispatcher \u003d createDispatcher();\n+      addIfService(dispatcher);\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       addIfService(clientService);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the staging directory cleaner before the history server but after\n       // the container allocator so the staging directory is cleaned after\n       // the history has been flushed but before unregistering with the RM.\n       addService(createStagingDirCleaningService());\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n      \n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the staging directory cleaner before the history server but after\n        // the container allocator so the staging directory is cleaned after\n        // the history has been flushed but before unregistering with the RM.\n        addService(createStagingDirCleaningService());\n\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n        \n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the staging directory cleaner before the history server but after\n      // the container allocator so the staging directory is cleaned after\n      // the history has been flushed but before unregistering with the RM.\n      addService(createStagingDirCleaningService());\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "46315a2d914058969c7234272420c063ce268bf5": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5062. Fix MR AM to read max-retries from the RM. Contributed by *Zhijie Shen.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1460923 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/03/13 3:33 PM",
      "commitName": "46315a2d914058969c7234272420c063ce268bf5",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "15/03/13 2:09 PM",
      "commitNameOld": "7d7553c4eb7d9a282410a3213d26a89fea9b7865",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 10.06,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,215 +1,216 @@\n   public void init(final Configuration conf) {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n-    \n-    //TODO this is a hack, we really need the RM to inform us when we\n-    // are the last one.  This would allow us to configure retries on\n-    // a per application basis.\n-    int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n-        YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n-    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n-    LOG.info(\"AM Retries: \" + numAMRetries + \n-        \" attempt num: \" + appAttemptID.getAttemptId() +\n+\n+    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n+    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n+        \" for application: \" + appAttemptID.getApplicationId().getId() +\n+        \". Attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n-    \n-    \n+\n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n+        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n+            \" is last retry: \" + isLastAMRetry +\n+            \" because the staging dir doesn\u0027t exist.\");\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n+        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n+            \" is last retry: \" + isLastAMRetry +\n+            \" because a commit was started.\");\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n       \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the staging directory cleaner before the history server but after\n         // the container allocator so the staging directory is cleaned after\n         // the history has been flushed but before unregistering with the RM.\n         addService(createStagingDirCleaningService());\n \n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n         \n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n       boolean recoveryEnabled \u003d conf.getBoolean(\n           MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n       boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n \n       // If a shuffle secret was not provided by the job client then this app\n       // attempt will generate one.  However that disables recovery if there\n       // are reducers as the shuffle secret would be app attempt specific.\n       boolean shuffleKeyValidForRecovery \u003d (numReduceTasks \u003e 0 \u0026\u0026\n           TokenCache.getShuffleSecretKey(fsTokens) !\u003d null);\n \n       if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n           \u0026\u0026 shuffleKeyValidForRecovery \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n         LOG.info(\"Recovery is enabled. \"\n             + \"Will try to recover from previous life on best effort basis.\");\n         recoveryServ \u003d createRecoveryService(context);\n         addIfService(recoveryServ);\n         dispatcher \u003d recoveryServ.getDispatcher();\n         clock \u003d recoveryServ.getClock();\n         inRecovery \u003d true;\n       } else {\n         LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n             + recoveryEnabled + \" recoverySupportedByCommitter: \"\n             + recoverySupportedByCommitter + \" shuffleKeyValidForRecovery: \"\n             + shuffleKeyValidForRecovery + \" ApplicationAttemptID: \"\n             + appAttemptID.getAttemptId());\n         dispatcher \u003d createDispatcher();\n         addIfService(dispatcher);\n       }\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       addIfService(clientService);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the staging directory cleaner before the history server but after\n       // the container allocator so the staging directory is cleaned after\n       // the history has been flushed but before unregistering with the RM.\n       addService(createStagingDirCleaningService());\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d maxAppAttempts;\n    LOG.info(\"The specific max attempts: \" + maxAppAttempts +\n        \" for application: \" + appAttemptID.getApplicationId().getId() +\n        \". Attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because the staging dir doesn\u0027t exist.\");\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() +\n            \" is last retry: \" + isLastAMRetry +\n            \" because a commit was started.\");\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n      \n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the staging directory cleaner before the history server but after\n        // the container allocator so the staging directory is cleaned after\n        // the history has been flushed but before unregistering with the RM.\n        addService(createStagingDirCleaningService());\n\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n        \n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n      boolean recoveryEnabled \u003d conf.getBoolean(\n          MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n      boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n\n      // If a shuffle secret was not provided by the job client then this app\n      // attempt will generate one.  However that disables recovery if there\n      // are reducers as the shuffle secret would be app attempt specific.\n      boolean shuffleKeyValidForRecovery \u003d (numReduceTasks \u003e 0 \u0026\u0026\n          TokenCache.getShuffleSecretKey(fsTokens) !\u003d null);\n\n      if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n          \u0026\u0026 shuffleKeyValidForRecovery \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n        LOG.info(\"Recovery is enabled. \"\n            + \"Will try to recover from previous life on best effort basis.\");\n        recoveryServ \u003d createRecoveryService(context);\n        addIfService(recoveryServ);\n        dispatcher \u003d recoveryServ.getDispatcher();\n        clock \u003d recoveryServ.getClock();\n        inRecovery \u003d true;\n      } else {\n        LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n            + recoveryEnabled + \" recoverySupportedByCommitter: \"\n            + recoverySupportedByCommitter + \" shuffleKeyValidForRecovery: \"\n            + shuffleKeyValidForRecovery + \" ApplicationAttemptID: \"\n            + appAttemptID.getAttemptId());\n        dispatcher \u003d createDispatcher();\n        addIfService(dispatcher);\n      }\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the staging directory cleaner before the history server but after\n      // the container allocator so the staging directory is cleaned after\n      // the history has been flushed but before unregistering with the RM.\n      addService(createStagingDirCleaningService());\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5042. Reducer unable to fetch for a map task that was recovered (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1457119 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/03/13 2:09 PM",
      "commitName": "7d7553c4eb7d9a282410a3213d26a89fea9b7865",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "09/01/13 2:56 PM",
      "commitNameOld": "b16dfc125dfd172900e34de1b46d3a06fe9aceb6",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 64.93,
      "commitsBetweenForRepo": 306,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,207 +1,215 @@\n   public void init(final Configuration conf) {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n     \n     //TODO this is a hack, we really need the RM to inform us when we\n     // are the last one.  This would allow us to configure retries on\n     // a per application basis.\n     int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n         YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n     LOG.info(\"AM Retries: \" + numAMRetries + \n         \" attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n     \n     \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n       NoopEventHandler eater \u003d new NoopEventHandler();\n       //We do not have a JobEventDispatcher in this path\n       dispatcher.register(JobEventType.class, eater);\n \n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n       } else {\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             eater);\n       }\n       \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the staging directory cleaner before the history server but after\n         // the container allocator so the staging directory is cleaned after\n         // the history has been flushed but before unregistering with the RM.\n         addService(createStagingDirCleaningService());\n \n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n         \n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n       boolean recoveryEnabled \u003d conf.getBoolean(\n           MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n       boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n+\n+      // If a shuffle secret was not provided by the job client then this app\n+      // attempt will generate one.  However that disables recovery if there\n+      // are reducers as the shuffle secret would be app attempt specific.\n+      boolean shuffleKeyValidForRecovery \u003d (numReduceTasks \u003e 0 \u0026\u0026\n+          TokenCache.getShuffleSecretKey(fsTokens) !\u003d null);\n+\n       if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n-          \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n+          \u0026\u0026 shuffleKeyValidForRecovery \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n         LOG.info(\"Recovery is enabled. \"\n             + \"Will try to recover from previous life on best effort basis.\");\n         recoveryServ \u003d createRecoveryService(context);\n         addIfService(recoveryServ);\n         dispatcher \u003d recoveryServ.getDispatcher();\n         clock \u003d recoveryServ.getClock();\n         inRecovery \u003d true;\n       } else {\n         LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n             + recoveryEnabled + \" recoverySupportedByCommitter: \"\n-            + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n+            + recoverySupportedByCommitter + \" shuffleKeyValidForRecovery: \"\n+            + shuffleKeyValidForRecovery + \" ApplicationAttemptID: \"\n             + appAttemptID.getAttemptId());\n         dispatcher \u003d createDispatcher();\n         addIfService(dispatcher);\n       }\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       addIfService(clientService);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the staging directory cleaner before the history server but after\n       // the container allocator so the staging directory is cleaned after\n       // the history has been flushed but before unregistering with the RM.\n       addService(createStagingDirCleaningService());\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n    \n    //TODO this is a hack, we really need the RM to inform us when we\n    // are the last one.  This would allow us to configure retries on\n    // a per application basis.\n    int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n        YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n    LOG.info(\"AM Retries: \" + numAMRetries + \n        \" attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n    \n    \n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n      \n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the staging directory cleaner before the history server but after\n        // the container allocator so the staging directory is cleaned after\n        // the history has been flushed but before unregistering with the RM.\n        addService(createStagingDirCleaningService());\n\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n        \n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n      boolean recoveryEnabled \u003d conf.getBoolean(\n          MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n      boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n\n      // If a shuffle secret was not provided by the job client then this app\n      // attempt will generate one.  However that disables recovery if there\n      // are reducers as the shuffle secret would be app attempt specific.\n      boolean shuffleKeyValidForRecovery \u003d (numReduceTasks \u003e 0 \u0026\u0026\n          TokenCache.getShuffleSecretKey(fsTokens) !\u003d null);\n\n      if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n          \u0026\u0026 shuffleKeyValidForRecovery \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n        LOG.info(\"Recovery is enabled. \"\n            + \"Will try to recover from previous life on best effort basis.\");\n        recoveryServ \u003d createRecoveryService(context);\n        addIfService(recoveryServ);\n        dispatcher \u003d recoveryServ.getDispatcher();\n        clock \u003d recoveryServ.getClock();\n        inRecovery \u003d true;\n      } else {\n        LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n            + recoveryEnabled + \" recoverySupportedByCommitter: \"\n            + recoverySupportedByCommitter + \" shuffleKeyValidForRecovery: \"\n            + shuffleKeyValidForRecovery + \" ApplicationAttemptID: \"\n            + appAttemptID.getAttemptId());\n        dispatcher \u003d createDispatcher();\n        addIfService(dispatcher);\n      }\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the staging directory cleaner before the history server but after\n      // the container allocator so the staging directory is cleaned after\n      // the history has been flushed but before unregistering with the RM.\n      addService(createStagingDirCleaningService());\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "bbc426f53ad9132104752074b78971a04d48634a": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4913. TestMRAppMaster#testMRAppMasterMissingStaging occasionally  exits (Jason Lowe via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1429371 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/01/13 12:07 PM",
      "commitName": "bbc426f53ad9132104752074b78971a04d48634a",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "04/01/13 12:35 PM",
      "commitNameOld": "64e4fb983e022d8d3375a3e1b8facbf95f7ba403",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 0.98,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,203 +1,207 @@\n   public void init(final Configuration conf) {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n     \n     //TODO this is a hack, we really need the RM to inform us when we\n     // are the last one.  This would allow us to configure retries on\n     // a per application basis.\n     int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n         YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n     LOG.info(\"AM Retries: \" + numAMRetries + \n         \" attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n     \n     \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n     \n     boolean copyHistory \u003d false;\n     try {\n       String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n       FileSystem fs \u003d getFileSystem(conf);\n       boolean stagingExists \u003d fs.exists(stagingDir);\n       Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n       boolean commitStarted \u003d fs.exists(startCommitFile);\n       Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n       boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n       Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n       boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n       if(!stagingExists) {\n         isLastAMRetry \u003d true;\n         errorHappenedShutDown \u003d true;\n         forcedState \u003d JobStateInternal.ERROR;\n         shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n         LOG.fatal(shutDownMessage);\n       } else if (commitStarted) {\n         //A commit was started so this is the last time, we just need to know\n         // what result we will use to notify, and how we will unregister\n         errorHappenedShutDown \u003d true;\n         isLastAMRetry \u003d true;\n         copyHistory \u003d true;\n         if (commitSuccess) {\n           shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n           forcedState \u003d JobStateInternal.SUCCEEDED;\n         } else if (commitFailure) {\n           shutDownMessage \u003d \"We crashed after a commit failure.\";\n           forcedState \u003d JobStateInternal.FAILED;\n         } else {\n           //The commit is still pending, commit error\n           shutDownMessage \u003d \"We crashed durring a commit\";\n           forcedState \u003d JobStateInternal.ERROR;\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnException(\"Error while initializing\", e);\n     }\n     \n     if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n       \n+      NoopEventHandler eater \u003d new NoopEventHandler();\n+      //We do not have a JobEventDispatcher in this path\n+      dispatcher.register(JobEventType.class, eater);\n+\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n       if (copyHistory) {\n         historyService \u003d \n           createJobHistoryHandler(context);\n         dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n             historyService);\n+      } else {\n+        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n+            eater);\n       }\n-      NoopEventHandler eater \u003d new NoopEventHandler();\n-      //We do not have a JobEventDispatcher in this path\n-      dispatcher.register(JobEventType.class, eater);\n       \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       containerAllocator \u003d createContainerAllocator(null, context);\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       if (copyHistory) {\n         // Add the staging directory cleaner before the history server but after\n         // the container allocator so the staging directory is cleaned after\n         // the history has been flushed but before unregistering with the RM.\n         addService(createStagingDirCleaningService());\n \n         // Add the JobHistoryEventHandler last so that it is properly stopped first.\n         // This will guarantee that all history-events are flushed before AM goes\n         // ahead with shutdown.\n         // Note: Even though JobHistoryEventHandler is started last, if any\n         // component creates a JobHistoryEvent in the meanwhile, it will be just be\n         // queued inside the JobHistoryEventHandler \n         addIfService(historyService);\n         \n \n         JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n             dispatcher.getEventHandler());\n         addIfService(cpHist);\n       }\n     } else {\n       committer \u003d createOutputCommitter(conf);\n       boolean recoveryEnabled \u003d conf.getBoolean(\n           MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n       boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n       if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n           \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n         LOG.info(\"Recovery is enabled. \"\n             + \"Will try to recover from previous life on best effort basis.\");\n         recoveryServ \u003d createRecoveryService(context);\n         addIfService(recoveryServ);\n         dispatcher \u003d recoveryServ.getDispatcher();\n         clock \u003d recoveryServ.getClock();\n         inRecovery \u003d true;\n       } else {\n         LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n             + recoveryEnabled + \" recoverySupportedByCommitter: \"\n             + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n             + appAttemptID.getAttemptId());\n         dispatcher \u003d createDispatcher();\n         addIfService(dispatcher);\n       }\n \n       //service to handle requests from JobClient\n       clientService \u003d createClientService(context);\n       addIfService(clientService);\n       \n       containerAllocator \u003d createContainerAllocator(clientService, context);\n       \n       //service to handle the output committer\n       committerEventHandler \u003d createCommitterEventHandler(context, committer);\n       addIfService(committerEventHandler);\n \n       //service to handle requests to TaskUmbilicalProtocol\n       taskAttemptListener \u003d createTaskAttemptListener(context);\n       addIfService(taskAttemptListener);\n \n       //service to log job history events\n       EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n       dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n           historyService);\n \n       this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n       //register the event dispatchers\n       dispatcher.register(JobEventType.class, jobEventDispatcher);\n       dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n       dispatcher.register(TaskAttemptEventType.class, \n           new TaskAttemptEventDispatcher());\n       dispatcher.register(CommitterEventType.class, committerEventHandler);\n \n       if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n           || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n         //optional service to speculate on task attempts\u0027 progress\n         speculator \u003d createSpeculator(conf, context);\n         addIfService(speculator);\n       }\n \n       speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n       dispatcher.register(Speculator.EventType.class,\n           speculatorEventDispatcher);\n \n       // service to allocate containers from RM (if non-uber) or to fake it (uber)\n       addIfService(containerAllocator);\n       dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n       // corresponding service to launch allocated containers via NodeManager\n       containerLauncher \u003d createContainerLauncher(context);\n       addIfService(containerLauncher);\n       dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n       // Add the staging directory cleaner before the history server but after\n       // the container allocator so the staging directory is cleaned after\n       // the history has been flushed but before unregistering with the RM.\n       addService(createStagingDirCleaningService());\n \n       // Add the JobHistoryEventHandler last so that it is properly stopped first.\n       // This will guarantee that all history-events are flushed before AM goes\n       // ahead with shutdown.\n       // Note: Even though JobHistoryEventHandler is started last, if any\n       // component creates a JobHistoryEvent in the meanwhile, it will be just be\n       // queued inside the JobHistoryEventHandler \n       addIfService(historyService);\n     }\n     \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n    \n    //TODO this is a hack, we really need the RM to inform us when we\n    // are the last one.  This would allow us to configure retries on\n    // a per application basis.\n    int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n        YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n    LOG.info(\"AM Retries: \" + numAMRetries + \n        \" attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n    \n    \n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      } else {\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            eater);\n      }\n      \n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the staging directory cleaner before the history server but after\n        // the container allocator so the staging directory is cleaned after\n        // the history has been flushed but before unregistering with the RM.\n        addService(createStagingDirCleaningService());\n\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n        \n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n      boolean recoveryEnabled \u003d conf.getBoolean(\n          MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n      boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n      if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n          \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n        LOG.info(\"Recovery is enabled. \"\n            + \"Will try to recover from previous life on best effort basis.\");\n        recoveryServ \u003d createRecoveryService(context);\n        addIfService(recoveryServ);\n        dispatcher \u003d recoveryServ.getDispatcher();\n        clock \u003d recoveryServ.getClock();\n        inRecovery \u003d true;\n      } else {\n        LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n            + recoveryEnabled + \" recoverySupportedByCommitter: \"\n            + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n            + appAttemptID.getAttemptId());\n        dispatcher \u003d createDispatcher();\n        addIfService(dispatcher);\n      }\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the staging directory cleaner before the history server but after\n      // the container allocator so the staging directory is cleaned after\n      // the history has been flushed but before unregistering with the RM.\n      addService(createStagingDirCleaningService());\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "64e4fb983e022d8d3375a3e1b8facbf95f7ba403": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4819. AM can rerun job after reporting final job status to the client (bobby and Bikas Saha via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1429114 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/01/13 12:35 PM",
      "commitName": "64e4fb983e022d8d3375a3e1b8facbf95f7ba403",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "04/01/13 11:15 AM",
      "commitNameOld": "78ab699fe93cafbaff8f496be53d26aff40a68b1",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,122 +1,203 @@\n   public void init(final Configuration conf) {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n     \n     //TODO this is a hack, we really need the RM to inform us when we\n     // are the last one.  This would allow us to configure retries on\n     // a per application basis.\n     int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n         YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n     LOG.info(\"AM Retries: \" + numAMRetries + \n         \" attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n     \n     \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n-\n-    committer \u003d createOutputCommitter(conf);\n-    boolean recoveryEnabled \u003d conf.getBoolean(\n-        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n-    boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n-    if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n-        \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n-      LOG.info(\"Recovery is enabled. \"\n-          + \"Will try to recover from previous life on best effort basis.\");\n-      recoveryServ \u003d createRecoveryService(context);\n-      addIfService(recoveryServ);\n-      dispatcher \u003d recoveryServ.getDispatcher();\n-      clock \u003d recoveryServ.getClock();\n-      inRecovery \u003d true;\n-    } else {\n-      LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n-          + recoveryEnabled + \" recoverySupportedByCommitter: \"\n-          + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n-          + appAttemptID.getAttemptId());\n+    \n+    boolean copyHistory \u003d false;\n+    try {\n+      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n+      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n+      FileSystem fs \u003d getFileSystem(conf);\n+      boolean stagingExists \u003d fs.exists(stagingDir);\n+      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n+      boolean commitStarted \u003d fs.exists(startCommitFile);\n+      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n+      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n+      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n+      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n+      if(!stagingExists) {\n+        isLastAMRetry \u003d true;\n+        errorHappenedShutDown \u003d true;\n+        forcedState \u003d JobStateInternal.ERROR;\n+        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n+        LOG.fatal(shutDownMessage);\n+      } else if (commitStarted) {\n+        //A commit was started so this is the last time, we just need to know\n+        // what result we will use to notify, and how we will unregister\n+        errorHappenedShutDown \u003d true;\n+        isLastAMRetry \u003d true;\n+        copyHistory \u003d true;\n+        if (commitSuccess) {\n+          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n+          forcedState \u003d JobStateInternal.SUCCEEDED;\n+        } else if (commitFailure) {\n+          shutDownMessage \u003d \"We crashed after a commit failure.\";\n+          forcedState \u003d JobStateInternal.FAILED;\n+        } else {\n+          //The commit is still pending, commit error\n+          shutDownMessage \u003d \"We crashed durring a commit\";\n+          forcedState \u003d JobStateInternal.ERROR;\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new YarnException(\"Error while initializing\", e);\n+    }\n+    \n+    if (errorHappenedShutDown) {\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n-    }\n+      \n+      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n+      if (copyHistory) {\n+        historyService \u003d \n+          createJobHistoryHandler(context);\n+        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n+            historyService);\n+      }\n+      NoopEventHandler eater \u003d new NoopEventHandler();\n+      //We do not have a JobEventDispatcher in this path\n+      dispatcher.register(JobEventType.class, eater);\n+      \n+      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n+      containerAllocator \u003d createContainerAllocator(null, context);\n+      addIfService(containerAllocator);\n+      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n-    //service to handle requests from JobClient\n-    clientService \u003d createClientService(context);\n-    addIfService(clientService);\n+      if (copyHistory) {\n+        // Add the staging directory cleaner before the history server but after\n+        // the container allocator so the staging directory is cleaned after\n+        // the history has been flushed but before unregistering with the RM.\n+        addService(createStagingDirCleaningService());\n \n-    containerAllocator \u003d createContainerAllocator(clientService, context);\n+        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n+        // This will guarantee that all history-events are flushed before AM goes\n+        // ahead with shutdown.\n+        // Note: Even though JobHistoryEventHandler is started last, if any\n+        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n+        // queued inside the JobHistoryEventHandler \n+        addIfService(historyService);\n+        \n \n-    //service to handle requests to TaskUmbilicalProtocol\n-    taskAttemptListener \u003d createTaskAttemptListener(context);\n-    addIfService(taskAttemptListener);\n+        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n+            dispatcher.getEventHandler());\n+        addIfService(cpHist);\n+      }\n+    } else {\n+      committer \u003d createOutputCommitter(conf);\n+      boolean recoveryEnabled \u003d conf.getBoolean(\n+          MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n+      boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n+      if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n+          \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n+        LOG.info(\"Recovery is enabled. \"\n+            + \"Will try to recover from previous life on best effort basis.\");\n+        recoveryServ \u003d createRecoveryService(context);\n+        addIfService(recoveryServ);\n+        dispatcher \u003d recoveryServ.getDispatcher();\n+        clock \u003d recoveryServ.getClock();\n+        inRecovery \u003d true;\n+      } else {\n+        LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n+            + recoveryEnabled + \" recoverySupportedByCommitter: \"\n+            + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n+            + appAttemptID.getAttemptId());\n+        dispatcher \u003d createDispatcher();\n+        addIfService(dispatcher);\n+      }\n \n-    //service to handle the output committer\n-    committerEventHandler \u003d createCommitterEventHandler(context, committer);\n-    addIfService(committerEventHandler);\n+      //service to handle requests from JobClient\n+      clientService \u003d createClientService(context);\n+      addIfService(clientService);\n+      \n+      containerAllocator \u003d createContainerAllocator(clientService, context);\n+      \n+      //service to handle the output committer\n+      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n+      addIfService(committerEventHandler);\n \n-    //service to log job history events\n-    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n+      //service to handle requests to TaskUmbilicalProtocol\n+      taskAttemptListener \u003d createTaskAttemptListener(context);\n+      addIfService(taskAttemptListener);\n+\n+      //service to log job history events\n+      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n-    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n-        historyService);\n+      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n+          historyService);\n \n-    this.jobEventDispatcher \u003d new JobEventDispatcher();\n+      this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n-    //register the event dispatchers\n-    dispatcher.register(JobEventType.class, jobEventDispatcher);\n-    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n-    dispatcher.register(TaskAttemptEventType.class, \n-        new TaskAttemptEventDispatcher());\n-    dispatcher.register(CommitterEventType.class, committerEventHandler);\n-   \n-    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n-        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n-      //optional service to speculate on task attempts\u0027 progress\n-      speculator \u003d createSpeculator(conf, context);\n-      addIfService(speculator);\n+      //register the event dispatchers\n+      dispatcher.register(JobEventType.class, jobEventDispatcher);\n+      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n+      dispatcher.register(TaskAttemptEventType.class, \n+          new TaskAttemptEventDispatcher());\n+      dispatcher.register(CommitterEventType.class, committerEventHandler);\n+\n+      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n+          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n+        //optional service to speculate on task attempts\u0027 progress\n+        speculator \u003d createSpeculator(conf, context);\n+        addIfService(speculator);\n+      }\n+\n+      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n+      dispatcher.register(Speculator.EventType.class,\n+          speculatorEventDispatcher);\n+\n+      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n+      addIfService(containerAllocator);\n+      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n+\n+      // corresponding service to launch allocated containers via NodeManager\n+      containerLauncher \u003d createContainerLauncher(context);\n+      addIfService(containerLauncher);\n+      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n+\n+      // Add the staging directory cleaner before the history server but after\n+      // the container allocator so the staging directory is cleaned after\n+      // the history has been flushed but before unregistering with the RM.\n+      addService(createStagingDirCleaningService());\n+\n+      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n+      // This will guarantee that all history-events are flushed before AM goes\n+      // ahead with shutdown.\n+      // Note: Even though JobHistoryEventHandler is started last, if any\n+      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n+      // queued inside the JobHistoryEventHandler \n+      addIfService(historyService);\n     }\n-\n-    speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n-    dispatcher.register(Speculator.EventType.class,\n-        speculatorEventDispatcher);\n-\n-    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n-    addIfService(containerAllocator);\n-    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n-\n-    // corresponding service to launch allocated containers via NodeManager\n-    containerLauncher \u003d createContainerLauncher(context);\n-    addIfService(containerLauncher);\n-    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n-\n-    // Add the staging directory cleaner before the history server but after\n-    // the container allocator so the staging directory is cleaned after\n-    // the history has been flushed but before unregistering with the RM.\n-    addService(createStagingDirCleaningService());\n-\n-    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n-    // This will guarantee that all history-events are flushed before AM goes\n-    // ahead with shutdown.\n-    // Note: Even though JobHistoryEventHandler is started last, if any\n-    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n-    // queued inside the JobHistoryEventHandler \n-    addIfService(historyService);\n-\n+    \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n    \n    //TODO this is a hack, we really need the RM to inform us when we\n    // are the last one.  This would allow us to configure retries on\n    // a per application basis.\n    int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n        YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n    LOG.info(\"AM Retries: \" + numAMRetries + \n        \" attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n    \n    \n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n    \n    boolean copyHistory \u003d false;\n    try {\n      String user \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      Path stagingDir \u003d MRApps.getStagingAreaDir(conf, user);\n      FileSystem fs \u003d getFileSystem(conf);\n      boolean stagingExists \u003d fs.exists(stagingDir);\n      Path startCommitFile \u003d MRApps.getStartJobCommitFile(conf, user, jobId);\n      boolean commitStarted \u003d fs.exists(startCommitFile);\n      Path endCommitSuccessFile \u003d MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\n      boolean commitSuccess \u003d fs.exists(endCommitSuccessFile);\n      Path endCommitFailureFile \u003d MRApps.getEndJobCommitFailureFile(conf, user, jobId);\n      boolean commitFailure \u003d fs.exists(endCommitFailureFile);\n      if(!stagingExists) {\n        isLastAMRetry \u003d true;\n        errorHappenedShutDown \u003d true;\n        forcedState \u003d JobStateInternal.ERROR;\n        shutDownMessage \u003d \"Staging dir does not exist \" + stagingDir;\n        LOG.fatal(shutDownMessage);\n      } else if (commitStarted) {\n        //A commit was started so this is the last time, we just need to know\n        // what result we will use to notify, and how we will unregister\n        errorHappenedShutDown \u003d true;\n        isLastAMRetry \u003d true;\n        copyHistory \u003d true;\n        if (commitSuccess) {\n          shutDownMessage \u003d \"We crashed after successfully committing. Recovering.\";\n          forcedState \u003d JobStateInternal.SUCCEEDED;\n        } else if (commitFailure) {\n          shutDownMessage \u003d \"We crashed after a commit failure.\";\n          forcedState \u003d JobStateInternal.FAILED;\n        } else {\n          //The commit is still pending, commit error\n          shutDownMessage \u003d \"We crashed durring a commit\";\n          forcedState \u003d JobStateInternal.ERROR;\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnException(\"Error while initializing\", e);\n    }\n    \n    if (errorHappenedShutDown) {\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n      \n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d null;\n      if (copyHistory) {\n        historyService \u003d \n          createJobHistoryHandler(context);\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n            historyService);\n      }\n      NoopEventHandler eater \u003d new NoopEventHandler();\n      //We do not have a JobEventDispatcher in this path\n      dispatcher.register(JobEventType.class, eater);\n      \n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      containerAllocator \u003d createContainerAllocator(null, context);\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      if (copyHistory) {\n        // Add the staging directory cleaner before the history server but after\n        // the container allocator so the staging directory is cleaned after\n        // the history has been flushed but before unregistering with the RM.\n        addService(createStagingDirCleaningService());\n\n        // Add the JobHistoryEventHandler last so that it is properly stopped first.\n        // This will guarantee that all history-events are flushed before AM goes\n        // ahead with shutdown.\n        // Note: Even though JobHistoryEventHandler is started last, if any\n        // component creates a JobHistoryEvent in the meanwhile, it will be just be\n        // queued inside the JobHistoryEventHandler \n        addIfService(historyService);\n        \n\n        JobHistoryCopyService cpHist \u003d new JobHistoryCopyService(appAttemptID,\n            dispatcher.getEventHandler());\n        addIfService(cpHist);\n      }\n    } else {\n      committer \u003d createOutputCommitter(conf);\n      boolean recoveryEnabled \u003d conf.getBoolean(\n          MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n      boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n      if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n          \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n        LOG.info(\"Recovery is enabled. \"\n            + \"Will try to recover from previous life on best effort basis.\");\n        recoveryServ \u003d createRecoveryService(context);\n        addIfService(recoveryServ);\n        dispatcher \u003d recoveryServ.getDispatcher();\n        clock \u003d recoveryServ.getClock();\n        inRecovery \u003d true;\n      } else {\n        LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n            + recoveryEnabled + \" recoverySupportedByCommitter: \"\n            + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n            + appAttemptID.getAttemptId());\n        dispatcher \u003d createDispatcher();\n        addIfService(dispatcher);\n      }\n\n      //service to handle requests from JobClient\n      clientService \u003d createClientService(context);\n      addIfService(clientService);\n      \n      containerAllocator \u003d createContainerAllocator(clientService, context);\n      \n      //service to handle the output committer\n      committerEventHandler \u003d createCommitterEventHandler(context, committer);\n      addIfService(committerEventHandler);\n\n      //service to handle requests to TaskUmbilicalProtocol\n      taskAttemptListener \u003d createTaskAttemptListener(context);\n      addIfService(taskAttemptListener);\n\n      //service to log job history events\n      EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n          historyService);\n\n      this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n      //register the event dispatchers\n      dispatcher.register(JobEventType.class, jobEventDispatcher);\n      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n      dispatcher.register(TaskAttemptEventType.class, \n          new TaskAttemptEventDispatcher());\n      dispatcher.register(CommitterEventType.class, committerEventHandler);\n\n      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n        //optional service to speculate on task attempts\u0027 progress\n        speculator \u003d createSpeculator(conf, context);\n        addIfService(speculator);\n      }\n\n      speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n      dispatcher.register(Speculator.EventType.class,\n          speculatorEventDispatcher);\n\n      // service to allocate containers from RM (if non-uber) or to fake it (uber)\n      addIfService(containerAllocator);\n      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n      // corresponding service to launch allocated containers via NodeManager\n      containerLauncher \u003d createContainerLauncher(context);\n      addIfService(containerLauncher);\n      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n      // Add the staging directory cleaner before the history server but after\n      // the container allocator so the staging directory is cleaned after\n      // the history has been flushed but before unregistering with the RM.\n      addService(createStagingDirCleaningService());\n\n      // Add the JobHistoryEventHandler last so that it is properly stopped first.\n      // This will guarantee that all history-events are flushed before AM goes\n      // ahead with shutdown.\n      // Note: Even though JobHistoryEventHandler is started last, if any\n      // component creates a JobHistoryEvent in the meanwhile, it will be just be\n      // queued inside the JobHistoryEventHandler \n      addIfService(historyService);\n    }\n    \n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "78ab699fe93cafbaff8f496be53d26aff40a68b1": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4832. MR AM can get in a split brain situation. Contributed by Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1429040 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/01/13 11:15 AM",
      "commitName": "78ab699fe93cafbaff8f496be53d26aff40a68b1",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "28/12/12 7:01 AM",
      "commitNameOld": "402eb1851341fce72c8e46266a2578bb67b5b684",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 7.18,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,121 +1,122 @@\n   public void init(final Configuration conf) {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n     \n     //TODO this is a hack, we really need the RM to inform us when we\n     // are the last one.  This would allow us to configure retries on\n     // a per application basis.\n     int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n         YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n     LOG.info(\"AM Retries: \" + numAMRetries + \n         \" attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n     \n     \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n \n     committer \u003d createOutputCommitter(conf);\n     boolean recoveryEnabled \u003d conf.getBoolean(\n         MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n     boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n     if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n       LOG.info(\"Recovery is enabled. \"\n           + \"Will try to recover from previous life on best effort basis.\");\n       recoveryServ \u003d createRecoveryService(context);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       inRecovery \u003d true;\n     } else {\n       LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n           + recoveryEnabled + \" recoverySupportedByCommitter: \"\n           + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n           + appAttemptID.getAttemptId());\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n     }\n \n+    //service to handle requests from JobClient\n+    clientService \u003d createClientService(context);\n+    addIfService(clientService);\n+\n+    containerAllocator \u003d createContainerAllocator(clientService, context);\n+\n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n-    //service to do the task cleanup\n+    //service to handle the output committer\n     committerEventHandler \u003d createCommitterEventHandler(context, committer);\n     addIfService(committerEventHandler);\n \n-    //service to handle requests from JobClient\n-    clientService \u003d createClientService(context);\n-    addIfService(clientService);\n-\n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(CommitterEventType.class, committerEventHandler);\n    \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n     dispatcher.register(Speculator.EventType.class,\n         speculatorEventDispatcher);\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n-    containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n     // Add the staging directory cleaner before the history server but after\n     // the container allocator so the staging directory is cleaned after\n     // the history has been flushed but before unregistering with the RM.\n     addService(createStagingDirCleaningService());\n \n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n    \n    //TODO this is a hack, we really need the RM to inform us when we\n    // are the last one.  This would allow us to configure retries on\n    // a per application basis.\n    int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n        YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n    LOG.info(\"AM Retries: \" + numAMRetries + \n        \" attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n    \n    \n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n\n    committer \u003d createOutputCommitter(conf);\n    boolean recoveryEnabled \u003d conf.getBoolean(\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n    boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n    if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n        \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. \"\n          + \"Will try to recover from previous life on best effort basis.\");\n      recoveryServ \u003d createRecoveryService(context);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      inRecovery \u003d true;\n    } else {\n      LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n          + recoveryEnabled + \" recoverySupportedByCommitter: \"\n          + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n          + appAttemptID.getAttemptId());\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to handle the output committer\n    committerEventHandler \u003d createCommitterEventHandler(context, committer);\n    addIfService(committerEventHandler);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(CommitterEventType.class, committerEventHandler);\n   \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n    dispatcher.register(Speculator.EventType.class,\n        speculatorEventDispatcher);\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the staging directory cleaner before the history server but after\n    // the container allocator so the staging directory is cleaned after\n    // the history has been flushed but before unregistering with the RM.\n    addService(createStagingDirCleaningService());\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "402eb1851341fce72c8e46266a2578bb67b5b684": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4813. AM timing out during job commit (jlowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1426536 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/12/12 7:01 AM",
      "commitName": "402eb1851341fce72c8e46266a2578bb67b5b684",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "01/11/12 3:59 PM",
      "commitNameOld": "42d1eaf237ef0a3a30c061888d35329b2a2e1453",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 56.67,
      "commitsBetweenForRepo": 239,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,121 +1,121 @@\n   public void init(final Configuration conf) {\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n     \n     //TODO this is a hack, we really need the RM to inform us when we\n     // are the last one.  This would allow us to configure retries on\n     // a per application basis.\n     int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n         YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n     isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n     LOG.info(\"AM Retries: \" + numAMRetries + \n         \" attempt num: \" + appAttemptID.getAttemptId() +\n         \" is last retry: \" + isLastAMRetry);\n     \n     \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n \n     committer \u003d createOutputCommitter(conf);\n     boolean recoveryEnabled \u003d conf.getBoolean(\n         MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n     boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n     if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n       LOG.info(\"Recovery is enabled. \"\n           + \"Will try to recover from previous life on best effort basis.\");\n       recoveryServ \u003d createRecoveryService(context);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       inRecovery \u003d true;\n     } else {\n       LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n           + recoveryEnabled + \" recoverySupportedByCommitter: \"\n           + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n           + appAttemptID.getAttemptId());\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n-    taskCleaner \u003d createTaskCleaner(context);\n-    addIfService(taskCleaner);\n+    committerEventHandler \u003d createCommitterEventHandler(context, committer);\n+    addIfService(committerEventHandler);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n-    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n+    dispatcher.register(CommitterEventType.class, committerEventHandler);\n    \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n     dispatcher.register(Speculator.EventType.class,\n         speculatorEventDispatcher);\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n     // Add the staging directory cleaner before the history server but after\n     // the container allocator so the staging directory is cleaned after\n     // the history has been flushed but before unregistering with the RM.\n     addService(createStagingDirCleaningService());\n \n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n    \n    //TODO this is a hack, we really need the RM to inform us when we\n    // are the last one.  This would allow us to configure retries on\n    // a per application basis.\n    int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n        YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n    LOG.info(\"AM Retries: \" + numAMRetries + \n        \" attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n    \n    \n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n\n    committer \u003d createOutputCommitter(conf);\n    boolean recoveryEnabled \u003d conf.getBoolean(\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n    boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n    if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n        \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. \"\n          + \"Will try to recover from previous life on best effort basis.\");\n      recoveryServ \u003d createRecoveryService(context);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      inRecovery \u003d true;\n    } else {\n      LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n          + recoveryEnabled + \" recoverySupportedByCommitter: \"\n          + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n          + appAttemptID.getAttemptId());\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    committerEventHandler \u003d createCommitterEventHandler(context, committer);\n    addIfService(committerEventHandler);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(CommitterEventType.class, committerEventHandler);\n   \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n    dispatcher.register(Speculator.EventType.class,\n        speculatorEventDispatcher);\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the staging directory cleaner before the history server but after\n    // the container allocator so the staging directory is cleaned after\n    // the history has been flushed but before unregistering with the RM.\n    addService(createStagingDirCleaningService());\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "25e96e455b3473387df865fbc1c3ad7ebf9ff1e4": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4611. MR AM dies badly when Node is decommissioned (Robert Evans via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1379599 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/08/12 1:43 PM",
      "commitName": "25e96e455b3473387df865fbc1c3ad7ebf9ff1e4",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "30/08/12 12:58 PM",
      "commitNameOld": "6f6e170325d39f9f7b543a39791b2cb54692f83d",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 1.03,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,111 +1,121 @@\n   public void init(final Configuration conf) {\n-\n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n-\n+    \n+    //TODO this is a hack, we really need the RM to inform us when we\n+    // are the last one.  This would allow us to configure retries on\n+    // a per application basis.\n+    int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n+        YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n+    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n+    LOG.info(\"AM Retries: \" + numAMRetries + \n+        \" attempt num: \" + appAttemptID.getAttemptId() +\n+        \" is last retry: \" + isLastAMRetry);\n+    \n+    \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n \n     committer \u003d createOutputCommitter(conf);\n     boolean recoveryEnabled \u003d conf.getBoolean(\n         MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n     boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n     if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n       LOG.info(\"Recovery is enabled. \"\n           + \"Will try to recover from previous life on best effort basis.\");\n       recoveryServ \u003d createRecoveryService(context);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       inRecovery \u003d true;\n     } else {\n       LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n           + recoveryEnabled + \" recoverySupportedByCommitter: \"\n           + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n           + appAttemptID.getAttemptId());\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n     dispatcher.register(Speculator.EventType.class,\n         speculatorEventDispatcher);\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n     // Add the staging directory cleaner before the history server but after\n     // the container allocator so the staging directory is cleaned after\n     // the history has been flushed but before unregistering with the RM.\n     addService(createStagingDirCleaningService());\n \n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n    \n    //TODO this is a hack, we really need the RM to inform us when we\n    // are the last one.  This would allow us to configure retries on\n    // a per application basis.\n    int numAMRetries \u003d conf.getInt(YarnConfiguration.RM_AM_MAX_RETRIES, \n        YarnConfiguration.DEFAULT_RM_AM_MAX_RETRIES);\n    isLastAMRetry \u003d appAttemptID.getAttemptId() \u003e\u003d numAMRetries;\n    LOG.info(\"AM Retries: \" + numAMRetries + \n        \" attempt num: \" + appAttemptID.getAttemptId() +\n        \" is last retry: \" + isLastAMRetry);\n    \n    \n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n\n    committer \u003d createOutputCommitter(conf);\n    boolean recoveryEnabled \u003d conf.getBoolean(\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n    boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n    if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n        \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. \"\n          + \"Will try to recover from previous life on best effort basis.\");\n      recoveryServ \u003d createRecoveryService(context);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      inRecovery \u003d true;\n    } else {\n      LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n          + recoveryEnabled + \" recoverySupportedByCommitter: \"\n          + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n          + appAttemptID.getAttemptId());\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n   \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n    dispatcher.register(Speculator.EventType.class,\n        speculatorEventDispatcher);\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the staging directory cleaner before the history server but after\n    // the container allocator so the staging directory is cleaned after\n    // the history has been flushed but before unregistering with the RM.\n    addService(createStagingDirCleaningService());\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "cfafd8c29dc3679e503c6155bcbf26f377b0ea8f": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4099 amendment. ApplicationMaster will remove staging directory after the history service is stopped. (Contributed by Jason Lowe)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1324866 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/04/12 10:09 AM",
      "commitName": "cfafd8c29dc3679e503c6155bcbf26f377b0ea8f",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "10/04/12 11:57 AM",
      "commitNameOld": "793746870b704a30fa0595d09da3d176ada75c35",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 0.93,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,106 +1,111 @@\n   public void init(final Configuration conf) {\n \n     conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n \n     downloadTokensAndSetupUGI(conf);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n \n     committer \u003d createOutputCommitter(conf);\n     boolean recoveryEnabled \u003d conf.getBoolean(\n         MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n     boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n     if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n       LOG.info(\"Recovery is enabled. \"\n           + \"Will try to recover from previous life on best effort basis.\");\n       recoveryServ \u003d createRecoveryService(context);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       inRecovery \u003d true;\n     } else {\n       LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n           + recoveryEnabled + \" recoverySupportedByCommitter: \"\n           + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n           + appAttemptID.getAttemptId());\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n     dispatcher.register(Speculator.EventType.class,\n         speculatorEventDispatcher);\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n+    // Add the staging directory cleaner before the history server but after\n+    // the container allocator so the staging directory is cleaned after\n+    // the history has been flushed but before unregistering with the RM.\n+    addService(createStagingDirCleaningService());\n+\n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n\n    committer \u003d createOutputCommitter(conf);\n    boolean recoveryEnabled \u003d conf.getBoolean(\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n    boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n    if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n        \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. \"\n          + \"Will try to recover from previous life on best effort basis.\");\n      recoveryServ \u003d createRecoveryService(context);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      inRecovery \u003d true;\n    } else {\n      LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n          + recoveryEnabled + \" recoverySupportedByCommitter: \"\n          + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n          + appAttemptID.getAttemptId());\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n   \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n    dispatcher.register(Speculator.EventType.class,\n        speculatorEventDispatcher);\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the staging directory cleaner before the history server but after\n    // the container allocator so the staging directory is cleaned after\n    // the history has been flushed but before unregistering with the RM.\n    addService(createStagingDirCleaningService());\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "5ee495e6f34faff231ad87ec890188eb63617393": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3634. Fixed all daemons to crash instead of hanging around when their EventHandlers get exceptions. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1291598 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/02/12 9:08 PM",
      "commitName": "5ee495e6f34faff231ad87ec890188eb63617393",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "13/02/12 4:06 PM",
      "commitNameOld": "0515b3322f9d94f1743504085967b29efe1dd7fe",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 7.21,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,106 @@\n   public void init(final Configuration conf) {\n \n+    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n+\n     downloadTokensAndSetupUGI(conf);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n \n     committer \u003d createOutputCommitter(conf);\n     boolean recoveryEnabled \u003d conf.getBoolean(\n         MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n     boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n     if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n       LOG.info(\"Recovery is enabled. \"\n           + \"Will try to recover from previous life on best effort basis.\");\n       recoveryServ \u003d createRecoveryService(context);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       inRecovery \u003d true;\n     } else {\n       LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n           + recoveryEnabled + \" recoverySupportedByCommitter: \"\n           + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n           + appAttemptID.getAttemptId());\n       dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n     dispatcher.register(Speculator.EventType.class,\n         speculatorEventDispatcher);\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    downloadTokensAndSetupUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n\n    committer \u003d createOutputCommitter(conf);\n    boolean recoveryEnabled \u003d conf.getBoolean(\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n    boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n    if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n        \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. \"\n          + \"Will try to recover from previous life on best effort basis.\");\n      recoveryServ \u003d createRecoveryService(context);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      inRecovery \u003d true;\n    } else {\n      LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n          + recoveryEnabled + \" recoverySupportedByCommitter: \"\n          + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n          + appAttemptID.getAttemptId());\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n   \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n    dispatcher.register(Speculator.EventType.class,\n        speculatorEventDispatcher);\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "fcbad14a3da7fadbb601bf245552ecca2fbc5026": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3568. Optimized Job\u0027s progress calculations in MR AM. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1224995 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/12/11 11:54 AM",
      "commitName": "fcbad14a3da7fadbb601bf245552ecca2fbc5026",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "15/12/11 6:09 PM",
      "commitNameOld": "f73bd5402e49ae6ed712eea70bb3a76314f0a695",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 11.74,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,104 @@\n   public void init(final Configuration conf) {\n \n     downloadTokensAndSetupUGI(conf);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n \n     committer \u003d createOutputCommitter(conf);\n     boolean recoveryEnabled \u003d conf.getBoolean(\n         MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n     boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n     if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n       LOG.info(\"Recovery is enabled. \"\n           + \"Will try to recover from previous life on best effort basis.\");\n       recoveryServ \u003d createRecoveryService(context);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       inRecovery \u003d true;\n     } else {\n       LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n           + recoveryEnabled + \" recoverySupportedByCommitter: \"\n           + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n           + appAttemptID.getAttemptId());\n-      dispatcher \u003d new AsyncDispatcher();\n+      dispatcher \u003d createDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n     \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n     dispatcher.register(Speculator.EventType.class,\n         speculatorEventDispatcher);\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n\n    downloadTokensAndSetupUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n\n    committer \u003d createOutputCommitter(conf);\n    boolean recoveryEnabled \u003d conf.getBoolean(\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n    boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n    if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n        \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. \"\n          + \"Will try to recover from previous life on best effort basis.\");\n      recoveryServ \u003d createRecoveryService(context);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      inRecovery \u003d true;\n    } else {\n      LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n          + recoveryEnabled + \" recoverySupportedByCommitter: \"\n          + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n          + appAttemptID.getAttemptId());\n      dispatcher \u003d createDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n    dispatcher.register(Speculator.EventType.class,\n        speculatorEventDispatcher);\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "b7ae5a6cb7b2d3e3112ac53007e984caeb07de58": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3426. Fixed MR AM in uber mode to write map intermediate outputs in the correct directory to work properly in secure mode. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213987 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:35 PM",
      "commitName": "b7ae5a6cb7b2d3e3112ac53007e984caeb07de58",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "01/12/11 12:35 AM",
      "commitNameOld": "08da8ea5db5359fc04010be486b842a5d2e6b9c2",
      "commitAuthorOld": "Mahadev Konar",
      "daysBetweenCommits": 12.62,
      "commitsBetweenForRepo": 81,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,104 @@\n   public void init(final Configuration conf) {\n \n     downloadTokensAndSetupUGI(conf);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n \n     committer \u003d createOutputCommitter(conf);\n     boolean recoveryEnabled \u003d conf.getBoolean(\n         MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n     boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n     if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n       LOG.info(\"Recovery is enabled. \"\n           + \"Will try to recover from previous life on best effort basis.\");\n       recoveryServ \u003d createRecoveryService(context);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       inRecovery \u003d true;\n     } else {\n       LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n           + recoveryEnabled + \" recoverySupportedByCommitter: \"\n           + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n           + appAttemptID.getAttemptId());\n       dispatcher \u003d new AsyncDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n     \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n+    speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n     dispatcher.register(Speculator.EventType.class,\n-        new SpeculatorEventDispatcher(conf));\n+        speculatorEventDispatcher);\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n\n    downloadTokensAndSetupUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n\n    committer \u003d createOutputCommitter(conf);\n    boolean recoveryEnabled \u003d conf.getBoolean(\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n    boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n    if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n        \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. \"\n          + \"Will try to recover from previous life on best effort basis.\");\n      recoveryServ \u003d createRecoveryService(context);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      inRecovery \u003d true;\n    } else {\n      LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n          + recoveryEnabled + \" recoverySupportedByCommitter: \"\n          + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n          + appAttemptID.getAttemptId());\n      dispatcher \u003d new AsyncDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    speculatorEventDispatcher \u003d new SpeculatorEventDispatcher(conf);\n    dispatcher.register(Speculator.EventType.class,\n        speculatorEventDispatcher);\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "08da8ea5db5359fc04010be486b842a5d2e6b9c2": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3463. Second AM fails to recover properly when first AM is killed with java.lang.IllegalArgumentException causing lost job. (Siddharth Seth via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1208994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/12/11 12:35 AM",
      "commitName": "08da8ea5db5359fc04010be486b842a5d2e6b9c2",
      "commitAuthor": "Mahadev Konar",
      "commitDateOld": "30/10/11 11:42 PM",
      "commitNameOld": "47a381e306877750b5a3ce5d76e0a5ff652ec188",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 31.08,
      "commitsBetweenForRepo": 175,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,103 @@\n   public void init(final Configuration conf) {\n \n     downloadTokensAndSetupUGI(conf);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n      \n     newApiCommitter \u003d false;\n     jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n         appAttemptID.getApplicationId().getId());\n     int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n     if ((numReduceTasks \u003e 0 \u0026\u0026 \n         conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n           (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n            conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n       newApiCommitter \u003d true;\n       LOG.info(\"Using mapred newApiCommitter.\");\n     }\n \n     committer \u003d createOutputCommitter(conf);\n     boolean recoveryEnabled \u003d conf.getBoolean(\n         MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n     boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n     if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n       LOG.info(\"Recovery is enabled. \"\n           + \"Will try to recover from previous life on best effort basis.\");\n-      recoveryServ \u003d new RecoveryService(appAttemptID, clock, \n-          committer);\n+      recoveryServ \u003d createRecoveryService(context);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       inRecovery \u003d true;\n     } else {\n       LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n           + recoveryEnabled + \" recoverySupportedByCommitter: \"\n           + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n           + appAttemptID.getAttemptId());\n       dispatcher \u003d new AsyncDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n     \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     dispatcher.register(Speculator.EventType.class,\n         new SpeculatorEventDispatcher(conf));\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n\n    downloadTokensAndSetupUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n\n    committer \u003d createOutputCommitter(conf);\n    boolean recoveryEnabled \u003d conf.getBoolean(\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n    boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n    if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n        \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. \"\n          + \"Will try to recover from previous life on best effort basis.\");\n      recoveryServ \u003d createRecoveryService(context);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      inRecovery \u003d true;\n    } else {\n      LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n          + recoveryEnabled + \" recoverySupportedByCommitter: \"\n          + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n          + appAttemptID.getAttemptId());\n      dispatcher \u003d new AsyncDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    dispatcher.register(Speculator.EventType.class,\n        new SpeculatorEventDispatcher(conf));\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "408656614495674992349fbda3981559ada3de0b": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2708. Designed and implemented MR Application Master recovery to make MR AMs resume their progress after restart. Contributed by Sharad Agarwal.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1188043 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/10/11 1:41 AM",
      "commitName": "408656614495674992349fbda3981559ada3de0b",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "18/10/11 10:21 PM",
      "commitNameOld": "13e4562924a6cb3d16c262e0f595b2ffbf9e0546",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 5.14,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,104 @@\n   public void init(final Configuration conf) {\n \n     downloadTokensAndSetupUGI(conf);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n-    if (conf.getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false)\n-         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n-      LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n-      recoveryServ \u003d new RecoveryService(appAttemptID, clock);\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n+     \n+    newApiCommitter \u003d false;\n+    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n+        appAttemptID.getApplicationId().getId());\n+    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n+    if ((numReduceTasks \u003e 0 \u0026\u0026 \n+        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n+          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n+           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n+      newApiCommitter \u003d true;\n+      LOG.info(\"Using mapred newApiCommitter.\");\n+    }\n+\n+    committer \u003d createOutputCommitter(conf);\n+    boolean recoveryEnabled \u003d conf.getBoolean(\n+        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n+    boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n+    if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n+        \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n+      LOG.info(\"Recovery is enabled. \"\n+          + \"Will try to recover from previous life on best effort basis.\");\n+      recoveryServ \u003d new RecoveryService(appAttemptID, clock, \n+          committer);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       inRecovery \u003d true;\n     } else {\n+      LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n+          + recoveryEnabled + \" recoverySupportedByCommitter: \"\n+          + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n+          + appAttemptID.getAttemptId());\n       dispatcher \u003d new AsyncDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n     \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     dispatcher.register(Speculator.EventType.class,\n         new SpeculatorEventDispatcher(conf));\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n\n    downloadTokensAndSetupUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\n     \n    newApiCommitter \u003d false;\n    jobId \u003d MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),\n        appAttemptID.getApplicationId().getId());\n    int numReduceTasks \u003d conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n    if ((numReduceTasks \u003e 0 \u0026\u0026 \n        conf.getBoolean(\"mapred.reducer.new-api\", false)) ||\n          (numReduceTasks \u003d\u003d 0 \u0026\u0026 \n           conf.getBoolean(\"mapred.mapper.new-api\", false)))  {\n      newApiCommitter \u003d true;\n      LOG.info(\"Using mapred newApiCommitter.\");\n    }\n\n    committer \u003d createOutputCommitter(conf);\n    boolean recoveryEnabled \u003d conf.getBoolean(\n        MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\n    boolean recoverySupportedByCommitter \u003d committer.isRecoverySupported();\n    if (recoveryEnabled \u0026\u0026 recoverySupportedByCommitter\n        \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. \"\n          + \"Will try to recover from previous life on best effort basis.\");\n      recoveryServ \u003d new RecoveryService(appAttemptID, clock, \n          committer);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      inRecovery \u003d true;\n    } else {\n      LOG.info(\"Not starting RecoveryService: recoveryEnabled: \"\n          + recoveryEnabled + \" recoverySupportedByCommitter: \"\n          + recoverySupportedByCommitter + \" ApplicationAttemptID: \"\n          + appAttemptID.getAttemptId());\n      dispatcher \u003d new AsyncDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    dispatcher.register(Speculator.EventType.class,\n        new SpeculatorEventDispatcher(conf));\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "13e4562924a6cb3d16c262e0f595b2ffbf9e0546": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3144. Augmented JobHistory with the information needed for serving aggregated logs. Contributed by Siddharth Seth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1185976 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/10/11 10:21 PM",
      "commitName": "13e4562924a6cb3d16c262e0f595b2ffbf9e0546",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "18/10/11 3:46 PM",
      "commitNameOld": "b4d20c707af6fe8b9b3e5ffa95a88012a1cdfc17",
      "commitAuthorOld": "Mahadev Konar",
      "daysBetweenCommits": 0.27,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,80 @@\n   public void init(final Configuration conf) {\n \n     downloadTokensAndSetupUGI(conf);\n \n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     if (conf.getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false)\n          \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n       LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n-      Recovery recoveryServ \u003d new RecoveryService(appAttemptID, clock);\n+      recoveryServ \u003d new RecoveryService(appAttemptID, clock);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n-      completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n+      inRecovery \u003d true;\n     } else {\n       dispatcher \u003d new AsyncDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n     \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     dispatcher.register(Speculator.EventType.class,\n         new SpeculatorEventDispatcher(conf));\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n\n    downloadTokensAndSetupUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    if (conf.getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false)\n         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n      recoveryServ \u003d new RecoveryService(appAttemptID, clock);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      inRecovery \u003d true;\n    } else {\n      dispatcher \u003d new AsyncDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    dispatcher.register(Speculator.EventType.class,\n        new SpeculatorEventDispatcher(conf));\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "fa2529c9317b2f27dec16411b99f296904ea095d": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3141. Fix the broken MRAppMaster to work over YARN in security mode.(vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1180007 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/10/11 4:35 AM",
      "commitName": "fa2529c9317b2f27dec16411b99f296904ea095d",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "04/10/11 2:37 AM",
      "commitNameOld": "e979a3ddb17f32582e36cdc9b826f06c80c473f2",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 3.08,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,80 @@\n   public void init(final Configuration conf) {\n+\n+    downloadTokensAndSetupUGI(conf);\n+\n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     if (conf.getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false)\n          \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n       LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n       Recovery recoveryServ \u003d new RecoveryService(appAttemptID, clock);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n     } else {\n       dispatcher \u003d new AsyncDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n     \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     dispatcher.register(Speculator.EventType.class,\n         new SpeculatorEventDispatcher(conf));\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n\n    downloadTokensAndSetupUGI(conf);\n\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    if (conf.getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false)\n         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n      Recovery recoveryServ \u003d new RecoveryService(appAttemptID, clock);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n    } else {\n      dispatcher \u003d new AsyncDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    dispatcher.register(Speculator.EventType.class,\n        new SpeculatorEventDispatcher(conf));\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "c9a7d3dbf902244902b636bf566154c09ecd1116": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3090. Fix MR AM to use ApplicationAttemptId rather than (ApplicationId, startCount) consistently. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1175718 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/09/11 1:44 AM",
      "commitName": "c9a7d3dbf902244902b636bf566154c09ecd1116",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "23/09/11 7:07 AM",
      "commitNameOld": "b549c107825581b15fd14494099a943ff3213c6f",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 2.78,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,77 @@\n   public void init(final Configuration conf) {\n     context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     if (conf.getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false)\n-         \u0026\u0026 startCount \u003e 1) {\n+         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n       LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n-      Recovery recoveryServ \u003d new RecoveryService(appID, clock, startCount);\n+      Recovery recoveryServ \u003d new RecoveryService(appAttemptID, clock);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n     } else {\n       dispatcher \u003d new AsyncDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n \n     this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n     \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     dispatcher.register(Speculator.EventType.class,\n         new SpeculatorEventDispatcher(conf));\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n \n     // Add the JobHistoryEventHandler last so that it is properly stopped first.\n     // This will guarantee that all history-events are flushed before AM goes\n     // ahead with shutdown.\n     // Note: Even though JobHistoryEventHandler is started last, if any\n     // component creates a JobHistoryEvent in the meanwhile, it will be just be\n     // queued inside the JobHistoryEventHandler \n     addIfService(historyService);\n \n     super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    if (conf.getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false)\n         \u0026\u0026 appAttemptID.getAttemptId() \u003e 1) {\n      LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n      Recovery recoveryServ \u003d new RecoveryService(appAttemptID, clock);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n    } else {\n      dispatcher \u003d new AsyncDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    dispatcher.register(Speculator.EventType.class,\n        new SpeculatorEventDispatcher(conf));\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "61900651b1b85cf235e01142acf2a51727fc5537": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3006. Fixed MapReduce AM to exit only after properly writing out history file. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1172206 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/11 12:16 AM",
      "commitName": "61900651b1b85cf235e01142acf2a51727fc5537",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "14/09/11 10:57 AM",
      "commitNameOld": "4ba2acf3363bdfd7fcdd9de496cd57f8af6f03ad",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 3.55,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,137 +1,77 @@\n   public void init(final Configuration conf) {\n-    context \u003d new RunningAppContext();\n+    context \u003d new RunningAppContext(conf);\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     if (conf.getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false)\n          \u0026\u0026 startCount \u003e 1) {\n       LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n       Recovery recoveryServ \u003d new RecoveryService(appID, clock, startCount);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n     } else {\n       dispatcher \u003d new AsyncDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n-    addIfService(historyService);\n+    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n+        historyService);\n \n-    JobEventDispatcher synchronousJobEventDispatcher \u003d new JobEventDispatcher();\n+    this.jobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n-    dispatcher.register(JobEventType.class, synchronousJobEventDispatcher);\n+    dispatcher.register(JobEventType.class, jobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n-    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n-        historyService);\n     \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     dispatcher.register(Speculator.EventType.class,\n-        new SpeculatorEventDispatcher());\n-\n-    Credentials fsTokens \u003d new Credentials();\n-\n-    UserGroupInformation currentUser \u003d null;\n-\n-    try {\n-      currentUser \u003d UserGroupInformation.getCurrentUser();\n-\n-      if (UserGroupInformation.isSecurityEnabled()) {\n-        // Read the file-system tokens from the localized tokens-file.\n-        Path jobSubmitDir \u003d \n-            FileContext.getLocalFSFileContext().makeQualified(\n-                new Path(new File(MRConstants.JOB_SUBMIT_DIR)\n-                    .getAbsolutePath()));\n-        Path jobTokenFile \u003d \n-            new Path(jobSubmitDir, MRConstants.APPLICATION_TOKENS_FILE);\n-        fsTokens.addAll(Credentials.readTokenStorageFile(jobTokenFile, conf));\n-        LOG.info(\"jobSubmitDir\u003d\" + jobSubmitDir + \" jobTokenFile\u003d\"\n-            + jobTokenFile);\n-\n-        for (Token\u003c? extends TokenIdentifier\u003e tk : fsTokens.getAllTokens()) {\n-          LOG.info(\" --- DEBUG: Token of kind \" + tk.getKind()\n-              + \"in current ugi in the AppMaster for service \"\n-              + tk.getService());\n-          currentUser.addToken(tk); // For use by AppMaster itself.\n-        }\n-      }\n-    } catch (IOException e) {\n-      throw new YarnException(e);\n-    }\n-\n-    super.init(conf);\n-\n-    //---- start of what used to be startJobs() code:\n-\n-    Configuration config \u003d getConfig();\n-\n-    job \u003d createJob(config, fsTokens, currentUser.getUserName());\n-\n-    /** create a job event for job intialization */\n-    JobEvent initJobEvent \u003d new JobEvent(job.getID(), JobEventType.JOB_INIT);\n-    /** send init to the job (this does NOT trigger job execution) */\n-    synchronousJobEventDispatcher.handle(initJobEvent);\n-\n-    // send init to speculator. This won\u0027t yest start as dispatcher isn\u0027t\n-    // started yet.\n-    dispatcher.getEventHandler().handle(\n-        new SpeculatorEvent(job.getID(), clock.getTime()));\n-\n-    // JobImpl\u0027s InitTransition is done (call above is synchronous), so the\n-    // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n-    // ubermode if appropriate (by registering different container-allocator\n-    // and container-launcher services/event-handlers).\n-\n-    if (job.isUber()) {\n-      LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n-               + \" in local container (\\\"uber-AM\\\").\");\n-    } else {\n-      LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n-               + \"job \" + job.getID() + \".\");\n-    }\n+        new SpeculatorEventDispatcher(conf));\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n-    containerAllocator \u003d\n-        createContainerAllocator(clientService, context, job.isUber());\n+    containerAllocator \u003d createContainerAllocator(clientService, context);\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n-    if (containerAllocator instanceof Service) {\n-      ((Service) containerAllocator).init(config);\n-    }\n \n     // corresponding service to launch allocated containers via NodeManager\n-    containerLauncher \u003d createContainerLauncher(context, job.isUber());\n+    containerLauncher \u003d createContainerLauncher(context);\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n-    if (containerLauncher instanceof Service) {\n-      ((Service) containerLauncher).init(config);\n-    }\n \n+    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n+    // This will guarantee that all history-events are flushed before AM goes\n+    // ahead with shutdown.\n+    // Note: Even though JobHistoryEventHandler is started last, if any\n+    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n+    // queued inside the JobHistoryEventHandler \n+    addIfService(historyService);\n+\n+    super.init(conf);\n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    context \u003d new RunningAppContext(conf);\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    if (conf.getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false)\n         \u0026\u0026 startCount \u003e 1) {\n      LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n      Recovery recoveryServ \u003d new RecoveryService(appID, clock, startCount);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n    } else {\n      dispatcher \u003d new AsyncDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n\n    this.jobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, jobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    dispatcher.register(Speculator.EventType.class,\n        new SpeculatorEventDispatcher(conf));\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d createContainerAllocator(clientService, context);\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context);\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n\n    // Add the JobHistoryEventHandler last so that it is properly stopped first.\n    // This will guarantee that all history-events are flushed before AM goes\n    // ahead with shutdown.\n    // Note: Even though JobHistoryEventHandler is started last, if any\n    // component creates a JobHistoryEvent in the meanwhile, it will be just be\n    // queued inside the JobHistoryEventHandler \n    addIfService(historyService);\n\n    super.init(conf);\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "fafe8cd28e726566509c679e19d7da622f29f90d": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2864. Normalize configuration variable names for YARN. Contributed by Robert Evans.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1166955 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/09/11 6:44 PM",
      "commitName": "fafe8cd28e726566509c679e19d7da622f29f90d",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 15.06,
      "commitsBetweenForRepo": 86,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,137 +1,137 @@\n   public void init(final Configuration conf) {\n     context \u003d new RunningAppContext();\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n-    if (conf.getBoolean(AMConstants.RECOVERY_ENABLE, false)\n+    if (conf.getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false)\n          \u0026\u0026 startCount \u003e 1) {\n       LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n       Recovery recoveryServ \u003d new RecoveryService(appID, clock, startCount);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n     } else {\n       dispatcher \u003d new AsyncDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     addIfService(historyService);\n \n     JobEventDispatcher synchronousJobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, synchronousJobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n     \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     dispatcher.register(Speculator.EventType.class,\n         new SpeculatorEventDispatcher());\n \n     Credentials fsTokens \u003d new Credentials();\n \n     UserGroupInformation currentUser \u003d null;\n \n     try {\n       currentUser \u003d UserGroupInformation.getCurrentUser();\n \n       if (UserGroupInformation.isSecurityEnabled()) {\n         // Read the file-system tokens from the localized tokens-file.\n         Path jobSubmitDir \u003d \n             FileContext.getLocalFSFileContext().makeQualified(\n                 new Path(new File(MRConstants.JOB_SUBMIT_DIR)\n                     .getAbsolutePath()));\n         Path jobTokenFile \u003d \n             new Path(jobSubmitDir, MRConstants.APPLICATION_TOKENS_FILE);\n         fsTokens.addAll(Credentials.readTokenStorageFile(jobTokenFile, conf));\n         LOG.info(\"jobSubmitDir\u003d\" + jobSubmitDir + \" jobTokenFile\u003d\"\n             + jobTokenFile);\n \n         for (Token\u003c? extends TokenIdentifier\u003e tk : fsTokens.getAllTokens()) {\n           LOG.info(\" --- DEBUG: Token of kind \" + tk.getKind()\n               + \"in current ugi in the AppMaster for service \"\n               + tk.getService());\n           currentUser.addToken(tk); // For use by AppMaster itself.\n         }\n       }\n     } catch (IOException e) {\n       throw new YarnException(e);\n     }\n \n     super.init(conf);\n \n     //---- start of what used to be startJobs() code:\n \n     Configuration config \u003d getConfig();\n \n     job \u003d createJob(config, fsTokens, currentUser.getUserName());\n \n     /** create a job event for job intialization */\n     JobEvent initJobEvent \u003d new JobEvent(job.getID(), JobEventType.JOB_INIT);\n     /** send init to the job (this does NOT trigger job execution) */\n     synchronousJobEventDispatcher.handle(initJobEvent);\n \n     // send init to speculator. This won\u0027t yest start as dispatcher isn\u0027t\n     // started yet.\n     dispatcher.getEventHandler().handle(\n         new SpeculatorEvent(job.getID(), clock.getTime()));\n \n     // JobImpl\u0027s InitTransition is done (call above is synchronous), so the\n     // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n     // ubermode if appropriate (by registering different container-allocator\n     // and container-launcher services/event-handlers).\n \n     if (job.isUber()) {\n       LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n                + \" in local container (\\\"uber-AM\\\").\");\n     } else {\n       LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n                + \"job \" + job.getID() + \".\");\n     }\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d\n         createContainerAllocator(clientService, context, job.isUber());\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n     if (containerAllocator instanceof Service) {\n       ((Service) containerAllocator).init(config);\n     }\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context, job.isUber());\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n     if (containerLauncher instanceof Service) {\n       ((Service) containerLauncher).init(config);\n     }\n \n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    context \u003d new RunningAppContext();\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    if (conf.getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false)\n         \u0026\u0026 startCount \u003e 1) {\n      LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n      Recovery recoveryServ \u003d new RecoveryService(appID, clock, startCount);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n    } else {\n      dispatcher \u003d new AsyncDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    addIfService(historyService);\n\n    JobEventDispatcher synchronousJobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, synchronousJobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    dispatcher.register(Speculator.EventType.class,\n        new SpeculatorEventDispatcher());\n\n    Credentials fsTokens \u003d new Credentials();\n\n    UserGroupInformation currentUser \u003d null;\n\n    try {\n      currentUser \u003d UserGroupInformation.getCurrentUser();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Read the file-system tokens from the localized tokens-file.\n        Path jobSubmitDir \u003d \n            FileContext.getLocalFSFileContext().makeQualified(\n                new Path(new File(MRConstants.JOB_SUBMIT_DIR)\n                    .getAbsolutePath()));\n        Path jobTokenFile \u003d \n            new Path(jobSubmitDir, MRConstants.APPLICATION_TOKENS_FILE);\n        fsTokens.addAll(Credentials.readTokenStorageFile(jobTokenFile, conf));\n        LOG.info(\"jobSubmitDir\u003d\" + jobSubmitDir + \" jobTokenFile\u003d\"\n            + jobTokenFile);\n\n        for (Token\u003c? extends TokenIdentifier\u003e tk : fsTokens.getAllTokens()) {\n          LOG.info(\" --- DEBUG: Token of kind \" + tk.getKind()\n              + \"in current ugi in the AppMaster for service \"\n              + tk.getService());\n          currentUser.addToken(tk); // For use by AppMaster itself.\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    super.init(conf);\n\n    //---- start of what used to be startJobs() code:\n\n    Configuration config \u003d getConfig();\n\n    job \u003d createJob(config, fsTokens, currentUser.getUserName());\n\n    /** create a job event for job intialization */\n    JobEvent initJobEvent \u003d new JobEvent(job.getID(), JobEventType.JOB_INIT);\n    /** send init to the job (this does NOT trigger job execution) */\n    synchronousJobEventDispatcher.handle(initJobEvent);\n\n    // send init to speculator. This won\u0027t yest start as dispatcher isn\u0027t\n    // started yet.\n    dispatcher.getEventHandler().handle(\n        new SpeculatorEvent(job.getID(), clock.getTime()));\n\n    // JobImpl\u0027s InitTransition is done (call above is synchronous), so the\n    // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n    // ubermode if appropriate (by registering different container-allocator\n    // and container-launcher services/event-handlers).\n\n    if (job.isUber()) {\n      LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n               + \" in local container (\\\"uber-AM\\\").\");\n    } else {\n      LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n               + \"job \" + job.getID() + \".\");\n    }\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d\n        createContainerAllocator(clientService, context, job.isUber());\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n    if (containerAllocator instanceof Service) {\n      ((Service) containerAllocator).init(config);\n    }\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context, job.isUber());\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n    if (containerLauncher instanceof Service) {\n      ((Service) containerLauncher).init(config);\n    }\n\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void init(final Configuration conf) {\n    context \u003d new RunningAppContext();\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    if (conf.getBoolean(AMConstants.RECOVERY_ENABLE, false)\n         \u0026\u0026 startCount \u003e 1) {\n      LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n      Recovery recoveryServ \u003d new RecoveryService(appID, clock, startCount);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n    } else {\n      dispatcher \u003d new AsyncDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    addIfService(historyService);\n\n    JobEventDispatcher synchronousJobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, synchronousJobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    dispatcher.register(Speculator.EventType.class,\n        new SpeculatorEventDispatcher());\n\n    Credentials fsTokens \u003d new Credentials();\n\n    UserGroupInformation currentUser \u003d null;\n\n    try {\n      currentUser \u003d UserGroupInformation.getCurrentUser();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Read the file-system tokens from the localized tokens-file.\n        Path jobSubmitDir \u003d \n            FileContext.getLocalFSFileContext().makeQualified(\n                new Path(new File(MRConstants.JOB_SUBMIT_DIR)\n                    .getAbsolutePath()));\n        Path jobTokenFile \u003d \n            new Path(jobSubmitDir, MRConstants.APPLICATION_TOKENS_FILE);\n        fsTokens.addAll(Credentials.readTokenStorageFile(jobTokenFile, conf));\n        LOG.info(\"jobSubmitDir\u003d\" + jobSubmitDir + \" jobTokenFile\u003d\"\n            + jobTokenFile);\n\n        for (Token\u003c? extends TokenIdentifier\u003e tk : fsTokens.getAllTokens()) {\n          LOG.info(\" --- DEBUG: Token of kind \" + tk.getKind()\n              + \"in current ugi in the AppMaster for service \"\n              + tk.getService());\n          currentUser.addToken(tk); // For use by AppMaster itself.\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    super.init(conf);\n\n    //---- start of what used to be startJobs() code:\n\n    Configuration config \u003d getConfig();\n\n    job \u003d createJob(config, fsTokens, currentUser.getUserName());\n\n    /** create a job event for job intialization */\n    JobEvent initJobEvent \u003d new JobEvent(job.getID(), JobEventType.JOB_INIT);\n    /** send init to the job (this does NOT trigger job execution) */\n    synchronousJobEventDispatcher.handle(initJobEvent);\n\n    // send init to speculator. This won\u0027t yest start as dispatcher isn\u0027t\n    // started yet.\n    dispatcher.getEventHandler().handle(\n        new SpeculatorEvent(job.getID(), clock.getTime()));\n\n    // JobImpl\u0027s InitTransition is done (call above is synchronous), so the\n    // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n    // ubermode if appropriate (by registering different container-allocator\n    // and container-launcher services/event-handlers).\n\n    if (job.isUber()) {\n      LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n               + \" in local container (\\\"uber-AM\\\").\");\n    } else {\n      LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n               + \"job \" + job.getID() + \".\");\n    }\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d\n        createContainerAllocator(clientService, context, job.isUber());\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n    if (containerAllocator instanceof Service) {\n      ((Service) containerAllocator).init(config);\n    }\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context, job.isUber());\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n    if (containerLauncher instanceof Service) {\n      ((Service) containerLauncher).init(config);\n    }\n\n  } // end of init()",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java"
      }
    },
    "7c8fcbecf14b2e24d54ccb276bb684fdbe62b669": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2701. app/Job.java needs UGI for the user that launched it. (Robert Evans via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1160392 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/08/11 12:36 PM",
      "commitName": "7c8fcbecf14b2e24d54ccb276bb684fdbe62b669",
      "commitAuthor": "Mahadev Konar",
      "commitDateOld": "18/08/11 4:07 AM",
      "commitNameOld": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 4.35,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,134 +1,137 @@\n   public void init(final Configuration conf) {\n     context \u003d new RunningAppContext();\n \n     // Job name is the same as the app name util we support DAG of jobs\n     // for an app later\n     appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n \n     if (conf.getBoolean(AMConstants.RECOVERY_ENABLE, false)\n          \u0026\u0026 startCount \u003e 1) {\n       LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n       Recovery recoveryServ \u003d new RecoveryService(appID, clock, startCount);\n       addIfService(recoveryServ);\n       dispatcher \u003d recoveryServ.getDispatcher();\n       clock \u003d recoveryServ.getClock();\n       completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n     } else {\n       dispatcher \u003d new AsyncDispatcher();\n       addIfService(dispatcher);\n     }\n \n     //service to handle requests to TaskUmbilicalProtocol\n     taskAttemptListener \u003d createTaskAttemptListener(context);\n     addIfService(taskAttemptListener);\n \n     //service to do the task cleanup\n     taskCleaner \u003d createTaskCleaner(context);\n     addIfService(taskCleaner);\n \n     //service to handle requests from JobClient\n     clientService \u003d createClientService(context);\n     addIfService(clientService);\n \n     //service to log job history events\n     EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n         createJobHistoryHandler(context);\n     addIfService(historyService);\n \n     JobEventDispatcher synchronousJobEventDispatcher \u003d new JobEventDispatcher();\n \n     //register the event dispatchers\n     dispatcher.register(JobEventType.class, synchronousJobEventDispatcher);\n     dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n     dispatcher.register(TaskAttemptEventType.class, \n         new TaskAttemptEventDispatcher());\n     dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n     dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n         historyService);\n     \n     if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n         || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n       //optional service to speculate on task attempts\u0027 progress\n       speculator \u003d createSpeculator(conf, context);\n       addIfService(speculator);\n     }\n \n     dispatcher.register(Speculator.EventType.class,\n         new SpeculatorEventDispatcher());\n \n     Credentials fsTokens \u003d new Credentials();\n-    if (UserGroupInformation.isSecurityEnabled()) {\n-      // Read the file-system tokens from the localized tokens-file.\n-      try {\n-        Path jobSubmitDir \u003d\n+\n+    UserGroupInformation currentUser \u003d null;\n+\n+    try {\n+      currentUser \u003d UserGroupInformation.getCurrentUser();\n+\n+      if (UserGroupInformation.isSecurityEnabled()) {\n+        // Read the file-system tokens from the localized tokens-file.\n+        Path jobSubmitDir \u003d \n             FileContext.getLocalFSFileContext().makeQualified(\n                 new Path(new File(MRConstants.JOB_SUBMIT_DIR)\n                     .getAbsolutePath()));\n-        Path jobTokenFile \u003d\n+        Path jobTokenFile \u003d \n             new Path(jobSubmitDir, MRConstants.APPLICATION_TOKENS_FILE);\n         fsTokens.addAll(Credentials.readTokenStorageFile(jobTokenFile, conf));\n         LOG.info(\"jobSubmitDir\u003d\" + jobSubmitDir + \" jobTokenFile\u003d\"\n             + jobTokenFile);\n \n-        UserGroupInformation currentUser \u003d\n-            UserGroupInformation.getCurrentUser();\n         for (Token\u003c? extends TokenIdentifier\u003e tk : fsTokens.getAllTokens()) {\n           LOG.info(\" --- DEBUG: Token of kind \" + tk.getKind()\n               + \"in current ugi in the AppMaster for service \"\n               + tk.getService());\n           currentUser.addToken(tk); // For use by AppMaster itself.\n         }\n-      } catch (IOException e) {\n-        throw new YarnException(e);\n       }\n+    } catch (IOException e) {\n+      throw new YarnException(e);\n     }\n \n     super.init(conf);\n \n     //---- start of what used to be startJobs() code:\n \n     Configuration config \u003d getConfig();\n \n-    job \u003d createJob(config, fsTokens);\n+    job \u003d createJob(config, fsTokens, currentUser.getUserName());\n \n     /** create a job event for job intialization */\n     JobEvent initJobEvent \u003d new JobEvent(job.getID(), JobEventType.JOB_INIT);\n     /** send init to the job (this does NOT trigger job execution) */\n     synchronousJobEventDispatcher.handle(initJobEvent);\n \n     // send init to speculator. This won\u0027t yest start as dispatcher isn\u0027t\n     // started yet.\n     dispatcher.getEventHandler().handle(\n         new SpeculatorEvent(job.getID(), clock.getTime()));\n \n     // JobImpl\u0027s InitTransition is done (call above is synchronous), so the\n     // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n     // ubermode if appropriate (by registering different container-allocator\n     // and container-launcher services/event-handlers).\n \n     if (job.isUber()) {\n       LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n                + \" in local container (\\\"uber-AM\\\").\");\n     } else {\n       LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n                + \"job \" + job.getID() + \".\");\n     }\n \n     // service to allocate containers from RM (if non-uber) or to fake it (uber)\n     containerAllocator \u003d\n         createContainerAllocator(clientService, context, job.isUber());\n     addIfService(containerAllocator);\n     dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n     if (containerAllocator instanceof Service) {\n       ((Service) containerAllocator).init(config);\n     }\n \n     // corresponding service to launch allocated containers via NodeManager\n     containerLauncher \u003d createContainerLauncher(context, job.isUber());\n     addIfService(containerLauncher);\n     dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n     if (containerLauncher instanceof Service) {\n       ((Service) containerLauncher).init(config);\n     }\n \n   } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    context \u003d new RunningAppContext();\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    if (conf.getBoolean(AMConstants.RECOVERY_ENABLE, false)\n         \u0026\u0026 startCount \u003e 1) {\n      LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n      Recovery recoveryServ \u003d new RecoveryService(appID, clock, startCount);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n    } else {\n      dispatcher \u003d new AsyncDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    addIfService(historyService);\n\n    JobEventDispatcher synchronousJobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, synchronousJobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    dispatcher.register(Speculator.EventType.class,\n        new SpeculatorEventDispatcher());\n\n    Credentials fsTokens \u003d new Credentials();\n\n    UserGroupInformation currentUser \u003d null;\n\n    try {\n      currentUser \u003d UserGroupInformation.getCurrentUser();\n\n      if (UserGroupInformation.isSecurityEnabled()) {\n        // Read the file-system tokens from the localized tokens-file.\n        Path jobSubmitDir \u003d \n            FileContext.getLocalFSFileContext().makeQualified(\n                new Path(new File(MRConstants.JOB_SUBMIT_DIR)\n                    .getAbsolutePath()));\n        Path jobTokenFile \u003d \n            new Path(jobSubmitDir, MRConstants.APPLICATION_TOKENS_FILE);\n        fsTokens.addAll(Credentials.readTokenStorageFile(jobTokenFile, conf));\n        LOG.info(\"jobSubmitDir\u003d\" + jobSubmitDir + \" jobTokenFile\u003d\"\n            + jobTokenFile);\n\n        for (Token\u003c? extends TokenIdentifier\u003e tk : fsTokens.getAllTokens()) {\n          LOG.info(\" --- DEBUG: Token of kind \" + tk.getKind()\n              + \"in current ugi in the AppMaster for service \"\n              + tk.getService());\n          currentUser.addToken(tk); // For use by AppMaster itself.\n        }\n      }\n    } catch (IOException e) {\n      throw new YarnException(e);\n    }\n\n    super.init(conf);\n\n    //---- start of what used to be startJobs() code:\n\n    Configuration config \u003d getConfig();\n\n    job \u003d createJob(config, fsTokens, currentUser.getUserName());\n\n    /** create a job event for job intialization */\n    JobEvent initJobEvent \u003d new JobEvent(job.getID(), JobEventType.JOB_INIT);\n    /** send init to the job (this does NOT trigger job execution) */\n    synchronousJobEventDispatcher.handle(initJobEvent);\n\n    // send init to speculator. This won\u0027t yest start as dispatcher isn\u0027t\n    // started yet.\n    dispatcher.getEventHandler().handle(\n        new SpeculatorEvent(job.getID(), clock.getTime()));\n\n    // JobImpl\u0027s InitTransition is done (call above is synchronous), so the\n    // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n    // ubermode if appropriate (by registering different container-allocator\n    // and container-launcher services/event-handlers).\n\n    if (job.isUber()) {\n      LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n               + \" in local container (\\\"uber-AM\\\").\");\n    } else {\n      LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n               + \"job \" + job.getID() + \".\");\n    }\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d\n        createContainerAllocator(clientService, context, job.isUber());\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n    if (containerAllocator instanceof Service) {\n      ((Service) containerAllocator).init(config);\n    }\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context, job.isUber());\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n    if (containerLauncher instanceof Service) {\n      ((Service) containerLauncher).init(config);\n    }\n\n  } // end of init()",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
      "extendedDetails": {}
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,134 @@\n+  public void init(final Configuration conf) {\n+    context \u003d new RunningAppContext();\n+\n+    // Job name is the same as the app name util we support DAG of jobs\n+    // for an app later\n+    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n+\n+    if (conf.getBoolean(AMConstants.RECOVERY_ENABLE, false)\n+         \u0026\u0026 startCount \u003e 1) {\n+      LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n+      Recovery recoveryServ \u003d new RecoveryService(appID, clock, startCount);\n+      addIfService(recoveryServ);\n+      dispatcher \u003d recoveryServ.getDispatcher();\n+      clock \u003d recoveryServ.getClock();\n+      completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n+    } else {\n+      dispatcher \u003d new AsyncDispatcher();\n+      addIfService(dispatcher);\n+    }\n+\n+    //service to handle requests to TaskUmbilicalProtocol\n+    taskAttemptListener \u003d createTaskAttemptListener(context);\n+    addIfService(taskAttemptListener);\n+\n+    //service to do the task cleanup\n+    taskCleaner \u003d createTaskCleaner(context);\n+    addIfService(taskCleaner);\n+\n+    //service to handle requests from JobClient\n+    clientService \u003d createClientService(context);\n+    addIfService(clientService);\n+\n+    //service to log job history events\n+    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n+        createJobHistoryHandler(context);\n+    addIfService(historyService);\n+\n+    JobEventDispatcher synchronousJobEventDispatcher \u003d new JobEventDispatcher();\n+\n+    //register the event dispatchers\n+    dispatcher.register(JobEventType.class, synchronousJobEventDispatcher);\n+    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n+    dispatcher.register(TaskAttemptEventType.class, \n+        new TaskAttemptEventDispatcher());\n+    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n+    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n+        historyService);\n+    \n+    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n+        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n+      //optional service to speculate on task attempts\u0027 progress\n+      speculator \u003d createSpeculator(conf, context);\n+      addIfService(speculator);\n+    }\n+\n+    dispatcher.register(Speculator.EventType.class,\n+        new SpeculatorEventDispatcher());\n+\n+    Credentials fsTokens \u003d new Credentials();\n+    if (UserGroupInformation.isSecurityEnabled()) {\n+      // Read the file-system tokens from the localized tokens-file.\n+      try {\n+        Path jobSubmitDir \u003d\n+            FileContext.getLocalFSFileContext().makeQualified(\n+                new Path(new File(MRConstants.JOB_SUBMIT_DIR)\n+                    .getAbsolutePath()));\n+        Path jobTokenFile \u003d\n+            new Path(jobSubmitDir, MRConstants.APPLICATION_TOKENS_FILE);\n+        fsTokens.addAll(Credentials.readTokenStorageFile(jobTokenFile, conf));\n+        LOG.info(\"jobSubmitDir\u003d\" + jobSubmitDir + \" jobTokenFile\u003d\"\n+            + jobTokenFile);\n+\n+        UserGroupInformation currentUser \u003d\n+            UserGroupInformation.getCurrentUser();\n+        for (Token\u003c? extends TokenIdentifier\u003e tk : fsTokens.getAllTokens()) {\n+          LOG.info(\" --- DEBUG: Token of kind \" + tk.getKind()\n+              + \"in current ugi in the AppMaster for service \"\n+              + tk.getService());\n+          currentUser.addToken(tk); // For use by AppMaster itself.\n+        }\n+      } catch (IOException e) {\n+        throw new YarnException(e);\n+      }\n+    }\n+\n+    super.init(conf);\n+\n+    //---- start of what used to be startJobs() code:\n+\n+    Configuration config \u003d getConfig();\n+\n+    job \u003d createJob(config, fsTokens);\n+\n+    /** create a job event for job intialization */\n+    JobEvent initJobEvent \u003d new JobEvent(job.getID(), JobEventType.JOB_INIT);\n+    /** send init to the job (this does NOT trigger job execution) */\n+    synchronousJobEventDispatcher.handle(initJobEvent);\n+\n+    // send init to speculator. This won\u0027t yest start as dispatcher isn\u0027t\n+    // started yet.\n+    dispatcher.getEventHandler().handle(\n+        new SpeculatorEvent(job.getID(), clock.getTime()));\n+\n+    // JobImpl\u0027s InitTransition is done (call above is synchronous), so the\n+    // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n+    // ubermode if appropriate (by registering different container-allocator\n+    // and container-launcher services/event-handlers).\n+\n+    if (job.isUber()) {\n+      LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n+               + \" in local container (\\\"uber-AM\\\").\");\n+    } else {\n+      LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n+               + \"job \" + job.getID() + \".\");\n+    }\n+\n+    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n+    containerAllocator \u003d\n+        createContainerAllocator(clientService, context, job.isUber());\n+    addIfService(containerAllocator);\n+    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n+    if (containerAllocator instanceof Service) {\n+      ((Service) containerAllocator).init(config);\n+    }\n+\n+    // corresponding service to launch allocated containers via NodeManager\n+    containerLauncher \u003d createContainerLauncher(context, job.isUber());\n+    addIfService(containerLauncher);\n+    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n+    if (containerLauncher instanceof Service) {\n+      ((Service) containerLauncher).init(config);\n+    }\n+\n+  } // end of init()\n\\ No newline at end of file\n",
      "actualSource": "  public void init(final Configuration conf) {\n    context \u003d new RunningAppContext();\n\n    // Job name is the same as the app name util we support DAG of jobs\n    // for an app later\n    appName \u003d conf.get(MRJobConfig.JOB_NAME, \"\u003cmissing app name\u003e\");\n\n    if (conf.getBoolean(AMConstants.RECOVERY_ENABLE, false)\n         \u0026\u0026 startCount \u003e 1) {\n      LOG.info(\"Recovery is enabled. Will try to recover from previous life.\");\n      Recovery recoveryServ \u003d new RecoveryService(appID, clock, startCount);\n      addIfService(recoveryServ);\n      dispatcher \u003d recoveryServ.getDispatcher();\n      clock \u003d recoveryServ.getClock();\n      completedTasksFromPreviousRun \u003d recoveryServ.getCompletedTasks();\n    } else {\n      dispatcher \u003d new AsyncDispatcher();\n      addIfService(dispatcher);\n    }\n\n    //service to handle requests to TaskUmbilicalProtocol\n    taskAttemptListener \u003d createTaskAttemptListener(context);\n    addIfService(taskAttemptListener);\n\n    //service to do the task cleanup\n    taskCleaner \u003d createTaskCleaner(context);\n    addIfService(taskCleaner);\n\n    //service to handle requests from JobClient\n    clientService \u003d createClientService(context);\n    addIfService(clientService);\n\n    //service to log job history events\n    EventHandler\u003cJobHistoryEvent\u003e historyService \u003d \n        createJobHistoryHandler(context);\n    addIfService(historyService);\n\n    JobEventDispatcher synchronousJobEventDispatcher \u003d new JobEventDispatcher();\n\n    //register the event dispatchers\n    dispatcher.register(JobEventType.class, synchronousJobEventDispatcher);\n    dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\n    dispatcher.register(TaskAttemptEventType.class, \n        new TaskAttemptEventDispatcher());\n    dispatcher.register(TaskCleaner.EventType.class, taskCleaner);\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,\n        historyService);\n    \n    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)\n        || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\n      //optional service to speculate on task attempts\u0027 progress\n      speculator \u003d createSpeculator(conf, context);\n      addIfService(speculator);\n    }\n\n    dispatcher.register(Speculator.EventType.class,\n        new SpeculatorEventDispatcher());\n\n    Credentials fsTokens \u003d new Credentials();\n    if (UserGroupInformation.isSecurityEnabled()) {\n      // Read the file-system tokens from the localized tokens-file.\n      try {\n        Path jobSubmitDir \u003d\n            FileContext.getLocalFSFileContext().makeQualified(\n                new Path(new File(MRConstants.JOB_SUBMIT_DIR)\n                    .getAbsolutePath()));\n        Path jobTokenFile \u003d\n            new Path(jobSubmitDir, MRConstants.APPLICATION_TOKENS_FILE);\n        fsTokens.addAll(Credentials.readTokenStorageFile(jobTokenFile, conf));\n        LOG.info(\"jobSubmitDir\u003d\" + jobSubmitDir + \" jobTokenFile\u003d\"\n            + jobTokenFile);\n\n        UserGroupInformation currentUser \u003d\n            UserGroupInformation.getCurrentUser();\n        for (Token\u003c? extends TokenIdentifier\u003e tk : fsTokens.getAllTokens()) {\n          LOG.info(\" --- DEBUG: Token of kind \" + tk.getKind()\n              + \"in current ugi in the AppMaster for service \"\n              + tk.getService());\n          currentUser.addToken(tk); // For use by AppMaster itself.\n        }\n      } catch (IOException e) {\n        throw new YarnException(e);\n      }\n    }\n\n    super.init(conf);\n\n    //---- start of what used to be startJobs() code:\n\n    Configuration config \u003d getConfig();\n\n    job \u003d createJob(config, fsTokens);\n\n    /** create a job event for job intialization */\n    JobEvent initJobEvent \u003d new JobEvent(job.getID(), JobEventType.JOB_INIT);\n    /** send init to the job (this does NOT trigger job execution) */\n    synchronousJobEventDispatcher.handle(initJobEvent);\n\n    // send init to speculator. This won\u0027t yest start as dispatcher isn\u0027t\n    // started yet.\n    dispatcher.getEventHandler().handle(\n        new SpeculatorEvent(job.getID(), clock.getTime()));\n\n    // JobImpl\u0027s InitTransition is done (call above is synchronous), so the\n    // \"uber-decision\" (MR-1220) has been made.  Query job and switch to\n    // ubermode if appropriate (by registering different container-allocator\n    // and container-launcher services/event-handlers).\n\n    if (job.isUber()) {\n      LOG.info(\"MRAppMaster uberizing job \" + job.getID()\n               + \" in local container (\\\"uber-AM\\\").\");\n    } else {\n      LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \"\n               + \"job \" + job.getID() + \".\");\n    }\n\n    // service to allocate containers from RM (if non-uber) or to fake it (uber)\n    containerAllocator \u003d\n        createContainerAllocator(clientService, context, job.isUber());\n    addIfService(containerAllocator);\n    dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\n    if (containerAllocator instanceof Service) {\n      ((Service) containerAllocator).init(config);\n    }\n\n    // corresponding service to launch allocated containers via NodeManager\n    containerLauncher \u003d createContainerLauncher(context, job.isUber());\n    addIfService(containerLauncher);\n    dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\n    if (containerLauncher instanceof Service) {\n      ((Service) containerLauncher).init(config);\n    }\n\n  } // end of init()",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java"
    }
  }
}