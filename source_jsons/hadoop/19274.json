{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ReduceTask.java",
  "functionName": "runOldReducer",
  "functionId": "runOldReducer___job-JobConf__umbilical-TaskUmbilicalProtocol__reporter-TaskReporter(modifiers-final)__rIter-RawKeyValueIterator__comparator-RawComparator__INKEY____keyClass-Class__INKEY____valueClass-Class__INVALUE__",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
  "functionStartLine": 403,
  "functionEndLine": 463,
  "numCommitsSeen": 16,
  "timeTaken": 9655,
  "changeHistory": [
    "178751ed8c9d47038acf8616c226f1f52e884feb",
    "c39012f4a0444f9e4b7d67957d5192127d143d90",
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "4796e1adcb912005198c9003305c97cf3a8b523e",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "178751ed8c9d47038acf8616c226f1f52e884feb": "Ybodychange",
    "c39012f4a0444f9e4b7d67957d5192127d143d90": "Ybodychange",
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymovefromfile",
    "4796e1adcb912005198c9003305c97cf3a8b523e": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "178751ed8c9d47038acf8616c226f1f52e884feb": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6983. Moving logging APIs over to slf4j in hadoop-mapreduce-client-core. Contributed by Jinjiang Ling.\n",
      "commitDate": "02/11/17 1:43 AM",
      "commitName": "178751ed8c9d47038acf8616c226f1f52e884feb",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "08/05/15 1:48 PM",
      "commitNameOld": "c39012f4a0444f9e4b7d67957d5192127d143d90",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 908.5,
      "commitsBetweenForRepo": 6292,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   void runOldReducer(JobConf job,\n                      TaskUmbilicalProtocol umbilical,\n                      final TaskReporter reporter,\n                      RawKeyValueIterator rIter,\n                      RawComparator\u003cINKEY\u003e comparator,\n                      Class\u003cINKEY\u003e keyClass,\n                      Class\u003cINVALUE\u003e valueClass) throws IOException {\n     Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n       ReflectionUtils.newInstance(job.getReducerClass(), job);\n     // make output collector\n     String finalName \u003d getOutputName(getPartition());\n \n     RecordWriter\u003cOUTKEY, OUTVALUE\u003e out \u003d new OldTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(\n         this, job, reporter, finalName);\n     final RecordWriter\u003cOUTKEY, OUTVALUE\u003e finalOut \u003d out;\n     \n     OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n       new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n         public void collect(OUTKEY key, OUTVALUE value)\n           throws IOException {\n           finalOut.write(key, value);\n           // indicate that progress update needs to be sent\n           reporter.progress();\n         }\n       };\n     \n     // apply reduce function\n     try {\n       //increment processed counter only if skipping feature is enabled\n       boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n         SkipBadRecords.getAutoIncrReducerProcCount(job);\n       \n       ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n           new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n               comparator, keyClass, valueClass, \n               job, reporter, umbilical) :\n           new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n           comparator, keyClass, valueClass,\n           job, reporter);\n       values.informReduceProgress();\n       while (values.more()) {\n         reduceInputKeyCounter.increment(1);\n         reducer.reduce(values.getKey(), values, collector, reporter);\n         if(incrProcCount) {\n           reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n               SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n         }\n         values.nextKey();\n         values.informReduceProgress();\n       }\n \n       reducer.close();\n       reducer \u003d null;\n       \n       out.close(reporter);\n       out \u003d null;\n     } finally {\n-      IOUtils.cleanup(LOG, reducer);\n+      IOUtils.cleanupWithLogger(LOG, reducer);\n       closeQuietly(out, reporter);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass) throws IOException {\n    Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName \u003d getOutputName(getPartition());\n\n    RecordWriter\u003cOUTKEY, OUTVALUE\u003e out \u003d new OldTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(\n        this, job, reporter, finalName);\n    final RecordWriter\u003cOUTKEY, OUTVALUE\u003e finalOut \u003d out;\n    \n    OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n      new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          finalOut.write(key, value);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    \n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      \n      ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n          new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n          comparator, keyClass, valueClass,\n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n\n      reducer.close();\n      reducer \u003d null;\n      \n      out.close(reporter);\n      out \u003d null;\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, reducer);\n      closeQuietly(out, reporter);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
      "extendedDetails": {}
    },
    "c39012f4a0444f9e4b7d67957d5192127d143d90": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3383. Duplicate job.getOutputValueGroupingComparator() in ReduceTask. Contributed by Binglin Chang\n",
      "commitDate": "08/05/15 1:48 PM",
      "commitName": "c39012f4a0444f9e4b7d67957d5192127d143d90",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "05/08/13 11:36 PM",
      "commitNameOld": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 640.59,
      "commitsBetweenForRepo": 4961,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   void runOldReducer(JobConf job,\n                      TaskUmbilicalProtocol umbilical,\n                      final TaskReporter reporter,\n                      RawKeyValueIterator rIter,\n                      RawComparator\u003cINKEY\u003e comparator,\n                      Class\u003cINKEY\u003e keyClass,\n                      Class\u003cINVALUE\u003e valueClass) throws IOException {\n     Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n       ReflectionUtils.newInstance(job.getReducerClass(), job);\n     // make output collector\n     String finalName \u003d getOutputName(getPartition());\n \n     RecordWriter\u003cOUTKEY, OUTVALUE\u003e out \u003d new OldTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(\n         this, job, reporter, finalName);\n     final RecordWriter\u003cOUTKEY, OUTVALUE\u003e finalOut \u003d out;\n     \n     OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n       new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n         public void collect(OUTKEY key, OUTVALUE value)\n           throws IOException {\n           finalOut.write(key, value);\n           // indicate that progress update needs to be sent\n           reporter.progress();\n         }\n       };\n     \n     // apply reduce function\n     try {\n       //increment processed counter only if skipping feature is enabled\n       boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n         SkipBadRecords.getAutoIncrReducerProcCount(job);\n       \n       ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n           new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n               comparator, keyClass, valueClass, \n               job, reporter, umbilical) :\n           new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n-          job.getOutputValueGroupingComparator(), keyClass, valueClass, \n+          comparator, keyClass, valueClass,\n           job, reporter);\n       values.informReduceProgress();\n       while (values.more()) {\n         reduceInputKeyCounter.increment(1);\n         reducer.reduce(values.getKey(), values, collector, reporter);\n         if(incrProcCount) {\n           reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n               SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n         }\n         values.nextKey();\n         values.informReduceProgress();\n       }\n \n       reducer.close();\n       reducer \u003d null;\n       \n       out.close(reporter);\n       out \u003d null;\n     } finally {\n       IOUtils.cleanup(LOG, reducer);\n       closeQuietly(out, reporter);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass) throws IOException {\n    Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName \u003d getOutputName(getPartition());\n\n    RecordWriter\u003cOUTKEY, OUTVALUE\u003e out \u003d new OldTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(\n        this, job, reporter, finalName);\n    final RecordWriter\u003cOUTKEY, OUTVALUE\u003e finalOut \u003d out;\n    \n    OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n      new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          finalOut.write(key, value);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    \n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      \n      ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n          new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n          comparator, keyClass, valueClass,\n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n\n      reducer.close();\n      reducer \u003d null;\n      \n      out.close(reporter);\n      out \u003d null;\n    } finally {\n      IOUtils.cleanup(LOG, reducer);\n      closeQuietly(out, reporter);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
      "extendedDetails": {}
    },
    "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4737. Ensure that mapreduce APIs are semantically consistent with mapred API w.r.t Mapper.cleanup and Reducer.cleanup; in the sense that cleanup is now called even if there is an error. The old mapred API already ensures that Mapper.close and Reducer.close are invoked during error handling. Note that it is an incompatible change, however end-users can override Mapper.run and Reducer.run to get the old (inconsistent) behaviour. Contributed by Arun C. Murthy.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1471556 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/04/13 10:38 AM",
      "commitName": "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "06/12/12 6:36 PM",
      "commitNameOld": "b096f61fe25752703785cad5fe0aaae8bf45da2f",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 138.63,
      "commitsBetweenForRepo": 666,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,61 @@\n   void runOldReducer(JobConf job,\n                      TaskUmbilicalProtocol umbilical,\n                      final TaskReporter reporter,\n                      RawKeyValueIterator rIter,\n                      RawComparator\u003cINKEY\u003e comparator,\n                      Class\u003cINKEY\u003e keyClass,\n                      Class\u003cINVALUE\u003e valueClass) throws IOException {\n     Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n       ReflectionUtils.newInstance(job.getReducerClass(), job);\n     // make output collector\n     String finalName \u003d getOutputName(getPartition());\n \n-    final RecordWriter\u003cOUTKEY, OUTVALUE\u003e out \u003d new OldTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(\n+    RecordWriter\u003cOUTKEY, OUTVALUE\u003e out \u003d new OldTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(\n         this, job, reporter, finalName);\n-\n+    final RecordWriter\u003cOUTKEY, OUTVALUE\u003e finalOut \u003d out;\n+    \n     OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n       new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n         public void collect(OUTKEY key, OUTVALUE value)\n           throws IOException {\n-          out.write(key, value);\n+          finalOut.write(key, value);\n           // indicate that progress update needs to be sent\n           reporter.progress();\n         }\n       };\n     \n     // apply reduce function\n     try {\n       //increment processed counter only if skipping feature is enabled\n       boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n         SkipBadRecords.getAutoIncrReducerProcCount(job);\n       \n       ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n           new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n               comparator, keyClass, valueClass, \n               job, reporter, umbilical) :\n           new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n           job.getOutputValueGroupingComparator(), keyClass, valueClass, \n           job, reporter);\n       values.informReduceProgress();\n       while (values.more()) {\n         reduceInputKeyCounter.increment(1);\n         reducer.reduce(values.getKey(), values, collector, reporter);\n         if(incrProcCount) {\n           reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n               SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n         }\n         values.nextKey();\n         values.informReduceProgress();\n       }\n \n-      //Clean up: repeated in catch block below\n       reducer.close();\n-      out.close(reporter);\n-      //End of clean up.\n-    } catch (IOException ioe) {\n-      try {\n-        reducer.close();\n-      } catch (IOException ignored) {}\n-        \n-      try {\n-        out.close(reporter);\n-      } catch (IOException ignored) {}\n+      reducer \u003d null;\n       \n-      throw ioe;\n+      out.close(reporter);\n+      out \u003d null;\n+    } finally {\n+      IOUtils.cleanup(LOG, reducer);\n+      closeQuietly(out, reporter);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass) throws IOException {\n    Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName \u003d getOutputName(getPartition());\n\n    RecordWriter\u003cOUTKEY, OUTVALUE\u003e out \u003d new OldTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(\n        this, job, reporter, finalName);\n    final RecordWriter\u003cOUTKEY, OUTVALUE\u003e finalOut \u003d out;\n    \n    OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n      new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          finalOut.write(key, value);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    \n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      \n      ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n          new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n          job.getOutputValueGroupingComparator(), keyClass, valueClass, \n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n\n      reducer.close();\n      reducer \u003d null;\n      \n      out.close(reporter);\n      out \u003d null;\n    } finally {\n      IOUtils.cleanup(LOG, reducer);\n      closeQuietly(out, reporter);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass) throws IOException {\n    Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName \u003d getOutputName(getPartition());\n\n    final RecordWriter\u003cOUTKEY, OUTVALUE\u003e out \u003d new OldTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(\n        this, job, reporter, finalName);\n\n    OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n      new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          out.write(key, value);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    \n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      \n      ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n          new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n          job.getOutputValueGroupingComparator(), keyClass, valueClass, \n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n\n      //Clean up: repeated in catch block below\n      reducer.close();\n      out.close(reporter);\n      //End of clean up.\n    } catch (IOException ioe) {\n      try {\n        reducer.close();\n      } catch (IOException ignored) {}\n        \n      try {\n        out.close(reporter);\n      } catch (IOException ignored) {}\n      \n      throw ioe;\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass) throws IOException {\n    Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName \u003d getOutputName(getPartition());\n\n    final RecordWriter\u003cOUTKEY, OUTVALUE\u003e out \u003d new OldTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(\n        this, job, reporter, finalName);\n\n    OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n      new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          out.write(key, value);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    \n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      \n      ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n          new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n          job.getOutputValueGroupingComparator(), keyClass, valueClass, \n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n\n      //Clean up: repeated in catch block below\n      reducer.close();\n      out.close(reporter);\n      //End of clean up.\n    } catch (IOException ioe) {\n      try {\n        reducer.close();\n      } catch (IOException ignored) {}\n        \n      try {\n        out.close(reporter);\n      } catch (IOException ignored) {}\n      \n      throw ioe;\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/ReduceTask.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
        "oldMethodName": "runOldReducer",
        "newMethodName": "runOldReducer"
      }
    },
    "4796e1adcb912005198c9003305c97cf3a8b523e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2365. Add counters to track bytes (read,written) via File(Input,Output)Format. Contributed by Siddharth Seth. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1146515 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/07/11 4:36 PM",
      "commitName": "4796e1adcb912005198c9003305c97cf3a8b523e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 31.07,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,66 @@\n   void runOldReducer(JobConf job,\n                      TaskUmbilicalProtocol umbilical,\n                      final TaskReporter reporter,\n                      RawKeyValueIterator rIter,\n                      RawComparator\u003cINKEY\u003e comparator,\n                      Class\u003cINKEY\u003e keyClass,\n                      Class\u003cINVALUE\u003e valueClass) throws IOException {\n     Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n       ReflectionUtils.newInstance(job.getReducerClass(), job);\n     // make output collector\n     String finalName \u003d getOutputName(getPartition());\n \n-    FileSystem fs \u003d FileSystem.get(job);\n+    final RecordWriter\u003cOUTKEY, OUTVALUE\u003e out \u003d new OldTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(\n+        this, job, reporter, finalName);\n \n-    final RecordWriter\u003cOUTKEY,OUTVALUE\u003e out \u003d \n-      job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter);  \n-    \n     OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n       new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n         public void collect(OUTKEY key, OUTVALUE value)\n           throws IOException {\n           out.write(key, value);\n-          reduceOutputCounter.increment(1);\n           // indicate that progress update needs to be sent\n           reporter.progress();\n         }\n       };\n     \n     // apply reduce function\n     try {\n       //increment processed counter only if skipping feature is enabled\n       boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n         SkipBadRecords.getAutoIncrReducerProcCount(job);\n       \n       ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n           new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n               comparator, keyClass, valueClass, \n               job, reporter, umbilical) :\n           new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n           job.getOutputValueGroupingComparator(), keyClass, valueClass, \n           job, reporter);\n       values.informReduceProgress();\n       while (values.more()) {\n         reduceInputKeyCounter.increment(1);\n         reducer.reduce(values.getKey(), values, collector, reporter);\n         if(incrProcCount) {\n           reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n               SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n         }\n         values.nextKey();\n         values.informReduceProgress();\n       }\n \n       //Clean up: repeated in catch block below\n       reducer.close();\n       out.close(reporter);\n       //End of clean up.\n     } catch (IOException ioe) {\n       try {\n         reducer.close();\n       } catch (IOException ignored) {}\n         \n       try {\n         out.close(reporter);\n       } catch (IOException ignored) {}\n       \n       throw ioe;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass) throws IOException {\n    Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName \u003d getOutputName(getPartition());\n\n    final RecordWriter\u003cOUTKEY, OUTVALUE\u003e out \u003d new OldTrackingRecordWriter\u003cOUTKEY, OUTVALUE\u003e(\n        this, job, reporter, finalName);\n\n    OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n      new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          out.write(key, value);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    \n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      \n      ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n          new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n          job.getOutputValueGroupingComparator(), keyClass, valueClass, \n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n\n      //Clean up: repeated in catch block below\n      reducer.close();\n      out.close(reporter);\n      //End of clean up.\n    } catch (IOException ioe) {\n      try {\n        reducer.close();\n      } catch (IOException ignored) {}\n        \n      try {\n        out.close(reporter);\n      } catch (IOException ignored) {}\n      \n      throw ioe;\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/ReduceTask.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,69 @@\n+  void runOldReducer(JobConf job,\n+                     TaskUmbilicalProtocol umbilical,\n+                     final TaskReporter reporter,\n+                     RawKeyValueIterator rIter,\n+                     RawComparator\u003cINKEY\u003e comparator,\n+                     Class\u003cINKEY\u003e keyClass,\n+                     Class\u003cINVALUE\u003e valueClass) throws IOException {\n+    Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n+      ReflectionUtils.newInstance(job.getReducerClass(), job);\n+    // make output collector\n+    String finalName \u003d getOutputName(getPartition());\n+\n+    FileSystem fs \u003d FileSystem.get(job);\n+\n+    final RecordWriter\u003cOUTKEY,OUTVALUE\u003e out \u003d \n+      job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter);  \n+    \n+    OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n+      new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n+        public void collect(OUTKEY key, OUTVALUE value)\n+          throws IOException {\n+          out.write(key, value);\n+          reduceOutputCounter.increment(1);\n+          // indicate that progress update needs to be sent\n+          reporter.progress();\n+        }\n+      };\n+    \n+    // apply reduce function\n+    try {\n+      //increment processed counter only if skipping feature is enabled\n+      boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n+        SkipBadRecords.getAutoIncrReducerProcCount(job);\n+      \n+      ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n+          new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n+              comparator, keyClass, valueClass, \n+              job, reporter, umbilical) :\n+          new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n+          job.getOutputValueGroupingComparator(), keyClass, valueClass, \n+          job, reporter);\n+      values.informReduceProgress();\n+      while (values.more()) {\n+        reduceInputKeyCounter.increment(1);\n+        reducer.reduce(values.getKey(), values, collector, reporter);\n+        if(incrProcCount) {\n+          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n+              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n+        }\n+        values.nextKey();\n+        values.informReduceProgress();\n+      }\n+\n+      //Clean up: repeated in catch block below\n+      reducer.close();\n+      out.close(reporter);\n+      //End of clean up.\n+    } catch (IOException ioe) {\n+      try {\n+        reducer.close();\n+      } catch (IOException ignored) {}\n+        \n+      try {\n+        out.close(reporter);\n+      } catch (IOException ignored) {}\n+      \n+      throw ioe;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator\u003cINKEY\u003e comparator,\n                     Class\u003cINKEY\u003e keyClass,\n                     Class\u003cINVALUE\u003e valueClass) throws IOException {\n    Reducer\u003cINKEY,INVALUE,OUTKEY,OUTVALUE\u003e reducer \u003d \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName \u003d getOutputName(getPartition());\n\n    FileSystem fs \u003d FileSystem.get(job);\n\n    final RecordWriter\u003cOUTKEY,OUTVALUE\u003e out \u003d \n      job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter);  \n    \n    OutputCollector\u003cOUTKEY,OUTVALUE\u003e collector \u003d \n      new OutputCollector\u003cOUTKEY,OUTVALUE\u003e() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          out.write(key, value);\n          reduceOutputCounter.increment(1);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    \n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount \u003d SkipBadRecords.getReducerMaxSkipGroups(job)\u003e0 \u0026\u0026\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      \n      ReduceValuesIterator\u003cINKEY,INVALUE\u003e values \u003d isSkipping() ? \n          new SkippingReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator\u003cINKEY,INVALUE\u003e(rIter, \n          job.getOutputValueGroupingComparator(), keyClass, valueClass, \n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n\n      //Clean up: repeated in catch block below\n      reducer.close();\n      out.close(reporter);\n      //End of clean up.\n    } catch (IOException ioe) {\n      try {\n        reducer.close();\n      } catch (IOException ignored) {}\n        \n      try {\n        out.close(reporter);\n      } catch (IOException ignored) {}\n      \n      throw ioe;\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/ReduceTask.java"
    }
  }
}