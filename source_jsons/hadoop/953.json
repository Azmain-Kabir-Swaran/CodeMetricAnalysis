{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSStripedOutputStream.java",
  "functionName": "enqueueCurrentPacketFull",
  "functionId": "enqueueCurrentPacketFull",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
  "functionStartLine": 604,
  "functionEndLine": 612,
  "numCommitsSeen": 38,
  "timeTaken": 1166,
  "changeHistory": [
    "8b7339312cb06b7e021f8f9ea6e3a20ebf009af3"
  ],
  "changeHistoryShort": {
    "8b7339312cb06b7e021f8f9ea6e3a20ebf009af3": "Ymodifierchange"
  },
  "changeHistoryDetails": {
    "8b7339312cb06b7e021f8f9ea6e3a20ebf009af3": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-9182. Cleanup the findbugs and other issues after HDFS EC merged to trunk. Contributed by Uma Maheswara Rao G.\n",
      "commitDate": "06/10/15 10:22 PM",
      "commitName": "8b7339312cb06b7e021f8f9ea6e3a20ebf009af3",
      "commitAuthor": "Uma Mahesh",
      "commitDateOld": "06/10/15 10:56 AM",
      "commitNameOld": "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.48,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,9 @@\n-  void enqueueCurrentPacketFull() throws IOException {\n+  synchronized void enqueueCurrentPacketFull() throws IOException {\n     LOG.debug(\"enqueue full {}, src\u003d{}, bytesCurBlock\u003d{}, blockSize\u003d{},\"\n             + \" appendChunk\u003d{}, {}\", currentPacket, src, getStreamer()\n             .getBytesCurBlock(), blockSize, getStreamer().getAppendChunk(),\n         getStreamer());\n     enqueueCurrentPacket();\n     adjustChunkBoundary();\n     // no need to end block here\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void enqueueCurrentPacketFull() throws IOException {\n    LOG.debug(\"enqueue full {}, src\u003d{}, bytesCurBlock\u003d{}, blockSize\u003d{},\"\n            + \" appendChunk\u003d{}, {}\", currentPacket, src, getStreamer()\n            .getBytesCurBlock(), blockSize, getStreamer().getAppendChunk(),\n        getStreamer());\n    enqueueCurrentPacket();\n    adjustChunkBoundary();\n    // no need to end block here\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[synchronized]"
      }
    }
  }
}