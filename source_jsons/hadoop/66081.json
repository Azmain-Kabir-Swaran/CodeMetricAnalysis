{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RenameOperation.java",
  "functionName": "execute",
  "functionId": "execute",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java",
  "functionStartLine": 213,
  "functionEndLine": 259,
  "numCommitsSeen": 11,
  "timeTaken": 7881,
  "changeHistory": [
    "511df1e837b19ccb9271520589452d82d50ac69d",
    "e02eb24e0a9139418120027b694492e0738df20a",
    "6d0bffe17eadedd60d4599427248b0db4a7c5502",
    "7f783970364930cc461d1a73833bc58cdd10553e",
    "d7152332b32a575c3a92e3f4c44b95e58462528d",
    "da9a39eed138210de29b59b90c449b28da1c04f9",
    "9a013b255f301c557c3868dc1ad657202e9e7a67"
  ],
  "changeHistoryShort": {
    "511df1e837b19ccb9271520589452d82d50ac69d": "Ymultichange(Yrename,Yreturntypechange,Ybodychange)",
    "e02eb24e0a9139418120027b694492e0738df20a": "Ymultichange(Ymovefromfile,Yreturntypechange,Yexceptionschange,Ybodychange,Yrename,Yparameterchange)",
    "6d0bffe17eadedd60d4599427248b0db4a7c5502": "Ybodychange",
    "7f783970364930cc461d1a73833bc58cdd10553e": "Ybodychange",
    "d7152332b32a575c3a92e3f4c44b95e58462528d": "Ybodychange",
    "da9a39eed138210de29b59b90c449b28da1c04f9": "Ybodychange",
    "9a013b255f301c557c3868dc1ad657202e9e7a67": "Yintroduced"
  },
  "changeHistoryDetails": {
    "511df1e837b19ccb9271520589452d82d50ac69d": {
      "type": "Ymultichange(Yrename,Yreturntypechange,Ybodychange)",
      "commitMessage": "HADOOP-16430. S3AFilesystem.delete to incrementally update s3guard with deletions\n\nContributed by Steve Loughran.\n\nThis overlaps the scanning for directory entries with batched calls to S3 DELETE and updates of the S3Guard tables.\nIt also uses S3Guard to list the files to delete, so find newly created files even when S3 listings are not use consistent.\n\nFor path which the client considers S3Guard to be authoritative, we also do a recursive LIST of the store and delete files; this is to find unindexed files and do guarantee that the delete(path, true) call really does delete everything underneath.\n\nChange-Id: Ice2f6e940c506e0b3a78fa534a99721b1698708e\n",
      "commitDate": "05/09/19 6:25 AM",
      "commitName": "511df1e837b19ccb9271520589452d82d50ac69d",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HADOOP-16430. S3AFilesystem.delete to incrementally update s3guard with deletions\n\nContributed by Steve Loughran.\n\nThis overlaps the scanning for directory entries with batched calls to S3 DELETE and updates of the S3Guard tables.\nIt also uses S3Guard to list the files to delete, so find newly created files even when S3 listings are not use consistent.\n\nFor path which the client considers S3Guard to be authoritative, we also do a recursive LIST of the store and delete files; this is to find unindexed files and do guarantee that the delete(path, true) call really does delete everything underneath.\n\nChange-Id: Ice2f6e940c506e0b3a78fa534a99721b1698708e\n",
          "commitDate": "05/09/19 6:25 AM",
          "commitName": "511df1e837b19ccb9271520589452d82d50ac69d",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "20/06/19 1:56 AM",
          "commitNameOld": "e02eb24e0a9139418120027b694492e0738df20a",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 77.19,
          "commitsBetweenForRepo": 691,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,47 @@\n-  public long executeRename() throws IOException {\n-    Preconditions.checkState(\n-        !executed.getAndSet(true),\n-        \"Rename attempted twice\");\n+  public Long execute() throws IOException {\n+    executeOnlyOnce();\n     final StoreContext storeContext \u003d getStoreContext();\n     final MetadataStore metadataStore \u003d checkNotNull(\n         storeContext.getMetadataStore(),\n         \"No metadata store in context\");\n \n     // Validation completed: time to begin the operation.\n     // The store-specific rename tracker is used to keep the store\n     // to date with the in-progress operation.\n     // for the null store, these are all no-ops.\n     renameTracker \u003d metadataStore.initiateRenameOperation(\n         storeContext,\n         sourcePath, sourceStatus, destPath);\n \n \n     // Ok! Time to start\n     try {\n       if (sourceStatus.isFile()) {\n         renameFileToDest();\n       } else {\n         recursiveDirectoryRename();\n       }\n     } catch (AmazonClientException | IOException ex) {\n       // rename failed.\n       // block for all ongoing copies to complete, successfully or not\n       try {\n         completeActiveCopies(\"failure handling\");\n       } catch (IOException e) {\n         // a failure to update the metastore after a rename failure is what\n         // we\u0027d see on a network problem, expired credentials and other\n         // unrecoverable errors.\n         // Downgrading to warn because an exception is already\n         // about to be thrown.\n         LOG.warn(\"While completing all active copies\", e);\n       }\n       // notify the rename tracker of the failure\n       throw renameTracker.renameFailed(ex);\n     }\n \n     // At this point the rename has completed successfully in the S3 store.\n     // Tell the metastore this fact and let it complete its changes\n     renameTracker.completeRename();\n \n     callbacks.finishRename(sourcePath, destPath);\n     return bytesCopied.get();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Long execute() throws IOException {\n    executeOnlyOnce();\n    final StoreContext storeContext \u003d getStoreContext();\n    final MetadataStore metadataStore \u003d checkNotNull(\n        storeContext.getMetadataStore(),\n        \"No metadata store in context\");\n\n    // Validation completed: time to begin the operation.\n    // The store-specific rename tracker is used to keep the store\n    // to date with the in-progress operation.\n    // for the null store, these are all no-ops.\n    renameTracker \u003d metadataStore.initiateRenameOperation(\n        storeContext,\n        sourcePath, sourceStatus, destPath);\n\n\n    // Ok! Time to start\n    try {\n      if (sourceStatus.isFile()) {\n        renameFileToDest();\n      } else {\n        recursiveDirectoryRename();\n      }\n    } catch (AmazonClientException | IOException ex) {\n      // rename failed.\n      // block for all ongoing copies to complete, successfully or not\n      try {\n        completeActiveCopies(\"failure handling\");\n      } catch (IOException e) {\n        // a failure to update the metastore after a rename failure is what\n        // we\u0027d see on a network problem, expired credentials and other\n        // unrecoverable errors.\n        // Downgrading to warn because an exception is already\n        // about to be thrown.\n        LOG.warn(\"While completing all active copies\", e);\n      }\n      // notify the rename tracker of the failure\n      throw renameTracker.renameFailed(ex);\n    }\n\n    // At this point the rename has completed successfully in the S3 store.\n    // Tell the metastore this fact and let it complete its changes\n    renameTracker.completeRename();\n\n    callbacks.finishRename(sourcePath, destPath);\n    return bytesCopied.get();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java",
          "extendedDetails": {
            "oldValue": "executeRename",
            "newValue": "execute"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HADOOP-16430. S3AFilesystem.delete to incrementally update s3guard with deletions\n\nContributed by Steve Loughran.\n\nThis overlaps the scanning for directory entries with batched calls to S3 DELETE and updates of the S3Guard tables.\nIt also uses S3Guard to list the files to delete, so find newly created files even when S3 listings are not use consistent.\n\nFor path which the client considers S3Guard to be authoritative, we also do a recursive LIST of the store and delete files; this is to find unindexed files and do guarantee that the delete(path, true) call really does delete everything underneath.\n\nChange-Id: Ice2f6e940c506e0b3a78fa534a99721b1698708e\n",
          "commitDate": "05/09/19 6:25 AM",
          "commitName": "511df1e837b19ccb9271520589452d82d50ac69d",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "20/06/19 1:56 AM",
          "commitNameOld": "e02eb24e0a9139418120027b694492e0738df20a",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 77.19,
          "commitsBetweenForRepo": 691,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,47 @@\n-  public long executeRename() throws IOException {\n-    Preconditions.checkState(\n-        !executed.getAndSet(true),\n-        \"Rename attempted twice\");\n+  public Long execute() throws IOException {\n+    executeOnlyOnce();\n     final StoreContext storeContext \u003d getStoreContext();\n     final MetadataStore metadataStore \u003d checkNotNull(\n         storeContext.getMetadataStore(),\n         \"No metadata store in context\");\n \n     // Validation completed: time to begin the operation.\n     // The store-specific rename tracker is used to keep the store\n     // to date with the in-progress operation.\n     // for the null store, these are all no-ops.\n     renameTracker \u003d metadataStore.initiateRenameOperation(\n         storeContext,\n         sourcePath, sourceStatus, destPath);\n \n \n     // Ok! Time to start\n     try {\n       if (sourceStatus.isFile()) {\n         renameFileToDest();\n       } else {\n         recursiveDirectoryRename();\n       }\n     } catch (AmazonClientException | IOException ex) {\n       // rename failed.\n       // block for all ongoing copies to complete, successfully or not\n       try {\n         completeActiveCopies(\"failure handling\");\n       } catch (IOException e) {\n         // a failure to update the metastore after a rename failure is what\n         // we\u0027d see on a network problem, expired credentials and other\n         // unrecoverable errors.\n         // Downgrading to warn because an exception is already\n         // about to be thrown.\n         LOG.warn(\"While completing all active copies\", e);\n       }\n       // notify the rename tracker of the failure\n       throw renameTracker.renameFailed(ex);\n     }\n \n     // At this point the rename has completed successfully in the S3 store.\n     // Tell the metastore this fact and let it complete its changes\n     renameTracker.completeRename();\n \n     callbacks.finishRename(sourcePath, destPath);\n     return bytesCopied.get();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Long execute() throws IOException {\n    executeOnlyOnce();\n    final StoreContext storeContext \u003d getStoreContext();\n    final MetadataStore metadataStore \u003d checkNotNull(\n        storeContext.getMetadataStore(),\n        \"No metadata store in context\");\n\n    // Validation completed: time to begin the operation.\n    // The store-specific rename tracker is used to keep the store\n    // to date with the in-progress operation.\n    // for the null store, these are all no-ops.\n    renameTracker \u003d metadataStore.initiateRenameOperation(\n        storeContext,\n        sourcePath, sourceStatus, destPath);\n\n\n    // Ok! Time to start\n    try {\n      if (sourceStatus.isFile()) {\n        renameFileToDest();\n      } else {\n        recursiveDirectoryRename();\n      }\n    } catch (AmazonClientException | IOException ex) {\n      // rename failed.\n      // block for all ongoing copies to complete, successfully or not\n      try {\n        completeActiveCopies(\"failure handling\");\n      } catch (IOException e) {\n        // a failure to update the metastore after a rename failure is what\n        // we\u0027d see on a network problem, expired credentials and other\n        // unrecoverable errors.\n        // Downgrading to warn because an exception is already\n        // about to be thrown.\n        LOG.warn(\"While completing all active copies\", e);\n      }\n      // notify the rename tracker of the failure\n      throw renameTracker.renameFailed(ex);\n    }\n\n    // At this point the rename has completed successfully in the S3 store.\n    // Tell the metastore this fact and let it complete its changes\n    renameTracker.completeRename();\n\n    callbacks.finishRename(sourcePath, destPath);\n    return bytesCopied.get();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java",
          "extendedDetails": {
            "oldValue": "long",
            "newValue": "Long"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-16430. S3AFilesystem.delete to incrementally update s3guard with deletions\n\nContributed by Steve Loughran.\n\nThis overlaps the scanning for directory entries with batched calls to S3 DELETE and updates of the S3Guard tables.\nIt also uses S3Guard to list the files to delete, so find newly created files even when S3 listings are not use consistent.\n\nFor path which the client considers S3Guard to be authoritative, we also do a recursive LIST of the store and delete files; this is to find unindexed files and do guarantee that the delete(path, true) call really does delete everything underneath.\n\nChange-Id: Ice2f6e940c506e0b3a78fa534a99721b1698708e\n",
          "commitDate": "05/09/19 6:25 AM",
          "commitName": "511df1e837b19ccb9271520589452d82d50ac69d",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "20/06/19 1:56 AM",
          "commitNameOld": "e02eb24e0a9139418120027b694492e0738df20a",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 77.19,
          "commitsBetweenForRepo": 691,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,47 @@\n-  public long executeRename() throws IOException {\n-    Preconditions.checkState(\n-        !executed.getAndSet(true),\n-        \"Rename attempted twice\");\n+  public Long execute() throws IOException {\n+    executeOnlyOnce();\n     final StoreContext storeContext \u003d getStoreContext();\n     final MetadataStore metadataStore \u003d checkNotNull(\n         storeContext.getMetadataStore(),\n         \"No metadata store in context\");\n \n     // Validation completed: time to begin the operation.\n     // The store-specific rename tracker is used to keep the store\n     // to date with the in-progress operation.\n     // for the null store, these are all no-ops.\n     renameTracker \u003d metadataStore.initiateRenameOperation(\n         storeContext,\n         sourcePath, sourceStatus, destPath);\n \n \n     // Ok! Time to start\n     try {\n       if (sourceStatus.isFile()) {\n         renameFileToDest();\n       } else {\n         recursiveDirectoryRename();\n       }\n     } catch (AmazonClientException | IOException ex) {\n       // rename failed.\n       // block for all ongoing copies to complete, successfully or not\n       try {\n         completeActiveCopies(\"failure handling\");\n       } catch (IOException e) {\n         // a failure to update the metastore after a rename failure is what\n         // we\u0027d see on a network problem, expired credentials and other\n         // unrecoverable errors.\n         // Downgrading to warn because an exception is already\n         // about to be thrown.\n         LOG.warn(\"While completing all active copies\", e);\n       }\n       // notify the rename tracker of the failure\n       throw renameTracker.renameFailed(ex);\n     }\n \n     // At this point the rename has completed successfully in the S3 store.\n     // Tell the metastore this fact and let it complete its changes\n     renameTracker.completeRename();\n \n     callbacks.finishRename(sourcePath, destPath);\n     return bytesCopied.get();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Long execute() throws IOException {\n    executeOnlyOnce();\n    final StoreContext storeContext \u003d getStoreContext();\n    final MetadataStore metadataStore \u003d checkNotNull(\n        storeContext.getMetadataStore(),\n        \"No metadata store in context\");\n\n    // Validation completed: time to begin the operation.\n    // The store-specific rename tracker is used to keep the store\n    // to date with the in-progress operation.\n    // for the null store, these are all no-ops.\n    renameTracker \u003d metadataStore.initiateRenameOperation(\n        storeContext,\n        sourcePath, sourceStatus, destPath);\n\n\n    // Ok! Time to start\n    try {\n      if (sourceStatus.isFile()) {\n        renameFileToDest();\n      } else {\n        recursiveDirectoryRename();\n      }\n    } catch (AmazonClientException | IOException ex) {\n      // rename failed.\n      // block for all ongoing copies to complete, successfully or not\n      try {\n        completeActiveCopies(\"failure handling\");\n      } catch (IOException e) {\n        // a failure to update the metastore after a rename failure is what\n        // we\u0027d see on a network problem, expired credentials and other\n        // unrecoverable errors.\n        // Downgrading to warn because an exception is already\n        // about to be thrown.\n        LOG.warn(\"While completing all active copies\", e);\n      }\n      // notify the rename tracker of the failure\n      throw renameTracker.renameFailed(ex);\n    }\n\n    // At this point the rename has completed successfully in the S3 store.\n    // Tell the metastore this fact and let it complete its changes\n    renameTracker.completeRename();\n\n    callbacks.finishRename(sourcePath, destPath);\n    return bytesCopied.get();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java",
          "extendedDetails": {}
        }
      ]
    },
    "e02eb24e0a9139418120027b694492e0738df20a": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Yexceptionschange,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
      "commitDate": "20/06/19 1:56 AM",
      "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
          "commitDate": "20/06/19 1:56 AM",
          "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "20/06/19 1:42 AM",
          "commitNameOld": "28291a9e8ade5dc4ebcebe1c9fbe9e92535c9333",
          "commitAuthorOld": "Sahil Takiar",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,49 @@\n-  public void executeRenameReadOnlyData(final Configuration conf)\n-      throws Exception {\n-    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n-    Path basePath \u003d methodPath();\n-    Path destDir \u003d new Path(basePath, \"renameDest\");\n-    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n-    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n+  public long executeRename() throws IOException {\n+    Preconditions.checkState(\n+        !executed.getAndSet(true),\n+        \"Rename attempted twice\");\n+    final StoreContext storeContext \u003d getStoreContext();\n+    final MetadataStore metadataStore \u003d checkNotNull(\n+        storeContext.getMetadataStore(),\n+        \"No metadata store in context\");\n \n-    // the full FS\n-    S3AFileSystem fs \u003d getFileSystem();\n-    fs.delete(basePath, true);\n+    // Validation completed: time to begin the operation.\n+    // The store-specific rename tracker is used to keep the store\n+    // to date with the in-progress operation.\n+    // for the null store, these are all no-ops.\n+    renameTracker \u003d metadataStore.initiateRenameOperation(\n+        storeContext,\n+        sourcePath, sourceStatus, destPath);\n \n-    // this file is readable by the roleFS, but cannot be deleted\n-    touch(fs, readOnlyFile);\n \n-    bindRolePolicyStatements(conf,\n-        STATEMENT_S3GUARD_CLIENT,\n-        STATEMENT_ALL_BUCKET_READ_ACCESS,\n-        new Statement(Effects.Allow)\n-            .addActions(S3_PATH_RW_OPERATIONS)\n-            .addResources(directory(destDir))\n-    );\n-    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n+    // Ok! Time to start\n+    try {\n+      if (sourceStatus.isFile()) {\n+        renameFileToDest();\n+      } else {\n+        recursiveDirectoryRename();\n+      }\n+    } catch (AmazonClientException | IOException ex) {\n+      // rename failed.\n+      // block for all ongoing copies to complete, successfully or not\n+      try {\n+        completeActiveCopies(\"failure handling\");\n+      } catch (IOException e) {\n+        // a failure to update the metastore after a rename failure is what\n+        // we\u0027d see on a network problem, expired credentials and other\n+        // unrecoverable errors.\n+        // Downgrading to warn because an exception is already\n+        // about to be thrown.\n+        LOG.warn(\"While completing all active copies\", e);\n+      }\n+      // notify the rename tracker of the failure\n+      throw renameTracker.renameFailed(ex);\n+    }\n \n-    roleFS.delete(destDir, true);\n-    roleFS.mkdirs(destDir);\n-    // rename will fail in the delete phase\n-    forbidden(readOnlyFile.toString(),\n-        () -\u003e roleFS.rename(readOnlyFile, destDir));\n+    // At this point the rename has completed successfully in the S3 store.\n+    // Tell the metastore this fact and let it complete its changes\n+    renameTracker.completeRename();\n \n-    // and the source file is still there\n-    assertIsFile(readOnlyFile);\n-\n-    // but so is the copied version, because there\u0027s no attempt\n-    // at rollback, or preflight checking on the delete permissions\n-    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n-\n-    assertIsFile(renamedFile);\n-\n-    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n-    assertFileCount(\"Empty Dest Dir\", roleFS,\n-        destDir, 0);\n-    // create a set of files\n-    // this is done in parallel as it is 10x faster on a long-haul test run.\n-    int range \u003d 10;\n-    touchFiles(fs, readOnlyDir, range);\n-    // don\u0027t forget about that original file!\n-    final long createdFiles \u003d range + 1;\n-    // are they all there?\n-    assertFileCount(\"files ready to rename\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // try to rename the directory\n-    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n-    AccessDeniedException ex \u003d forbidden(\"\",\n-        () -\u003e roleFS.rename(readOnlyDir, destDir));\n-    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n-    assertFileCount(\"files copied to the destination\", roleFS,\n-        destDir, createdFiles);\n-    assertFileCount(\"files in the source directory\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // and finally (so as to avoid the delay of POSTing some more objects,\n-    // delete that r/o source\n-    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n+    callbacks.finishRename(sourcePath, destPath);\n+    return bytesCopied.get();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long executeRename() throws IOException {\n    Preconditions.checkState(\n        !executed.getAndSet(true),\n        \"Rename attempted twice\");\n    final StoreContext storeContext \u003d getStoreContext();\n    final MetadataStore metadataStore \u003d checkNotNull(\n        storeContext.getMetadataStore(),\n        \"No metadata store in context\");\n\n    // Validation completed: time to begin the operation.\n    // The store-specific rename tracker is used to keep the store\n    // to date with the in-progress operation.\n    // for the null store, these are all no-ops.\n    renameTracker \u003d metadataStore.initiateRenameOperation(\n        storeContext,\n        sourcePath, sourceStatus, destPath);\n\n\n    // Ok! Time to start\n    try {\n      if (sourceStatus.isFile()) {\n        renameFileToDest();\n      } else {\n        recursiveDirectoryRename();\n      }\n    } catch (AmazonClientException | IOException ex) {\n      // rename failed.\n      // block for all ongoing copies to complete, successfully or not\n      try {\n        completeActiveCopies(\"failure handling\");\n      } catch (IOException e) {\n        // a failure to update the metastore after a rename failure is what\n        // we\u0027d see on a network problem, expired credentials and other\n        // unrecoverable errors.\n        // Downgrading to warn because an exception is already\n        // about to be thrown.\n        LOG.warn(\"While completing all active copies\", e);\n      }\n      // notify the rename tracker of the failure\n      throw renameTracker.renameFailed(ex);\n    }\n\n    // At this point the rename has completed successfully in the S3 store.\n    // Tell the metastore this fact and let it complete its changes\n    renameTracker.completeRename();\n\n    callbacks.finishRename(sourcePath, destPath);\n    return bytesCopied.get();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java",
          "extendedDetails": {
            "oldPath": "hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java",
            "newPath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java",
            "oldMethodName": "executeRenameReadOnlyData",
            "newMethodName": "executeRename"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
          "commitDate": "20/06/19 1:56 AM",
          "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "20/06/19 1:42 AM",
          "commitNameOld": "28291a9e8ade5dc4ebcebe1c9fbe9e92535c9333",
          "commitAuthorOld": "Sahil Takiar",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,49 @@\n-  public void executeRenameReadOnlyData(final Configuration conf)\n-      throws Exception {\n-    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n-    Path basePath \u003d methodPath();\n-    Path destDir \u003d new Path(basePath, \"renameDest\");\n-    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n-    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n+  public long executeRename() throws IOException {\n+    Preconditions.checkState(\n+        !executed.getAndSet(true),\n+        \"Rename attempted twice\");\n+    final StoreContext storeContext \u003d getStoreContext();\n+    final MetadataStore metadataStore \u003d checkNotNull(\n+        storeContext.getMetadataStore(),\n+        \"No metadata store in context\");\n \n-    // the full FS\n-    S3AFileSystem fs \u003d getFileSystem();\n-    fs.delete(basePath, true);\n+    // Validation completed: time to begin the operation.\n+    // The store-specific rename tracker is used to keep the store\n+    // to date with the in-progress operation.\n+    // for the null store, these are all no-ops.\n+    renameTracker \u003d metadataStore.initiateRenameOperation(\n+        storeContext,\n+        sourcePath, sourceStatus, destPath);\n \n-    // this file is readable by the roleFS, but cannot be deleted\n-    touch(fs, readOnlyFile);\n \n-    bindRolePolicyStatements(conf,\n-        STATEMENT_S3GUARD_CLIENT,\n-        STATEMENT_ALL_BUCKET_READ_ACCESS,\n-        new Statement(Effects.Allow)\n-            .addActions(S3_PATH_RW_OPERATIONS)\n-            .addResources(directory(destDir))\n-    );\n-    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n+    // Ok! Time to start\n+    try {\n+      if (sourceStatus.isFile()) {\n+        renameFileToDest();\n+      } else {\n+        recursiveDirectoryRename();\n+      }\n+    } catch (AmazonClientException | IOException ex) {\n+      // rename failed.\n+      // block for all ongoing copies to complete, successfully or not\n+      try {\n+        completeActiveCopies(\"failure handling\");\n+      } catch (IOException e) {\n+        // a failure to update the metastore after a rename failure is what\n+        // we\u0027d see on a network problem, expired credentials and other\n+        // unrecoverable errors.\n+        // Downgrading to warn because an exception is already\n+        // about to be thrown.\n+        LOG.warn(\"While completing all active copies\", e);\n+      }\n+      // notify the rename tracker of the failure\n+      throw renameTracker.renameFailed(ex);\n+    }\n \n-    roleFS.delete(destDir, true);\n-    roleFS.mkdirs(destDir);\n-    // rename will fail in the delete phase\n-    forbidden(readOnlyFile.toString(),\n-        () -\u003e roleFS.rename(readOnlyFile, destDir));\n+    // At this point the rename has completed successfully in the S3 store.\n+    // Tell the metastore this fact and let it complete its changes\n+    renameTracker.completeRename();\n \n-    // and the source file is still there\n-    assertIsFile(readOnlyFile);\n-\n-    // but so is the copied version, because there\u0027s no attempt\n-    // at rollback, or preflight checking on the delete permissions\n-    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n-\n-    assertIsFile(renamedFile);\n-\n-    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n-    assertFileCount(\"Empty Dest Dir\", roleFS,\n-        destDir, 0);\n-    // create a set of files\n-    // this is done in parallel as it is 10x faster on a long-haul test run.\n-    int range \u003d 10;\n-    touchFiles(fs, readOnlyDir, range);\n-    // don\u0027t forget about that original file!\n-    final long createdFiles \u003d range + 1;\n-    // are they all there?\n-    assertFileCount(\"files ready to rename\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // try to rename the directory\n-    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n-    AccessDeniedException ex \u003d forbidden(\"\",\n-        () -\u003e roleFS.rename(readOnlyDir, destDir));\n-    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n-    assertFileCount(\"files copied to the destination\", roleFS,\n-        destDir, createdFiles);\n-    assertFileCount(\"files in the source directory\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // and finally (so as to avoid the delay of POSTing some more objects,\n-    // delete that r/o source\n-    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n+    callbacks.finishRename(sourcePath, destPath);\n+    return bytesCopied.get();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long executeRename() throws IOException {\n    Preconditions.checkState(\n        !executed.getAndSet(true),\n        \"Rename attempted twice\");\n    final StoreContext storeContext \u003d getStoreContext();\n    final MetadataStore metadataStore \u003d checkNotNull(\n        storeContext.getMetadataStore(),\n        \"No metadata store in context\");\n\n    // Validation completed: time to begin the operation.\n    // The store-specific rename tracker is used to keep the store\n    // to date with the in-progress operation.\n    // for the null store, these are all no-ops.\n    renameTracker \u003d metadataStore.initiateRenameOperation(\n        storeContext,\n        sourcePath, sourceStatus, destPath);\n\n\n    // Ok! Time to start\n    try {\n      if (sourceStatus.isFile()) {\n        renameFileToDest();\n      } else {\n        recursiveDirectoryRename();\n      }\n    } catch (AmazonClientException | IOException ex) {\n      // rename failed.\n      // block for all ongoing copies to complete, successfully or not\n      try {\n        completeActiveCopies(\"failure handling\");\n      } catch (IOException e) {\n        // a failure to update the metastore after a rename failure is what\n        // we\u0027d see on a network problem, expired credentials and other\n        // unrecoverable errors.\n        // Downgrading to warn because an exception is already\n        // about to be thrown.\n        LOG.warn(\"While completing all active copies\", e);\n      }\n      // notify the rename tracker of the failure\n      throw renameTracker.renameFailed(ex);\n    }\n\n    // At this point the rename has completed successfully in the S3 store.\n    // Tell the metastore this fact and let it complete its changes\n    renameTracker.completeRename();\n\n    callbacks.finishRename(sourcePath, destPath);\n    return bytesCopied.get();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "long"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
          "commitDate": "20/06/19 1:56 AM",
          "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "20/06/19 1:42 AM",
          "commitNameOld": "28291a9e8ade5dc4ebcebe1c9fbe9e92535c9333",
          "commitAuthorOld": "Sahil Takiar",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,49 @@\n-  public void executeRenameReadOnlyData(final Configuration conf)\n-      throws Exception {\n-    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n-    Path basePath \u003d methodPath();\n-    Path destDir \u003d new Path(basePath, \"renameDest\");\n-    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n-    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n+  public long executeRename() throws IOException {\n+    Preconditions.checkState(\n+        !executed.getAndSet(true),\n+        \"Rename attempted twice\");\n+    final StoreContext storeContext \u003d getStoreContext();\n+    final MetadataStore metadataStore \u003d checkNotNull(\n+        storeContext.getMetadataStore(),\n+        \"No metadata store in context\");\n \n-    // the full FS\n-    S3AFileSystem fs \u003d getFileSystem();\n-    fs.delete(basePath, true);\n+    // Validation completed: time to begin the operation.\n+    // The store-specific rename tracker is used to keep the store\n+    // to date with the in-progress operation.\n+    // for the null store, these are all no-ops.\n+    renameTracker \u003d metadataStore.initiateRenameOperation(\n+        storeContext,\n+        sourcePath, sourceStatus, destPath);\n \n-    // this file is readable by the roleFS, but cannot be deleted\n-    touch(fs, readOnlyFile);\n \n-    bindRolePolicyStatements(conf,\n-        STATEMENT_S3GUARD_CLIENT,\n-        STATEMENT_ALL_BUCKET_READ_ACCESS,\n-        new Statement(Effects.Allow)\n-            .addActions(S3_PATH_RW_OPERATIONS)\n-            .addResources(directory(destDir))\n-    );\n-    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n+    // Ok! Time to start\n+    try {\n+      if (sourceStatus.isFile()) {\n+        renameFileToDest();\n+      } else {\n+        recursiveDirectoryRename();\n+      }\n+    } catch (AmazonClientException | IOException ex) {\n+      // rename failed.\n+      // block for all ongoing copies to complete, successfully or not\n+      try {\n+        completeActiveCopies(\"failure handling\");\n+      } catch (IOException e) {\n+        // a failure to update the metastore after a rename failure is what\n+        // we\u0027d see on a network problem, expired credentials and other\n+        // unrecoverable errors.\n+        // Downgrading to warn because an exception is already\n+        // about to be thrown.\n+        LOG.warn(\"While completing all active copies\", e);\n+      }\n+      // notify the rename tracker of the failure\n+      throw renameTracker.renameFailed(ex);\n+    }\n \n-    roleFS.delete(destDir, true);\n-    roleFS.mkdirs(destDir);\n-    // rename will fail in the delete phase\n-    forbidden(readOnlyFile.toString(),\n-        () -\u003e roleFS.rename(readOnlyFile, destDir));\n+    // At this point the rename has completed successfully in the S3 store.\n+    // Tell the metastore this fact and let it complete its changes\n+    renameTracker.completeRename();\n \n-    // and the source file is still there\n-    assertIsFile(readOnlyFile);\n-\n-    // but so is the copied version, because there\u0027s no attempt\n-    // at rollback, or preflight checking on the delete permissions\n-    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n-\n-    assertIsFile(renamedFile);\n-\n-    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n-    assertFileCount(\"Empty Dest Dir\", roleFS,\n-        destDir, 0);\n-    // create a set of files\n-    // this is done in parallel as it is 10x faster on a long-haul test run.\n-    int range \u003d 10;\n-    touchFiles(fs, readOnlyDir, range);\n-    // don\u0027t forget about that original file!\n-    final long createdFiles \u003d range + 1;\n-    // are they all there?\n-    assertFileCount(\"files ready to rename\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // try to rename the directory\n-    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n-    AccessDeniedException ex \u003d forbidden(\"\",\n-        () -\u003e roleFS.rename(readOnlyDir, destDir));\n-    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n-    assertFileCount(\"files copied to the destination\", roleFS,\n-        destDir, createdFiles);\n-    assertFileCount(\"files in the source directory\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // and finally (so as to avoid the delay of POSTing some more objects,\n-    // delete that r/o source\n-    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n+    callbacks.finishRename(sourcePath, destPath);\n+    return bytesCopied.get();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long executeRename() throws IOException {\n    Preconditions.checkState(\n        !executed.getAndSet(true),\n        \"Rename attempted twice\");\n    final StoreContext storeContext \u003d getStoreContext();\n    final MetadataStore metadataStore \u003d checkNotNull(\n        storeContext.getMetadataStore(),\n        \"No metadata store in context\");\n\n    // Validation completed: time to begin the operation.\n    // The store-specific rename tracker is used to keep the store\n    // to date with the in-progress operation.\n    // for the null store, these are all no-ops.\n    renameTracker \u003d metadataStore.initiateRenameOperation(\n        storeContext,\n        sourcePath, sourceStatus, destPath);\n\n\n    // Ok! Time to start\n    try {\n      if (sourceStatus.isFile()) {\n        renameFileToDest();\n      } else {\n        recursiveDirectoryRename();\n      }\n    } catch (AmazonClientException | IOException ex) {\n      // rename failed.\n      // block for all ongoing copies to complete, successfully or not\n      try {\n        completeActiveCopies(\"failure handling\");\n      } catch (IOException e) {\n        // a failure to update the metastore after a rename failure is what\n        // we\u0027d see on a network problem, expired credentials and other\n        // unrecoverable errors.\n        // Downgrading to warn because an exception is already\n        // about to be thrown.\n        LOG.warn(\"While completing all active copies\", e);\n      }\n      // notify the rename tracker of the failure\n      throw renameTracker.renameFailed(ex);\n    }\n\n    // At this point the rename has completed successfully in the S3 store.\n    // Tell the metastore this fact and let it complete its changes\n    renameTracker.completeRename();\n\n    callbacks.finishRename(sourcePath, destPath);\n    return bytesCopied.get();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java",
          "extendedDetails": {
            "oldValue": "[Exception]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
          "commitDate": "20/06/19 1:56 AM",
          "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "20/06/19 1:42 AM",
          "commitNameOld": "28291a9e8ade5dc4ebcebe1c9fbe9e92535c9333",
          "commitAuthorOld": "Sahil Takiar",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,49 @@\n-  public void executeRenameReadOnlyData(final Configuration conf)\n-      throws Exception {\n-    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n-    Path basePath \u003d methodPath();\n-    Path destDir \u003d new Path(basePath, \"renameDest\");\n-    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n-    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n+  public long executeRename() throws IOException {\n+    Preconditions.checkState(\n+        !executed.getAndSet(true),\n+        \"Rename attempted twice\");\n+    final StoreContext storeContext \u003d getStoreContext();\n+    final MetadataStore metadataStore \u003d checkNotNull(\n+        storeContext.getMetadataStore(),\n+        \"No metadata store in context\");\n \n-    // the full FS\n-    S3AFileSystem fs \u003d getFileSystem();\n-    fs.delete(basePath, true);\n+    // Validation completed: time to begin the operation.\n+    // The store-specific rename tracker is used to keep the store\n+    // to date with the in-progress operation.\n+    // for the null store, these are all no-ops.\n+    renameTracker \u003d metadataStore.initiateRenameOperation(\n+        storeContext,\n+        sourcePath, sourceStatus, destPath);\n \n-    // this file is readable by the roleFS, but cannot be deleted\n-    touch(fs, readOnlyFile);\n \n-    bindRolePolicyStatements(conf,\n-        STATEMENT_S3GUARD_CLIENT,\n-        STATEMENT_ALL_BUCKET_READ_ACCESS,\n-        new Statement(Effects.Allow)\n-            .addActions(S3_PATH_RW_OPERATIONS)\n-            .addResources(directory(destDir))\n-    );\n-    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n+    // Ok! Time to start\n+    try {\n+      if (sourceStatus.isFile()) {\n+        renameFileToDest();\n+      } else {\n+        recursiveDirectoryRename();\n+      }\n+    } catch (AmazonClientException | IOException ex) {\n+      // rename failed.\n+      // block for all ongoing copies to complete, successfully or not\n+      try {\n+        completeActiveCopies(\"failure handling\");\n+      } catch (IOException e) {\n+        // a failure to update the metastore after a rename failure is what\n+        // we\u0027d see on a network problem, expired credentials and other\n+        // unrecoverable errors.\n+        // Downgrading to warn because an exception is already\n+        // about to be thrown.\n+        LOG.warn(\"While completing all active copies\", e);\n+      }\n+      // notify the rename tracker of the failure\n+      throw renameTracker.renameFailed(ex);\n+    }\n \n-    roleFS.delete(destDir, true);\n-    roleFS.mkdirs(destDir);\n-    // rename will fail in the delete phase\n-    forbidden(readOnlyFile.toString(),\n-        () -\u003e roleFS.rename(readOnlyFile, destDir));\n+    // At this point the rename has completed successfully in the S3 store.\n+    // Tell the metastore this fact and let it complete its changes\n+    renameTracker.completeRename();\n \n-    // and the source file is still there\n-    assertIsFile(readOnlyFile);\n-\n-    // but so is the copied version, because there\u0027s no attempt\n-    // at rollback, or preflight checking on the delete permissions\n-    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n-\n-    assertIsFile(renamedFile);\n-\n-    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n-    assertFileCount(\"Empty Dest Dir\", roleFS,\n-        destDir, 0);\n-    // create a set of files\n-    // this is done in parallel as it is 10x faster on a long-haul test run.\n-    int range \u003d 10;\n-    touchFiles(fs, readOnlyDir, range);\n-    // don\u0027t forget about that original file!\n-    final long createdFiles \u003d range + 1;\n-    // are they all there?\n-    assertFileCount(\"files ready to rename\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // try to rename the directory\n-    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n-    AccessDeniedException ex \u003d forbidden(\"\",\n-        () -\u003e roleFS.rename(readOnlyDir, destDir));\n-    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n-    assertFileCount(\"files copied to the destination\", roleFS,\n-        destDir, createdFiles);\n-    assertFileCount(\"files in the source directory\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // and finally (so as to avoid the delay of POSTing some more objects,\n-    // delete that r/o source\n-    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n+    callbacks.finishRename(sourcePath, destPath);\n+    return bytesCopied.get();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long executeRename() throws IOException {\n    Preconditions.checkState(\n        !executed.getAndSet(true),\n        \"Rename attempted twice\");\n    final StoreContext storeContext \u003d getStoreContext();\n    final MetadataStore metadataStore \u003d checkNotNull(\n        storeContext.getMetadataStore(),\n        \"No metadata store in context\");\n\n    // Validation completed: time to begin the operation.\n    // The store-specific rename tracker is used to keep the store\n    // to date with the in-progress operation.\n    // for the null store, these are all no-ops.\n    renameTracker \u003d metadataStore.initiateRenameOperation(\n        storeContext,\n        sourcePath, sourceStatus, destPath);\n\n\n    // Ok! Time to start\n    try {\n      if (sourceStatus.isFile()) {\n        renameFileToDest();\n      } else {\n        recursiveDirectoryRename();\n      }\n    } catch (AmazonClientException | IOException ex) {\n      // rename failed.\n      // block for all ongoing copies to complete, successfully or not\n      try {\n        completeActiveCopies(\"failure handling\");\n      } catch (IOException e) {\n        // a failure to update the metastore after a rename failure is what\n        // we\u0027d see on a network problem, expired credentials and other\n        // unrecoverable errors.\n        // Downgrading to warn because an exception is already\n        // about to be thrown.\n        LOG.warn(\"While completing all active copies\", e);\n      }\n      // notify the rename tracker of the failure\n      throw renameTracker.renameFailed(ex);\n    }\n\n    // At this point the rename has completed successfully in the S3 store.\n    // Tell the metastore this fact and let it complete its changes\n    renameTracker.completeRename();\n\n    callbacks.finishRename(sourcePath, destPath);\n    return bytesCopied.get();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
          "commitDate": "20/06/19 1:56 AM",
          "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "20/06/19 1:42 AM",
          "commitNameOld": "28291a9e8ade5dc4ebcebe1c9fbe9e92535c9333",
          "commitAuthorOld": "Sahil Takiar",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,49 @@\n-  public void executeRenameReadOnlyData(final Configuration conf)\n-      throws Exception {\n-    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n-    Path basePath \u003d methodPath();\n-    Path destDir \u003d new Path(basePath, \"renameDest\");\n-    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n-    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n+  public long executeRename() throws IOException {\n+    Preconditions.checkState(\n+        !executed.getAndSet(true),\n+        \"Rename attempted twice\");\n+    final StoreContext storeContext \u003d getStoreContext();\n+    final MetadataStore metadataStore \u003d checkNotNull(\n+        storeContext.getMetadataStore(),\n+        \"No metadata store in context\");\n \n-    // the full FS\n-    S3AFileSystem fs \u003d getFileSystem();\n-    fs.delete(basePath, true);\n+    // Validation completed: time to begin the operation.\n+    // The store-specific rename tracker is used to keep the store\n+    // to date with the in-progress operation.\n+    // for the null store, these are all no-ops.\n+    renameTracker \u003d metadataStore.initiateRenameOperation(\n+        storeContext,\n+        sourcePath, sourceStatus, destPath);\n \n-    // this file is readable by the roleFS, but cannot be deleted\n-    touch(fs, readOnlyFile);\n \n-    bindRolePolicyStatements(conf,\n-        STATEMENT_S3GUARD_CLIENT,\n-        STATEMENT_ALL_BUCKET_READ_ACCESS,\n-        new Statement(Effects.Allow)\n-            .addActions(S3_PATH_RW_OPERATIONS)\n-            .addResources(directory(destDir))\n-    );\n-    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n+    // Ok! Time to start\n+    try {\n+      if (sourceStatus.isFile()) {\n+        renameFileToDest();\n+      } else {\n+        recursiveDirectoryRename();\n+      }\n+    } catch (AmazonClientException | IOException ex) {\n+      // rename failed.\n+      // block for all ongoing copies to complete, successfully or not\n+      try {\n+        completeActiveCopies(\"failure handling\");\n+      } catch (IOException e) {\n+        // a failure to update the metastore after a rename failure is what\n+        // we\u0027d see on a network problem, expired credentials and other\n+        // unrecoverable errors.\n+        // Downgrading to warn because an exception is already\n+        // about to be thrown.\n+        LOG.warn(\"While completing all active copies\", e);\n+      }\n+      // notify the rename tracker of the failure\n+      throw renameTracker.renameFailed(ex);\n+    }\n \n-    roleFS.delete(destDir, true);\n-    roleFS.mkdirs(destDir);\n-    // rename will fail in the delete phase\n-    forbidden(readOnlyFile.toString(),\n-        () -\u003e roleFS.rename(readOnlyFile, destDir));\n+    // At this point the rename has completed successfully in the S3 store.\n+    // Tell the metastore this fact and let it complete its changes\n+    renameTracker.completeRename();\n \n-    // and the source file is still there\n-    assertIsFile(readOnlyFile);\n-\n-    // but so is the copied version, because there\u0027s no attempt\n-    // at rollback, or preflight checking on the delete permissions\n-    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n-\n-    assertIsFile(renamedFile);\n-\n-    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n-    assertFileCount(\"Empty Dest Dir\", roleFS,\n-        destDir, 0);\n-    // create a set of files\n-    // this is done in parallel as it is 10x faster on a long-haul test run.\n-    int range \u003d 10;\n-    touchFiles(fs, readOnlyDir, range);\n-    // don\u0027t forget about that original file!\n-    final long createdFiles \u003d range + 1;\n-    // are they all there?\n-    assertFileCount(\"files ready to rename\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // try to rename the directory\n-    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n-    AccessDeniedException ex \u003d forbidden(\"\",\n-        () -\u003e roleFS.rename(readOnlyDir, destDir));\n-    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n-    assertFileCount(\"files copied to the destination\", roleFS,\n-        destDir, createdFiles);\n-    assertFileCount(\"files in the source directory\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // and finally (so as to avoid the delay of POSTing some more objects,\n-    // delete that r/o source\n-    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n+    callbacks.finishRename(sourcePath, destPath);\n+    return bytesCopied.get();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long executeRename() throws IOException {\n    Preconditions.checkState(\n        !executed.getAndSet(true),\n        \"Rename attempted twice\");\n    final StoreContext storeContext \u003d getStoreContext();\n    final MetadataStore metadataStore \u003d checkNotNull(\n        storeContext.getMetadataStore(),\n        \"No metadata store in context\");\n\n    // Validation completed: time to begin the operation.\n    // The store-specific rename tracker is used to keep the store\n    // to date with the in-progress operation.\n    // for the null store, these are all no-ops.\n    renameTracker \u003d metadataStore.initiateRenameOperation(\n        storeContext,\n        sourcePath, sourceStatus, destPath);\n\n\n    // Ok! Time to start\n    try {\n      if (sourceStatus.isFile()) {\n        renameFileToDest();\n      } else {\n        recursiveDirectoryRename();\n      }\n    } catch (AmazonClientException | IOException ex) {\n      // rename failed.\n      // block for all ongoing copies to complete, successfully or not\n      try {\n        completeActiveCopies(\"failure handling\");\n      } catch (IOException e) {\n        // a failure to update the metastore after a rename failure is what\n        // we\u0027d see on a network problem, expired credentials and other\n        // unrecoverable errors.\n        // Downgrading to warn because an exception is already\n        // about to be thrown.\n        LOG.warn(\"While completing all active copies\", e);\n      }\n      // notify the rename tracker of the failure\n      throw renameTracker.renameFailed(ex);\n    }\n\n    // At this point the rename has completed successfully in the S3 store.\n    // Tell the metastore this fact and let it complete its changes\n    renameTracker.completeRename();\n\n    callbacks.finishRename(sourcePath, destPath);\n    return bytesCopied.get();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java",
          "extendedDetails": {
            "oldValue": "executeRenameReadOnlyData",
            "newValue": "executeRename"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
          "commitDate": "20/06/19 1:56 AM",
          "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "20/06/19 1:42 AM",
          "commitNameOld": "28291a9e8ade5dc4ebcebe1c9fbe9e92535c9333",
          "commitAuthorOld": "Sahil Takiar",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,49 @@\n-  public void executeRenameReadOnlyData(final Configuration conf)\n-      throws Exception {\n-    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n-    Path basePath \u003d methodPath();\n-    Path destDir \u003d new Path(basePath, \"renameDest\");\n-    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n-    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n+  public long executeRename() throws IOException {\n+    Preconditions.checkState(\n+        !executed.getAndSet(true),\n+        \"Rename attempted twice\");\n+    final StoreContext storeContext \u003d getStoreContext();\n+    final MetadataStore metadataStore \u003d checkNotNull(\n+        storeContext.getMetadataStore(),\n+        \"No metadata store in context\");\n \n-    // the full FS\n-    S3AFileSystem fs \u003d getFileSystem();\n-    fs.delete(basePath, true);\n+    // Validation completed: time to begin the operation.\n+    // The store-specific rename tracker is used to keep the store\n+    // to date with the in-progress operation.\n+    // for the null store, these are all no-ops.\n+    renameTracker \u003d metadataStore.initiateRenameOperation(\n+        storeContext,\n+        sourcePath, sourceStatus, destPath);\n \n-    // this file is readable by the roleFS, but cannot be deleted\n-    touch(fs, readOnlyFile);\n \n-    bindRolePolicyStatements(conf,\n-        STATEMENT_S3GUARD_CLIENT,\n-        STATEMENT_ALL_BUCKET_READ_ACCESS,\n-        new Statement(Effects.Allow)\n-            .addActions(S3_PATH_RW_OPERATIONS)\n-            .addResources(directory(destDir))\n-    );\n-    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n+    // Ok! Time to start\n+    try {\n+      if (sourceStatus.isFile()) {\n+        renameFileToDest();\n+      } else {\n+        recursiveDirectoryRename();\n+      }\n+    } catch (AmazonClientException | IOException ex) {\n+      // rename failed.\n+      // block for all ongoing copies to complete, successfully or not\n+      try {\n+        completeActiveCopies(\"failure handling\");\n+      } catch (IOException e) {\n+        // a failure to update the metastore after a rename failure is what\n+        // we\u0027d see on a network problem, expired credentials and other\n+        // unrecoverable errors.\n+        // Downgrading to warn because an exception is already\n+        // about to be thrown.\n+        LOG.warn(\"While completing all active copies\", e);\n+      }\n+      // notify the rename tracker of the failure\n+      throw renameTracker.renameFailed(ex);\n+    }\n \n-    roleFS.delete(destDir, true);\n-    roleFS.mkdirs(destDir);\n-    // rename will fail in the delete phase\n-    forbidden(readOnlyFile.toString(),\n-        () -\u003e roleFS.rename(readOnlyFile, destDir));\n+    // At this point the rename has completed successfully in the S3 store.\n+    // Tell the metastore this fact and let it complete its changes\n+    renameTracker.completeRename();\n \n-    // and the source file is still there\n-    assertIsFile(readOnlyFile);\n-\n-    // but so is the copied version, because there\u0027s no attempt\n-    // at rollback, or preflight checking on the delete permissions\n-    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n-\n-    assertIsFile(renamedFile);\n-\n-    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n-    assertFileCount(\"Empty Dest Dir\", roleFS,\n-        destDir, 0);\n-    // create a set of files\n-    // this is done in parallel as it is 10x faster on a long-haul test run.\n-    int range \u003d 10;\n-    touchFiles(fs, readOnlyDir, range);\n-    // don\u0027t forget about that original file!\n-    final long createdFiles \u003d range + 1;\n-    // are they all there?\n-    assertFileCount(\"files ready to rename\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // try to rename the directory\n-    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n-    AccessDeniedException ex \u003d forbidden(\"\",\n-        () -\u003e roleFS.rename(readOnlyDir, destDir));\n-    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n-    assertFileCount(\"files copied to the destination\", roleFS,\n-        destDir, createdFiles);\n-    assertFileCount(\"files in the source directory\", roleFS,\n-        readOnlyDir, createdFiles);\n-\n-    // and finally (so as to avoid the delay of POSTing some more objects,\n-    // delete that r/o source\n-    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n+    callbacks.finishRename(sourcePath, destPath);\n+    return bytesCopied.get();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public long executeRename() throws IOException {\n    Preconditions.checkState(\n        !executed.getAndSet(true),\n        \"Rename attempted twice\");\n    final StoreContext storeContext \u003d getStoreContext();\n    final MetadataStore metadataStore \u003d checkNotNull(\n        storeContext.getMetadataStore(),\n        \"No metadata store in context\");\n\n    // Validation completed: time to begin the operation.\n    // The store-specific rename tracker is used to keep the store\n    // to date with the in-progress operation.\n    // for the null store, these are all no-ops.\n    renameTracker \u003d metadataStore.initiateRenameOperation(\n        storeContext,\n        sourcePath, sourceStatus, destPath);\n\n\n    // Ok! Time to start\n    try {\n      if (sourceStatus.isFile()) {\n        renameFileToDest();\n      } else {\n        recursiveDirectoryRename();\n      }\n    } catch (AmazonClientException | IOException ex) {\n      // rename failed.\n      // block for all ongoing copies to complete, successfully or not\n      try {\n        completeActiveCopies(\"failure handling\");\n      } catch (IOException e) {\n        // a failure to update the metastore after a rename failure is what\n        // we\u0027d see on a network problem, expired credentials and other\n        // unrecoverable errors.\n        // Downgrading to warn because an exception is already\n        // about to be thrown.\n        LOG.warn(\"While completing all active copies\", e);\n      }\n      // notify the rename tracker of the failure\n      throw renameTracker.renameFailed(ex);\n    }\n\n    // At this point the rename has completed successfully in the S3 store.\n    // Tell the metastore this fact and let it complete its changes\n    renameTracker.completeRename();\n\n    callbacks.finishRename(sourcePath, destPath);\n    return bytesCopied.get();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java",
          "extendedDetails": {
            "oldValue": "[conf-Configuration(modifiers-final)]",
            "newValue": "[]"
          }
        }
      ]
    },
    "6d0bffe17eadedd60d4599427248b0db4a7c5502": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14556. S3A to support Delegation Tokens.\n\nContributed by Steve Loughran and Daryn Sharp.\n",
      "commitDate": "14/01/19 9:59 AM",
      "commitName": "6d0bffe17eadedd60d4599427248b0db4a7c5502",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "07/01/19 9:51 PM",
      "commitNameOld": "7f783970364930cc461d1a73833bc58cdd10553e",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 6.51,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,66 @@\n   public void executeRenameReadOnlyData(final Configuration conf)\n       throws Exception {\n     assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n     Path basePath \u003d methodPath();\n     Path destDir \u003d new Path(basePath, \"renameDest\");\n     Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n     Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n \n     // the full FS\n     S3AFileSystem fs \u003d getFileSystem();\n     fs.delete(basePath, true);\n \n     // this file is readable by the roleFS, but cannot be deleted\n     touch(fs, readOnlyFile);\n \n     bindRolePolicyStatements(conf,\n         STATEMENT_S3GUARD_CLIENT,\n-        statement(true, S3_ALL_BUCKETS, S3_ROOT_READ_OPERATIONS),\n-          new Statement(Effects.Allow)\n+        STATEMENT_ALL_BUCKET_READ_ACCESS,\n+        new Statement(Effects.Allow)\n             .addActions(S3_PATH_RW_OPERATIONS)\n             .addResources(directory(destDir))\n     );\n     roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n \n     roleFS.delete(destDir, true);\n     roleFS.mkdirs(destDir);\n     // rename will fail in the delete phase\n     forbidden(readOnlyFile.toString(),\n         () -\u003e roleFS.rename(readOnlyFile, destDir));\n \n     // and the source file is still there\n     assertIsFile(readOnlyFile);\n \n     // but so is the copied version, because there\u0027s no attempt\n     // at rollback, or preflight checking on the delete permissions\n     Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n \n     assertIsFile(renamedFile);\n \n     ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n     assertFileCount(\"Empty Dest Dir\", roleFS,\n         destDir, 0);\n     // create a set of files\n     // this is done in parallel as it is 10x faster on a long-haul test run.\n     int range \u003d 10;\n     touchFiles(fs, readOnlyDir, range);\n     // don\u0027t forget about that original file!\n     final long createdFiles \u003d range + 1;\n     // are they all there?\n     assertFileCount(\"files ready to rename\", roleFS,\n         readOnlyDir, createdFiles);\n \n     // try to rename the directory\n     LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n     AccessDeniedException ex \u003d forbidden(\"\",\n         () -\u003e roleFS.rename(readOnlyDir, destDir));\n     LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n     assertFileCount(\"files copied to the destination\", roleFS,\n         destDir, createdFiles);\n     assertFileCount(\"files in the source directory\", roleFS,\n         readOnlyDir, createdFiles);\n \n     // and finally (so as to avoid the delay of POSTing some more objects,\n     // delete that r/o source\n     forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void executeRenameReadOnlyData(final Configuration conf)\n      throws Exception {\n    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n    Path basePath \u003d methodPath();\n    Path destDir \u003d new Path(basePath, \"renameDest\");\n    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n\n    // the full FS\n    S3AFileSystem fs \u003d getFileSystem();\n    fs.delete(basePath, true);\n\n    // this file is readable by the roleFS, but cannot be deleted\n    touch(fs, readOnlyFile);\n\n    bindRolePolicyStatements(conf,\n        STATEMENT_S3GUARD_CLIENT,\n        STATEMENT_ALL_BUCKET_READ_ACCESS,\n        new Statement(Effects.Allow)\n            .addActions(S3_PATH_RW_OPERATIONS)\n            .addResources(directory(destDir))\n    );\n    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n\n    roleFS.delete(destDir, true);\n    roleFS.mkdirs(destDir);\n    // rename will fail in the delete phase\n    forbidden(readOnlyFile.toString(),\n        () -\u003e roleFS.rename(readOnlyFile, destDir));\n\n    // and the source file is still there\n    assertIsFile(readOnlyFile);\n\n    // but so is the copied version, because there\u0027s no attempt\n    // at rollback, or preflight checking on the delete permissions\n    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n\n    assertIsFile(renamedFile);\n\n    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n    assertFileCount(\"Empty Dest Dir\", roleFS,\n        destDir, 0);\n    // create a set of files\n    // this is done in parallel as it is 10x faster on a long-haul test run.\n    int range \u003d 10;\n    touchFiles(fs, readOnlyDir, range);\n    // don\u0027t forget about that original file!\n    final long createdFiles \u003d range + 1;\n    // are they all there?\n    assertFileCount(\"files ready to rename\", roleFS,\n        readOnlyDir, createdFiles);\n\n    // try to rename the directory\n    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n    AccessDeniedException ex \u003d forbidden(\"\",\n        () -\u003e roleFS.rename(readOnlyDir, destDir));\n    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n    assertFileCount(\"files copied to the destination\", roleFS,\n        destDir, createdFiles);\n    assertFileCount(\"files in the source directory\", roleFS,\n        readOnlyDir, createdFiles);\n\n    // and finally (so as to avoid the delay of POSTing some more objects,\n    // delete that r/o source\n    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n  }",
      "path": "hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java",
      "extendedDetails": {}
    },
    "7f783970364930cc461d1a73833bc58cdd10553e": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HADOOP-14556. S3A to support Delegation Tokens.\"\n\nThis reverts commit d7152332b32a575c3a92e3f4c44b95e58462528d.\n",
      "commitDate": "07/01/19 9:51 PM",
      "commitName": "7f783970364930cc461d1a73833bc58cdd10553e",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "07/01/19 5:18 AM",
      "commitNameOld": "d7152332b32a575c3a92e3f4c44b95e58462528d",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 0.69,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,66 @@\n   public void executeRenameReadOnlyData(final Configuration conf)\n       throws Exception {\n     assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n     Path basePath \u003d methodPath();\n     Path destDir \u003d new Path(basePath, \"renameDest\");\n     Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n     Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n \n     // the full FS\n     S3AFileSystem fs \u003d getFileSystem();\n     fs.delete(basePath, true);\n \n     // this file is readable by the roleFS, but cannot be deleted\n     touch(fs, readOnlyFile);\n \n     bindRolePolicyStatements(conf,\n         STATEMENT_S3GUARD_CLIENT,\n-        STATEMENT_ALL_BUCKET_READ_ACCESS,\n-        new Statement(Effects.Allow)\n+        statement(true, S3_ALL_BUCKETS, S3_ROOT_READ_OPERATIONS),\n+          new Statement(Effects.Allow)\n             .addActions(S3_PATH_RW_OPERATIONS)\n             .addResources(directory(destDir))\n     );\n     roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n \n     roleFS.delete(destDir, true);\n     roleFS.mkdirs(destDir);\n     // rename will fail in the delete phase\n     forbidden(readOnlyFile.toString(),\n         () -\u003e roleFS.rename(readOnlyFile, destDir));\n \n     // and the source file is still there\n     assertIsFile(readOnlyFile);\n \n     // but so is the copied version, because there\u0027s no attempt\n     // at rollback, or preflight checking on the delete permissions\n     Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n \n     assertIsFile(renamedFile);\n \n     ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n     assertFileCount(\"Empty Dest Dir\", roleFS,\n         destDir, 0);\n     // create a set of files\n     // this is done in parallel as it is 10x faster on a long-haul test run.\n     int range \u003d 10;\n     touchFiles(fs, readOnlyDir, range);\n     // don\u0027t forget about that original file!\n     final long createdFiles \u003d range + 1;\n     // are they all there?\n     assertFileCount(\"files ready to rename\", roleFS,\n         readOnlyDir, createdFiles);\n \n     // try to rename the directory\n     LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n     AccessDeniedException ex \u003d forbidden(\"\",\n         () -\u003e roleFS.rename(readOnlyDir, destDir));\n     LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n     assertFileCount(\"files copied to the destination\", roleFS,\n         destDir, createdFiles);\n     assertFileCount(\"files in the source directory\", roleFS,\n         readOnlyDir, createdFiles);\n \n     // and finally (so as to avoid the delay of POSTing some more objects,\n     // delete that r/o source\n     forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void executeRenameReadOnlyData(final Configuration conf)\n      throws Exception {\n    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n    Path basePath \u003d methodPath();\n    Path destDir \u003d new Path(basePath, \"renameDest\");\n    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n\n    // the full FS\n    S3AFileSystem fs \u003d getFileSystem();\n    fs.delete(basePath, true);\n\n    // this file is readable by the roleFS, but cannot be deleted\n    touch(fs, readOnlyFile);\n\n    bindRolePolicyStatements(conf,\n        STATEMENT_S3GUARD_CLIENT,\n        statement(true, S3_ALL_BUCKETS, S3_ROOT_READ_OPERATIONS),\n          new Statement(Effects.Allow)\n            .addActions(S3_PATH_RW_OPERATIONS)\n            .addResources(directory(destDir))\n    );\n    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n\n    roleFS.delete(destDir, true);\n    roleFS.mkdirs(destDir);\n    // rename will fail in the delete phase\n    forbidden(readOnlyFile.toString(),\n        () -\u003e roleFS.rename(readOnlyFile, destDir));\n\n    // and the source file is still there\n    assertIsFile(readOnlyFile);\n\n    // but so is the copied version, because there\u0027s no attempt\n    // at rollback, or preflight checking on the delete permissions\n    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n\n    assertIsFile(renamedFile);\n\n    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n    assertFileCount(\"Empty Dest Dir\", roleFS,\n        destDir, 0);\n    // create a set of files\n    // this is done in parallel as it is 10x faster on a long-haul test run.\n    int range \u003d 10;\n    touchFiles(fs, readOnlyDir, range);\n    // don\u0027t forget about that original file!\n    final long createdFiles \u003d range + 1;\n    // are they all there?\n    assertFileCount(\"files ready to rename\", roleFS,\n        readOnlyDir, createdFiles);\n\n    // try to rename the directory\n    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n    AccessDeniedException ex \u003d forbidden(\"\",\n        () -\u003e roleFS.rename(readOnlyDir, destDir));\n    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n    assertFileCount(\"files copied to the destination\", roleFS,\n        destDir, createdFiles);\n    assertFileCount(\"files in the source directory\", roleFS,\n        readOnlyDir, createdFiles);\n\n    // and finally (so as to avoid the delay of POSTing some more objects,\n    // delete that r/o source\n    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n  }",
      "path": "hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java",
      "extendedDetails": {}
    },
    "d7152332b32a575c3a92e3f4c44b95e58462528d": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14556. S3A to support Delegation Tokens.\n\nContributed by Steve Loughran.\n",
      "commitDate": "07/01/19 5:18 AM",
      "commitName": "d7152332b32a575c3a92e3f4c44b95e58462528d",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "24/09/18 12:53 PM",
      "commitNameOld": "c07715e37895a2e40602737fe3695746789ac78f",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 104.73,
      "commitsBetweenForRepo": 789,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,66 @@\n   public void executeRenameReadOnlyData(final Configuration conf)\n       throws Exception {\n     assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n     Path basePath \u003d methodPath();\n     Path destDir \u003d new Path(basePath, \"renameDest\");\n     Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n     Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n \n     // the full FS\n     S3AFileSystem fs \u003d getFileSystem();\n     fs.delete(basePath, true);\n \n     // this file is readable by the roleFS, but cannot be deleted\n     touch(fs, readOnlyFile);\n \n     bindRolePolicyStatements(conf,\n         STATEMENT_S3GUARD_CLIENT,\n-        statement(true, S3_ALL_BUCKETS, S3_ROOT_READ_OPERATIONS),\n-          new Statement(Effects.Allow)\n+        STATEMENT_ALL_BUCKET_READ_ACCESS,\n+        new Statement(Effects.Allow)\n             .addActions(S3_PATH_RW_OPERATIONS)\n             .addResources(directory(destDir))\n     );\n     roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n \n     roleFS.delete(destDir, true);\n     roleFS.mkdirs(destDir);\n     // rename will fail in the delete phase\n     forbidden(readOnlyFile.toString(),\n         () -\u003e roleFS.rename(readOnlyFile, destDir));\n \n     // and the source file is still there\n     assertIsFile(readOnlyFile);\n \n     // but so is the copied version, because there\u0027s no attempt\n     // at rollback, or preflight checking on the delete permissions\n     Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n \n     assertIsFile(renamedFile);\n \n     ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n     assertFileCount(\"Empty Dest Dir\", roleFS,\n         destDir, 0);\n     // create a set of files\n     // this is done in parallel as it is 10x faster on a long-haul test run.\n     int range \u003d 10;\n     touchFiles(fs, readOnlyDir, range);\n     // don\u0027t forget about that original file!\n     final long createdFiles \u003d range + 1;\n     // are they all there?\n     assertFileCount(\"files ready to rename\", roleFS,\n         readOnlyDir, createdFiles);\n \n     // try to rename the directory\n     LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n     AccessDeniedException ex \u003d forbidden(\"\",\n         () -\u003e roleFS.rename(readOnlyDir, destDir));\n     LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n     assertFileCount(\"files copied to the destination\", roleFS,\n         destDir, createdFiles);\n     assertFileCount(\"files in the source directory\", roleFS,\n         readOnlyDir, createdFiles);\n \n     // and finally (so as to avoid the delay of POSTing some more objects,\n     // delete that r/o source\n     forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void executeRenameReadOnlyData(final Configuration conf)\n      throws Exception {\n    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n    Path basePath \u003d methodPath();\n    Path destDir \u003d new Path(basePath, \"renameDest\");\n    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n\n    // the full FS\n    S3AFileSystem fs \u003d getFileSystem();\n    fs.delete(basePath, true);\n\n    // this file is readable by the roleFS, but cannot be deleted\n    touch(fs, readOnlyFile);\n\n    bindRolePolicyStatements(conf,\n        STATEMENT_S3GUARD_CLIENT,\n        STATEMENT_ALL_BUCKET_READ_ACCESS,\n        new Statement(Effects.Allow)\n            .addActions(S3_PATH_RW_OPERATIONS)\n            .addResources(directory(destDir))\n    );\n    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n\n    roleFS.delete(destDir, true);\n    roleFS.mkdirs(destDir);\n    // rename will fail in the delete phase\n    forbidden(readOnlyFile.toString(),\n        () -\u003e roleFS.rename(readOnlyFile, destDir));\n\n    // and the source file is still there\n    assertIsFile(readOnlyFile);\n\n    // but so is the copied version, because there\u0027s no attempt\n    // at rollback, or preflight checking on the delete permissions\n    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n\n    assertIsFile(renamedFile);\n\n    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n    assertFileCount(\"Empty Dest Dir\", roleFS,\n        destDir, 0);\n    // create a set of files\n    // this is done in parallel as it is 10x faster on a long-haul test run.\n    int range \u003d 10;\n    touchFiles(fs, readOnlyDir, range);\n    // don\u0027t forget about that original file!\n    final long createdFiles \u003d range + 1;\n    // are they all there?\n    assertFileCount(\"files ready to rename\", roleFS,\n        readOnlyDir, createdFiles);\n\n    // try to rename the directory\n    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n    AccessDeniedException ex \u003d forbidden(\"\",\n        () -\u003e roleFS.rename(readOnlyDir, destDir));\n    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n    assertFileCount(\"files copied to the destination\", roleFS,\n        destDir, createdFiles);\n    assertFileCount(\"files in the source directory\", roleFS,\n        readOnlyDir, createdFiles);\n\n    // and finally (so as to avoid the delay of POSTing some more objects,\n    // delete that r/o source\n    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n  }",
      "path": "hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java",
      "extendedDetails": {}
    },
    "da9a39eed138210de29b59b90c449b28da1c04f9": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15583. Stabilize S3A Assumed Role support.\nContributed by Steve Loughran.\n",
      "commitDate": "08/08/18 10:57 PM",
      "commitName": "da9a39eed138210de29b59b90c449b28da1c04f9",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "16/02/18 8:37 AM",
      "commitNameOld": "7ac88244c54ce483729af3d2736d9f4731e230ca",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 173.56,
      "commitsBetweenForRepo": 1828,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,66 @@\n   public void executeRenameReadOnlyData(final Configuration conf)\n       throws Exception {\n     assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n     Path basePath \u003d methodPath();\n     Path destDir \u003d new Path(basePath, \"renameDest\");\n     Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n     Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n \n     // the full FS\n     S3AFileSystem fs \u003d getFileSystem();\n     fs.delete(basePath, true);\n \n     // this file is readable by the roleFS, but cannot be deleted\n     touch(fs, readOnlyFile);\n \n     bindRolePolicyStatements(conf,\n-        STATEMENT_ALL_DDB,\n+        STATEMENT_S3GUARD_CLIENT,\n         statement(true, S3_ALL_BUCKETS, S3_ROOT_READ_OPERATIONS),\n           new Statement(Effects.Allow)\n             .addActions(S3_PATH_RW_OPERATIONS)\n             .addResources(directory(destDir))\n     );\n     roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n \n     roleFS.delete(destDir, true);\n     roleFS.mkdirs(destDir);\n     // rename will fail in the delete phase\n     forbidden(readOnlyFile.toString(),\n         () -\u003e roleFS.rename(readOnlyFile, destDir));\n \n     // and the source file is still there\n     assertIsFile(readOnlyFile);\n \n     // but so is the copied version, because there\u0027s no attempt\n     // at rollback, or preflight checking on the delete permissions\n     Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n \n     assertIsFile(renamedFile);\n \n     ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n     assertFileCount(\"Empty Dest Dir\", roleFS,\n         destDir, 0);\n     // create a set of files\n     // this is done in parallel as it is 10x faster on a long-haul test run.\n     int range \u003d 10;\n     touchFiles(fs, readOnlyDir, range);\n     // don\u0027t forget about that original file!\n     final long createdFiles \u003d range + 1;\n     // are they all there?\n     assertFileCount(\"files ready to rename\", roleFS,\n         readOnlyDir, createdFiles);\n \n     // try to rename the directory\n     LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n     AccessDeniedException ex \u003d forbidden(\"\",\n         () -\u003e roleFS.rename(readOnlyDir, destDir));\n     LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n     assertFileCount(\"files copied to the destination\", roleFS,\n         destDir, createdFiles);\n     assertFileCount(\"files in the source directory\", roleFS,\n         readOnlyDir, createdFiles);\n \n     // and finally (so as to avoid the delay of POSTing some more objects,\n     // delete that r/o source\n     forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void executeRenameReadOnlyData(final Configuration conf)\n      throws Exception {\n    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n    Path basePath \u003d methodPath();\n    Path destDir \u003d new Path(basePath, \"renameDest\");\n    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n\n    // the full FS\n    S3AFileSystem fs \u003d getFileSystem();\n    fs.delete(basePath, true);\n\n    // this file is readable by the roleFS, but cannot be deleted\n    touch(fs, readOnlyFile);\n\n    bindRolePolicyStatements(conf,\n        STATEMENT_S3GUARD_CLIENT,\n        statement(true, S3_ALL_BUCKETS, S3_ROOT_READ_OPERATIONS),\n          new Statement(Effects.Allow)\n            .addActions(S3_PATH_RW_OPERATIONS)\n            .addResources(directory(destDir))\n    );\n    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n\n    roleFS.delete(destDir, true);\n    roleFS.mkdirs(destDir);\n    // rename will fail in the delete phase\n    forbidden(readOnlyFile.toString(),\n        () -\u003e roleFS.rename(readOnlyFile, destDir));\n\n    // and the source file is still there\n    assertIsFile(readOnlyFile);\n\n    // but so is the copied version, because there\u0027s no attempt\n    // at rollback, or preflight checking on the delete permissions\n    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n\n    assertIsFile(renamedFile);\n\n    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n    assertFileCount(\"Empty Dest Dir\", roleFS,\n        destDir, 0);\n    // create a set of files\n    // this is done in parallel as it is 10x faster on a long-haul test run.\n    int range \u003d 10;\n    touchFiles(fs, readOnlyDir, range);\n    // don\u0027t forget about that original file!\n    final long createdFiles \u003d range + 1;\n    // are they all there?\n    assertFileCount(\"files ready to rename\", roleFS,\n        readOnlyDir, createdFiles);\n\n    // try to rename the directory\n    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n    AccessDeniedException ex \u003d forbidden(\"\",\n        () -\u003e roleFS.rename(readOnlyDir, destDir));\n    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n    assertFileCount(\"files copied to the destination\", roleFS,\n        destDir, createdFiles);\n    assertFileCount(\"files in the source directory\", roleFS,\n        readOnlyDir, createdFiles);\n\n    // and finally (so as to avoid the delay of POSTing some more objects,\n    // delete that r/o source\n    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n  }",
      "path": "hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java",
      "extendedDetails": {}
    },
    "9a013b255f301c557c3868dc1ad657202e9e7a67": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-15176. Enhance IAM Assumed Role support in S3A client.\nContributed by Steve Loughran\n\n(cherry picked from commit 96c047fbb98c2378eed9693a724d4cbbd03c00fd)\n",
      "commitDate": "15/02/18 7:57 AM",
      "commitName": "9a013b255f301c557c3868dc1ad657202e9e7a67",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,66 @@\n+  public void executeRenameReadOnlyData(final Configuration conf)\n+      throws Exception {\n+    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n+    Path basePath \u003d methodPath();\n+    Path destDir \u003d new Path(basePath, \"renameDest\");\n+    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n+    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n+\n+    // the full FS\n+    S3AFileSystem fs \u003d getFileSystem();\n+    fs.delete(basePath, true);\n+\n+    // this file is readable by the roleFS, but cannot be deleted\n+    touch(fs, readOnlyFile);\n+\n+    bindRolePolicyStatements(conf,\n+        STATEMENT_ALL_DDB,\n+        statement(true, S3_ALL_BUCKETS, S3_ROOT_READ_OPERATIONS),\n+          new Statement(Effects.Allow)\n+            .addActions(S3_PATH_RW_OPERATIONS)\n+            .addResources(directory(destDir))\n+    );\n+    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n+\n+    roleFS.delete(destDir, true);\n+    roleFS.mkdirs(destDir);\n+    // rename will fail in the delete phase\n+    forbidden(readOnlyFile.toString(),\n+        () -\u003e roleFS.rename(readOnlyFile, destDir));\n+\n+    // and the source file is still there\n+    assertIsFile(readOnlyFile);\n+\n+    // but so is the copied version, because there\u0027s no attempt\n+    // at rollback, or preflight checking on the delete permissions\n+    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n+\n+    assertIsFile(renamedFile);\n+\n+    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n+    assertFileCount(\"Empty Dest Dir\", roleFS,\n+        destDir, 0);\n+    // create a set of files\n+    // this is done in parallel as it is 10x faster on a long-haul test run.\n+    int range \u003d 10;\n+    touchFiles(fs, readOnlyDir, range);\n+    // don\u0027t forget about that original file!\n+    final long createdFiles \u003d range + 1;\n+    // are they all there?\n+    assertFileCount(\"files ready to rename\", roleFS,\n+        readOnlyDir, createdFiles);\n+\n+    // try to rename the directory\n+    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n+    AccessDeniedException ex \u003d forbidden(\"\",\n+        () -\u003e roleFS.rename(readOnlyDir, destDir));\n+    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n+    assertFileCount(\"files copied to the destination\", roleFS,\n+        destDir, createdFiles);\n+    assertFileCount(\"files in the source directory\", roleFS,\n+        readOnlyDir, createdFiles);\n+\n+    // and finally (so as to avoid the delay of POSTing some more objects,\n+    // delete that r/o source\n+    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void executeRenameReadOnlyData(final Configuration conf)\n      throws Exception {\n    assume(\"Does not work with S3Guard\", !getFileSystem().hasMetadataStore());\n    Path basePath \u003d methodPath();\n    Path destDir \u003d new Path(basePath, \"renameDest\");\n    Path readOnlyDir \u003d new Path(basePath, \"readonlyDir\");\n    Path readOnlyFile \u003d new Path(readOnlyDir, \"readonlyChild\");\n\n    // the full FS\n    S3AFileSystem fs \u003d getFileSystem();\n    fs.delete(basePath, true);\n\n    // this file is readable by the roleFS, but cannot be deleted\n    touch(fs, readOnlyFile);\n\n    bindRolePolicyStatements(conf,\n        STATEMENT_ALL_DDB,\n        statement(true, S3_ALL_BUCKETS, S3_ROOT_READ_OPERATIONS),\n          new Statement(Effects.Allow)\n            .addActions(S3_PATH_RW_OPERATIONS)\n            .addResources(directory(destDir))\n    );\n    roleFS \u003d (S3AFileSystem) destDir.getFileSystem(conf);\n\n    roleFS.delete(destDir, true);\n    roleFS.mkdirs(destDir);\n    // rename will fail in the delete phase\n    forbidden(readOnlyFile.toString(),\n        () -\u003e roleFS.rename(readOnlyFile, destDir));\n\n    // and the source file is still there\n    assertIsFile(readOnlyFile);\n\n    // but so is the copied version, because there\u0027s no attempt\n    // at rollback, or preflight checking on the delete permissions\n    Path renamedFile \u003d new Path(destDir, readOnlyFile.getName());\n\n    assertIsFile(renamedFile);\n\n    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\n    assertFileCount(\"Empty Dest Dir\", roleFS,\n        destDir, 0);\n    // create a set of files\n    // this is done in parallel as it is 10x faster on a long-haul test run.\n    int range \u003d 10;\n    touchFiles(fs, readOnlyDir, range);\n    // don\u0027t forget about that original file!\n    final long createdFiles \u003d range + 1;\n    // are they all there?\n    assertFileCount(\"files ready to rename\", roleFS,\n        readOnlyDir, createdFiles);\n\n    // try to rename the directory\n    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, destDir);\n    AccessDeniedException ex \u003d forbidden(\"\",\n        () -\u003e roleFS.rename(readOnlyDir, destDir));\n    LOG.info(\"Result of renaming read-only files is AccessDeniedException\", ex);\n    assertFileCount(\"files copied to the destination\", roleFS,\n        destDir, createdFiles);\n    assertFileCount(\"files in the source directory\", roleFS,\n        readOnlyDir, createdFiles);\n\n    // and finally (so as to avoid the delay of POSTing some more objects,\n    // delete that r/o source\n    forbidden(\"\", () -\u003e roleFS.delete(readOnlyDir, true));\n  }",
      "path": "hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java"
    }
  }
}