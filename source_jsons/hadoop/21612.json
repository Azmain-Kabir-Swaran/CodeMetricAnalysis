{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "JobSubmitter.java",
  "functionName": "submitJobInternal",
  "functionId": "submitJobInternal___job-Job__cluster-Cluster",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
  "functionStartLine": 139,
  "functionEndLine": 266,
  "numCommitsSeen": 41,
  "timeTaken": 8236,
  "changeHistory": [
    "fbb7d6bcbb887ce52ab1e9d5a1fed67a7f8a4be8",
    "21d2b90213e8e188bcac31f9360c5176ac89d083",
    "6b710a42e00acca405e085724c89cda016cf7442",
    "e8a31f2e1c34514fba2f480e8db652f6e2ed65d8",
    "3f282762d1afc916de9207d3adeda852ca344853",
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
    "db06f1bcb98270cd1c36e314f818886f1ef7fd77",
    "e60b057c0a1da225c633fc1d7acc37043b482616",
    "2166fa201ed9ee22d613d2edbac1e2af4e71ab76",
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865",
    "b03023110805a3479ef6b42f7c095de3f1a883e2",
    "8aabd3d4e67cad8dc7e46f5339981135badc7421",
    "51a667bef8300d6499c9867b50eee352311a4185",
    "d0016c6120a37d5056e3c9e72b67588d3e793ac1",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "fbb7d6bcbb887ce52ab1e9d5a1fed67a7f8a4be8": "Ybodychange",
    "21d2b90213e8e188bcac31f9360c5176ac89d083": "Ybodychange",
    "6b710a42e00acca405e085724c89cda016cf7442": "Ybodychange",
    "e8a31f2e1c34514fba2f480e8db652f6e2ed65d8": "Ybodychange",
    "3f282762d1afc916de9207d3adeda852ca344853": "Ybodychange",
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": "Ybodychange",
    "db06f1bcb98270cd1c36e314f818886f1ef7fd77": "Ybodychange",
    "e60b057c0a1da225c633fc1d7acc37043b482616": "Ybodychange",
    "2166fa201ed9ee22d613d2edbac1e2af4e71ab76": "Ybodychange",
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865": "Ybodychange",
    "b03023110805a3479ef6b42f7c095de3f1a883e2": "Ybodychange",
    "8aabd3d4e67cad8dc7e46f5339981135badc7421": "Ybodychange",
    "51a667bef8300d6499c9867b50eee352311a4185": "Ybodychange",
    "d0016c6120a37d5056e3c9e72b67588d3e793ac1": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "fbb7d6bcbb887ce52ab1e9d5a1fed67a7f8a4be8": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"MAPREDUCE-5875. Make Counter limits consistent across JobClient, MRAppMaster, and YarnChild. (Gera Shegalov via kasha)\"\n\nThis reverts commit e8a31f2e1c34514fba2f480e8db652f6e2ed65d8.\n",
      "commitDate": "31/07/17 2:09 PM",
      "commitName": "fbb7d6bcbb887ce52ab1e9d5a1fed67a7f8a4be8",
      "commitAuthor": "Junping Du",
      "commitDateOld": "29/03/17 10:10 PM",
      "commitNameOld": "de7efd2687ccb74608249abf4c83c84a9345ec0a",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 123.67,
      "commitsBetweenForRepo": 627,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,129 +1,128 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n \n     Configuration conf \u003d job.getConfiguration();\n     addMRFrameworkToDistributedCache(conf);\n \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n     //configure the command line options correctly on the submitting dfs\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n       conf.set(MRJobConfig.USER_NAME,\n           UserGroupInformation.getCurrentUser().getShortUserName());\n       conf.set(\"hadoop.http.filter.initializers\", \n           \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       // generate a secret to authenticate shuffle transfers\n       if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n         KeyGenerator keyGen;\n         try {\n           keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n           keyGen.init(SHUFFLE_KEY_LENGTH);\n         } catch (NoSuchAlgorithmException e) {\n           throw new IOException(\"Error generating shuffle secret key\", e);\n         }\n         SecretKey shuffleKey \u003d keyGen.generateKey();\n         TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n             job.getCredentials());\n       }\n       if (CryptoUtils.isEncryptedSpillEnabled(conf)) {\n         conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);\n         LOG.warn(\"Max job attempts set to 1 since encrypted intermediate\" +\n                 \"data spill is enabled\");\n       }\n \n       copyAndConfigureFiles(job, submitJobDir);\n \n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n       \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       int maxMaps \u003d conf.getInt(MRJobConfig.JOB_MAX_MAP,\n           MRJobConfig.DEFAULT_JOB_MAX_MAP);\n       if (maxMaps \u003e\u003d 0 \u0026\u0026 maxMaps \u003c maps) {\n         throw new IllegalArgumentException(\"The number of map tasks \" + maps +\n             \" exceeded limit \" + maxMaps);\n       }\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // removing jobtoken referrals before copying the jobconf to HDFS\n       // as the tasks don\u0027t need this setting, actually they may break\n       // because of it if present as the referral will point to a\n       // different job.\n       TokenCache.cleanUpTokenReferral(conf);\n \n       if (conf.getBoolean(\n           MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n           MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n         // Add HDFS tracking ids\n         ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n         for (Token\u003c? extends TokenIdentifier\u003e t :\n             job.getCredentials().getAllTokens()) {\n           trackingIds.add(t.decodeIdentifier().getTrackingId());\n         }\n         conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n             trackingIds.toArray(new String[trackingIds.size()]));\n       }\n \n       // Set reservation info if it exists\n       ReservationId reservationId \u003d job.getReservationId();\n       if (reservationId !\u003d null) {\n         conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\n       }\n \n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n-      Limits.reset(conf);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n\n    Configuration conf \u003d job.getConfiguration();\n    addMRFrameworkToDistributedCache(conf);\n\n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n    //configure the command line options correctly on the submitting dfs\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(MRJobConfig.USER_NAME,\n          UserGroupInformation.getCurrentUser().getShortUserName());\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      // generate a secret to authenticate shuffle transfers\n      if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n        KeyGenerator keyGen;\n        try {\n          keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n          keyGen.init(SHUFFLE_KEY_LENGTH);\n        } catch (NoSuchAlgorithmException e) {\n          throw new IOException(\"Error generating shuffle secret key\", e);\n        }\n        SecretKey shuffleKey \u003d keyGen.generateKey();\n        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n            job.getCredentials());\n      }\n      if (CryptoUtils.isEncryptedSpillEnabled(conf)) {\n        conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);\n        LOG.warn(\"Max job attempts set to 1 since encrypted intermediate\" +\n                \"data spill is enabled\");\n      }\n\n      copyAndConfigureFiles(job, submitJobDir);\n\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      int maxMaps \u003d conf.getInt(MRJobConfig.JOB_MAX_MAP,\n          MRJobConfig.DEFAULT_JOB_MAX_MAP);\n      if (maxMaps \u003e\u003d 0 \u0026\u0026 maxMaps \u003c maps) {\n        throw new IllegalArgumentException(\"The number of map tasks \" + maps +\n            \" exceeded limit \" + maxMaps);\n      }\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don\u0027t need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      if (conf.getBoolean(\n          MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n          MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n        // Add HDFS tracking ids\n        ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n        for (Token\u003c? extends TokenIdentifier\u003e t :\n            job.getCredentials().getAllTokens()) {\n          trackingIds.add(t.decodeIdentifier().getTrackingId());\n        }\n        conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n            trackingIds.toArray(new String[trackingIds.size()]));\n      }\n\n      // Set reservation info if it exists\n      ReservationId reservationId \u003d job.getReservationId();\n      if (reservationId !\u003d null) {\n        conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\n      }\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "21d2b90213e8e188bcac31f9360c5176ac89d083": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6696. Add a configuration to limit the number of map tasks allowed per job. Contributed by Zhihai Xu\n",
      "commitDate": "19/05/16 7:25 PM",
      "commitName": "21d2b90213e8e188bcac31f9360c5176ac89d083",
      "commitAuthor": "Jian He",
      "commitDateOld": "09/02/16 10:05 AM",
      "commitNameOld": "a0b1f10a30dc2736cc136f257b0d3bf0140158bb",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 100.35,
      "commitsBetweenForRepo": 647,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,122 +1,129 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n \n     Configuration conf \u003d job.getConfiguration();\n     addMRFrameworkToDistributedCache(conf);\n \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n     //configure the command line options correctly on the submitting dfs\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n       conf.set(MRJobConfig.USER_NAME,\n           UserGroupInformation.getCurrentUser().getShortUserName());\n       conf.set(\"hadoop.http.filter.initializers\", \n           \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       // generate a secret to authenticate shuffle transfers\n       if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n         KeyGenerator keyGen;\n         try {\n           keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n           keyGen.init(SHUFFLE_KEY_LENGTH);\n         } catch (NoSuchAlgorithmException e) {\n           throw new IOException(\"Error generating shuffle secret key\", e);\n         }\n         SecretKey shuffleKey \u003d keyGen.generateKey();\n         TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n             job.getCredentials());\n       }\n       if (CryptoUtils.isEncryptedSpillEnabled(conf)) {\n         conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);\n         LOG.warn(\"Max job attempts set to 1 since encrypted intermediate\" +\n                 \"data spill is enabled\");\n       }\n \n       copyAndConfigureFiles(job, submitJobDir);\n \n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n       \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n+      int maxMaps \u003d conf.getInt(MRJobConfig.JOB_MAX_MAP,\n+          MRJobConfig.DEFAULT_JOB_MAX_MAP);\n+      if (maxMaps \u003e\u003d 0 \u0026\u0026 maxMaps \u003c maps) {\n+        throw new IllegalArgumentException(\"The number of map tasks \" + maps +\n+            \" exceeded limit \" + maxMaps);\n+      }\n+\n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // removing jobtoken referrals before copying the jobconf to HDFS\n       // as the tasks don\u0027t need this setting, actually they may break\n       // because of it if present as the referral will point to a\n       // different job.\n       TokenCache.cleanUpTokenReferral(conf);\n \n       if (conf.getBoolean(\n           MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n           MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n         // Add HDFS tracking ids\n         ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n         for (Token\u003c? extends TokenIdentifier\u003e t :\n             job.getCredentials().getAllTokens()) {\n           trackingIds.add(t.decodeIdentifier().getTrackingId());\n         }\n         conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n             trackingIds.toArray(new String[trackingIds.size()]));\n       }\n \n       // Set reservation info if it exists\n       ReservationId reservationId \u003d job.getReservationId();\n       if (reservationId !\u003d null) {\n         conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\n       }\n \n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       Limits.reset(conf);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n\n    Configuration conf \u003d job.getConfiguration();\n    addMRFrameworkToDistributedCache(conf);\n\n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n    //configure the command line options correctly on the submitting dfs\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(MRJobConfig.USER_NAME,\n          UserGroupInformation.getCurrentUser().getShortUserName());\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      // generate a secret to authenticate shuffle transfers\n      if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n        KeyGenerator keyGen;\n        try {\n          keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n          keyGen.init(SHUFFLE_KEY_LENGTH);\n        } catch (NoSuchAlgorithmException e) {\n          throw new IOException(\"Error generating shuffle secret key\", e);\n        }\n        SecretKey shuffleKey \u003d keyGen.generateKey();\n        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n            job.getCredentials());\n      }\n      if (CryptoUtils.isEncryptedSpillEnabled(conf)) {\n        conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);\n        LOG.warn(\"Max job attempts set to 1 since encrypted intermediate\" +\n                \"data spill is enabled\");\n      }\n\n      copyAndConfigureFiles(job, submitJobDir);\n\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      int maxMaps \u003d conf.getInt(MRJobConfig.JOB_MAX_MAP,\n          MRJobConfig.DEFAULT_JOB_MAX_MAP);\n      if (maxMaps \u003e\u003d 0 \u0026\u0026 maxMaps \u003c maps) {\n        throw new IllegalArgumentException(\"The number of map tasks \" + maps +\n            \" exceeded limit \" + maxMaps);\n      }\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don\u0027t need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      if (conf.getBoolean(\n          MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n          MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n        // Add HDFS tracking ids\n        ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n        for (Token\u003c? extends TokenIdentifier\u003e t :\n            job.getCredentials().getAllTokens()) {\n          trackingIds.add(t.decodeIdentifier().getTrackingId());\n        }\n        conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n            trackingIds.toArray(new String[trackingIds.size()]));\n      }\n\n      // Set reservation info if it exists\n      ReservationId reservationId \u003d job.getReservationId();\n      if (reservationId !\u003d null) {\n        conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\n      }\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      Limits.reset(conf);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "6b710a42e00acca405e085724c89cda016cf7442": {
      "type": "Ybodychange",
      "commitMessage": "Fixing MR intermediate spills. Contributed by Arun Suresh.\n",
      "commitDate": "14/05/15 4:07 PM",
      "commitName": "6b710a42e00acca405e085724c89cda016cf7442",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "16/03/15 10:36 PM",
      "commitNameOld": "f222bde273cc10a38945dc31e85206a0c4f06a12",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 58.73,
      "commitsBetweenForRepo": 610,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,122 +1,122 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n \n     Configuration conf \u003d job.getConfiguration();\n     addMRFrameworkToDistributedCache(conf);\n \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n     //configure the command line options correctly on the submitting dfs\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n       conf.set(MRJobConfig.USER_NAME,\n           UserGroupInformation.getCurrentUser().getShortUserName());\n       conf.set(\"hadoop.http.filter.initializers\", \n           \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       // generate a secret to authenticate shuffle transfers\n       if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n         KeyGenerator keyGen;\n         try {\n-         \n-          int keyLen \u003d CryptoUtils.isShuffleEncrypted(conf) \n-              ? conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS, \n-                  MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS)\n-              : SHUFFLE_KEY_LENGTH;\n           keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n-          keyGen.init(keyLen);\n+          keyGen.init(SHUFFLE_KEY_LENGTH);\n         } catch (NoSuchAlgorithmException e) {\n           throw new IOException(\"Error generating shuffle secret key\", e);\n         }\n         SecretKey shuffleKey \u003d keyGen.generateKey();\n         TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n             job.getCredentials());\n       }\n+      if (CryptoUtils.isEncryptedSpillEnabled(conf)) {\n+        conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);\n+        LOG.warn(\"Max job attempts set to 1 since encrypted intermediate\" +\n+                \"data spill is enabled\");\n+      }\n \n       copyAndConfigureFiles(job, submitJobDir);\n \n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n       \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // removing jobtoken referrals before copying the jobconf to HDFS\n       // as the tasks don\u0027t need this setting, actually they may break\n       // because of it if present as the referral will point to a\n       // different job.\n       TokenCache.cleanUpTokenReferral(conf);\n \n       if (conf.getBoolean(\n           MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n           MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n         // Add HDFS tracking ids\n         ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n         for (Token\u003c? extends TokenIdentifier\u003e t :\n             job.getCredentials().getAllTokens()) {\n           trackingIds.add(t.decodeIdentifier().getTrackingId());\n         }\n         conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n             trackingIds.toArray(new String[trackingIds.size()]));\n       }\n \n       // Set reservation info if it exists\n       ReservationId reservationId \u003d job.getReservationId();\n       if (reservationId !\u003d null) {\n         conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\n       }\n \n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       Limits.reset(conf);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n\n    Configuration conf \u003d job.getConfiguration();\n    addMRFrameworkToDistributedCache(conf);\n\n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n    //configure the command line options correctly on the submitting dfs\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(MRJobConfig.USER_NAME,\n          UserGroupInformation.getCurrentUser().getShortUserName());\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      // generate a secret to authenticate shuffle transfers\n      if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n        KeyGenerator keyGen;\n        try {\n          keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n          keyGen.init(SHUFFLE_KEY_LENGTH);\n        } catch (NoSuchAlgorithmException e) {\n          throw new IOException(\"Error generating shuffle secret key\", e);\n        }\n        SecretKey shuffleKey \u003d keyGen.generateKey();\n        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n            job.getCredentials());\n      }\n      if (CryptoUtils.isEncryptedSpillEnabled(conf)) {\n        conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);\n        LOG.warn(\"Max job attempts set to 1 since encrypted intermediate\" +\n                \"data spill is enabled\");\n      }\n\n      copyAndConfigureFiles(job, submitJobDir);\n\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don\u0027t need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      if (conf.getBoolean(\n          MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n          MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n        // Add HDFS tracking ids\n        ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n        for (Token\u003c? extends TokenIdentifier\u003e t :\n            job.getCredentials().getAllTokens()) {\n          trackingIds.add(t.decodeIdentifier().getTrackingId());\n        }\n        conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n            trackingIds.toArray(new String[trackingIds.size()]));\n      }\n\n      // Set reservation info if it exists\n      ReservationId reservationId \u003d job.getReservationId();\n      if (reservationId !\u003d null) {\n        conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\n      }\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      Limits.reset(conf);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "e8a31f2e1c34514fba2f480e8db652f6e2ed65d8": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5875. Make Counter limits consistent across JobClient, MRAppMaster, and YarnChild. (Gera Shegalov via kasha)\n",
      "commitDate": "11/10/14 10:49 PM",
      "commitName": "e8a31f2e1c34514fba2f480e8db652f6e2ed65d8",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "03/10/14 3:42 PM",
      "commitNameOld": "3f282762d1afc916de9207d3adeda852ca344853",
      "commitAuthorOld": "subru",
      "daysBetweenCommits": 8.3,
      "commitsBetweenForRepo": 64,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,120 +1,121 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n \n     Configuration conf \u003d job.getConfiguration();\n     addMRFrameworkToDistributedCache(conf);\n \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n     //configure the command line options correctly on the submitting dfs\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n       conf.set(MRJobConfig.USER_NAME,\n           UserGroupInformation.getCurrentUser().getShortUserName());\n       conf.set(\"hadoop.http.filter.initializers\", \n           \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       // generate a secret to authenticate shuffle transfers\n       if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n         KeyGenerator keyGen;\n         try {\n          \n           int keyLen \u003d CryptoUtils.isShuffleEncrypted(conf) \n               ? conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS, \n                   MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS)\n               : SHUFFLE_KEY_LENGTH;\n           keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n           keyGen.init(keyLen);\n         } catch (NoSuchAlgorithmException e) {\n           throw new IOException(\"Error generating shuffle secret key\", e);\n         }\n         SecretKey shuffleKey \u003d keyGen.generateKey();\n         TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n             job.getCredentials());\n       }\n \n       copyAndConfigureFiles(job, submitJobDir);\n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n       \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // removing jobtoken referrals before copying the jobconf to HDFS\n       // as the tasks don\u0027t need this setting, actually they may break\n       // because of it if present as the referral will point to a\n       // different job.\n       TokenCache.cleanUpTokenReferral(conf);\n \n       if (conf.getBoolean(\n           MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n           MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n         // Add HDFS tracking ids\n         ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n         for (Token\u003c? extends TokenIdentifier\u003e t :\n             job.getCredentials().getAllTokens()) {\n           trackingIds.add(t.decodeIdentifier().getTrackingId());\n         }\n         conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n             trackingIds.toArray(new String[trackingIds.size()]));\n       }\n \n       // Set reservation info if it exists\n       ReservationId reservationId \u003d job.getReservationId();\n       if (reservationId !\u003d null) {\n         conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\n       }\n \n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n+      Limits.reset(conf);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n\n    Configuration conf \u003d job.getConfiguration();\n    addMRFrameworkToDistributedCache(conf);\n\n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n    //configure the command line options correctly on the submitting dfs\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(MRJobConfig.USER_NAME,\n          UserGroupInformation.getCurrentUser().getShortUserName());\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      // generate a secret to authenticate shuffle transfers\n      if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n        KeyGenerator keyGen;\n        try {\n         \n          int keyLen \u003d CryptoUtils.isShuffleEncrypted(conf) \n              ? conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS, \n                  MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS)\n              : SHUFFLE_KEY_LENGTH;\n          keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n          keyGen.init(keyLen);\n        } catch (NoSuchAlgorithmException e) {\n          throw new IOException(\"Error generating shuffle secret key\", e);\n        }\n        SecretKey shuffleKey \u003d keyGen.generateKey();\n        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n            job.getCredentials());\n      }\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don\u0027t need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      if (conf.getBoolean(\n          MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n          MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n        // Add HDFS tracking ids\n        ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n        for (Token\u003c? extends TokenIdentifier\u003e t :\n            job.getCredentials().getAllTokens()) {\n          trackingIds.add(t.decodeIdentifier().getTrackingId());\n        }\n        conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n            trackingIds.toArray(new String[trackingIds.size()]));\n      }\n\n      // Set reservation info if it exists\n      ReservationId reservationId \u003d job.getReservationId();\n      if (reservationId !\u003d null) {\n        conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\n      }\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      Limits.reset(conf);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "3f282762d1afc916de9207d3adeda852ca344853": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6103.Adding reservation APIs to MR resource manager delegate. Contributed by Subru Krishnan and Carlo Curino.\n(cherry picked from commit aa92dd45f2d8c89a8a17ad2e4449aa3ff08bc53a)\n",
      "commitDate": "03/10/14 3:42 PM",
      "commitName": "3f282762d1afc916de9207d3adeda852ca344853",
      "commitAuthor": "subru",
      "commitDateOld": "18/09/14 3:46 PM",
      "commitNameOld": "52945a33cc3fcb3b961cce0e7e3ca01291f2a223",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 15.0,
      "commitsBetweenForRepo": 181,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,120 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n \n     Configuration conf \u003d job.getConfiguration();\n     addMRFrameworkToDistributedCache(conf);\n \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n     //configure the command line options correctly on the submitting dfs\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n       conf.set(MRJobConfig.USER_NAME,\n           UserGroupInformation.getCurrentUser().getShortUserName());\n       conf.set(\"hadoop.http.filter.initializers\", \n           \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       // generate a secret to authenticate shuffle transfers\n       if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n         KeyGenerator keyGen;\n         try {\n          \n           int keyLen \u003d CryptoUtils.isShuffleEncrypted(conf) \n               ? conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS, \n                   MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS)\n               : SHUFFLE_KEY_LENGTH;\n           keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n           keyGen.init(keyLen);\n         } catch (NoSuchAlgorithmException e) {\n           throw new IOException(\"Error generating shuffle secret key\", e);\n         }\n         SecretKey shuffleKey \u003d keyGen.generateKey();\n         TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n             job.getCredentials());\n       }\n \n       copyAndConfigureFiles(job, submitJobDir);\n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n       \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // removing jobtoken referrals before copying the jobconf to HDFS\n       // as the tasks don\u0027t need this setting, actually they may break\n       // because of it if present as the referral will point to a\n       // different job.\n       TokenCache.cleanUpTokenReferral(conf);\n \n       if (conf.getBoolean(\n           MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n           MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n         // Add HDFS tracking ids\n         ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n         for (Token\u003c? extends TokenIdentifier\u003e t :\n             job.getCredentials().getAllTokens()) {\n           trackingIds.add(t.decodeIdentifier().getTrackingId());\n         }\n         conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n             trackingIds.toArray(new String[trackingIds.size()]));\n       }\n \n+      // Set reservation info if it exists\n+      ReservationId reservationId \u003d job.getReservationId();\n+      if (reservationId !\u003d null) {\n+        conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\n+      }\n+\n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n\n    Configuration conf \u003d job.getConfiguration();\n    addMRFrameworkToDistributedCache(conf);\n\n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n    //configure the command line options correctly on the submitting dfs\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(MRJobConfig.USER_NAME,\n          UserGroupInformation.getCurrentUser().getShortUserName());\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      // generate a secret to authenticate shuffle transfers\n      if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n        KeyGenerator keyGen;\n        try {\n         \n          int keyLen \u003d CryptoUtils.isShuffleEncrypted(conf) \n              ? conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS, \n                  MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS)\n              : SHUFFLE_KEY_LENGTH;\n          keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n          keyGen.init(keyLen);\n        } catch (NoSuchAlgorithmException e) {\n          throw new IOException(\"Error generating shuffle secret key\", e);\n        }\n        SecretKey shuffleKey \u003d keyGen.generateKey();\n        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n            job.getCredentials());\n      }\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don\u0027t need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      if (conf.getBoolean(\n          MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n          MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n        // Add HDFS tracking ids\n        ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n        for (Token\u003c? extends TokenIdentifier\u003e t :\n            job.getCredentials().getAllTokens()) {\n          trackingIds.add(t.decodeIdentifier().getTrackingId());\n        }\n        conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n            trackingIds.toArray(new String[trackingIds.size()]));\n      }\n\n      // Set reservation info if it exists\n      ReservationId reservationId \u003d job.getReservationId();\n      if (reservationId !\u003d null) {\n        conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\n      }\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5890. Support for encrypting Intermediate data and spills in local filesystem. (asuresh via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1609597 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/07/14 5:43 PM",
      "commitName": "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "01/10/13 3:34 PM",
      "commitNameOld": "db06f1bcb98270cd1c36e314f818886f1ef7fd77",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 282.09,
      "commitsBetweenForRepo": 1902,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,114 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n \n     Configuration conf \u003d job.getConfiguration();\n     addMRFrameworkToDistributedCache(conf);\n \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n     //configure the command line options correctly on the submitting dfs\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n       conf.set(MRJobConfig.USER_NAME,\n           UserGroupInformation.getCurrentUser().getShortUserName());\n       conf.set(\"hadoop.http.filter.initializers\", \n           \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       // generate a secret to authenticate shuffle transfers\n       if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n         KeyGenerator keyGen;\n         try {\n+         \n+          int keyLen \u003d CryptoUtils.isShuffleEncrypted(conf) \n+              ? conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS, \n+                  MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS)\n+              : SHUFFLE_KEY_LENGTH;\n           keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n-          keyGen.init(SHUFFLE_KEY_LENGTH);\n+          keyGen.init(keyLen);\n         } catch (NoSuchAlgorithmException e) {\n           throw new IOException(\"Error generating shuffle secret key\", e);\n         }\n         SecretKey shuffleKey \u003d keyGen.generateKey();\n         TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n             job.getCredentials());\n       }\n \n       copyAndConfigureFiles(job, submitJobDir);\n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n       \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // removing jobtoken referrals before copying the jobconf to HDFS\n       // as the tasks don\u0027t need this setting, actually they may break\n       // because of it if present as the referral will point to a\n       // different job.\n       TokenCache.cleanUpTokenReferral(conf);\n \n       if (conf.getBoolean(\n           MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n           MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n         // Add HDFS tracking ids\n         ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n         for (Token\u003c? extends TokenIdentifier\u003e t :\n             job.getCredentials().getAllTokens()) {\n           trackingIds.add(t.decodeIdentifier().getTrackingId());\n         }\n         conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n             trackingIds.toArray(new String[trackingIds.size()]));\n       }\n \n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n\n    Configuration conf \u003d job.getConfiguration();\n    addMRFrameworkToDistributedCache(conf);\n\n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n    //configure the command line options correctly on the submitting dfs\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(MRJobConfig.USER_NAME,\n          UserGroupInformation.getCurrentUser().getShortUserName());\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      // generate a secret to authenticate shuffle transfers\n      if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n        KeyGenerator keyGen;\n        try {\n         \n          int keyLen \u003d CryptoUtils.isShuffleEncrypted(conf) \n              ? conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS, \n                  MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS)\n              : SHUFFLE_KEY_LENGTH;\n          keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n          keyGen.init(keyLen);\n        } catch (NoSuchAlgorithmException e) {\n          throw new IOException(\"Error generating shuffle secret key\", e);\n        }\n        SecretKey shuffleKey \u003d keyGen.generateKey();\n        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n            job.getCredentials());\n      }\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don\u0027t need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      if (conf.getBoolean(\n          MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n          MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n        // Add HDFS tracking ids\n        ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n        for (Token\u003c? extends TokenIdentifier\u003e t :\n            job.getCredentials().getAllTokens()) {\n          trackingIds.add(t.decodeIdentifier().getTrackingId());\n        }\n        conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n            trackingIds.toArray(new String[trackingIds.size()]));\n      }\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "db06f1bcb98270cd1c36e314f818886f1ef7fd77": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4421. Run MapReduce framework via the distributed cache. Contributed by Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1528237 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/10/13 3:34 PM",
      "commitName": "db06f1bcb98270cd1c36e314f818886f1ef7fd77",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "16/09/13 5:17 AM",
      "commitNameOld": "e60b057c0a1da225c633fc1d7acc37043b482616",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 15.43,
      "commitsBetweenForRepo": 92,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,108 +1,109 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n-    \n-    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n-                                                     job.getConfiguration());\n-    //configure the command line options correctly on the submitting dfs\n+\n     Configuration conf \u003d job.getConfiguration();\n+    addMRFrameworkToDistributedCache(conf);\n+\n+    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n+    //configure the command line options correctly on the submitting dfs\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n       conf.set(MRJobConfig.USER_NAME,\n           UserGroupInformation.getCurrentUser().getShortUserName());\n       conf.set(\"hadoop.http.filter.initializers\", \n           \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       // generate a secret to authenticate shuffle transfers\n       if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n         KeyGenerator keyGen;\n         try {\n           keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n           keyGen.init(SHUFFLE_KEY_LENGTH);\n         } catch (NoSuchAlgorithmException e) {\n           throw new IOException(\"Error generating shuffle secret key\", e);\n         }\n         SecretKey shuffleKey \u003d keyGen.generateKey();\n         TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n             job.getCredentials());\n       }\n \n       copyAndConfigureFiles(job, submitJobDir);\n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n       \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // removing jobtoken referrals before copying the jobconf to HDFS\n       // as the tasks don\u0027t need this setting, actually they may break\n       // because of it if present as the referral will point to a\n       // different job.\n       TokenCache.cleanUpTokenReferral(conf);\n \n       if (conf.getBoolean(\n           MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n           MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n         // Add HDFS tracking ids\n         ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n         for (Token\u003c? extends TokenIdentifier\u003e t :\n             job.getCredentials().getAllTokens()) {\n           trackingIds.add(t.decodeIdentifier().getTrackingId());\n         }\n         conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n             trackingIds.toArray(new String[trackingIds.size()]));\n       }\n \n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n\n    Configuration conf \u003d job.getConfiguration();\n    addMRFrameworkToDistributedCache(conf);\n\n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, conf);\n    //configure the command line options correctly on the submitting dfs\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(MRJobConfig.USER_NAME,\n          UserGroupInformation.getCurrentUser().getShortUserName());\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      // generate a secret to authenticate shuffle transfers\n      if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n        KeyGenerator keyGen;\n        try {\n          keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n          keyGen.init(SHUFFLE_KEY_LENGTH);\n        } catch (NoSuchAlgorithmException e) {\n          throw new IOException(\"Error generating shuffle secret key\", e);\n        }\n        SecretKey shuffleKey \u003d keyGen.generateKey();\n        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n            job.getCredentials());\n      }\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don\u0027t need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      if (conf.getBoolean(\n          MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n          MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n        // Add HDFS tracking ids\n        ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n        for (Token\u003c? extends TokenIdentifier\u003e t :\n            job.getCredentials().getAllTokens()) {\n          trackingIds.add(t.decodeIdentifier().getTrackingId());\n        }\n        conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n            trackingIds.toArray(new String[trackingIds.size()]));\n      }\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "e60b057c0a1da225c633fc1d7acc37043b482616": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5379. Include token tracking ids in jobconf. (kkambatl via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1523605 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/09/13 5:17 AM",
      "commitName": "e60b057c0a1da225c633fc1d7acc37043b482616",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "13/06/13 11:26 PM",
      "commitNameOld": "2166fa201ed9ee22d613d2edbac1e2af4e71ab76",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 94.24,
      "commitsBetweenForRepo": 544,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,95 +1,108 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n     \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                      job.getConfiguration());\n     //configure the command line options correctly on the submitting dfs\n     Configuration conf \u003d job.getConfiguration();\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n       conf.set(MRJobConfig.USER_NAME,\n           UserGroupInformation.getCurrentUser().getShortUserName());\n       conf.set(\"hadoop.http.filter.initializers\", \n           \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       // generate a secret to authenticate shuffle transfers\n       if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n         KeyGenerator keyGen;\n         try {\n           keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n           keyGen.init(SHUFFLE_KEY_LENGTH);\n         } catch (NoSuchAlgorithmException e) {\n           throw new IOException(\"Error generating shuffle secret key\", e);\n         }\n         SecretKey shuffleKey \u003d keyGen.generateKey();\n         TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n             job.getCredentials());\n       }\n \n       copyAndConfigureFiles(job, submitJobDir);\n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n       \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // removing jobtoken referrals before copying the jobconf to HDFS\n       // as the tasks don\u0027t need this setting, actually they may break\n       // because of it if present as the referral will point to a\n       // different job.\n       TokenCache.cleanUpTokenReferral(conf);\n \n+      if (conf.getBoolean(\n+          MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n+          MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n+        // Add HDFS tracking ids\n+        ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n+        for (Token\u003c? extends TokenIdentifier\u003e t :\n+            job.getCredentials().getAllTokens()) {\n+          trackingIds.add(t.decodeIdentifier().getTrackingId());\n+        }\n+        conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n+            trackingIds.toArray(new String[trackingIds.size()]));\n+      }\n+\n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n    \n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                     job.getConfiguration());\n    //configure the command line options correctly on the submitting dfs\n    Configuration conf \u003d job.getConfiguration();\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(MRJobConfig.USER_NAME,\n          UserGroupInformation.getCurrentUser().getShortUserName());\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      // generate a secret to authenticate shuffle transfers\n      if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n        KeyGenerator keyGen;\n        try {\n          keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n          keyGen.init(SHUFFLE_KEY_LENGTH);\n        } catch (NoSuchAlgorithmException e) {\n          throw new IOException(\"Error generating shuffle secret key\", e);\n        }\n        SecretKey shuffleKey \u003d keyGen.generateKey();\n        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n            job.getCredentials());\n      }\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don\u0027t need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      if (conf.getBoolean(\n          MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,\n          MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\n        // Add HDFS tracking ids\n        ArrayList\u003cString\u003e trackingIds \u003d new ArrayList\u003cString\u003e();\n        for (Token\u003c? extends TokenIdentifier\u003e t :\n            job.getCredentials().getAllTokens()) {\n          trackingIds.add(t.decodeIdentifier().getTrackingId());\n        }\n        conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,\n            trackingIds.toArray(new String[trackingIds.size()]));\n      }\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "2166fa201ed9ee22d613d2edbac1e2af4e71ab76": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5319. Set user.name in job.xml. Contributed by Xuan Gong.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1492962 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/06/13 11:26 PM",
      "commitName": "2166fa201ed9ee22d613d2edbac1e2af4e71ab76",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "15/03/13 2:09 PM",
      "commitNameOld": "7d7553c4eb7d9a282410a3213d26a89fea9b7865",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 90.39,
      "commitsBetweenForRepo": 561,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,93 +1,95 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n     \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                      job.getConfiguration());\n     //configure the command line options correctly on the submitting dfs\n     Configuration conf \u003d job.getConfiguration();\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n+      conf.set(MRJobConfig.USER_NAME,\n+          UserGroupInformation.getCurrentUser().getShortUserName());\n       conf.set(\"hadoop.http.filter.initializers\", \n           \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       // generate a secret to authenticate shuffle transfers\n       if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n         KeyGenerator keyGen;\n         try {\n           keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n           keyGen.init(SHUFFLE_KEY_LENGTH);\n         } catch (NoSuchAlgorithmException e) {\n           throw new IOException(\"Error generating shuffle secret key\", e);\n         }\n         SecretKey shuffleKey \u003d keyGen.generateKey();\n         TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n             job.getCredentials());\n       }\n \n       copyAndConfigureFiles(job, submitJobDir);\n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n       \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // removing jobtoken referrals before copying the jobconf to HDFS\n       // as the tasks don\u0027t need this setting, actually they may break\n       // because of it if present as the referral will point to a\n       // different job.\n       TokenCache.cleanUpTokenReferral(conf);\n \n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n    \n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                     job.getConfiguration());\n    //configure the command line options correctly on the submitting dfs\n    Configuration conf \u003d job.getConfiguration();\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(MRJobConfig.USER_NAME,\n          UserGroupInformation.getCurrentUser().getShortUserName());\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      // generate a secret to authenticate shuffle transfers\n      if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n        KeyGenerator keyGen;\n        try {\n          keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n          keyGen.init(SHUFFLE_KEY_LENGTH);\n        } catch (NoSuchAlgorithmException e) {\n          throw new IOException(\"Error generating shuffle secret key\", e);\n        }\n        SecretKey shuffleKey \u003d keyGen.generateKey();\n        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n            job.getCredentials());\n      }\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don\u0027t need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "7d7553c4eb7d9a282410a3213d26a89fea9b7865": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5042. Reducer unable to fetch for a map task that was recovered (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1457119 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/03/13 2:09 PM",
      "commitName": "7d7553c4eb7d9a282410a3213d26a89fea9b7865",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "09/01/13 4:50 PM",
      "commitNameOld": "59d9d8bca93bf714f8ec846a27009e5690f1c05d",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 64.85,
      "commitsBetweenForRepo": 304,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,79 +1,93 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n     \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                      job.getConfiguration());\n     //configure the command line options correctly on the submitting dfs\n     Configuration conf \u003d job.getConfiguration();\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n       conf.set(\"hadoop.http.filter.initializers\", \n           \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n+      // generate a secret to authenticate shuffle transfers\n+      if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n+        KeyGenerator keyGen;\n+        try {\n+          keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n+          keyGen.init(SHUFFLE_KEY_LENGTH);\n+        } catch (NoSuchAlgorithmException e) {\n+          throw new IOException(\"Error generating shuffle secret key\", e);\n+        }\n+        SecretKey shuffleKey \u003d keyGen.generateKey();\n+        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n+            job.getCredentials());\n+      }\n+\n       copyAndConfigureFiles(job, submitJobDir);\n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n       \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // removing jobtoken referrals before copying the jobconf to HDFS\n       // as the tasks don\u0027t need this setting, actually they may break\n       // because of it if present as the referral will point to a\n       // different job.\n       TokenCache.cleanUpTokenReferral(conf);\n \n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n    \n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                     job.getConfiguration());\n    //configure the command line options correctly on the submitting dfs\n    Configuration conf \u003d job.getConfiguration();\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      // generate a secret to authenticate shuffle transfers\n      if (TokenCache.getShuffleSecretKey(job.getCredentials()) \u003d\u003d null) {\n        KeyGenerator keyGen;\n        try {\n          keyGen \u003d KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n          keyGen.init(SHUFFLE_KEY_LENGTH);\n        } catch (NoSuchAlgorithmException e) {\n          throw new IOException(\"Error generating shuffle secret key\", e);\n        }\n        SecretKey shuffleKey \u003d keyGen.generateKey();\n        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n            job.getCredentials());\n      }\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don\u0027t need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "b03023110805a3479ef6b42f7c095de3f1a883e2": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3727. jobtoken location property in jobconf refers to wrong jobtoken file (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1240410 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/02/12 3:48 PM",
      "commitName": "b03023110805a3479ef6b42f7c095de3f1a883e2",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "27/10/11 11:45 PM",
      "commitNameOld": "670fa24b48acb407c22fbfdde87ae3123dcbf449",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 98.71,
      "commitsBetweenForRepo": 534,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,79 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n     \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                      job.getConfiguration());\n     //configure the command line options correctly on the submitting dfs\n     Configuration conf \u003d job.getConfiguration();\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n       conf.set(\"hadoop.http.filter.initializers\", \n           \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       copyAndConfigureFiles(job, submitJobDir);\n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n       \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n+      // removing jobtoken referrals before copying the jobconf to HDFS\n+      // as the tasks don\u0027t need this setting, actually they may break\n+      // because of it if present as the referral will point to a\n+      // different job.\n+      TokenCache.cleanUpTokenReferral(conf);\n+\n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n    \n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                     job.getConfiguration());\n    //configure the command line options correctly on the submitting dfs\n    Configuration conf \u003d job.getConfiguration();\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // removing jobtoken referrals before copying the jobconf to HDFS\n      // as the tasks don\u0027t need this setting, actually they may break\n      // because of it if present as the referral will point to a\n      // different job.\n      TokenCache.cleanUpTokenReferral(conf);\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "8aabd3d4e67cad8dc7e46f5339981135badc7421": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2858. Added a WebApp Proxy for applications. Contributed by Robert Evans.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189036 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/10/11 11:29 PM",
      "commitName": "8aabd3d4e67cad8dc7e46f5339981135badc7421",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "18/10/11 2:37 PM",
      "commitNameOld": "51a667bef8300d6499c9867b50eee352311a4185",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 7.37,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,73 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n     \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                      job.getConfiguration());\n     //configure the command line options correctly on the submitting dfs\n     Configuration conf \u003d job.getConfiguration();\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n+      conf.set(\"hadoop.http.filter.initializers\", \n+          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n       conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       copyAndConfigureFiles(job, submitJobDir);\n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n    \n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                     job.getConfiguration());\n    //configure the command line options correctly on the submitting dfs\n    Configuration conf \u003d job.getConfiguration();\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(\"hadoop.http.filter.initializers\", \n          \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n\n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "51a667bef8300d6499c9867b50eee352311a4185": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2762. Cleanup MR staging directory on completion. Contributed by Mahadev Konar.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1185880 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/10/11 2:37 PM",
      "commitName": "51a667bef8300d6499c9867b50eee352311a4185",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "09/10/11 8:29 PM",
      "commitNameOld": "d0016c6120a37d5056e3c9e72b67588d3e793ac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 8.76,
      "commitsBetweenForRepo": 64,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,71 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n     //validate the jobs output specs \n     checkSpecs(job);\n     \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                      job.getConfiguration());\n     //configure the command line options correctly on the submitting dfs\n     Configuration conf \u003d job.getConfiguration();\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n-      conf.set(\"mapreduce.job.dir\", submitJobDir.toString());\n+      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       copyAndConfigureFiles(job, submitJobDir);\n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n    \n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                     job.getConfiguration());\n    //configure the command line options correctly on the submitting dfs\n    Configuration conf \u003d job.getConfiguration();\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n\n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "d0016c6120a37d5056e3c9e72b67588d3e793ac1": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3154. Fix JobSubmitter to check for output specs before copying job submission files to fail fast. Contributed by Abhijit Suresh Shingate.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1180774 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/10/11 8:29 PM",
      "commitName": "d0016c6120a37d5056e3c9e72b67588d3e793ac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "17/09/11 7:50 PM",
      "commitNameOld": "e1acb1222dd6fdb8fa688c815cbca6ae4193745d",
      "commitAuthorOld": "Mahadev Konar",
      "daysBetweenCommits": 22.03,
      "commitsBetweenForRepo": 163,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,71 @@\n   JobStatus submitJobInternal(Job job, Cluster cluster) \n   throws ClassNotFoundException, InterruptedException, IOException {\n \n+    //validate the jobs output specs \n+    checkSpecs(job);\n+    \n     Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                      job.getConfiguration());\n     //configure the command line options correctly on the submitting dfs\n     Configuration conf \u003d job.getConfiguration();\n     InetAddress ip \u003d InetAddress.getLocalHost();\n     if (ip !\u003d null) {\n       submitHostAddress \u003d ip.getHostAddress();\n       submitHostName \u003d ip.getHostName();\n       conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n       conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n     }\n     JobID jobId \u003d submitClient.getNewJobID();\n     job.setJobID(jobId);\n     Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n     JobStatus status \u003d null;\n     try {\n       conf.set(\"mapreduce.job.dir\", submitJobDir.toString());\n       LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n           + \" as the submit dir\");\n       // get delegation token for the dir\n       TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n           new Path[] { submitJobDir }, conf);\n       \n       populateTokenCache(conf, job.getCredentials());\n \n       copyAndConfigureFiles(job, submitJobDir);\n       Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n \n-      checkSpecs(job);\n-      \n       // Create the splits for the job\n       LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n       int maps \u003d writeSplits(job, submitJobDir);\n       conf.setInt(MRJobConfig.NUM_MAPS, maps);\n       LOG.info(\"number of splits:\" + maps);\n \n       // write \"queue admins of the queue to which job is being submitted\"\n       // to job file.\n       String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n           JobConf.DEFAULT_QUEUE_NAME);\n       AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n       conf.set(toFullPropertyName(queue,\n           QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n \n       // Write job file to submit dir\n       writeConf(conf, submitJobFile);\n       \n       //\n       // Now, actually submit the job (using the submit name)\n       //\n       printTokens(jobId, job.getCredentials());\n       status \u003d submitClient.submitJob(\n           jobId, submitJobDir.toString(), job.getCredentials());\n       if (status !\u003d null) {\n         return status;\n       } else {\n         throw new IOException(\"Could not launch job\");\n       }\n     } finally {\n       if (status \u003d\u003d null) {\n         LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n         if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n           jtFs.delete(submitJobDir, true);\n \n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    //validate the jobs output specs \n    checkSpecs(job);\n    \n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                     job.getConfiguration());\n    //configure the command line options correctly on the submitting dfs\n    Configuration conf \u003d job.getConfiguration();\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(\"mapreduce.job.dir\", submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n\n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                     job.getConfiguration());\n    //configure the command line options correctly on the submitting dfs\n    Configuration conf \u003d job.getConfiguration();\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(\"mapreduce.job.dir\", submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n\n      checkSpecs(job);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                     job.getConfiguration());\n    //configure the command line options correctly on the submitting dfs\n    Configuration conf \u003d job.getConfiguration();\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(\"mapreduce.job.dir\", submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n\n      checkSpecs(job);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,70 @@\n+  JobStatus submitJobInternal(Job job, Cluster cluster) \n+  throws ClassNotFoundException, InterruptedException, IOException {\n+\n+    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n+                                                     job.getConfiguration());\n+    //configure the command line options correctly on the submitting dfs\n+    Configuration conf \u003d job.getConfiguration();\n+    InetAddress ip \u003d InetAddress.getLocalHost();\n+    if (ip !\u003d null) {\n+      submitHostAddress \u003d ip.getHostAddress();\n+      submitHostName \u003d ip.getHostName();\n+      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n+      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n+    }\n+    JobID jobId \u003d submitClient.getNewJobID();\n+    job.setJobID(jobId);\n+    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n+    JobStatus status \u003d null;\n+    try {\n+      conf.set(\"mapreduce.job.dir\", submitJobDir.toString());\n+      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n+          + \" as the submit dir\");\n+      // get delegation token for the dir\n+      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n+          new Path[] { submitJobDir }, conf);\n+      \n+      populateTokenCache(conf, job.getCredentials());\n+\n+      copyAndConfigureFiles(job, submitJobDir);\n+      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n+\n+      checkSpecs(job);\n+      \n+      // Create the splits for the job\n+      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n+      int maps \u003d writeSplits(job, submitJobDir);\n+      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n+      LOG.info(\"number of splits:\" + maps);\n+\n+      // write \"queue admins of the queue to which job is being submitted\"\n+      // to job file.\n+      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n+          JobConf.DEFAULT_QUEUE_NAME);\n+      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n+      conf.set(toFullPropertyName(queue,\n+          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n+\n+      // Write job file to submit dir\n+      writeConf(conf, submitJobFile);\n+      \n+      //\n+      // Now, actually submit the job (using the submit name)\n+      //\n+      printTokens(jobId, job.getCredentials());\n+      status \u003d submitClient.submitJob(\n+          jobId, submitJobDir.toString(), job.getCredentials());\n+      if (status !\u003d null) {\n+        return status;\n+      } else {\n+        throw new IOException(\"Could not launch job\");\n+      }\n+    } finally {\n+      if (status \u003d\u003d null) {\n+        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n+        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n+          jtFs.delete(submitJobDir, true);\n+\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  JobStatus submitJobInternal(Job job, Cluster cluster) \n  throws ClassNotFoundException, InterruptedException, IOException {\n\n    Path jobStagingArea \u003d JobSubmissionFiles.getStagingDir(cluster, \n                                                     job.getConfiguration());\n    //configure the command line options correctly on the submitting dfs\n    Configuration conf \u003d job.getConfiguration();\n    InetAddress ip \u003d InetAddress.getLocalHost();\n    if (ip !\u003d null) {\n      submitHostAddress \u003d ip.getHostAddress();\n      submitHostName \u003d ip.getHostName();\n      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);\n      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);\n    }\n    JobID jobId \u003d submitClient.getNewJobID();\n    job.setJobID(jobId);\n    Path submitJobDir \u003d new Path(jobStagingArea, jobId.toString());\n    JobStatus status \u003d null;\n    try {\n      conf.set(\"mapreduce.job.dir\", submitJobDir.toString());\n      LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir \n          + \" as the submit dir\");\n      // get delegation token for the dir\n      TokenCache.obtainTokensForNamenodes(job.getCredentials(),\n          new Path[] { submitJobDir }, conf);\n      \n      populateTokenCache(conf, job.getCredentials());\n\n      copyAndConfigureFiles(job, submitJobDir);\n      Path submitJobFile \u003d JobSubmissionFiles.getJobConfPath(submitJobDir);\n\n      checkSpecs(job);\n      \n      // Create the splits for the job\n      LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\n      int maps \u003d writeSplits(job, submitJobDir);\n      conf.setInt(MRJobConfig.NUM_MAPS, maps);\n      LOG.info(\"number of splits:\" + maps);\n\n      // write \"queue admins of the queue to which job is being submitted\"\n      // to job file.\n      String queue \u003d conf.get(MRJobConfig.QUEUE_NAME,\n          JobConf.DEFAULT_QUEUE_NAME);\n      AccessControlList acl \u003d submitClient.getQueueAdmins(queue);\n      conf.set(toFullPropertyName(queue,\n          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\n\n      // Write job file to submit dir\n      writeConf(conf, submitJobFile);\n      \n      //\n      // Now, actually submit the job (using the submit name)\n      //\n      printTokens(jobId, job.getCredentials());\n      status \u003d submitClient.submitJob(\n          jobId, submitJobDir.toString(), job.getCredentials());\n      if (status !\u003d null) {\n        return status;\n      } else {\n        throw new IOException(\"Could not launch job\");\n      }\n    } finally {\n      if (status \u003d\u003d null) {\n        LOG.info(\"Cleaning up the staging area \" + submitJobDir);\n        if (jtFs !\u003d null \u0026\u0026 submitJobDir !\u003d null)\n          jtFs.delete(submitJobDir, true);\n\n      }\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/JobSubmitter.java"
    }
  }
}