{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "NamenodeFsck.java",
  "functionName": "check",
  "functionId": "check___parent-String__file-HdfsFileStatus__replRes-Result__ecRes-Result",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
  "functionStartLine": 504,
  "functionEndLine": 532,
  "numCommitsSeen": 120,
  "timeTaken": 3167,
  "changeHistory": [
    "1e3a0b0d931676b191cb4813ed1a283ebb24d4eb",
    "10a1f557e775e7a55958b106dd10021ac7394843",
    "645a8f2a4d09acb5a21820f52ee78784d9e4cc8a"
  ],
  "changeHistoryShort": {
    "1e3a0b0d931676b191cb4813ed1a283ebb24d4eb": "Ybodychange",
    "10a1f557e775e7a55958b106dd10021ac7394843": "Ybodychange",
    "645a8f2a4d09acb5a21820f52ee78784d9e4cc8a": "Ybodychange"
  },
  "changeHistoryDetails": {
    "1e3a0b0d931676b191cb4813ed1a283ebb24d4eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7175. Client-side SocketTimeoutException during Fsck. Contributed by Stephen O\u0027Donnell, Akira Ajisaka.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\nCo-authored-by: Akira Ajisaka \u003caajisaka@apache.org\u003e\n",
      "commitDate": "31/01/20 4:13 PM",
      "commitName": "1e3a0b0d931676b191cb4813ed1a283ebb24d4eb",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "29/11/19 10:25 AM",
      "commitNameOld": "6b2d6d4aafb110bef1b77d4ccbba4350e624b57d",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 63.24,
      "commitsBetweenForRepo": 216,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,29 @@\n   void check(String parent, HdfsFileStatus file, Result replRes, Result ecRes)\n       throws IOException {\n     String path \u003d file.getFullName(parent);\n-    if (showprogress \u0026\u0026\n-        (totalDirs + totalSymlinks + replRes.totalFiles + ecRes.totalFiles)\n-            % 100 \u003d\u003d 0) {\n+    if ((totalDirs + totalSymlinks + replRes.totalFiles + ecRes.totalFiles)\n+            % 1000 \u003d\u003d 0) {\n       out.println();\n       out.flush();\n     }\n \n     if (file.isDirectory()) {\n       checkDir(path, replRes, ecRes);\n       return;\n     }\n     if (file.isSymlink()) {\n       if (showFiles) {\n         out.println(path + \" \u003csymlink\u003e\");\n       }\n       totalSymlinks++;\n       return;\n     }\n     LocatedBlocks blocks \u003d getBlockLocations(path, file);\n     if (blocks \u003d\u003d null) { // the file is deleted\n       return;\n     }\n \n     final Result r \u003d file.getErasureCodingPolicy() !\u003d null ? ecRes: replRes;\n     collectFileSummary(path, file, r, blocks);\n     collectBlocksSummary(parent, file, r, blocks);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void check(String parent, HdfsFileStatus file, Result replRes, Result ecRes)\n      throws IOException {\n    String path \u003d file.getFullName(parent);\n    if ((totalDirs + totalSymlinks + replRes.totalFiles + ecRes.totalFiles)\n            % 1000 \u003d\u003d 0) {\n      out.println();\n      out.flush();\n    }\n\n    if (file.isDirectory()) {\n      checkDir(path, replRes, ecRes);\n      return;\n    }\n    if (file.isSymlink()) {\n      if (showFiles) {\n        out.println(path + \" \u003csymlink\u003e\");\n      }\n      totalSymlinks++;\n      return;\n    }\n    LocatedBlocks blocks \u003d getBlockLocations(path, file);\n    if (blocks \u003d\u003d null) { // the file is deleted\n      return;\n    }\n\n    final Result r \u003d file.getErasureCodingPolicy() !\u003d null ? ecRes: replRes;\n    collectFileSummary(path, file, r, blocks);\n    collectBlocksSummary(parent, file, r, blocks);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "10a1f557e775e7a55958b106dd10021ac7394843": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12791. NameNode Fsck http Connection can timeout for directories with multiple levels. Contributed by Mukul Kumar Singh.\n",
      "commitDate": "09/11/17 6:47 PM",
      "commitName": "10a1f557e775e7a55958b106dd10021ac7394843",
      "commitAuthor": "Chen Liang",
      "commitDateOld": "14/08/17 9:57 PM",
      "commitNameOld": "645a8f2a4d09acb5a21820f52ee78784d9e4cc8a",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 86.91,
      "commitsBetweenForRepo": 816,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,30 @@\n   void check(String parent, HdfsFileStatus file, Result replRes, Result ecRes)\n       throws IOException {\n     String path \u003d file.getFullName(parent);\n+    if (showprogress \u0026\u0026\n+        (totalDirs + totalSymlinks + replRes.totalFiles + ecRes.totalFiles)\n+            % 100 \u003d\u003d 0) {\n+      out.println();\n+      out.flush();\n+    }\n+\n     if (file.isDirectory()) {\n       checkDir(path, replRes, ecRes);\n       return;\n     }\n     if (file.isSymlink()) {\n       if (showFiles) {\n         out.println(path + \" \u003csymlink\u003e\");\n       }\n       totalSymlinks++;\n       return;\n     }\n     LocatedBlocks blocks \u003d getBlockLocations(path, file);\n     if (blocks \u003d\u003d null) { // the file is deleted\n       return;\n     }\n \n     final Result r \u003d file.getErasureCodingPolicy() !\u003d null ? ecRes: replRes;\n     collectFileSummary(path, file, r, blocks);\n-    if (showprogress \u0026\u0026 (replRes.totalFiles + ecRes.totalFiles) % 100 \u003d\u003d 0) {\n-      out.println();\n-      out.flush();\n-    }\n     collectBlocksSummary(parent, file, r, blocks);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void check(String parent, HdfsFileStatus file, Result replRes, Result ecRes)\n      throws IOException {\n    String path \u003d file.getFullName(parent);\n    if (showprogress \u0026\u0026\n        (totalDirs + totalSymlinks + replRes.totalFiles + ecRes.totalFiles)\n            % 100 \u003d\u003d 0) {\n      out.println();\n      out.flush();\n    }\n\n    if (file.isDirectory()) {\n      checkDir(path, replRes, ecRes);\n      return;\n    }\n    if (file.isSymlink()) {\n      if (showFiles) {\n        out.println(path + \" \u003csymlink\u003e\");\n      }\n      totalSymlinks++;\n      return;\n    }\n    LocatedBlocks blocks \u003d getBlockLocations(path, file);\n    if (blocks \u003d\u003d null) { // the file is deleted\n      return;\n    }\n\n    final Result r \u003d file.getErasureCodingPolicy() !\u003d null ? ecRes: replRes;\n    collectFileSummary(path, file, r, blocks);\n    collectBlocksSummary(parent, file, r, blocks);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    },
    "645a8f2a4d09acb5a21820f52ee78784d9e4cc8a": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14726. Mark FileStatus::isDir as final\n",
      "commitDate": "14/08/17 9:57 PM",
      "commitName": "645a8f2a4d09acb5a21820f52ee78784d9e4cc8a",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "02/03/17 7:58 PM",
      "commitNameOld": "3749152b661d0359b3b941ab1d17177230f3b8dc",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 165.04,
      "commitsBetweenForRepo": 953,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   void check(String parent, HdfsFileStatus file, Result replRes, Result ecRes)\n       throws IOException {\n     String path \u003d file.getFullName(parent);\n-    if (file.isDir()) {\n+    if (file.isDirectory()) {\n       checkDir(path, replRes, ecRes);\n       return;\n     }\n     if (file.isSymlink()) {\n       if (showFiles) {\n         out.println(path + \" \u003csymlink\u003e\");\n       }\n       totalSymlinks++;\n       return;\n     }\n     LocatedBlocks blocks \u003d getBlockLocations(path, file);\n     if (blocks \u003d\u003d null) { // the file is deleted\n       return;\n     }\n \n     final Result r \u003d file.getErasureCodingPolicy() !\u003d null ? ecRes: replRes;\n     collectFileSummary(path, file, r, blocks);\n     if (showprogress \u0026\u0026 (replRes.totalFiles + ecRes.totalFiles) % 100 \u003d\u003d 0) {\n       out.println();\n       out.flush();\n     }\n     collectBlocksSummary(parent, file, r, blocks);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void check(String parent, HdfsFileStatus file, Result replRes, Result ecRes)\n      throws IOException {\n    String path \u003d file.getFullName(parent);\n    if (file.isDirectory()) {\n      checkDir(path, replRes, ecRes);\n      return;\n    }\n    if (file.isSymlink()) {\n      if (showFiles) {\n        out.println(path + \" \u003csymlink\u003e\");\n      }\n      totalSymlinks++;\n      return;\n    }\n    LocatedBlocks blocks \u003d getBlockLocations(path, file);\n    if (blocks \u003d\u003d null) { // the file is deleted\n      return;\n    }\n\n    final Result r \u003d file.getErasureCodingPolicy() !\u003d null ? ecRes: replRes;\n    collectFileSummary(path, file, r, blocks);\n    if (showprogress \u0026\u0026 (replRes.totalFiles + ecRes.totalFiles) % 100 \u003d\u003d 0) {\n      out.println();\n      out.flush();\n    }\n    collectBlocksSummary(parent, file, r, blocks);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
      "extendedDetails": {}
    }
  }
}