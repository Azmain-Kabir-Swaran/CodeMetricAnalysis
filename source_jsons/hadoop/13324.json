{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeManager.java",
  "functionName": "getDatanodeListForReport",
  "functionId": "getDatanodeListForReport___type-DatanodeReportType(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
  "functionStartLine": 1482,
  "functionEndLine": 1561,
  "numCommitsSeen": 195,
  "timeTaken": 11561,
  "changeHistory": [
    "8d9084eb62f4593d4dfeb618abacf6ae89019109",
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978",
    "fde8ac5d8514f5146f438f8d0794116aaef20416",
    "8602692338d6f493647205e0241e4116211fab75",
    "1c4951a7a09433fbbcfe26f243d6c2d8043c71bb",
    "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
    "2ffd84273ac490724fe7e7825664bb6d09ef0e99",
    "7a7960be41c32f20ffec9fea811878b113da62db",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "9445859930b8653cb0b9a0e1abf38cc05dbe2658",
    "2002dc63c9409de733a374d810c529e95895df44",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
    "8e0804666189ce9a66b7b41b744776bad29770dd",
    "a7bfb25d2bbab0a329712d1efb143edc49a4076d",
    "be94bf6b57895846853d3e0ebc5c33b4f725ae2c",
    "e505b7e704ff83893a40190695977ce1393f6248",
    "be7dd8333a7e56e732171db0781786987de03195",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "969a263188f7015261719fe45fa1505121ebb80e",
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
    "42863b9bafb9f77b76d9dcd26f0fcac859ea6f6a",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "8d9084eb62f4593d4dfeb618abacf6ae89019109": "Ybodychange",
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978": "Ybodychange",
    "fde8ac5d8514f5146f438f8d0794116aaef20416": "Ybodychange",
    "8602692338d6f493647205e0241e4116211fab75": "Ybodychange",
    "1c4951a7a09433fbbcfe26f243d6c2d8043c71bb": "Ybodychange",
    "b94b56806d3d6e04984e229b479f7ac15b62bbfa": "Ybodychange",
    "2ffd84273ac490724fe7e7825664bb6d09ef0e99": "Ybodychange",
    "7a7960be41c32f20ffec9fea811878b113da62db": "Ybodychange",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "9445859930b8653cb0b9a0e1abf38cc05dbe2658": "Ybodychange",
    "2002dc63c9409de733a374d810c529e95895df44": "Ybodychange",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": "Ybodychange",
    "8e0804666189ce9a66b7b41b744776bad29770dd": "Ybodychange",
    "a7bfb25d2bbab0a329712d1efb143edc49a4076d": "Ybodychange",
    "be94bf6b57895846853d3e0ebc5c33b4f725ae2c": "Ybodychange",
    "e505b7e704ff83893a40190695977ce1393f6248": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "969a263188f7015261719fe45fa1505121ebb80e": "Ybodychange",
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparametermetachange)",
    "42863b9bafb9f77b76d9dcd26f0fcac859ea6f6a": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8d9084eb62f4593d4dfeb618abacf6ae89019109": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11359. DFSAdmin report command supports displaying maintenance state datanodes. Contributed by Yiqun Lin.\n",
      "commitDate": "01/06/17 9:48 PM",
      "commitName": "8d9084eb62f4593d4dfeb618abacf6ae89019109",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "25/05/17 3:17 PM",
      "commitNameOld": "2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 7.27,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,80 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     final boolean listLiveNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.LIVE;\n     final boolean listDeadNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DEAD;\n     final boolean listDecommissioningNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n     final boolean listEnteringMaintenanceNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.ENTERING_MAINTENANCE;\n+    final boolean listInMaintenanceNodes \u003d\n+        type \u003d\u003d DatanodeReportType.ALL ||\n+        type \u003d\u003d DatanodeReportType.IN_MAINTENANCE;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes;\n     final HostSet foundNodes \u003d new HostSet();\n     final Iterable\u003cInetSocketAddress\u003e includedNodes \u003d\n         hostConfigManager.getIncludes();\n \n     synchronized(this) {\n       nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n       for (DatanodeDescriptor dn : datanodeMap.values()) {\n         final boolean isDead \u003d isDatanodeDead(dn);\n         final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n         final boolean isEnteringMaintenance \u003d dn.isEnteringMaintenance();\n+        final boolean isInMaintenance \u003d dn.isInMaintenance();\n \n         if (((listLiveNodes \u0026\u0026 !isDead) ||\n             (listDeadNodes \u0026\u0026 isDead) ||\n             (listDecommissioningNodes \u0026\u0026 isDecommissioning) ||\n-            (listEnteringMaintenanceNodes \u0026\u0026 isEnteringMaintenance)) \u0026\u0026\n+            (listEnteringMaintenanceNodes \u0026\u0026 isEnteringMaintenance) ||\n+            (listInMaintenanceNodes \u0026\u0026 isInMaintenance)) \u0026\u0026\n             hostConfigManager.isIncluded(dn)) {\n           nodes.add(dn);\n         }\n \n         foundNodes.add(dn.getResolvedAddress());\n       }\n     }\n     Collections.sort(nodes);\n \n     if (listDeadNodes) {\n       for (InetSocketAddress addr : includedNodes) {\n         if (foundNodes.matchedBy(addr)) {\n           continue;\n         }\n         // The remaining nodes are ones that are referenced by the hosts\n         // files but that we do not know about, ie that we have never\n         // head from. Eg. an entry that is no longer part of the cluster\n         // or a bogus entry was given in the hosts files\n         //\n         // If the host file entry specified the xferPort, we use that.\n         // Otherwise, we guess that it is the default xfer port.\n         // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n         // dead.\n         DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                 .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                 addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                 defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n         setDatanodeDead(dn);\n         if (hostConfigManager.isExcluded(dn)) {\n           dn.setDecommissioned();\n         }\n         nodes.add(dn);\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostConfigManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostConfigManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    final boolean listLiveNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.LIVE;\n    final boolean listDeadNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DEAD;\n    final boolean listDecommissioningNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n    final boolean listEnteringMaintenanceNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.ENTERING_MAINTENANCE;\n    final boolean listInMaintenanceNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.IN_MAINTENANCE;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n    final HostSet foundNodes \u003d new HostSet();\n    final Iterable\u003cInetSocketAddress\u003e includedNodes \u003d\n        hostConfigManager.getIncludes();\n\n    synchronized(this) {\n      nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead \u003d isDatanodeDead(dn);\n        final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n        final boolean isEnteringMaintenance \u003d dn.isEnteringMaintenance();\n        final boolean isInMaintenance \u003d dn.isInMaintenance();\n\n        if (((listLiveNodes \u0026\u0026 !isDead) ||\n            (listDeadNodes \u0026\u0026 isDead) ||\n            (listDecommissioningNodes \u0026\u0026 isDecommissioning) ||\n            (listEnteringMaintenanceNodes \u0026\u0026 isEnteringMaintenance) ||\n            (listInMaintenanceNodes \u0026\u0026 isInMaintenance)) \u0026\u0026\n            hostConfigManager.isIncluded(dn)) {\n          nodes.add(dn);\n        }\n\n        foundNodes.add(dn.getResolvedAddress());\n      }\n    }\n    Collections.sort(nodes);\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n        // dead.\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        setDatanodeDead(dn);\n        if (hostConfigManager.isExcluded(dn)) {\n          dn.setDecommissioned();\n        }\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostConfigManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostConfigManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9392. Admins support for maintenance state. Contributed by Ming Ma.\n",
      "commitDate": "30/08/16 2:00 PM",
      "commitName": "9dcbdbdb5a34d85910707f81ebc1bb1f81c99978",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "12/04/16 1:38 PM",
      "commitNameOld": "6ef42873a02bfcbff5521869f4d6f66539d1db41",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 140.01,
      "commitsBetweenForRepo": 1065,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,75 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     final boolean listLiveNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.LIVE;\n     final boolean listDeadNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DEAD;\n     final boolean listDecommissioningNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n+    final boolean listEnteringMaintenanceNodes \u003d\n+        type \u003d\u003d DatanodeReportType.ALL ||\n+        type \u003d\u003d DatanodeReportType.ENTERING_MAINTENANCE;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes;\n     final HostSet foundNodes \u003d new HostSet();\n     final Iterable\u003cInetSocketAddress\u003e includedNodes \u003d\n         hostConfigManager.getIncludes();\n \n     synchronized(this) {\n       nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n       for (DatanodeDescriptor dn : datanodeMap.values()) {\n         final boolean isDead \u003d isDatanodeDead(dn);\n         final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n+        final boolean isEnteringMaintenance \u003d dn.isEnteringMaintenance();\n \n         if (((listLiveNodes \u0026\u0026 !isDead) ||\n             (listDeadNodes \u0026\u0026 isDead) ||\n-            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) \u0026\u0026\n+            (listDecommissioningNodes \u0026\u0026 isDecommissioning) ||\n+            (listEnteringMaintenanceNodes \u0026\u0026 isEnteringMaintenance)) \u0026\u0026\n             hostConfigManager.isIncluded(dn)) {\n           nodes.add(dn);\n         }\n \n         foundNodes.add(dn.getResolvedAddress());\n       }\n     }\n     Collections.sort(nodes);\n \n     if (listDeadNodes) {\n       for (InetSocketAddress addr : includedNodes) {\n         if (foundNodes.matchedBy(addr)) {\n           continue;\n         }\n         // The remaining nodes are ones that are referenced by the hosts\n         // files but that we do not know about, ie that we have never\n         // head from. Eg. an entry that is no longer part of the cluster\n         // or a bogus entry was given in the hosts files\n         //\n         // If the host file entry specified the xferPort, we use that.\n         // Otherwise, we guess that it is the default xfer port.\n         // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n         // dead.\n         DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                 .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                 addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                 defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n         setDatanodeDead(dn);\n         if (hostConfigManager.isExcluded(dn)) {\n           dn.setDecommissioned();\n         }\n         nodes.add(dn);\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostConfigManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostConfigManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    final boolean listLiveNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.LIVE;\n    final boolean listDeadNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DEAD;\n    final boolean listDecommissioningNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n    final boolean listEnteringMaintenanceNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.ENTERING_MAINTENANCE;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n    final HostSet foundNodes \u003d new HostSet();\n    final Iterable\u003cInetSocketAddress\u003e includedNodes \u003d\n        hostConfigManager.getIncludes();\n\n    synchronized(this) {\n      nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead \u003d isDatanodeDead(dn);\n        final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n        final boolean isEnteringMaintenance \u003d dn.isEnteringMaintenance();\n\n        if (((listLiveNodes \u0026\u0026 !isDead) ||\n            (listDeadNodes \u0026\u0026 isDead) ||\n            (listDecommissioningNodes \u0026\u0026 isDecommissioning) ||\n            (listEnteringMaintenanceNodes \u0026\u0026 isEnteringMaintenance)) \u0026\u0026\n            hostConfigManager.isIncluded(dn)) {\n          nodes.add(dn);\n        }\n\n        foundNodes.add(dn.getResolvedAddress());\n      }\n    }\n    Collections.sort(nodes);\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n        // dead.\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        setDatanodeDead(dn);\n        if (hostConfigManager.isExcluded(dn)) {\n          dn.setDecommissioned();\n        }\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostConfigManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostConfigManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "fde8ac5d8514f5146f438f8d0794116aaef20416": {
      "type": "Ybodychange",
      "commitMessage": "Add missing files from HDFS-9005. (lei)\n",
      "commitDate": "25/03/16 5:11 PM",
      "commitName": "fde8ac5d8514f5146f438f8d0794116aaef20416",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 14.88,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,70 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     final boolean listLiveNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.LIVE;\n     final boolean listDeadNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DEAD;\n     final boolean listDecommissioningNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes;\n-    final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n-    final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n-    final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n+    final HostSet foundNodes \u003d new HostSet();\n+    final Iterable\u003cInetSocketAddress\u003e includedNodes \u003d\n+        hostConfigManager.getIncludes();\n \n     synchronized(this) {\n       nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n       for (DatanodeDescriptor dn : datanodeMap.values()) {\n         final boolean isDead \u003d isDatanodeDead(dn);\n         final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n \n         if (((listLiveNodes \u0026\u0026 !isDead) ||\n             (listDeadNodes \u0026\u0026 isDead) ||\n             (listDecommissioningNodes \u0026\u0026 isDecommissioning)) \u0026\u0026\n-            hostFileManager.isIncluded(dn)) {\n+            hostConfigManager.isIncluded(dn)) {\n           nodes.add(dn);\n         }\n \n-        foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n+        foundNodes.add(dn.getResolvedAddress());\n       }\n     }\n     Collections.sort(nodes);\n \n     if (listDeadNodes) {\n       for (InetSocketAddress addr : includedNodes) {\n         if (foundNodes.matchedBy(addr)) {\n           continue;\n         }\n         // The remaining nodes are ones that are referenced by the hosts\n         // files but that we do not know about, ie that we have never\n         // head from. Eg. an entry that is no longer part of the cluster\n         // or a bogus entry was given in the hosts files\n         //\n         // If the host file entry specified the xferPort, we use that.\n         // Otherwise, we guess that it is the default xfer port.\n         // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n         // dead.\n         DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                 .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                 addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                 defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n         setDatanodeDead(dn);\n-        if (excludedNodes.match(addr)) {\n+        if (hostConfigManager.isExcluded(dn)) {\n           dn.setDecommissioned();\n         }\n         nodes.add(dn);\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n-          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n-          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n+          \"includedNodes \u003d \" + hostConfigManager.getIncludes() +\n+          \", excludedNodes \u003d \" + hostConfigManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    final boolean listLiveNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.LIVE;\n    final boolean listDeadNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DEAD;\n    final boolean listDecommissioningNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n    final HostSet foundNodes \u003d new HostSet();\n    final Iterable\u003cInetSocketAddress\u003e includedNodes \u003d\n        hostConfigManager.getIncludes();\n\n    synchronized(this) {\n      nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead \u003d isDatanodeDead(dn);\n        final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n\n        if (((listLiveNodes \u0026\u0026 !isDead) ||\n            (listDeadNodes \u0026\u0026 isDead) ||\n            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) \u0026\u0026\n            hostConfigManager.isIncluded(dn)) {\n          nodes.add(dn);\n        }\n\n        foundNodes.add(dn.getResolvedAddress());\n      }\n    }\n    Collections.sort(nodes);\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n        // dead.\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        setDatanodeDead(dn);\n        if (hostConfigManager.isExcluded(dn)) {\n          dn.setDecommissioned();\n        }\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostConfigManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostConfigManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "8602692338d6f493647205e0241e4116211fab75": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9371. Code cleanup for DatanodeManager. Contributed by Jing Zhao.\n",
      "commitDate": "15/12/15 10:47 AM",
      "commitName": "8602692338d6f493647205e0241e4116211fab75",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "01/12/15 4:09 PM",
      "commitNameOld": "a49cc74b4c72195dee1dfb6f9548e5e411dff553",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 13.78,
      "commitsBetweenForRepo": 73,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,70 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     final boolean listLiveNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.LIVE;\n     final boolean listDeadNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DEAD;\n     final boolean listDecommissioningNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes;\n     final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n     final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n     final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n \n-    synchronized(datanodeMap) {\n+    synchronized(this) {\n       nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n       for (DatanodeDescriptor dn : datanodeMap.values()) {\n         final boolean isDead \u003d isDatanodeDead(dn);\n         final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n \n         if (((listLiveNodes \u0026\u0026 !isDead) ||\n             (listDeadNodes \u0026\u0026 isDead) ||\n             (listDecommissioningNodes \u0026\u0026 isDecommissioning)) \u0026\u0026\n             hostFileManager.isIncluded(dn)) {\n           nodes.add(dn);\n         }\n \n         foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n       }\n     }\n     Collections.sort(nodes);\n \n     if (listDeadNodes) {\n       for (InetSocketAddress addr : includedNodes) {\n         if (foundNodes.matchedBy(addr)) {\n           continue;\n         }\n         // The remaining nodes are ones that are referenced by the hosts\n         // files but that we do not know about, ie that we have never\n         // head from. Eg. an entry that is no longer part of the cluster\n         // or a bogus entry was given in the hosts files\n         //\n         // If the host file entry specified the xferPort, we use that.\n         // Otherwise, we guess that it is the default xfer port.\n         // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n         // dead.\n         DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                 .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                 addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                 defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n         setDatanodeDead(dn);\n         if (excludedNodes.match(addr)) {\n           dn.setDecommissioned();\n         }\n         nodes.add(dn);\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    final boolean listLiveNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.LIVE;\n    final boolean listDeadNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DEAD;\n    final boolean listDecommissioningNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n    final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n    final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n    final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n\n    synchronized(this) {\n      nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead \u003d isDatanodeDead(dn);\n        final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n\n        if (((listLiveNodes \u0026\u0026 !isDead) ||\n            (listDeadNodes \u0026\u0026 isDead) ||\n            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) \u0026\u0026\n            hostFileManager.isIncluded(dn)) {\n          nodes.add(dn);\n        }\n\n        foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n      }\n    }\n    Collections.sort(nodes);\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n        // dead.\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        setDatanodeDead(dn);\n        if (excludedNodes.match(addr)) {\n          dn.setDecommissioned();\n        }\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "1c4951a7a09433fbbcfe26f243d6c2d8043c71bb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8056. Decommissioned dead nodes should continue to be counted as dead after NN restart. (mingma)\n",
      "commitDate": "19/11/15 10:04 AM",
      "commitName": "1c4951a7a09433fbbcfe26f243d6c2d8043c71bb",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "06/11/15 10:15 AM",
      "commitNameOld": "0b18e5e8c69b40c9a446fff448d38e0dd10cb45e",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 12.99,
      "commitsBetweenForRepo": 62,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,70 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     final boolean listLiveNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.LIVE;\n     final boolean listDeadNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DEAD;\n     final boolean listDecommissioningNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes;\n     final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n     final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n     final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n \n     synchronized(datanodeMap) {\n       nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n       for (DatanodeDescriptor dn : datanodeMap.values()) {\n         final boolean isDead \u003d isDatanodeDead(dn);\n         final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n \n         if (((listLiveNodes \u0026\u0026 !isDead) ||\n             (listDeadNodes \u0026\u0026 isDead) ||\n             (listDecommissioningNodes \u0026\u0026 isDecommissioning)) \u0026\u0026\n             hostFileManager.isIncluded(dn)) {\n           nodes.add(dn);\n         }\n \n         foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n       }\n     }\n     Collections.sort(nodes);\n \n     if (listDeadNodes) {\n       for (InetSocketAddress addr : includedNodes) {\n-        if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n+        if (foundNodes.matchedBy(addr)) {\n           continue;\n         }\n         // The remaining nodes are ones that are referenced by the hosts\n         // files but that we do not know about, ie that we have never\n         // head from. Eg. an entry that is no longer part of the cluster\n         // or a bogus entry was given in the hosts files\n         //\n         // If the host file entry specified the xferPort, we use that.\n         // Otherwise, we guess that it is the default xfer port.\n         // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n         // dead.\n         DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                 .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                 addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                 defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n         setDatanodeDead(dn);\n+        if (excludedNodes.match(addr)) {\n+          dn.setDecommissioned();\n+        }\n         nodes.add(dn);\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    final boolean listLiveNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.LIVE;\n    final boolean listDeadNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DEAD;\n    final boolean listDecommissioningNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n    final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n    final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n    final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n\n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead \u003d isDatanodeDead(dn);\n        final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n\n        if (((listLiveNodes \u0026\u0026 !isDead) ||\n            (listDeadNodes \u0026\u0026 isDead) ||\n            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) \u0026\u0026\n            hostFileManager.isIncluded(dn)) {\n          nodes.add(dn);\n        }\n\n        foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n      }\n    }\n    Collections.sort(nodes);\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n        // dead.\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        setDatanodeDead(dn);\n        if (excludedNodes.match(addr)) {\n          dn.setDecommissioned();\n        }\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "b94b56806d3d6e04984e229b479f7ac15b62bbfa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8950. NameNode refresh doesn\u0027t remove DataNodes that are no longer in the allowed list (Daniel Templeton)\n",
      "commitDate": "28/08/15 2:21 PM",
      "commitName": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "17/08/15 11:28 AM",
      "commitNameOld": "e535e0f05b5fbd087c93238deb888cc985254b4c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 11.12,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,67 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     final boolean listLiveNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.LIVE;\n     final boolean listDeadNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DEAD;\n     final boolean listDecommissioningNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes;\n     final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n     final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n     final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n \n     synchronized(datanodeMap) {\n       nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n       for (DatanodeDescriptor dn : datanodeMap.values()) {\n         final boolean isDead \u003d isDatanodeDead(dn);\n         final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n-        if ((listLiveNodes \u0026\u0026 !isDead) ||\n+\n+        if (((listLiveNodes \u0026\u0026 !isDead) ||\n             (listDeadNodes \u0026\u0026 isDead) ||\n-            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) {\n-            nodes.add(dn);\n+            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) \u0026\u0026\n+            hostFileManager.isIncluded(dn)) {\n+          nodes.add(dn);\n         }\n+\n         foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n       }\n     }\n     Collections.sort(nodes);\n \n     if (listDeadNodes) {\n       for (InetSocketAddress addr : includedNodes) {\n         if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n           continue;\n         }\n         // The remaining nodes are ones that are referenced by the hosts\n         // files but that we do not know about, ie that we have never\n         // head from. Eg. an entry that is no longer part of the cluster\n         // or a bogus entry was given in the hosts files\n         //\n         // If the host file entry specified the xferPort, we use that.\n         // Otherwise, we guess that it is the default xfer port.\n         // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n         // dead.\n         DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                 .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                 addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                 defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n         setDatanodeDead(dn);\n         nodes.add(dn);\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    final boolean listLiveNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.LIVE;\n    final boolean listDeadNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DEAD;\n    final boolean listDecommissioningNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n    final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n    final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n    final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n\n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead \u003d isDatanodeDead(dn);\n        final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n\n        if (((listLiveNodes \u0026\u0026 !isDead) ||\n            (listDeadNodes \u0026\u0026 isDead) ||\n            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) \u0026\u0026\n            hostFileManager.isIncluded(dn)) {\n          nodes.add(dn);\n        }\n\n        foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n      }\n    }\n    Collections.sort(nodes);\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n        // dead.\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        setDatanodeDead(dn);\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "2ffd84273ac490724fe7e7825664bb6d09ef0e99": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8653. Code cleanup for DatanodeManager, DatanodeDescriptor and DatanodeStorageInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "29/06/15 12:12 PM",
      "commitName": "2ffd84273ac490724fe7e7825664bb6d09ef0e99",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "12/06/15 11:38 AM",
      "commitNameOld": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 17.02,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     final boolean listLiveNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.LIVE;\n     final boolean listDeadNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DEAD;\n     final boolean listDecommissioningNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes;\n     final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n     final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n     final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n \n     synchronized(datanodeMap) {\n-      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n+      nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n       for (DatanodeDescriptor dn : datanodeMap.values()) {\n         final boolean isDead \u003d isDatanodeDead(dn);\n         final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n         if ((listLiveNodes \u0026\u0026 !isDead) ||\n             (listDeadNodes \u0026\u0026 isDead) ||\n             (listDecommissioningNodes \u0026\u0026 isDecommissioning)) {\n             nodes.add(dn);\n         }\n         foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n       }\n     }\n     Collections.sort(nodes);\n \n     if (listDeadNodes) {\n       for (InetSocketAddress addr : includedNodes) {\n         if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n           continue;\n         }\n         // The remaining nodes are ones that are referenced by the hosts\n         // files but that we do not know about, ie that we have never\n         // head from. Eg. an entry that is no longer part of the cluster\n         // or a bogus entry was given in the hosts files\n         //\n         // If the host file entry specified the xferPort, we use that.\n         // Otherwise, we guess that it is the default xfer port.\n         // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n         // dead.\n         DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                 .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                 addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                 defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n         setDatanodeDead(dn);\n         nodes.add(dn);\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    final boolean listLiveNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.LIVE;\n    final boolean listDeadNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DEAD;\n    final boolean listDecommissioningNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n    final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n    final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n    final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n\n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003c\u003e(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead \u003d isDatanodeDead(dn);\n        final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n        if ((listLiveNodes \u0026\u0026 !isDead) ||\n            (listDeadNodes \u0026\u0026 isDead) ||\n            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) {\n            nodes.add(dn);\n        }\n        foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n      }\n    }\n    Collections.sort(nodes);\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n        // dead.\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        setDatanodeDead(dn);\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "7a7960be41c32f20ffec9fea811878b113da62db": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7433. Optimize performance of DatanodeManager\u0027s node map. Contributed by Daryn Sharp.\n",
      "commitDate": "08/05/15 1:18 PM",
      "commitName": "7a7960be41c32f20ffec9fea811878b113da62db",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "14/04/15 10:19 AM",
      "commitNameOld": "fef596df038112cbbc86c4dc49314e274fca0190",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 24.12,
      "commitsBetweenForRepo": 261,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,64 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     final boolean listLiveNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.LIVE;\n     final boolean listDeadNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DEAD;\n     final boolean listDecommissioningNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes;\n     final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n     final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n     final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n \n     synchronized(datanodeMap) {\n       nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n       for (DatanodeDescriptor dn : datanodeMap.values()) {\n         final boolean isDead \u003d isDatanodeDead(dn);\n         final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n         if ((listLiveNodes \u0026\u0026 !isDead) ||\n             (listDeadNodes \u0026\u0026 isDead) ||\n             (listDecommissioningNodes \u0026\u0026 isDecommissioning)) {\n             nodes.add(dn);\n         }\n         foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n       }\n     }\n+    Collections.sort(nodes);\n \n     if (listDeadNodes) {\n       for (InetSocketAddress addr : includedNodes) {\n         if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n           continue;\n         }\n         // The remaining nodes are ones that are referenced by the hosts\n         // files but that we do not know about, ie that we have never\n         // head from. Eg. an entry that is no longer part of the cluster\n         // or a bogus entry was given in the hosts files\n         //\n         // If the host file entry specified the xferPort, we use that.\n         // Otherwise, we guess that it is the default xfer port.\n         // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n         // dead.\n         DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                 .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                 addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                 defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n         setDatanodeDead(dn);\n         nodes.add(dn);\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    final boolean listLiveNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.LIVE;\n    final boolean listDeadNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DEAD;\n    final boolean listDecommissioningNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n    final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n    final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n    final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n\n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead \u003d isDatanodeDead(dn);\n        final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n        if ((listLiveNodes \u0026\u0026 !isDead) ||\n            (listDeadNodes \u0026\u0026 isDead) ||\n            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) {\n            nodes.add(dn);\n        }\n        foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n      }\n    }\n    Collections.sort(nodes);\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n        // dead.\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        setDatanodeDead(dn);\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "08/03/15 6:31 PM",
      "commitNameOld": "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 11.73,
      "commitsBetweenForRepo": 118,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,63 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     final boolean listLiveNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.LIVE;\n     final boolean listDeadNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DEAD;\n     final boolean listDecommissioningNodes \u003d\n         type \u003d\u003d DatanodeReportType.ALL ||\n         type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes;\n     final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n     final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n     final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n \n     synchronized(datanodeMap) {\n       nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n       for (DatanodeDescriptor dn : datanodeMap.values()) {\n         final boolean isDead \u003d isDatanodeDead(dn);\n         final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n         if ((listLiveNodes \u0026\u0026 !isDead) ||\n             (listDeadNodes \u0026\u0026 isDead) ||\n             (listDecommissioningNodes \u0026\u0026 isDecommissioning)) {\n             nodes.add(dn);\n         }\n         foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n       }\n     }\n \n     if (listDeadNodes) {\n       for (InetSocketAddress addr : includedNodes) {\n         if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n           continue;\n         }\n         // The remaining nodes are ones that are referenced by the hosts\n         // files but that we do not know about, ie that we have never\n         // head from. Eg. an entry that is no longer part of the cluster\n         // or a bogus entry was given in the hosts files\n         //\n         // If the host file entry specified the xferPort, we use that.\n         // Otherwise, we guess that it is the default xfer port.\n         // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n         // dead.\n         DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                 .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                 addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                 defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n-        dn.setLastUpdate(0); // Consider this node dead for reporting\n+        setDatanodeDead(dn);\n         nodes.add(dn);\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    final boolean listLiveNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.LIVE;\n    final boolean listDeadNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DEAD;\n    final boolean listDecommissioningNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n    final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n    final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n    final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n\n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead \u003d isDatanodeDead(dn);\n        final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n        if ((listLiveNodes \u0026\u0026 !isDead) ||\n            (listDeadNodes \u0026\u0026 isDead) ||\n            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) {\n            nodes.add(dn);\n        }\n        foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n      }\n    }\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n        // dead.\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        setDatanodeDead(dn);\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "9445859930b8653cb0b9a0e1abf38cc05dbe2658": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6295. Add decommissioning state and node state filtering to dfsadmin. Contributed by Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1592438 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/05/14 8:38 PM",
      "commitName": "9445859930b8653cb0b9a0e1abf38cc05dbe2658",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "03/05/14 4:02 AM",
      "commitNameOld": "b2f65c276da2c4420a0974a7e2d75e081abf5d63",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.69,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,63 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n-    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                            type \u003d\u003d DatanodeReportType.LIVE;\n-    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                            type \u003d\u003d DatanodeReportType.DEAD;\n+    final boolean listLiveNodes \u003d\n+        type \u003d\u003d DatanodeReportType.ALL ||\n+        type \u003d\u003d DatanodeReportType.LIVE;\n+    final boolean listDeadNodes \u003d\n+        type \u003d\u003d DatanodeReportType.ALL ||\n+        type \u003d\u003d DatanodeReportType.DEAD;\n+    final boolean listDecommissioningNodes \u003d\n+        type \u003d\u003d DatanodeReportType.ALL ||\n+        type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes;\n     final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n     final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n     final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n \n     synchronized(datanodeMap) {\n       nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n       for (DatanodeDescriptor dn : datanodeMap.values()) {\n         final boolean isDead \u003d isDatanodeDead(dn);\n-        if ((listLiveNodes \u0026\u0026 !isDead) || (listDeadNodes \u0026\u0026 isDead)) {\n+        final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n+        if ((listLiveNodes \u0026\u0026 !isDead) ||\n+            (listDeadNodes \u0026\u0026 isDead) ||\n+            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) {\n             nodes.add(dn);\n         }\n         foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n       }\n     }\n \n     if (listDeadNodes) {\n       for (InetSocketAddress addr : includedNodes) {\n         if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n           continue;\n         }\n         // The remaining nodes are ones that are referenced by the hosts\n         // files but that we do not know about, ie that we have never\n         // head from. Eg. an entry that is no longer part of the cluster\n         // or a bogus entry was given in the hosts files\n         //\n         // If the host file entry specified the xferPort, we use that.\n         // Otherwise, we guess that it is the default xfer port.\n         // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n         // dead.\n         DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                 .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                 addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                 defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n         dn.setLastUpdate(0); // Consider this node dead for reporting\n         nodes.add(dn);\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    final boolean listLiveNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.LIVE;\n    final boolean listDeadNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DEAD;\n    final boolean listDecommissioningNodes \u003d\n        type \u003d\u003d DatanodeReportType.ALL ||\n        type \u003d\u003d DatanodeReportType.DECOMMISSIONING;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n    final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n    final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n    final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n\n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead \u003d isDatanodeDead(dn);\n        final boolean isDecommissioning \u003d dn.isDecommissionInProgress();\n        if ((listLiveNodes \u0026\u0026 !isDead) ||\n            (listDeadNodes \u0026\u0026 isDead) ||\n            (listDecommissioningNodes \u0026\u0026 isDecommissioning)) {\n            nodes.add(dn);\n        }\n        foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n      }\n    }\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n        // dead.\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        dn.setLastUpdate(0); // Consider this node dead for reporting\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "2002dc63c9409de733a374d810c529e95895df44": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6180. Dead node count / listing is very broken in JMX and old GUI. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585625 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/04/14 4:55 PM",
      "commitName": "2002dc63c9409de733a374d810c529e95895df44",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/03/14 2:27 PM",
      "commitNameOld": "14556cc5d8fee8f8a846e4f65572828553be386c",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 12.1,
      "commitsBetweenForRepo": 57,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,55 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.LIVE;\n     boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.DEAD;\n \n-    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n-    final MutableEntrySet foundNodes \u003d new MutableEntrySet();\n+    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n+    final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n+    final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n+    final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n+\n     synchronized(datanodeMap) {\n       nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n-      Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n-      while (it.hasNext()) { \n-        DatanodeDescriptor dn \u003d it.next();\n+      for (DatanodeDescriptor dn : datanodeMap.values()) {\n         final boolean isDead \u003d isDatanodeDead(dn);\n-        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n-          nodes.add(dn);\n+        if ((listLiveNodes \u0026\u0026 !isDead) || (listDeadNodes \u0026\u0026 isDead)) {\n+            nodes.add(dn);\n         }\n-        foundNodes.add(dn);\n+        foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n       }\n     }\n \n     if (listDeadNodes) {\n-      final EntrySet includedNodes \u003d hostFileManager.getIncludes();\n-      final EntrySet excludedNodes \u003d hostFileManager.getExcludes();\n-      for (Entry entry : includedNodes) {\n-        if ((foundNodes.find(entry) \u003d\u003d null) \u0026\u0026\n-            (excludedNodes.find(entry) \u003d\u003d null)) {\n-          // The remaining nodes are ones that are referenced by the hosts\n-          // files but that we do not know about, ie that we have never\n-          // head from. Eg. an entry that is no longer part of the cluster\n-          // or a bogus entry was given in the hosts files\n-          //\n-          // If the host file entry specified the xferPort, we use that.\n-          // Otherwise, we guess that it is the default xfer port.\n-          // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n-          // dead.\n-          DatanodeDescriptor dn \u003d\n-              new DatanodeDescriptor(new DatanodeID(entry.getIpAddress(),\n-                  entry.getPrefix(), \"\",\n-                  entry.getPort() \u003d\u003d 0 ? defaultXferPort : entry.getPort(),\n-                  defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n-          dn.setLastUpdate(0); // Consider this node dead for reporting\n-          nodes.add(dn);\n+      for (InetSocketAddress addr : includedNodes) {\n+        if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n+          continue;\n         }\n+        // The remaining nodes are ones that are referenced by the hosts\n+        // files but that we do not know about, ie that we have never\n+        // head from. Eg. an entry that is no longer part of the cluster\n+        // or a bogus entry was given in the hosts files\n+        //\n+        // If the host file entry specified the xferPort, we use that.\n+        // Otherwise, we guess that it is the default xfer port.\n+        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n+        // dead.\n+        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n+                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n+                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n+                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n+        dn.setLastUpdate(0); // Consider this node dead for reporting\n+        nodes.add(dn);\n       }\n     }\n+\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes;\n    final HostFileManager.HostSet foundNodes \u003d new HostFileManager.HostSet();\n    final HostFileManager.HostSet includedNodes \u003d hostFileManager.getIncludes();\n    final HostFileManager.HostSet excludedNodes \u003d hostFileManager.getExcludes();\n\n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n      for (DatanodeDescriptor dn : datanodeMap.values()) {\n        final boolean isDead \u003d isDatanodeDead(dn);\n        if ((listLiveNodes \u0026\u0026 !isDead) || (listDeadNodes \u0026\u0026 isDead)) {\n            nodes.add(dn);\n        }\n        foundNodes.add(HostFileManager.resolvedAddressFromDatanodeID(dn));\n      }\n    }\n\n    if (listDeadNodes) {\n      for (InetSocketAddress addr : includedNodes) {\n        if (foundNodes.matchedBy(addr) || excludedNodes.match(addr)) {\n          continue;\n        }\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. an entry that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        //\n        // If the host file entry specified the xferPort, we use that.\n        // Otherwise, we guess that it is the default xfer port.\n        // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n        // dead.\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(new DatanodeID(addr\n                .getAddress().getHostAddress(), addr.getHostName(), \"\",\n                addr.getPort() \u003d\u003d 0 ? defaultXferPort : addr.getPort(),\n                defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n        dn.setLastUpdate(0); // Consider this node dead for reporting\n        nodes.add(dn);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": {
      "type": "Ybodychange",
      "commitMessage": "merge trunk to branch HDFS-4949\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532952 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 7:14 PM",
      "commitName": "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "16/10/13 3:15 PM",
      "commitNameOld": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 0.17,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.LIVE;\n     boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.DEAD;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n     final MutableEntrySet foundNodes \u003d new MutableEntrySet();\n     synchronized(datanodeMap) {\n       nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n       Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n       while (it.hasNext()) { \n         DatanodeDescriptor dn \u003d it.next();\n         final boolean isDead \u003d isDatanodeDead(dn);\n         if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n         foundNodes.add(dn);\n       }\n     }\n \n     if (listDeadNodes) {\n       final EntrySet includedNodes \u003d hostFileManager.getIncludes();\n       final EntrySet excludedNodes \u003d hostFileManager.getExcludes();\n       for (Entry entry : includedNodes) {\n         if ((foundNodes.find(entry) \u003d\u003d null) \u0026\u0026\n             (excludedNodes.find(entry) \u003d\u003d null)) {\n           // The remaining nodes are ones that are referenced by the hosts\n           // files but that we do not know about, ie that we have never\n           // head from. Eg. an entry that is no longer part of the cluster\n           // or a bogus entry was given in the hosts files\n           //\n           // If the host file entry specified the xferPort, we use that.\n           // Otherwise, we guess that it is the default xfer port.\n           // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n           // dead.\n           DatanodeDescriptor dn \u003d\n               new DatanodeDescriptor(new DatanodeID(entry.getIpAddress(),\n                   entry.getPrefix(), \"\",\n                   entry.getPort() \u003d\u003d 0 ? defaultXferPort : entry.getPort(),\n-                  defaultInfoPort, defaultIpcPort));\n+                  defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n           dn.setLastUpdate(0); // Consider this node dead for reporting\n           nodes.add(dn);\n         }\n       }\n     }\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    final MutableEntrySet foundNodes \u003d new MutableEntrySet();\n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        final boolean isDead \u003d isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        foundNodes.add(dn);\n      }\n    }\n\n    if (listDeadNodes) {\n      final EntrySet includedNodes \u003d hostFileManager.getIncludes();\n      final EntrySet excludedNodes \u003d hostFileManager.getExcludes();\n      for (Entry entry : includedNodes) {\n        if ((foundNodes.find(entry) \u003d\u003d null) \u0026\u0026\n            (excludedNodes.find(entry) \u003d\u003d null)) {\n          // The remaining nodes are ones that are referenced by the hosts\n          // files but that we do not know about, ie that we have never\n          // head from. Eg. an entry that is no longer part of the cluster\n          // or a bogus entry was given in the hosts files\n          //\n          // If the host file entry specified the xferPort, we use that.\n          // Otherwise, we guess that it is the default xfer port.\n          // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n          // dead.\n          DatanodeDescriptor dn \u003d\n              new DatanodeDescriptor(new DatanodeID(entry.getIpAddress(),\n                  entry.getPrefix(), \"\",\n                  entry.getPort() \u003d\u003d 0 ? defaultXferPort : entry.getPort(),\n                  defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n          dn.setLastUpdate(0); // Consider this node dead for reporting\n          nodes.add(dn);\n        }\n      }\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "8e0804666189ce9a66b7b41b744776bad29770dd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5306. Datanode https port is not available at the namenode. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529562 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/13 8:22 PM",
      "commitName": "8e0804666189ce9a66b7b41b744776bad29770dd",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "26/09/13 8:24 AM",
      "commitNameOld": "8a66e493ba03f710b353638647013401d18f413c",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 9.5,
      "commitsBetweenForRepo": 87,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.LIVE;\n     boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.DEAD;\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n     final MutableEntrySet foundNodes \u003d new MutableEntrySet();\n     synchronized(datanodeMap) {\n       nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n       Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n       while (it.hasNext()) { \n         DatanodeDescriptor dn \u003d it.next();\n         final boolean isDead \u003d isDatanodeDead(dn);\n         if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n         foundNodes.add(dn);\n       }\n     }\n \n     if (listDeadNodes) {\n       final EntrySet includedNodes \u003d hostFileManager.getIncludes();\n       final EntrySet excludedNodes \u003d hostFileManager.getExcludes();\n       for (Entry entry : includedNodes) {\n         if ((foundNodes.find(entry) \u003d\u003d null) \u0026\u0026\n             (excludedNodes.find(entry) \u003d\u003d null)) {\n           // The remaining nodes are ones that are referenced by the hosts\n           // files but that we do not know about, ie that we have never\n           // head from. Eg. an entry that is no longer part of the cluster\n           // or a bogus entry was given in the hosts files\n           //\n           // If the host file entry specified the xferPort, we use that.\n           // Otherwise, we guess that it is the default xfer port.\n           // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n           // dead.\n           DatanodeDescriptor dn \u003d\n               new DatanodeDescriptor(new DatanodeID(entry.getIpAddress(),\n                   entry.getPrefix(), \"\",\n                   entry.getPort() \u003d\u003d 0 ? defaultXferPort : entry.getPort(),\n-                  defaultInfoPort, defaultIpcPort));\n+                  defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n           dn.setLastUpdate(0); // Consider this node dead for reporting\n           nodes.add(dn);\n         }\n       }\n     }\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getDatanodeListForReport with \" +\n           \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n           \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n           \", foundNodes \u003d \" + foundNodes +\n           \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    final MutableEntrySet foundNodes \u003d new MutableEntrySet();\n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        final boolean isDead \u003d isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        foundNodes.add(dn);\n      }\n    }\n\n    if (listDeadNodes) {\n      final EntrySet includedNodes \u003d hostFileManager.getIncludes();\n      final EntrySet excludedNodes \u003d hostFileManager.getExcludes();\n      for (Entry entry : includedNodes) {\n        if ((foundNodes.find(entry) \u003d\u003d null) \u0026\u0026\n            (excludedNodes.find(entry) \u003d\u003d null)) {\n          // The remaining nodes are ones that are referenced by the hosts\n          // files but that we do not know about, ie that we have never\n          // head from. Eg. an entry that is no longer part of the cluster\n          // or a bogus entry was given in the hosts files\n          //\n          // If the host file entry specified the xferPort, we use that.\n          // Otherwise, we guess that it is the default xfer port.\n          // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n          // dead.\n          DatanodeDescriptor dn \u003d\n              new DatanodeDescriptor(new DatanodeID(entry.getIpAddress(),\n                  entry.getPrefix(), \"\",\n                  entry.getPort() \u003d\u003d 0 ? defaultXferPort : entry.getPort(),\n                  defaultInfoPort, defaultInfoSecurePort, defaultIpcPort));\n          dn.setLastUpdate(0); // Consider this node dead for reporting\n          nodes.add(dn);\n        }\n      }\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "a7bfb25d2bbab0a329712d1efb143edc49a4076d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3934. duplicative dfs_hosts entries handled wrong. (cmccabe)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489065 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/06/13 10:14 AM",
      "commitName": "a7bfb25d2bbab0a329712d1efb143edc49a4076d",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "28/05/13 1:17 PM",
      "commitNameOld": "4bb72210c266707806f3ce3e974968a9a137b25b",
      "commitAuthorOld": "Devaraj Das",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 49,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,56 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.LIVE;\n     boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.DEAD;\n \n-    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n-\n-    if (listDeadNodes) {\n-      // Put all nodes referenced in the hosts files in the map\n-      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n-      while (it.hasNext()) {\n-        mustList.put(it.next(), \"\");\n-      }\n-      it \u003d hostsReader.getExcludedHosts().iterator(); \n-      while (it.hasNext()) {\n-        mustList.put(it.next(), \"\");\n-      }\n-    }\n-\n     ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n-    \n+    final MutableEntrySet foundNodes \u003d new MutableEntrySet();\n     synchronized(datanodeMap) {\n-      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n-                                                mustList.size());\n+      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n       Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n       while (it.hasNext()) { \n         DatanodeDescriptor dn \u003d it.next();\n         final boolean isDead \u003d isDatanodeDead(dn);\n         if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n-        for (String name : getNodeNamesForHostFiltering(dn)) {\n-          mustList.remove(name);\n+        foundNodes.add(dn);\n+      }\n+    }\n+\n+    if (listDeadNodes) {\n+      final EntrySet includedNodes \u003d hostFileManager.getIncludes();\n+      final EntrySet excludedNodes \u003d hostFileManager.getExcludes();\n+      for (Entry entry : includedNodes) {\n+        if ((foundNodes.find(entry) \u003d\u003d null) \u0026\u0026\n+            (excludedNodes.find(entry) \u003d\u003d null)) {\n+          // The remaining nodes are ones that are referenced by the hosts\n+          // files but that we do not know about, ie that we have never\n+          // head from. Eg. an entry that is no longer part of the cluster\n+          // or a bogus entry was given in the hosts files\n+          //\n+          // If the host file entry specified the xferPort, we use that.\n+          // Otherwise, we guess that it is the default xfer port.\n+          // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n+          // dead.\n+          DatanodeDescriptor dn \u003d\n+              new DatanodeDescriptor(new DatanodeID(entry.getIpAddress(),\n+                  entry.getPrefix(), \"\",\n+                  entry.getPort() \u003d\u003d 0 ? defaultXferPort : entry.getPort(),\n+                  defaultInfoPort, defaultIpcPort));\n+          dn.setLastUpdate(0); // Consider this node dead for reporting\n+          nodes.add(dn);\n         }\n       }\n     }\n-    \n-    if (listDeadNodes) {\n-      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n-      while (it.hasNext()) {\n-        // The remaining nodes are ones that are referenced by the hosts\n-        // files but that we do not know about, ie that we have never\n-        // head from. Eg. a host that is no longer part of the cluster\n-        // or a bogus entry was given in the hosts files\n-        DatanodeID dnId \u003d parseDNFromHostsEntry(it.next());\n-        DatanodeDescriptor dn \u003d new DatanodeDescriptor(dnId); \n-        dn.setLastUpdate(0); // Consider this node dead for reporting\n-        nodes.add(dn);\n-      }\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"getDatanodeListForReport with \" +\n+          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n+          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n+          \", foundNodes \u003d \" + foundNodes +\n+          \", nodes \u003d \" + nodes);\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    final MutableEntrySet foundNodes \u003d new MutableEntrySet();\n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        final boolean isDead \u003d isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        foundNodes.add(dn);\n      }\n    }\n\n    if (listDeadNodes) {\n      final EntrySet includedNodes \u003d hostFileManager.getIncludes();\n      final EntrySet excludedNodes \u003d hostFileManager.getExcludes();\n      for (Entry entry : includedNodes) {\n        if ((foundNodes.find(entry) \u003d\u003d null) \u0026\u0026\n            (excludedNodes.find(entry) \u003d\u003d null)) {\n          // The remaining nodes are ones that are referenced by the hosts\n          // files but that we do not know about, ie that we have never\n          // head from. Eg. an entry that is no longer part of the cluster\n          // or a bogus entry was given in the hosts files\n          //\n          // If the host file entry specified the xferPort, we use that.\n          // Otherwise, we guess that it is the default xfer port.\n          // We can\u0027t ask the DataNode what it had configured, because it\u0027s\n          // dead.\n          DatanodeDescriptor dn \u003d\n              new DatanodeDescriptor(new DatanodeID(entry.getIpAddress(),\n                  entry.getPrefix(), \"\",\n                  entry.getPort() \u003d\u003d 0 ? defaultXferPort : entry.getPort(),\n                  defaultInfoPort, defaultIpcPort));\n          dn.setLastUpdate(0); // Consider this node dead for reporting\n          nodes.add(dn);\n        }\n      }\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getDatanodeListForReport with \" +\n          \"includedNodes \u003d \" + hostFileManager.getIncludes() +\n          \", excludedNodes \u003d \" + hostFileManager.getExcludes() +\n          \", foundNodes \u003d \" + foundNodes +\n          \", nodes \u003d \" + nodes);\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "be94bf6b57895846853d3e0ebc5c33b4f725ae2c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3990.  NN\u0027s health report has severe performance problems (daryn)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1407333 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/11/12 4:53 PM",
      "commitName": "be94bf6b57895846853d3e0ebc5c33b4f725ae2c",
      "commitAuthor": "Daryn Sharp",
      "commitDateOld": "06/11/12 11:27 AM",
      "commitNameOld": "54b70db347c2ebf577919f2c42f171c6801e9ba1",
      "commitAuthorOld": "Daryn Sharp",
      "daysBetweenCommits": 2.23,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,54 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.LIVE;\n     boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.DEAD;\n \n     HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n \n     if (listDeadNodes) {\n       // Put all nodes referenced in the hosts files in the map\n       Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n       while (it.hasNext()) {\n         mustList.put(it.next(), \"\");\n       }\n       it \u003d hostsReader.getExcludedHosts().iterator(); \n       while (it.hasNext()) {\n         mustList.put(it.next(), \"\");\n       }\n     }\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n     \n     synchronized(datanodeMap) {\n       nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                 mustList.size());\n       Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n       while (it.hasNext()) { \n         DatanodeDescriptor dn \u003d it.next();\n         final boolean isDead \u003d isDatanodeDead(dn);\n         if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n-        // Remove any nodes we know about from the map\n-        try {\n-          InetAddress inet \u003d InetAddress.getByName(dn.getIpAddr());\n-          // compare hostname(:port)\n-          mustList.remove(inet.getHostName());\n-          mustList.remove(inet.getHostName()+\":\"+dn.getXferPort());\n-          // compare ipaddress(:port)\n-          mustList.remove(inet.getHostAddress().toString());\n-          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getXferPort());\n-        } catch (UnknownHostException e) {\n-          mustList.remove(dn.getName());\n-          mustList.remove(dn.getIpAddr());\n-          LOG.warn(e);\n+        for (String name : getNodeNamesForHostFiltering(dn)) {\n+          mustList.remove(name);\n         }\n       }\n     }\n     \n     if (listDeadNodes) {\n       Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n       while (it.hasNext()) {\n         // The remaining nodes are ones that are referenced by the hosts\n         // files but that we do not know about, ie that we have never\n         // head from. Eg. a host that is no longer part of the cluster\n         // or a bogus entry was given in the hosts files\n         DatanodeID dnId \u003d parseDNFromHostsEntry(it.next());\n         DatanodeDescriptor dn \u003d new DatanodeDescriptor(dnId); \n         dn.setLastUpdate(0); // Consider this node dead for reporting\n         nodes.add(dn);\n       }\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n\n    if (listDeadNodes) {\n      // Put all nodes referenced in the hosts files in the map\n      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n      it \u003d hostsReader.getExcludedHosts().iterator(); \n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n    }\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    \n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                mustList.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        final boolean isDead \u003d isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        for (String name : getNodeNamesForHostFiltering(dn)) {\n          mustList.remove(name);\n        }\n      }\n    }\n    \n    if (listDeadNodes) {\n      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n      while (it.hasNext()) {\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. a host that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        DatanodeID dnId \u003d parseDNFromHostsEntry(it.next());\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(dnId); \n        dn.setLastUpdate(0); // Consider this node dead for reporting\n        nodes.add(dn);\n      }\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "e505b7e704ff83893a40190695977ce1393f6248": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3208. Bogus entries in hosts files are incorrectly displayed in the report. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1310138 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/04/12 5:20 PM",
      "commitName": "e505b7e704ff83893a40190695977ce1393f6248",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "01/04/12 3:12 PM",
      "commitNameOld": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 4.09,
      "commitsBetweenForRepo": 58,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,65 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.LIVE;\n     boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.DEAD;\n \n     HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n \n     if (listDeadNodes) {\n-      //first load all the nodes listed in include and exclude files.\n+      // Put all nodes referenced in the hosts files in the map\n       Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n       while (it.hasNext()) {\n         mustList.put(it.next(), \"\");\n       }\n       it \u003d hostsReader.getExcludedHosts().iterator(); \n       while (it.hasNext()) {\n         mustList.put(it.next(), \"\");\n       }\n     }\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n     \n     synchronized(datanodeMap) {\n       nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                 mustList.size());\n       Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n       while (it.hasNext()) { \n         DatanodeDescriptor dn \u003d it.next();\n         final boolean isDead \u003d isDatanodeDead(dn);\n         if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n-        //Remove any form of the this datanode in include/exclude lists.\n+        // Remove any nodes we know about from the map\n         try {\n           InetAddress inet \u003d InetAddress.getByName(dn.getIpAddr());\n           // compare hostname(:port)\n           mustList.remove(inet.getHostName());\n           mustList.remove(inet.getHostName()+\":\"+dn.getXferPort());\n           // compare ipaddress(:port)\n           mustList.remove(inet.getHostAddress().toString());\n           mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getXferPort());\n-        } catch ( UnknownHostException e ) {\n+        } catch (UnknownHostException e) {\n           mustList.remove(dn.getName());\n           mustList.remove(dn.getIpAddr());\n           LOG.warn(e);\n         }\n       }\n     }\n     \n     if (listDeadNodes) {\n       Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n       while (it.hasNext()) {\n-        DatanodeDescriptor dn \u003d \n-            new DatanodeDescriptor(new DatanodeID(it.next()));\n-        dn.setLastUpdate(0);\n+        // The remaining nodes are ones that are referenced by the hosts\n+        // files but that we do not know about, ie that we have never\n+        // head from. Eg. a host that is no longer part of the cluster\n+        // or a bogus entry was given in the hosts files\n+        DatanodeID dnId \u003d parseDNFromHostsEntry(it.next());\n+        DatanodeDescriptor dn \u003d new DatanodeDescriptor(dnId); \n+        dn.setLastUpdate(0); // Consider this node dead for reporting\n         nodes.add(dn);\n       }\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n\n    if (listDeadNodes) {\n      // Put all nodes referenced in the hosts files in the map\n      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n      it \u003d hostsReader.getExcludedHosts().iterator(); \n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n    }\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    \n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                mustList.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        final boolean isDead \u003d isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        // Remove any nodes we know about from the map\n        try {\n          InetAddress inet \u003d InetAddress.getByName(dn.getIpAddr());\n          // compare hostname(:port)\n          mustList.remove(inet.getHostName());\n          mustList.remove(inet.getHostName()+\":\"+dn.getXferPort());\n          // compare ipaddress(:port)\n          mustList.remove(inet.getHostAddress().toString());\n          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getXferPort());\n        } catch (UnknownHostException e) {\n          mustList.remove(dn.getName());\n          mustList.remove(dn.getIpAddr());\n          LOG.warn(e);\n        }\n      }\n    }\n    \n    if (listDeadNodes) {\n      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n      while (it.hasNext()) {\n        // The remaining nodes are ones that are referenced by the hosts\n        // files but that we do not know about, ie that we have never\n        // head from. Eg. a host that is no longer part of the cluster\n        // or a bogus entry was given in the hosts files\n        DatanodeID dnId \u003d parseDNFromHostsEntry(it.next());\n        DatanodeDescriptor dn \u003d new DatanodeDescriptor(dnId); \n        dn.setLastUpdate(0); // Consider this node dead for reporting\n        nodes.add(dn);\n      }\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "31/03/12 8:41 PM",
      "commitNameOld": "0663dbaac0a19719ddf9cd4290ba893bfca69da2",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.77,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.LIVE;\n     boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.DEAD;\n \n     HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n \n     if (listDeadNodes) {\n       //first load all the nodes listed in include and exclude files.\n       Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n       while (it.hasNext()) {\n         mustList.put(it.next(), \"\");\n       }\n       it \u003d hostsReader.getExcludedHosts().iterator(); \n       while (it.hasNext()) {\n         mustList.put(it.next(), \"\");\n       }\n     }\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n     \n     synchronized(datanodeMap) {\n       nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                 mustList.size());\n       Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n       while (it.hasNext()) { \n         DatanodeDescriptor dn \u003d it.next();\n         final boolean isDead \u003d isDatanodeDead(dn);\n         if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n         //Remove any form of the this datanode in include/exclude lists.\n         try {\n-          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n+          InetAddress inet \u003d InetAddress.getByName(dn.getIpAddr());\n           // compare hostname(:port)\n           mustList.remove(inet.getHostName());\n-          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n+          mustList.remove(inet.getHostName()+\":\"+dn.getXferPort());\n           // compare ipaddress(:port)\n           mustList.remove(inet.getHostAddress().toString());\n-          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n+          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getXferPort());\n         } catch ( UnknownHostException e ) {\n           mustList.remove(dn.getName());\n-          mustList.remove(dn.getHost());\n+          mustList.remove(dn.getIpAddr());\n           LOG.warn(e);\n         }\n       }\n     }\n     \n     if (listDeadNodes) {\n       Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n       while (it.hasNext()) {\n         DatanodeDescriptor dn \u003d \n             new DatanodeDescriptor(new DatanodeID(it.next()));\n         dn.setLastUpdate(0);\n         nodes.add(dn);\n       }\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n\n    if (listDeadNodes) {\n      //first load all the nodes listed in include and exclude files.\n      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n      it \u003d hostsReader.getExcludedHosts().iterator(); \n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n    }\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    \n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                mustList.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        final boolean isDead \u003d isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        //Remove any form of the this datanode in include/exclude lists.\n        try {\n          InetAddress inet \u003d InetAddress.getByName(dn.getIpAddr());\n          // compare hostname(:port)\n          mustList.remove(inet.getHostName());\n          mustList.remove(inet.getHostName()+\":\"+dn.getXferPort());\n          // compare ipaddress(:port)\n          mustList.remove(inet.getHostAddress().toString());\n          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getXferPort());\n        } catch ( UnknownHostException e ) {\n          mustList.remove(dn.getName());\n          mustList.remove(dn.getIpAddr());\n          LOG.warn(e);\n        }\n      }\n    }\n    \n    if (listDeadNodes) {\n      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n      while (it.hasNext()) {\n        DatanodeDescriptor dn \u003d \n            new DatanodeDescriptor(new DatanodeID(it.next()));\n        dn.setLastUpdate(0);\n        nodes.add(dn);\n      }\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n\n    if (listDeadNodes) {\n      //first load all the nodes listed in include and exclude files.\n      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n      it \u003d hostsReader.getExcludedHosts().iterator(); \n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n    }\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    \n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                mustList.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        final boolean isDead \u003d isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        //Remove any form of the this datanode in include/exclude lists.\n        try {\n          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n          // compare hostname(:port)\n          mustList.remove(inet.getHostName());\n          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n          // compare ipaddress(:port)\n          mustList.remove(inet.getHostAddress().toString());\n          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n        } catch ( UnknownHostException e ) {\n          mustList.remove(dn.getName());\n          mustList.remove(dn.getHost());\n          LOG.warn(e);\n        }\n      }\n    }\n    \n    if (listDeadNodes) {\n      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n      while (it.hasNext()) {\n        DatanodeDescriptor dn \u003d \n            new DatanodeDescriptor(new DatanodeID(it.next()));\n        dn.setLastUpdate(0);\n        nodes.add(dn);\n      }\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n\n    if (listDeadNodes) {\n      //first load all the nodes listed in include and exclude files.\n      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n      it \u003d hostsReader.getExcludedHosts().iterator(); \n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n    }\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    \n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                mustList.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        final boolean isDead \u003d isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        //Remove any form of the this datanode in include/exclude lists.\n        try {\n          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n          // compare hostname(:port)\n          mustList.remove(inet.getHostName());\n          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n          // compare ipaddress(:port)\n          mustList.remove(inet.getHostAddress().toString());\n          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n        } catch ( UnknownHostException e ) {\n          mustList.remove(dn.getName());\n          mustList.remove(dn.getHost());\n          LOG.warn(e);\n        }\n      }\n    }\n    \n    if (listDeadNodes) {\n      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n      while (it.hasNext()) {\n        DatanodeDescriptor dn \u003d \n            new DatanodeDescriptor(new DatanodeID(it.next()));\n        dn.setLastUpdate(0);\n        nodes.add(dn);\n      }\n    }\n    return nodes;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java"
      }
    },
    "969a263188f7015261719fe45fa1505121ebb80e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2191.  Move datanodeMap from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151339 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/11 10:46 PM",
      "commitName": "969a263188f7015261719fe45fa1505121ebb80e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "21/07/11 9:20 PM",
      "commitNameOld": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.06,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       final DatanodeReportType type) {\n     boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.LIVE;\n     boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                             type \u003d\u003d DatanodeReportType.DEAD;\n \n     HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n \n     if (listDeadNodes) {\n       //first load all the nodes listed in include and exclude files.\n       Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n       while (it.hasNext()) {\n         mustList.put(it.next(), \"\");\n       }\n       it \u003d hostsReader.getExcludedHosts().iterator(); \n       while (it.hasNext()) {\n         mustList.put(it.next(), \"\");\n       }\n     }\n \n     ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n     \n-    synchronized (namesystem.datanodeMap) {\n-      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(namesystem.datanodeMap.size() + \n+    synchronized(datanodeMap) {\n+      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                 mustList.size());\n-      Iterator\u003cDatanodeDescriptor\u003e it \u003d namesystem.datanodeMap.values().iterator();\n+      Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n       while (it.hasNext()) { \n         DatanodeDescriptor dn \u003d it.next();\n-        boolean isDead \u003d namesystem.isDatanodeDead(dn);\n+        final boolean isDead \u003d isDatanodeDead(dn);\n         if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n         //Remove any form of the this datanode in include/exclude lists.\n         try {\n           InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n           // compare hostname(:port)\n           mustList.remove(inet.getHostName());\n           mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n           // compare ipaddress(:port)\n           mustList.remove(inet.getHostAddress().toString());\n           mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n         } catch ( UnknownHostException e ) {\n           mustList.remove(dn.getName());\n           mustList.remove(dn.getHost());\n           LOG.warn(e);\n         }\n       }\n     }\n     \n     if (listDeadNodes) {\n       Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n       while (it.hasNext()) {\n         DatanodeDescriptor dn \u003d \n             new DatanodeDescriptor(new DatanodeID(it.next()));\n         dn.setLastUpdate(0);\n         nodes.add(dn);\n       }\n     }\n     return nodes;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n\n    if (listDeadNodes) {\n      //first load all the nodes listed in include and exclude files.\n      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n      it \u003d hostsReader.getExcludedHosts().iterator(); \n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n    }\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    \n    synchronized(datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                mustList.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        final boolean isDead \u003d isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        //Remove any form of the this datanode in include/exclude lists.\n        try {\n          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n          // compare hostname(:port)\n          mustList.remove(inet.getHostName());\n          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n          // compare ipaddress(:port)\n          mustList.remove(inet.getHostAddress().toString());\n          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n        } catch ( UnknownHostException e ) {\n          mustList.remove(dn.getName());\n          mustList.remove(dn.getHost());\n          LOG.warn(e);\n        }\n      }\n    }\n    \n    if (listDeadNodes) {\n      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n      while (it.hasNext()) {\n        DatanodeDescriptor dn \u003d \n            new DatanodeDescriptor(new DatanodeID(it.next()));\n        dn.setLastUpdate(0);\n        nodes.add(dn);\n      }\n    }\n    return nodes;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparametermetachange)",
      "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/07/11 9:20 PM",
      "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/07/11 9:20 PM",
          "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/07/11 12:16 PM",
          "commitNameOld": "c187bdc0a28e4f3b9378e2b1daa964c23b599383",
          "commitAuthorOld": "Owen O\u0027Malley",
          "daysBetweenCommits": 0.38,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,61 @@\n-  private ArrayList\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n-      DatanodeReportType type) {\n-    readLock();\n-    try {    \n-      boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                              type \u003d\u003d DatanodeReportType.LIVE;\n-      boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                              type \u003d\u003d DatanodeReportType.DEAD;\n-  \n-      HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n-  \n-      if (listDeadNodes) {\n-        //first load all the nodes listed in include and exclude files.\n-        Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n-        while (it.hasNext()) {\n-          mustList.put(it.next(), \"\");\n-        }\n-        it \u003d hostsReader.getExcludedHosts().iterator(); \n-        while (it.hasNext()) {\n-          mustList.put(it.next(), \"\");\n-        }\n-      }\n+  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n+      final DatanodeReportType type) {\n+    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                            type \u003d\u003d DatanodeReportType.LIVE;\n+    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                            type \u003d\u003d DatanodeReportType.DEAD;\n \n-      ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n-      \n-      synchronized (datanodeMap) {\n-        nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n-                                                  mustList.size());\n-        Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n-        while (it.hasNext()) { \n-          DatanodeDescriptor dn \u003d it.next();\n-          boolean isDead \u003d isDatanodeDead(dn);\n-          if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n-            nodes.add(dn);\n-          }\n-          //Remove any form of the this datanode in include/exclude lists.\n-          try {\n-            InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n-            // compare hostname(:port)\n-            mustList.remove(inet.getHostName());\n-            mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n-            // compare ipaddress(:port)\n-            mustList.remove(inet.getHostAddress().toString());\n-            mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n-          } catch ( UnknownHostException e ) {\n-            mustList.remove(dn.getName());\n-            mustList.remove(dn.getHost());\n-            LOG.warn(e);\n-          }\n-        }\n+    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n+\n+    if (listDeadNodes) {\n+      //first load all the nodes listed in include and exclude files.\n+      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n+      while (it.hasNext()) {\n+        mustList.put(it.next(), \"\");\n       }\n-      \n-      if (listDeadNodes) {\n-        Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n-        while (it.hasNext()) {\n-          DatanodeDescriptor dn \u003d \n-              new DatanodeDescriptor(new DatanodeID(it.next()));\n-          dn.setLastUpdate(0);\n+      it \u003d hostsReader.getExcludedHosts().iterator(); \n+      while (it.hasNext()) {\n+        mustList.put(it.next(), \"\");\n+      }\n+    }\n+\n+    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n+    \n+    synchronized (namesystem.datanodeMap) {\n+      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(namesystem.datanodeMap.size() + \n+                                                mustList.size());\n+      Iterator\u003cDatanodeDescriptor\u003e it \u003d namesystem.datanodeMap.values().iterator();\n+      while (it.hasNext()) { \n+        DatanodeDescriptor dn \u003d it.next();\n+        boolean isDead \u003d namesystem.isDatanodeDead(dn);\n+        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n+        //Remove any form of the this datanode in include/exclude lists.\n+        try {\n+          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n+          // compare hostname(:port)\n+          mustList.remove(inet.getHostName());\n+          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n+          // compare ipaddress(:port)\n+          mustList.remove(inet.getHostAddress().toString());\n+          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n+        } catch ( UnknownHostException e ) {\n+          mustList.remove(dn.getName());\n+          mustList.remove(dn.getHost());\n+          LOG.warn(e);\n+        }\n       }\n-      return nodes;\n-    } finally {\n-      readUnlock();\n     }\n+    \n+    if (listDeadNodes) {\n+      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n+      while (it.hasNext()) {\n+        DatanodeDescriptor dn \u003d \n+            new DatanodeDescriptor(new DatanodeID(it.next()));\n+        dn.setLastUpdate(0);\n+        nodes.add(dn);\n+      }\n+    }\n+    return nodes;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n\n    if (listDeadNodes) {\n      //first load all the nodes listed in include and exclude files.\n      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n      it \u003d hostsReader.getExcludedHosts().iterator(); \n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n    }\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    \n    synchronized (namesystem.datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(namesystem.datanodeMap.size() + \n                                                mustList.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d namesystem.datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        boolean isDead \u003d namesystem.isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        //Remove any form of the this datanode in include/exclude lists.\n        try {\n          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n          // compare hostname(:port)\n          mustList.remove(inet.getHostName());\n          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n          // compare ipaddress(:port)\n          mustList.remove(inet.getHostAddress().toString());\n          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n        } catch ( UnknownHostException e ) {\n          mustList.remove(dn.getName());\n          mustList.remove(dn.getHost());\n          LOG.warn(e);\n        }\n      }\n    }\n    \n    if (listDeadNodes) {\n      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n      while (it.hasNext()) {\n        DatanodeDescriptor dn \u003d \n            new DatanodeDescriptor(new DatanodeID(it.next()));\n        dn.setLastUpdate(0);\n        nodes.add(dn);\n      }\n    }\n    return nodes;\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
            "oldMethodName": "getDatanodeListForReport",
            "newMethodName": "getDatanodeListForReport"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/07/11 9:20 PM",
          "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/07/11 12:16 PM",
          "commitNameOld": "c187bdc0a28e4f3b9378e2b1daa964c23b599383",
          "commitAuthorOld": "Owen O\u0027Malley",
          "daysBetweenCommits": 0.38,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,61 @@\n-  private ArrayList\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n-      DatanodeReportType type) {\n-    readLock();\n-    try {    \n-      boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                              type \u003d\u003d DatanodeReportType.LIVE;\n-      boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                              type \u003d\u003d DatanodeReportType.DEAD;\n-  \n-      HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n-  \n-      if (listDeadNodes) {\n-        //first load all the nodes listed in include and exclude files.\n-        Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n-        while (it.hasNext()) {\n-          mustList.put(it.next(), \"\");\n-        }\n-        it \u003d hostsReader.getExcludedHosts().iterator(); \n-        while (it.hasNext()) {\n-          mustList.put(it.next(), \"\");\n-        }\n-      }\n+  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n+      final DatanodeReportType type) {\n+    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                            type \u003d\u003d DatanodeReportType.LIVE;\n+    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                            type \u003d\u003d DatanodeReportType.DEAD;\n \n-      ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n-      \n-      synchronized (datanodeMap) {\n-        nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n-                                                  mustList.size());\n-        Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n-        while (it.hasNext()) { \n-          DatanodeDescriptor dn \u003d it.next();\n-          boolean isDead \u003d isDatanodeDead(dn);\n-          if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n-            nodes.add(dn);\n-          }\n-          //Remove any form of the this datanode in include/exclude lists.\n-          try {\n-            InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n-            // compare hostname(:port)\n-            mustList.remove(inet.getHostName());\n-            mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n-            // compare ipaddress(:port)\n-            mustList.remove(inet.getHostAddress().toString());\n-            mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n-          } catch ( UnknownHostException e ) {\n-            mustList.remove(dn.getName());\n-            mustList.remove(dn.getHost());\n-            LOG.warn(e);\n-          }\n-        }\n+    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n+\n+    if (listDeadNodes) {\n+      //first load all the nodes listed in include and exclude files.\n+      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n+      while (it.hasNext()) {\n+        mustList.put(it.next(), \"\");\n       }\n-      \n-      if (listDeadNodes) {\n-        Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n-        while (it.hasNext()) {\n-          DatanodeDescriptor dn \u003d \n-              new DatanodeDescriptor(new DatanodeID(it.next()));\n-          dn.setLastUpdate(0);\n+      it \u003d hostsReader.getExcludedHosts().iterator(); \n+      while (it.hasNext()) {\n+        mustList.put(it.next(), \"\");\n+      }\n+    }\n+\n+    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n+    \n+    synchronized (namesystem.datanodeMap) {\n+      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(namesystem.datanodeMap.size() + \n+                                                mustList.size());\n+      Iterator\u003cDatanodeDescriptor\u003e it \u003d namesystem.datanodeMap.values().iterator();\n+      while (it.hasNext()) { \n+        DatanodeDescriptor dn \u003d it.next();\n+        boolean isDead \u003d namesystem.isDatanodeDead(dn);\n+        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n+        //Remove any form of the this datanode in include/exclude lists.\n+        try {\n+          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n+          // compare hostname(:port)\n+          mustList.remove(inet.getHostName());\n+          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n+          // compare ipaddress(:port)\n+          mustList.remove(inet.getHostAddress().toString());\n+          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n+        } catch ( UnknownHostException e ) {\n+          mustList.remove(dn.getName());\n+          mustList.remove(dn.getHost());\n+          LOG.warn(e);\n+        }\n       }\n-      return nodes;\n-    } finally {\n-      readUnlock();\n     }\n+    \n+    if (listDeadNodes) {\n+      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n+      while (it.hasNext()) {\n+        DatanodeDescriptor dn \u003d \n+            new DatanodeDescriptor(new DatanodeID(it.next()));\n+        dn.setLastUpdate(0);\n+        nodes.add(dn);\n+      }\n+    }\n+    return nodes;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n\n    if (listDeadNodes) {\n      //first load all the nodes listed in include and exclude files.\n      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n      it \u003d hostsReader.getExcludedHosts().iterator(); \n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n    }\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    \n    synchronized (namesystem.datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(namesystem.datanodeMap.size() + \n                                                mustList.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d namesystem.datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        boolean isDead \u003d namesystem.isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        //Remove any form of the this datanode in include/exclude lists.\n        try {\n          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n          // compare hostname(:port)\n          mustList.remove(inet.getHostName());\n          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n          // compare ipaddress(:port)\n          mustList.remove(inet.getHostAddress().toString());\n          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n        } catch ( UnknownHostException e ) {\n          mustList.remove(dn.getName());\n          mustList.remove(dn.getHost());\n          LOG.warn(e);\n        }\n      }\n    }\n    \n    if (listDeadNodes) {\n      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n      while (it.hasNext()) {\n        DatanodeDescriptor dn \u003d \n            new DatanodeDescriptor(new DatanodeID(it.next()));\n        dn.setLastUpdate(0);\n        nodes.add(dn);\n      }\n    }\n    return nodes;\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {
            "oldValue": "ArrayList\u003cDatanodeDescriptor\u003e",
            "newValue": "List\u003cDatanodeDescriptor\u003e"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/07/11 9:20 PM",
          "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/07/11 12:16 PM",
          "commitNameOld": "c187bdc0a28e4f3b9378e2b1daa964c23b599383",
          "commitAuthorOld": "Owen O\u0027Malley",
          "daysBetweenCommits": 0.38,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,61 @@\n-  private ArrayList\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n-      DatanodeReportType type) {\n-    readLock();\n-    try {    \n-      boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                              type \u003d\u003d DatanodeReportType.LIVE;\n-      boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                              type \u003d\u003d DatanodeReportType.DEAD;\n-  \n-      HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n-  \n-      if (listDeadNodes) {\n-        //first load all the nodes listed in include and exclude files.\n-        Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n-        while (it.hasNext()) {\n-          mustList.put(it.next(), \"\");\n-        }\n-        it \u003d hostsReader.getExcludedHosts().iterator(); \n-        while (it.hasNext()) {\n-          mustList.put(it.next(), \"\");\n-        }\n-      }\n+  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n+      final DatanodeReportType type) {\n+    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                            type \u003d\u003d DatanodeReportType.LIVE;\n+    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                            type \u003d\u003d DatanodeReportType.DEAD;\n \n-      ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n-      \n-      synchronized (datanodeMap) {\n-        nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n-                                                  mustList.size());\n-        Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n-        while (it.hasNext()) { \n-          DatanodeDescriptor dn \u003d it.next();\n-          boolean isDead \u003d isDatanodeDead(dn);\n-          if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n-            nodes.add(dn);\n-          }\n-          //Remove any form of the this datanode in include/exclude lists.\n-          try {\n-            InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n-            // compare hostname(:port)\n-            mustList.remove(inet.getHostName());\n-            mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n-            // compare ipaddress(:port)\n-            mustList.remove(inet.getHostAddress().toString());\n-            mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n-          } catch ( UnknownHostException e ) {\n-            mustList.remove(dn.getName());\n-            mustList.remove(dn.getHost());\n-            LOG.warn(e);\n-          }\n-        }\n+    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n+\n+    if (listDeadNodes) {\n+      //first load all the nodes listed in include and exclude files.\n+      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n+      while (it.hasNext()) {\n+        mustList.put(it.next(), \"\");\n       }\n-      \n-      if (listDeadNodes) {\n-        Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n-        while (it.hasNext()) {\n-          DatanodeDescriptor dn \u003d \n-              new DatanodeDescriptor(new DatanodeID(it.next()));\n-          dn.setLastUpdate(0);\n+      it \u003d hostsReader.getExcludedHosts().iterator(); \n+      while (it.hasNext()) {\n+        mustList.put(it.next(), \"\");\n+      }\n+    }\n+\n+    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n+    \n+    synchronized (namesystem.datanodeMap) {\n+      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(namesystem.datanodeMap.size() + \n+                                                mustList.size());\n+      Iterator\u003cDatanodeDescriptor\u003e it \u003d namesystem.datanodeMap.values().iterator();\n+      while (it.hasNext()) { \n+        DatanodeDescriptor dn \u003d it.next();\n+        boolean isDead \u003d namesystem.isDatanodeDead(dn);\n+        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n+        //Remove any form of the this datanode in include/exclude lists.\n+        try {\n+          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n+          // compare hostname(:port)\n+          mustList.remove(inet.getHostName());\n+          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n+          // compare ipaddress(:port)\n+          mustList.remove(inet.getHostAddress().toString());\n+          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n+        } catch ( UnknownHostException e ) {\n+          mustList.remove(dn.getName());\n+          mustList.remove(dn.getHost());\n+          LOG.warn(e);\n+        }\n       }\n-      return nodes;\n-    } finally {\n-      readUnlock();\n     }\n+    \n+    if (listDeadNodes) {\n+      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n+      while (it.hasNext()) {\n+        DatanodeDescriptor dn \u003d \n+            new DatanodeDescriptor(new DatanodeID(it.next()));\n+        dn.setLastUpdate(0);\n+        nodes.add(dn);\n+      }\n+    }\n+    return nodes;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n\n    if (listDeadNodes) {\n      //first load all the nodes listed in include and exclude files.\n      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n      it \u003d hostsReader.getExcludedHosts().iterator(); \n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n    }\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    \n    synchronized (namesystem.datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(namesystem.datanodeMap.size() + \n                                                mustList.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d namesystem.datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        boolean isDead \u003d namesystem.isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        //Remove any form of the this datanode in include/exclude lists.\n        try {\n          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n          // compare hostname(:port)\n          mustList.remove(inet.getHostName());\n          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n          // compare ipaddress(:port)\n          mustList.remove(inet.getHostAddress().toString());\n          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n        } catch ( UnknownHostException e ) {\n          mustList.remove(dn.getName());\n          mustList.remove(dn.getHost());\n          LOG.warn(e);\n        }\n      }\n    }\n    \n    if (listDeadNodes) {\n      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n      while (it.hasNext()) {\n        DatanodeDescriptor dn \u003d \n            new DatanodeDescriptor(new DatanodeID(it.next()));\n        dn.setLastUpdate(0);\n        nodes.add(dn);\n      }\n    }\n    return nodes;\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/07/11 9:20 PM",
          "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/07/11 12:16 PM",
          "commitNameOld": "c187bdc0a28e4f3b9378e2b1daa964c23b599383",
          "commitAuthorOld": "Owen O\u0027Malley",
          "daysBetweenCommits": 0.38,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,61 @@\n-  private ArrayList\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n-      DatanodeReportType type) {\n-    readLock();\n-    try {    \n-      boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                              type \u003d\u003d DatanodeReportType.LIVE;\n-      boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                              type \u003d\u003d DatanodeReportType.DEAD;\n-  \n-      HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n-  \n-      if (listDeadNodes) {\n-        //first load all the nodes listed in include and exclude files.\n-        Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n-        while (it.hasNext()) {\n-          mustList.put(it.next(), \"\");\n-        }\n-        it \u003d hostsReader.getExcludedHosts().iterator(); \n-        while (it.hasNext()) {\n-          mustList.put(it.next(), \"\");\n-        }\n-      }\n+  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n+      final DatanodeReportType type) {\n+    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                            type \u003d\u003d DatanodeReportType.LIVE;\n+    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                            type \u003d\u003d DatanodeReportType.DEAD;\n \n-      ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n-      \n-      synchronized (datanodeMap) {\n-        nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n-                                                  mustList.size());\n-        Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n-        while (it.hasNext()) { \n-          DatanodeDescriptor dn \u003d it.next();\n-          boolean isDead \u003d isDatanodeDead(dn);\n-          if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n-            nodes.add(dn);\n-          }\n-          //Remove any form of the this datanode in include/exclude lists.\n-          try {\n-            InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n-            // compare hostname(:port)\n-            mustList.remove(inet.getHostName());\n-            mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n-            // compare ipaddress(:port)\n-            mustList.remove(inet.getHostAddress().toString());\n-            mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n-          } catch ( UnknownHostException e ) {\n-            mustList.remove(dn.getName());\n-            mustList.remove(dn.getHost());\n-            LOG.warn(e);\n-          }\n-        }\n+    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n+\n+    if (listDeadNodes) {\n+      //first load all the nodes listed in include and exclude files.\n+      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n+      while (it.hasNext()) {\n+        mustList.put(it.next(), \"\");\n       }\n-      \n-      if (listDeadNodes) {\n-        Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n-        while (it.hasNext()) {\n-          DatanodeDescriptor dn \u003d \n-              new DatanodeDescriptor(new DatanodeID(it.next()));\n-          dn.setLastUpdate(0);\n+      it \u003d hostsReader.getExcludedHosts().iterator(); \n+      while (it.hasNext()) {\n+        mustList.put(it.next(), \"\");\n+      }\n+    }\n+\n+    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n+    \n+    synchronized (namesystem.datanodeMap) {\n+      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(namesystem.datanodeMap.size() + \n+                                                mustList.size());\n+      Iterator\u003cDatanodeDescriptor\u003e it \u003d namesystem.datanodeMap.values().iterator();\n+      while (it.hasNext()) { \n+        DatanodeDescriptor dn \u003d it.next();\n+        boolean isDead \u003d namesystem.isDatanodeDead(dn);\n+        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n+        //Remove any form of the this datanode in include/exclude lists.\n+        try {\n+          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n+          // compare hostname(:port)\n+          mustList.remove(inet.getHostName());\n+          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n+          // compare ipaddress(:port)\n+          mustList.remove(inet.getHostAddress().toString());\n+          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n+        } catch ( UnknownHostException e ) {\n+          mustList.remove(dn.getName());\n+          mustList.remove(dn.getHost());\n+          LOG.warn(e);\n+        }\n       }\n-      return nodes;\n-    } finally {\n-      readUnlock();\n     }\n+    \n+    if (listDeadNodes) {\n+      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n+      while (it.hasNext()) {\n+        DatanodeDescriptor dn \u003d \n+            new DatanodeDescriptor(new DatanodeID(it.next()));\n+        dn.setLastUpdate(0);\n+        nodes.add(dn);\n+      }\n+    }\n+    return nodes;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n\n    if (listDeadNodes) {\n      //first load all the nodes listed in include and exclude files.\n      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n      it \u003d hostsReader.getExcludedHosts().iterator(); \n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n    }\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    \n    synchronized (namesystem.datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(namesystem.datanodeMap.size() + \n                                                mustList.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d namesystem.datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        boolean isDead \u003d namesystem.isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        //Remove any form of the this datanode in include/exclude lists.\n        try {\n          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n          // compare hostname(:port)\n          mustList.remove(inet.getHostName());\n          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n          // compare ipaddress(:port)\n          mustList.remove(inet.getHostAddress().toString());\n          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n        } catch ( UnknownHostException e ) {\n          mustList.remove(dn.getName());\n          mustList.remove(dn.getHost());\n          LOG.warn(e);\n        }\n      }\n    }\n    \n    if (listDeadNodes) {\n      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n      while (it.hasNext()) {\n        DatanodeDescriptor dn \u003d \n            new DatanodeDescriptor(new DatanodeID(it.next()));\n        dn.setLastUpdate(0);\n        nodes.add(dn);\n      }\n    }\n    return nodes;\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/07/11 9:20 PM",
          "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "21/07/11 12:16 PM",
          "commitNameOld": "c187bdc0a28e4f3b9378e2b1daa964c23b599383",
          "commitAuthorOld": "Owen O\u0027Malley",
          "daysBetweenCommits": 0.38,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,61 @@\n-  private ArrayList\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n-      DatanodeReportType type) {\n-    readLock();\n-    try {    \n-      boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                              type \u003d\u003d DatanodeReportType.LIVE;\n-      boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n-                              type \u003d\u003d DatanodeReportType.DEAD;\n-  \n-      HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n-  \n-      if (listDeadNodes) {\n-        //first load all the nodes listed in include and exclude files.\n-        Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n-        while (it.hasNext()) {\n-          mustList.put(it.next(), \"\");\n-        }\n-        it \u003d hostsReader.getExcludedHosts().iterator(); \n-        while (it.hasNext()) {\n-          mustList.put(it.next(), \"\");\n-        }\n-      }\n+  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n+      final DatanodeReportType type) {\n+    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                            type \u003d\u003d DatanodeReportType.LIVE;\n+    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                            type \u003d\u003d DatanodeReportType.DEAD;\n \n-      ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n-      \n-      synchronized (datanodeMap) {\n-        nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n-                                                  mustList.size());\n-        Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n-        while (it.hasNext()) { \n-          DatanodeDescriptor dn \u003d it.next();\n-          boolean isDead \u003d isDatanodeDead(dn);\n-          if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n-            nodes.add(dn);\n-          }\n-          //Remove any form of the this datanode in include/exclude lists.\n-          try {\n-            InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n-            // compare hostname(:port)\n-            mustList.remove(inet.getHostName());\n-            mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n-            // compare ipaddress(:port)\n-            mustList.remove(inet.getHostAddress().toString());\n-            mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n-          } catch ( UnknownHostException e ) {\n-            mustList.remove(dn.getName());\n-            mustList.remove(dn.getHost());\n-            LOG.warn(e);\n-          }\n-        }\n+    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n+\n+    if (listDeadNodes) {\n+      //first load all the nodes listed in include and exclude files.\n+      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n+      while (it.hasNext()) {\n+        mustList.put(it.next(), \"\");\n       }\n-      \n-      if (listDeadNodes) {\n-        Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n-        while (it.hasNext()) {\n-          DatanodeDescriptor dn \u003d \n-              new DatanodeDescriptor(new DatanodeID(it.next()));\n-          dn.setLastUpdate(0);\n+      it \u003d hostsReader.getExcludedHosts().iterator(); \n+      while (it.hasNext()) {\n+        mustList.put(it.next(), \"\");\n+      }\n+    }\n+\n+    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n+    \n+    synchronized (namesystem.datanodeMap) {\n+      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(namesystem.datanodeMap.size() + \n+                                                mustList.size());\n+      Iterator\u003cDatanodeDescriptor\u003e it \u003d namesystem.datanodeMap.values().iterator();\n+      while (it.hasNext()) { \n+        DatanodeDescriptor dn \u003d it.next();\n+        boolean isDead \u003d namesystem.isDatanodeDead(dn);\n+        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n           nodes.add(dn);\n         }\n+        //Remove any form of the this datanode in include/exclude lists.\n+        try {\n+          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n+          // compare hostname(:port)\n+          mustList.remove(inet.getHostName());\n+          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n+          // compare ipaddress(:port)\n+          mustList.remove(inet.getHostAddress().toString());\n+          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n+        } catch ( UnknownHostException e ) {\n+          mustList.remove(dn.getName());\n+          mustList.remove(dn.getHost());\n+          LOG.warn(e);\n+        }\n       }\n-      return nodes;\n-    } finally {\n-      readUnlock();\n     }\n+    \n+    if (listDeadNodes) {\n+      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n+      while (it.hasNext()) {\n+        DatanodeDescriptor dn \u003d \n+            new DatanodeDescriptor(new DatanodeID(it.next()));\n+        dn.setLastUpdate(0);\n+        nodes.add(dn);\n+      }\n+    }\n+    return nodes;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public List\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      final DatanodeReportType type) {\n    boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.LIVE;\n    boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                            type \u003d\u003d DatanodeReportType.DEAD;\n\n    HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n\n    if (listDeadNodes) {\n      //first load all the nodes listed in include and exclude files.\n      Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n      it \u003d hostsReader.getExcludedHosts().iterator(); \n      while (it.hasNext()) {\n        mustList.put(it.next(), \"\");\n      }\n    }\n\n    ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n    \n    synchronized (namesystem.datanodeMap) {\n      nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(namesystem.datanodeMap.size() + \n                                                mustList.size());\n      Iterator\u003cDatanodeDescriptor\u003e it \u003d namesystem.datanodeMap.values().iterator();\n      while (it.hasNext()) { \n        DatanodeDescriptor dn \u003d it.next();\n        boolean isDead \u003d namesystem.isDatanodeDead(dn);\n        if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n          nodes.add(dn);\n        }\n        //Remove any form of the this datanode in include/exclude lists.\n        try {\n          InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n          // compare hostname(:port)\n          mustList.remove(inet.getHostName());\n          mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n          // compare ipaddress(:port)\n          mustList.remove(inet.getHostAddress().toString());\n          mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n        } catch ( UnknownHostException e ) {\n          mustList.remove(dn.getName());\n          mustList.remove(dn.getHost());\n          LOG.warn(e);\n        }\n      }\n    }\n    \n    if (listDeadNodes) {\n      Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n      while (it.hasNext()) {\n        DatanodeDescriptor dn \u003d \n            new DatanodeDescriptor(new DatanodeID(it.next()));\n        dn.setLastUpdate(0);\n        nodes.add(dn);\n      }\n    }\n    return nodes;\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {
            "oldValue": "[type-DatanodeReportType]",
            "newValue": "[type-DatanodeReportType(modifiers-final)]"
          }
        }
      ]
    },
    "42863b9bafb9f77b76d9dcd26f0fcac859ea6f6a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2086. If the include hosts list contains host names, after restarting namenode, data nodes registration is denied.  Contributed by Tanping Wang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139090 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/06/11 3:13 PM",
      "commitName": "42863b9bafb9f77b76d9dcd26f0fcac859ea6f6a",
      "commitAuthor": "Tanping Wang",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 11.01,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,66 @@\n   private ArrayList\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n       DatanodeReportType type) {\n     readLock();\n     try {    \n       boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                               type \u003d\u003d DatanodeReportType.LIVE;\n       boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                               type \u003d\u003d DatanodeReportType.DEAD;\n   \n       HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n   \n       if (listDeadNodes) {\n         //first load all the nodes listed in include and exclude files.\n         Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n         while (it.hasNext()) {\n           mustList.put(it.next(), \"\");\n         }\n         it \u003d hostsReader.getExcludedHosts().iterator(); \n         while (it.hasNext()) {\n           mustList.put(it.next(), \"\");\n         }\n       }\n \n       ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n       \n       synchronized (datanodeMap) {\n         nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                   mustList.size());\n         Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n         while (it.hasNext()) { \n           DatanodeDescriptor dn \u003d it.next();\n           boolean isDead \u003d isDatanodeDead(dn);\n           if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n             nodes.add(dn);\n           }\n           //Remove any form of the this datanode in include/exclude lists.\n-          mustList.remove(dn.getName());\n-          mustList.remove(dn.getHost());\n-          mustList.remove(dn.getHostName());\n+          try {\n+            InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n+            // compare hostname(:port)\n+            mustList.remove(inet.getHostName());\n+            mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n+            // compare ipaddress(:port)\n+            mustList.remove(inet.getHostAddress().toString());\n+            mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n+          } catch ( UnknownHostException e ) {\n+            mustList.remove(dn.getName());\n+            mustList.remove(dn.getHost());\n+            LOG.warn(e);\n+          }\n         }\n       }\n       \n       if (listDeadNodes) {\n         Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n         while (it.hasNext()) {\n           DatanodeDescriptor dn \u003d \n               new DatanodeDescriptor(new DatanodeID(it.next()));\n           dn.setLastUpdate(0);\n           nodes.add(dn);\n         }\n       }\n       return nodes;\n     } finally {\n       readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ArrayList\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      DatanodeReportType type) {\n    readLock();\n    try {    \n      boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                              type \u003d\u003d DatanodeReportType.LIVE;\n      boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                              type \u003d\u003d DatanodeReportType.DEAD;\n  \n      HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n  \n      if (listDeadNodes) {\n        //first load all the nodes listed in include and exclude files.\n        Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n        while (it.hasNext()) {\n          mustList.put(it.next(), \"\");\n        }\n        it \u003d hostsReader.getExcludedHosts().iterator(); \n        while (it.hasNext()) {\n          mustList.put(it.next(), \"\");\n        }\n      }\n\n      ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n      \n      synchronized (datanodeMap) {\n        nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                  mustList.size());\n        Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n        while (it.hasNext()) { \n          DatanodeDescriptor dn \u003d it.next();\n          boolean isDead \u003d isDatanodeDead(dn);\n          if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n            nodes.add(dn);\n          }\n          //Remove any form of the this datanode in include/exclude lists.\n          try {\n            InetAddress inet \u003d InetAddress.getByName(dn.getHost());\n            // compare hostname(:port)\n            mustList.remove(inet.getHostName());\n            mustList.remove(inet.getHostName()+\":\"+dn.getPort());\n            // compare ipaddress(:port)\n            mustList.remove(inet.getHostAddress().toString());\n            mustList.remove(inet.getHostAddress().toString()+ \":\" +dn.getPort());\n          } catch ( UnknownHostException e ) {\n            mustList.remove(dn.getName());\n            mustList.remove(dn.getHost());\n            LOG.warn(e);\n          }\n        }\n      }\n      \n      if (listDeadNodes) {\n        Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n        while (it.hasNext()) {\n          DatanodeDescriptor dn \u003d \n              new DatanodeDescriptor(new DatanodeID(it.next()));\n          dn.setLastUpdate(0);\n          nodes.add(dn);\n        }\n      }\n      return nodes;\n    } finally {\n      readUnlock();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,56 @@\n+  private ArrayList\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n+      DatanodeReportType type) {\n+    readLock();\n+    try {    \n+      boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                              type \u003d\u003d DatanodeReportType.LIVE;\n+      boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n+                              type \u003d\u003d DatanodeReportType.DEAD;\n+  \n+      HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n+  \n+      if (listDeadNodes) {\n+        //first load all the nodes listed in include and exclude files.\n+        Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n+        while (it.hasNext()) {\n+          mustList.put(it.next(), \"\");\n+        }\n+        it \u003d hostsReader.getExcludedHosts().iterator(); \n+        while (it.hasNext()) {\n+          mustList.put(it.next(), \"\");\n+        }\n+      }\n+\n+      ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n+      \n+      synchronized (datanodeMap) {\n+        nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n+                                                  mustList.size());\n+        Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n+        while (it.hasNext()) { \n+          DatanodeDescriptor dn \u003d it.next();\n+          boolean isDead \u003d isDatanodeDead(dn);\n+          if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n+            nodes.add(dn);\n+          }\n+          //Remove any form of the this datanode in include/exclude lists.\n+          mustList.remove(dn.getName());\n+          mustList.remove(dn.getHost());\n+          mustList.remove(dn.getHostName());\n+        }\n+      }\n+      \n+      if (listDeadNodes) {\n+        Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n+        while (it.hasNext()) {\n+          DatanodeDescriptor dn \u003d \n+              new DatanodeDescriptor(new DatanodeID(it.next()));\n+          dn.setLastUpdate(0);\n+          nodes.add(dn);\n+        }\n+      }\n+      return nodes;\n+    } finally {\n+      readUnlock();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private ArrayList\u003cDatanodeDescriptor\u003e getDatanodeListForReport(\n      DatanodeReportType type) {\n    readLock();\n    try {    \n      boolean listLiveNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                              type \u003d\u003d DatanodeReportType.LIVE;\n      boolean listDeadNodes \u003d type \u003d\u003d DatanodeReportType.ALL ||\n                              type \u003d\u003d DatanodeReportType.DEAD;\n  \n      HashMap\u003cString, String\u003e mustList \u003d new HashMap\u003cString, String\u003e();\n  \n      if (listDeadNodes) {\n        //first load all the nodes listed in include and exclude files.\n        Iterator\u003cString\u003e it \u003d hostsReader.getHosts().iterator();\n        while (it.hasNext()) {\n          mustList.put(it.next(), \"\");\n        }\n        it \u003d hostsReader.getExcludedHosts().iterator(); \n        while (it.hasNext()) {\n          mustList.put(it.next(), \"\");\n        }\n      }\n\n      ArrayList\u003cDatanodeDescriptor\u003e nodes \u003d null;\n      \n      synchronized (datanodeMap) {\n        nodes \u003d new ArrayList\u003cDatanodeDescriptor\u003e(datanodeMap.size() + \n                                                  mustList.size());\n        Iterator\u003cDatanodeDescriptor\u003e it \u003d datanodeMap.values().iterator();\n        while (it.hasNext()) { \n          DatanodeDescriptor dn \u003d it.next();\n          boolean isDead \u003d isDatanodeDead(dn);\n          if ( (isDead \u0026\u0026 listDeadNodes) || (!isDead \u0026\u0026 listLiveNodes) ) {\n            nodes.add(dn);\n          }\n          //Remove any form of the this datanode in include/exclude lists.\n          mustList.remove(dn.getName());\n          mustList.remove(dn.getHost());\n          mustList.remove(dn.getHostName());\n        }\n      }\n      \n      if (listDeadNodes) {\n        Iterator\u003cString\u003e it \u003d mustList.keySet().iterator();\n        while (it.hasNext()) {\n          DatanodeDescriptor dn \u003d \n              new DatanodeDescriptor(new DatanodeID(it.next()));\n          dn.setLastUpdate(0);\n          nodes.add(dn);\n        }\n      }\n      return nodes;\n    } finally {\n      readUnlock();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}