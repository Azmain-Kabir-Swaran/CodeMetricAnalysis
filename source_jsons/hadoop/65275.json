{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "S3ABlockOutputStream.java",
  "functionName": "close",
  "functionId": "close",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
  "functionStartLine": 347,
  "functionEndLine": 414,
  "numCommitsSeen": 18,
  "timeTaken": 4204,
  "changeHistory": [
    "29b19cd59245c8809b697b3d7d7445813a685aad",
    "990063d2af0a37e9474949f33128805e34c3f016",
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
    "621b43e254afaff708cd6fc4698b29628f6abc33",
    "dab00da19f25619ccc71c7f803a235b21766bf1e",
    "6c348c56918973fd988b110e79231324a8befe12",
    "c58a59f7081d55dd2108545ebf9ee48cf43ca944",
    "39ec1515a205952eda7e171408a8b83eceb4abde",
    "15b7076ad5f2ae92d231140b2f8cebc392a92c87"
  ],
  "changeHistoryShort": {
    "29b19cd59245c8809b697b3d7d7445813a685aad": "Ybodychange",
    "990063d2af0a37e9474949f33128805e34c3f016": "Ybodychange",
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c": "Ybodychange",
    "621b43e254afaff708cd6fc4698b29628f6abc33": "Ybodychange",
    "dab00da19f25619ccc71c7f803a235b21766bf1e": "Ybodychange",
    "6c348c56918973fd988b110e79231324a8befe12": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange)",
    "c58a59f7081d55dd2108545ebf9ee48cf43ca944": "Ybodychange",
    "39ec1515a205952eda7e171408a8b83eceb4abde": "Ybodychange",
    "15b7076ad5f2ae92d231140b2f8cebc392a92c87": "Yintroduced"
  },
  "changeHistoryDetails": {
    "29b19cd59245c8809b697b3d7d7445813a685aad": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16900. Very large files can be truncated when written through the S3A FileSystem.\n\nContributed by Mukund Thakur and Steve Loughran.\n\nThis patch ensures that writes to S3A fail when more than 10,000 blocks are\nwritten. That upper bound still exists. To write massive files, make sure\nthat the value of fs.s3a.multipart.size is set to a size which is large\nenough to upload the files in fewer than 10,000 blocks.\n\nChange-Id: Icec604e2a357ffd38d7ae7bc3f887ff55f2d721a\n",
      "commitDate": "20/05/20 5:42 AM",
      "commitName": "29b19cd59245c8809b697b3d7d7445813a685aad",
      "commitAuthor": "Mukund Thakur",
      "commitDateOld": "12/11/19 10:17 AM",
      "commitNameOld": "990063d2af0a37e9474949f33128805e34c3f016",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 189.77,
      "commitsBetweenForRepo": 658,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,68 @@\n   public void close() throws IOException {\n     if (closed.getAndSet(true)) {\n       // already closed\n       LOG.debug(\"Ignoring close() as stream is already closed\");\n       return;\n     }\n     S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n     boolean hasBlock \u003d hasActiveBlock();\n     LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n         this,\n         blockCount,\n         hasBlock ? block : \"(none)\");\n     long bytes \u003d 0;\n     try {\n       if (multiPartUpload \u003d\u003d null) {\n         if (hasBlock) {\n           // no uploads of data have taken place, put the single block up.\n           // This must happen even if there is no data, so that 0 byte files\n           // are created.\n           bytes \u003d putObject();\n           bytesSubmitted \u003d bytes;\n         }\n       } else {\n         // there\u0027s an MPU in progress\u0027;\n         // IF there is more data to upload, or no data has yet been uploaded,\n         // PUT the final block\n         if (hasBlock \u0026\u0026\n             (block.hasData() || multiPartUpload.getPartsSubmitted() \u003d\u003d 0)) {\n           //send last part\n           uploadCurrentBlock();\n         }\n         // wait for the partial uploads to finish\n         final List\u003cPartETag\u003e partETags \u003d\n             multiPartUpload.waitForAllPartUploads();\n         bytes \u003d bytesSubmitted;\n         // then complete the operation\n         if (putTracker.aboutToComplete(multiPartUpload.getUploadId(),\n             partETags,\n             bytes)) {\n           multiPartUpload.complete(partETags);\n         } else {\n           LOG.info(\"File {} will be visible when the job is committed\", key);\n         }\n       }\n       if (!putTracker.outputImmediatelyVisible()) {\n         // track the number of bytes uploaded as commit operations.\n         statistics.commitUploaded(bytes);\n       }\n       LOG.debug(\"Upload complete to {} by {}\", key, writeOperationHelper);\n     } catch (IOException ioe) {\n+      // the operation failed.\n+      // if this happened during a multipart upload, abort the\n+      // operation, so as to not leave (billable) data\n+      // pending on the bucket\n+      if (multiPartUpload !\u003d null) {\n+        multiPartUpload.abort();\n+      }\n       writeOperationHelper.writeFailed(ioe);\n       throw ioe;\n     } finally {\n       cleanupWithLogger(LOG, block, blockFactory);\n       LOG.debug(\"Statistics: {}\", statistics);\n       cleanupWithLogger(LOG, statistics);\n       clearActiveBlock();\n     }\n     // Note end of write. This does not change the state of the remote FS.\n     writeOperationHelper.writeSuccessful(bytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (closed.getAndSet(true)) {\n      // already closed\n      LOG.debug(\"Ignoring close() as stream is already closed\");\n      return;\n    }\n    S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n    boolean hasBlock \u003d hasActiveBlock();\n    LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n        this,\n        blockCount,\n        hasBlock ? block : \"(none)\");\n    long bytes \u003d 0;\n    try {\n      if (multiPartUpload \u003d\u003d null) {\n        if (hasBlock) {\n          // no uploads of data have taken place, put the single block up.\n          // This must happen even if there is no data, so that 0 byte files\n          // are created.\n          bytes \u003d putObject();\n          bytesSubmitted \u003d bytes;\n        }\n      } else {\n        // there\u0027s an MPU in progress\u0027;\n        // IF there is more data to upload, or no data has yet been uploaded,\n        // PUT the final block\n        if (hasBlock \u0026\u0026\n            (block.hasData() || multiPartUpload.getPartsSubmitted() \u003d\u003d 0)) {\n          //send last part\n          uploadCurrentBlock();\n        }\n        // wait for the partial uploads to finish\n        final List\u003cPartETag\u003e partETags \u003d\n            multiPartUpload.waitForAllPartUploads();\n        bytes \u003d bytesSubmitted;\n        // then complete the operation\n        if (putTracker.aboutToComplete(multiPartUpload.getUploadId(),\n            partETags,\n            bytes)) {\n          multiPartUpload.complete(partETags);\n        } else {\n          LOG.info(\"File {} will be visible when the job is committed\", key);\n        }\n      }\n      if (!putTracker.outputImmediatelyVisible()) {\n        // track the number of bytes uploaded as commit operations.\n        statistics.commitUploaded(bytes);\n      }\n      LOG.debug(\"Upload complete to {} by {}\", key, writeOperationHelper);\n    } catch (IOException ioe) {\n      // the operation failed.\n      // if this happened during a multipart upload, abort the\n      // operation, so as to not leave (billable) data\n      // pending on the bucket\n      if (multiPartUpload !\u003d null) {\n        multiPartUpload.abort();\n      }\n      writeOperationHelper.writeFailed(ioe);\n      throw ioe;\n    } finally {\n      cleanupWithLogger(LOG, block, blockFactory);\n      LOG.debug(\"Statistics: {}\", statistics);\n      cleanupWithLogger(LOG, statistics);\n      clearActiveBlock();\n    }\n    // Note end of write. This does not change the state of the remote FS.\n    writeOperationHelper.writeSuccessful(bytes);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
      "extendedDetails": {}
    },
    "990063d2af0a37e9474949f33128805e34c3f016": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16665. Filesystems to be closed if they failed during initialize().\n\nContributed by Steve Loughran.\n\nThis FileSystem instantiation so if an IOException or RuntimeException is\nraised in the invocation of FileSystem.initialize() then a best-effort\nattempt is made to close the FS instance; exceptions raised that there\nare swallowed.\n\nThe S3AFileSystem is also modified to do its own cleanup if an\nIOException is raised during its initialize() process, it being the\nFS we know has the \"potential\" to leak threads, especially in\nextension points (e.g AWS Authenticators) which spawn threads.\n\nChange-Id: Ib84073a606c9d53bf53cbfca4629876a03894f04\n",
      "commitDate": "12/11/19 10:17 AM",
      "commitName": "990063d2af0a37e9474949f33128805e34c3f016",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "25/09/19 4:16 AM",
      "commitNameOld": "e346e3638c595a512cd582739ff51fb64c3b4950",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 48.29,
      "commitsBetweenForRepo": 247,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   public void close() throws IOException {\n     if (closed.getAndSet(true)) {\n       // already closed\n       LOG.debug(\"Ignoring close() as stream is already closed\");\n       return;\n     }\n     S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n     boolean hasBlock \u003d hasActiveBlock();\n     LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n         this,\n         blockCount,\n         hasBlock ? block : \"(none)\");\n     long bytes \u003d 0;\n     try {\n       if (multiPartUpload \u003d\u003d null) {\n         if (hasBlock) {\n           // no uploads of data have taken place, put the single block up.\n           // This must happen even if there is no data, so that 0 byte files\n           // are created.\n           bytes \u003d putObject();\n           bytesSubmitted \u003d bytes;\n         }\n       } else {\n         // there\u0027s an MPU in progress\u0027;\n         // IF there is more data to upload, or no data has yet been uploaded,\n         // PUT the final block\n         if (hasBlock \u0026\u0026\n             (block.hasData() || multiPartUpload.getPartsSubmitted() \u003d\u003d 0)) {\n           //send last part\n           uploadCurrentBlock();\n         }\n         // wait for the partial uploads to finish\n         final List\u003cPartETag\u003e partETags \u003d\n             multiPartUpload.waitForAllPartUploads();\n         bytes \u003d bytesSubmitted;\n         // then complete the operation\n         if (putTracker.aboutToComplete(multiPartUpload.getUploadId(),\n             partETags,\n             bytes)) {\n           multiPartUpload.complete(partETags);\n         } else {\n           LOG.info(\"File {} will be visible when the job is committed\", key);\n         }\n       }\n       if (!putTracker.outputImmediatelyVisible()) {\n         // track the number of bytes uploaded as commit operations.\n         statistics.commitUploaded(bytes);\n       }\n       LOG.debug(\"Upload complete to {} by {}\", key, writeOperationHelper);\n     } catch (IOException ioe) {\n       writeOperationHelper.writeFailed(ioe);\n       throw ioe;\n     } finally {\n-      closeAll(LOG, block, blockFactory);\n+      cleanupWithLogger(LOG, block, blockFactory);\n       LOG.debug(\"Statistics: {}\", statistics);\n-      closeAll(LOG, statistics);\n+      cleanupWithLogger(LOG, statistics);\n       clearActiveBlock();\n     }\n     // Note end of write. This does not change the state of the remote FS.\n     writeOperationHelper.writeSuccessful(bytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (closed.getAndSet(true)) {\n      // already closed\n      LOG.debug(\"Ignoring close() as stream is already closed\");\n      return;\n    }\n    S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n    boolean hasBlock \u003d hasActiveBlock();\n    LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n        this,\n        blockCount,\n        hasBlock ? block : \"(none)\");\n    long bytes \u003d 0;\n    try {\n      if (multiPartUpload \u003d\u003d null) {\n        if (hasBlock) {\n          // no uploads of data have taken place, put the single block up.\n          // This must happen even if there is no data, so that 0 byte files\n          // are created.\n          bytes \u003d putObject();\n          bytesSubmitted \u003d bytes;\n        }\n      } else {\n        // there\u0027s an MPU in progress\u0027;\n        // IF there is more data to upload, or no data has yet been uploaded,\n        // PUT the final block\n        if (hasBlock \u0026\u0026\n            (block.hasData() || multiPartUpload.getPartsSubmitted() \u003d\u003d 0)) {\n          //send last part\n          uploadCurrentBlock();\n        }\n        // wait for the partial uploads to finish\n        final List\u003cPartETag\u003e partETags \u003d\n            multiPartUpload.waitForAllPartUploads();\n        bytes \u003d bytesSubmitted;\n        // then complete the operation\n        if (putTracker.aboutToComplete(multiPartUpload.getUploadId(),\n            partETags,\n            bytes)) {\n          multiPartUpload.complete(partETags);\n        } else {\n          LOG.info(\"File {} will be visible when the job is committed\", key);\n        }\n      }\n      if (!putTracker.outputImmediatelyVisible()) {\n        // track the number of bytes uploaded as commit operations.\n        statistics.commitUploaded(bytes);\n      }\n      LOG.debug(\"Upload complete to {} by {}\", key, writeOperationHelper);\n    } catch (IOException ioe) {\n      writeOperationHelper.writeFailed(ioe);\n      throw ioe;\n    } finally {\n      cleanupWithLogger(LOG, block, blockFactory);\n      LOG.debug(\"Statistics: {}\", statistics);\n      cleanupWithLogger(LOG, statistics);\n      clearActiveBlock();\n    }\n    // Note end of write. This does not change the state of the remote FS.\n    writeOperationHelper.writeSuccessful(bytes);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
      "extendedDetails": {}
    },
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13786 Add S3A committer for zero-rename commits to S3 endpoints.\nContributed by Steve Loughran and Ryan Blue.\n",
      "commitDate": "22/11/17 7:28 AM",
      "commitName": "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "01/09/17 6:13 AM",
      "commitNameOld": "621b43e254afaff708cd6fc4698b29628f6abc33",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 82.09,
      "commitsBetweenForRepo": 710,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,61 @@\n   public void close() throws IOException {\n     if (closed.getAndSet(true)) {\n       // already closed\n       LOG.debug(\"Ignoring close() as stream is already closed\");\n       return;\n     }\n     S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n     boolean hasBlock \u003d hasActiveBlock();\n     LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n         this,\n         blockCount,\n         hasBlock ? block : \"(none)\");\n     long bytes \u003d 0;\n     try {\n       if (multiPartUpload \u003d\u003d null) {\n         if (hasBlock) {\n           // no uploads of data have taken place, put the single block up.\n           // This must happen even if there is no data, so that 0 byte files\n           // are created.\n           bytes \u003d putObject();\n+          bytesSubmitted \u003d bytes;\n         }\n       } else {\n-        // there has already been at least one block scheduled for upload;\n-        // put up the current then wait\n-        if (hasBlock \u0026\u0026 block.hasData()) {\n+        // there\u0027s an MPU in progress\u0027;\n+        // IF there is more data to upload, or no data has yet been uploaded,\n+        // PUT the final block\n+        if (hasBlock \u0026\u0026\n+            (block.hasData() || multiPartUpload.getPartsSubmitted() \u003d\u003d 0)) {\n           //send last part\n           uploadCurrentBlock();\n         }\n         // wait for the partial uploads to finish\n         final List\u003cPartETag\u003e partETags \u003d\n             multiPartUpload.waitForAllPartUploads();\n-        // then complete the operation\n-        multiPartUpload.complete(partETags);\n         bytes \u003d bytesSubmitted;\n+        // then complete the operation\n+        if (putTracker.aboutToComplete(multiPartUpload.getUploadId(),\n+            partETags,\n+            bytes)) {\n+          multiPartUpload.complete(partETags);\n+        } else {\n+          LOG.info(\"File {} will be visible when the job is committed\", key);\n+        }\n       }\n-      LOG.debug(\"Upload complete for {}\", writeOperationHelper);\n+      if (!putTracker.outputImmediatelyVisible()) {\n+        // track the number of bytes uploaded as commit operations.\n+        statistics.commitUploaded(bytes);\n+      }\n+      LOG.debug(\"Upload complete to {} by {}\", key, writeOperationHelper);\n     } catch (IOException ioe) {\n       writeOperationHelper.writeFailed(ioe);\n       throw ioe;\n     } finally {\n       closeAll(LOG, block, blockFactory);\n       LOG.debug(\"Statistics: {}\", statistics);\n       closeAll(LOG, statistics);\n       clearActiveBlock();\n     }\n-    // All end of write operations, including deleting fake parent directories\n+    // Note end of write. This does not change the state of the remote FS.\n     writeOperationHelper.writeSuccessful(bytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (closed.getAndSet(true)) {\n      // already closed\n      LOG.debug(\"Ignoring close() as stream is already closed\");\n      return;\n    }\n    S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n    boolean hasBlock \u003d hasActiveBlock();\n    LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n        this,\n        blockCount,\n        hasBlock ? block : \"(none)\");\n    long bytes \u003d 0;\n    try {\n      if (multiPartUpload \u003d\u003d null) {\n        if (hasBlock) {\n          // no uploads of data have taken place, put the single block up.\n          // This must happen even if there is no data, so that 0 byte files\n          // are created.\n          bytes \u003d putObject();\n          bytesSubmitted \u003d bytes;\n        }\n      } else {\n        // there\u0027s an MPU in progress\u0027;\n        // IF there is more data to upload, or no data has yet been uploaded,\n        // PUT the final block\n        if (hasBlock \u0026\u0026\n            (block.hasData() || multiPartUpload.getPartsSubmitted() \u003d\u003d 0)) {\n          //send last part\n          uploadCurrentBlock();\n        }\n        // wait for the partial uploads to finish\n        final List\u003cPartETag\u003e partETags \u003d\n            multiPartUpload.waitForAllPartUploads();\n        bytes \u003d bytesSubmitted;\n        // then complete the operation\n        if (putTracker.aboutToComplete(multiPartUpload.getUploadId(),\n            partETags,\n            bytes)) {\n          multiPartUpload.complete(partETags);\n        } else {\n          LOG.info(\"File {} will be visible when the job is committed\", key);\n        }\n      }\n      if (!putTracker.outputImmediatelyVisible()) {\n        // track the number of bytes uploaded as commit operations.\n        statistics.commitUploaded(bytes);\n      }\n      LOG.debug(\"Upload complete to {} by {}\", key, writeOperationHelper);\n    } catch (IOException ioe) {\n      writeOperationHelper.writeFailed(ioe);\n      throw ioe;\n    } finally {\n      closeAll(LOG, block, blockFactory);\n      LOG.debug(\"Statistics: {}\", statistics);\n      closeAll(LOG, statistics);\n      clearActiveBlock();\n    }\n    // Note end of write. This does not change the state of the remote FS.\n    writeOperationHelper.writeSuccessful(bytes);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
      "extendedDetails": {}
    },
    "621b43e254afaff708cd6fc4698b29628f6abc33": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13345 HS3Guard: Improved Consistency for S3A.\nContributed by: Chris Nauroth, Aaron Fabbri, Mingliang Liu, Lei (Eddy) Xu,\nSean Mackrory, Steve Loughran and others.\n",
      "commitDate": "01/09/17 6:13 AM",
      "commitName": "621b43e254afaff708cd6fc4698b29628f6abc33",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "23/03/17 5:54 AM",
      "commitNameOld": "a5a4867f3b193a137a6260d539da7e21f02ffab3",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 162.01,
      "commitsBetweenForRepo": 999,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,48 @@\n   public void close() throws IOException {\n     if (closed.getAndSet(true)) {\n       // already closed\n       LOG.debug(\"Ignoring close() as stream is already closed\");\n       return;\n     }\n     S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n     boolean hasBlock \u003d hasActiveBlock();\n     LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n         this,\n         blockCount,\n         hasBlock ? block : \"(none)\");\n+    long bytes \u003d 0;\n     try {\n       if (multiPartUpload \u003d\u003d null) {\n         if (hasBlock) {\n           // no uploads of data have taken place, put the single block up.\n           // This must happen even if there is no data, so that 0 byte files\n           // are created.\n-          putObject();\n+          bytes \u003d putObject();\n         }\n       } else {\n         // there has already been at least one block scheduled for upload;\n         // put up the current then wait\n         if (hasBlock \u0026\u0026 block.hasData()) {\n           //send last part\n           uploadCurrentBlock();\n         }\n         // wait for the partial uploads to finish\n         final List\u003cPartETag\u003e partETags \u003d\n             multiPartUpload.waitForAllPartUploads();\n         // then complete the operation\n         multiPartUpload.complete(partETags);\n+        bytes \u003d bytesSubmitted;\n       }\n       LOG.debug(\"Upload complete for {}\", writeOperationHelper);\n     } catch (IOException ioe) {\n       writeOperationHelper.writeFailed(ioe);\n       throw ioe;\n     } finally {\n       closeAll(LOG, block, blockFactory);\n       LOG.debug(\"Statistics: {}\", statistics);\n       closeAll(LOG, statistics);\n       clearActiveBlock();\n     }\n     // All end of write operations, including deleting fake parent directories\n-    writeOperationHelper.writeSuccessful();\n+    writeOperationHelper.writeSuccessful(bytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (closed.getAndSet(true)) {\n      // already closed\n      LOG.debug(\"Ignoring close() as stream is already closed\");\n      return;\n    }\n    S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n    boolean hasBlock \u003d hasActiveBlock();\n    LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n        this,\n        blockCount,\n        hasBlock ? block : \"(none)\");\n    long bytes \u003d 0;\n    try {\n      if (multiPartUpload \u003d\u003d null) {\n        if (hasBlock) {\n          // no uploads of data have taken place, put the single block up.\n          // This must happen even if there is no data, so that 0 byte files\n          // are created.\n          bytes \u003d putObject();\n        }\n      } else {\n        // there has already been at least one block scheduled for upload;\n        // put up the current then wait\n        if (hasBlock \u0026\u0026 block.hasData()) {\n          //send last part\n          uploadCurrentBlock();\n        }\n        // wait for the partial uploads to finish\n        final List\u003cPartETag\u003e partETags \u003d\n            multiPartUpload.waitForAllPartUploads();\n        // then complete the operation\n        multiPartUpload.complete(partETags);\n        bytes \u003d bytesSubmitted;\n      }\n      LOG.debug(\"Upload complete for {}\", writeOperationHelper);\n    } catch (IOException ioe) {\n      writeOperationHelper.writeFailed(ioe);\n      throw ioe;\n    } finally {\n      closeAll(LOG, block, blockFactory);\n      LOG.debug(\"Statistics: {}\", statistics);\n      closeAll(LOG, statistics);\n      clearActiveBlock();\n    }\n    // All end of write operations, including deleting fake parent directories\n    writeOperationHelper.writeSuccessful(bytes);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
      "extendedDetails": {}
    },
    "dab00da19f25619ccc71c7f803a235b21766bf1e": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14028. S3A BlockOutputStreams doesn\u0027t delete temporary files in multipart uploads or handle part upload failures.\nContributed by Steve Loughran.\n\n(cherry picked from commit 29fe5af017b945d8750c074ca39031b5b777eddd)\n",
      "commitDate": "25/02/17 7:35 AM",
      "commitName": "dab00da19f25619ccc71c7f803a235b21766bf1e",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "11/02/17 1:59 PM",
      "commitNameOld": "839b690ed5edc2ac4984640d58c005bb63cd8a07",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 13.73,
      "commitsBetweenForRepo": 86,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,46 @@\n   public void close() throws IOException {\n     if (closed.getAndSet(true)) {\n       // already closed\n       LOG.debug(\"Ignoring close() as stream is already closed\");\n       return;\n     }\n     S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n     boolean hasBlock \u003d hasActiveBlock();\n     LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n         this,\n         blockCount,\n         hasBlock ? block : \"(none)\");\n     try {\n       if (multiPartUpload \u003d\u003d null) {\n         if (hasBlock) {\n           // no uploads of data have taken place, put the single block up.\n           // This must happen even if there is no data, so that 0 byte files\n           // are created.\n           putObject();\n         }\n       } else {\n         // there has already been at least one block scheduled for upload;\n         // put up the current then wait\n         if (hasBlock \u0026\u0026 block.hasData()) {\n           //send last part\n           uploadCurrentBlock();\n         }\n         // wait for the partial uploads to finish\n         final List\u003cPartETag\u003e partETags \u003d\n             multiPartUpload.waitForAllPartUploads();\n         // then complete the operation\n         multiPartUpload.complete(partETags);\n       }\n       LOG.debug(\"Upload complete for {}\", writeOperationHelper);\n     } catch (IOException ioe) {\n       writeOperationHelper.writeFailed(ioe);\n       throw ioe;\n     } finally {\n-      LOG.debug(\"Closing block and factory\");\n-      IOUtils.closeStream(block);\n-      IOUtils.closeStream(blockFactory);\n+      closeAll(LOG, block, blockFactory);\n       LOG.debug(\"Statistics: {}\", statistics);\n-      IOUtils.closeStream(statistics);\n+      closeAll(LOG, statistics);\n       clearActiveBlock();\n     }\n     // All end of write operations, including deleting fake parent directories\n     writeOperationHelper.writeSuccessful();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    if (closed.getAndSet(true)) {\n      // already closed\n      LOG.debug(\"Ignoring close() as stream is already closed\");\n      return;\n    }\n    S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n    boolean hasBlock \u003d hasActiveBlock();\n    LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n        this,\n        blockCount,\n        hasBlock ? block : \"(none)\");\n    try {\n      if (multiPartUpload \u003d\u003d null) {\n        if (hasBlock) {\n          // no uploads of data have taken place, put the single block up.\n          // This must happen even if there is no data, so that 0 byte files\n          // are created.\n          putObject();\n        }\n      } else {\n        // there has already been at least one block scheduled for upload;\n        // put up the current then wait\n        if (hasBlock \u0026\u0026 block.hasData()) {\n          //send last part\n          uploadCurrentBlock();\n        }\n        // wait for the partial uploads to finish\n        final List\u003cPartETag\u003e partETags \u003d\n            multiPartUpload.waitForAllPartUploads();\n        // then complete the operation\n        multiPartUpload.complete(partETags);\n      }\n      LOG.debug(\"Upload complete for {}\", writeOperationHelper);\n    } catch (IOException ioe) {\n      writeOperationHelper.writeFailed(ioe);\n      throw ioe;\n    } finally {\n      closeAll(LOG, block, blockFactory);\n      LOG.debug(\"Statistics: {}\", statistics);\n      closeAll(LOG, statistics);\n      clearActiveBlock();\n    }\n    // All end of write operations, including deleting fake parent directories\n    writeOperationHelper.writeSuccessful();\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
      "extendedDetails": {}
    },
    "6c348c56918973fd988b110e79231324a8befe12": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange)",
      "commitMessage": "HADOOP-13560. S3ABlockOutputStream to support huge (many GB) file writes. Contributed by Steve Loughran\n",
      "commitDate": "18/10/16 1:16 PM",
      "commitName": "6c348c56918973fd988b110e79231324a8befe12",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HADOOP-13560. S3ABlockOutputStream to support huge (many GB) file writes. Contributed by Steve Loughran\n",
          "commitDate": "18/10/16 1:16 PM",
          "commitName": "6c348c56918973fd988b110e79231324a8befe12",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "18/10/16 11:06 AM",
          "commitNameOld": "b733a6f86262522e535cebc972baecbe6a6eab50",
          "commitAuthorOld": "Xuan",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,48 @@\n-  public synchronized void close() throws IOException {\n-    if (closed) {\n+  public void close() throws IOException {\n+    if (closed.getAndSet(true)) {\n+      // already closed\n+      LOG.debug(\"Ignoring close() as stream is already closed\");\n       return;\n     }\n-    closed \u003d true;\n+    S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n+    boolean hasBlock \u003d hasActiveBlock();\n+    LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n+        this,\n+        blockCount,\n+        hasBlock ? block : \"(none)\");\n     try {\n       if (multiPartUpload \u003d\u003d null) {\n-        putObject();\n-      } else {\n-        int size \u003d buffer.size();\n-        if (size \u003e 0) {\n-          fs.incrementPutStartStatistics(size);\n-          //send last part\n-          multiPartUpload.uploadPartAsync(new ByteArrayInputStream(buffer\n-              .toByteArray()), size);\n+        if (hasBlock) {\n+          // no uploads of data have taken place, put the single block up.\n+          // This must happen even if there is no data, so that 0 byte files\n+          // are created.\n+          putObject();\n         }\n-        final List\u003cPartETag\u003e partETags \u003d multiPartUpload\n-            .waitForAllPartUploads();\n+      } else {\n+        // there has already been at least one block scheduled for upload;\n+        // put up the current then wait\n+        if (hasBlock \u0026\u0026 block.hasData()) {\n+          //send last part\n+          uploadCurrentBlock();\n+        }\n+        // wait for the partial uploads to finish\n+        final List\u003cPartETag\u003e partETags \u003d\n+            multiPartUpload.waitForAllPartUploads();\n+        // then complete the operation\n         multiPartUpload.complete(partETags);\n       }\n-      // This will delete unnecessary fake parent directories\n-      fs.finishedWrite(key);\n-      LOG.debug(\"Upload complete for bucket \u0027{}\u0027 key \u0027{}\u0027\", bucket, key);\n+      LOG.debug(\"Upload complete for {}\", writeOperationHelper);\n+    } catch (IOException ioe) {\n+      writeOperationHelper.writeFailed(ioe);\n+      throw ioe;\n     } finally {\n-      buffer \u003d null;\n-      super.close();\n+      LOG.debug(\"Closing block and factory\");\n+      IOUtils.closeStream(block);\n+      IOUtils.closeStream(blockFactory);\n+      LOG.debug(\"Statistics: {}\", statistics);\n+      IOUtils.closeStream(statistics);\n+      clearActiveBlock();\n     }\n+    // All end of write operations, including deleting fake parent directories\n+    writeOperationHelper.writeSuccessful();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void close() throws IOException {\n    if (closed.getAndSet(true)) {\n      // already closed\n      LOG.debug(\"Ignoring close() as stream is already closed\");\n      return;\n    }\n    S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n    boolean hasBlock \u003d hasActiveBlock();\n    LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n        this,\n        blockCount,\n        hasBlock ? block : \"(none)\");\n    try {\n      if (multiPartUpload \u003d\u003d null) {\n        if (hasBlock) {\n          // no uploads of data have taken place, put the single block up.\n          // This must happen even if there is no data, so that 0 byte files\n          // are created.\n          putObject();\n        }\n      } else {\n        // there has already been at least one block scheduled for upload;\n        // put up the current then wait\n        if (hasBlock \u0026\u0026 block.hasData()) {\n          //send last part\n          uploadCurrentBlock();\n        }\n        // wait for the partial uploads to finish\n        final List\u003cPartETag\u003e partETags \u003d\n            multiPartUpload.waitForAllPartUploads();\n        // then complete the operation\n        multiPartUpload.complete(partETags);\n      }\n      LOG.debug(\"Upload complete for {}\", writeOperationHelper);\n    } catch (IOException ioe) {\n      writeOperationHelper.writeFailed(ioe);\n      throw ioe;\n    } finally {\n      LOG.debug(\"Closing block and factory\");\n      IOUtils.closeStream(block);\n      IOUtils.closeStream(blockFactory);\n      LOG.debug(\"Statistics: {}\", statistics);\n      IOUtils.closeStream(statistics);\n      clearActiveBlock();\n    }\n    // All end of write operations, including deleting fake parent directories\n    writeOperationHelper.writeSuccessful();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
          "extendedDetails": {
            "oldPath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFastOutputStream.java",
            "newPath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
            "oldMethodName": "close",
            "newMethodName": "close"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HADOOP-13560. S3ABlockOutputStream to support huge (many GB) file writes. Contributed by Steve Loughran\n",
          "commitDate": "18/10/16 1:16 PM",
          "commitName": "6c348c56918973fd988b110e79231324a8befe12",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "18/10/16 11:06 AM",
          "commitNameOld": "b733a6f86262522e535cebc972baecbe6a6eab50",
          "commitAuthorOld": "Xuan",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,48 @@\n-  public synchronized void close() throws IOException {\n-    if (closed) {\n+  public void close() throws IOException {\n+    if (closed.getAndSet(true)) {\n+      // already closed\n+      LOG.debug(\"Ignoring close() as stream is already closed\");\n       return;\n     }\n-    closed \u003d true;\n+    S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n+    boolean hasBlock \u003d hasActiveBlock();\n+    LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n+        this,\n+        blockCount,\n+        hasBlock ? block : \"(none)\");\n     try {\n       if (multiPartUpload \u003d\u003d null) {\n-        putObject();\n-      } else {\n-        int size \u003d buffer.size();\n-        if (size \u003e 0) {\n-          fs.incrementPutStartStatistics(size);\n-          //send last part\n-          multiPartUpload.uploadPartAsync(new ByteArrayInputStream(buffer\n-              .toByteArray()), size);\n+        if (hasBlock) {\n+          // no uploads of data have taken place, put the single block up.\n+          // This must happen even if there is no data, so that 0 byte files\n+          // are created.\n+          putObject();\n         }\n-        final List\u003cPartETag\u003e partETags \u003d multiPartUpload\n-            .waitForAllPartUploads();\n+      } else {\n+        // there has already been at least one block scheduled for upload;\n+        // put up the current then wait\n+        if (hasBlock \u0026\u0026 block.hasData()) {\n+          //send last part\n+          uploadCurrentBlock();\n+        }\n+        // wait for the partial uploads to finish\n+        final List\u003cPartETag\u003e partETags \u003d\n+            multiPartUpload.waitForAllPartUploads();\n+        // then complete the operation\n         multiPartUpload.complete(partETags);\n       }\n-      // This will delete unnecessary fake parent directories\n-      fs.finishedWrite(key);\n-      LOG.debug(\"Upload complete for bucket \u0027{}\u0027 key \u0027{}\u0027\", bucket, key);\n+      LOG.debug(\"Upload complete for {}\", writeOperationHelper);\n+    } catch (IOException ioe) {\n+      writeOperationHelper.writeFailed(ioe);\n+      throw ioe;\n     } finally {\n-      buffer \u003d null;\n-      super.close();\n+      LOG.debug(\"Closing block and factory\");\n+      IOUtils.closeStream(block);\n+      IOUtils.closeStream(blockFactory);\n+      LOG.debug(\"Statistics: {}\", statistics);\n+      IOUtils.closeStream(statistics);\n+      clearActiveBlock();\n     }\n+    // All end of write operations, including deleting fake parent directories\n+    writeOperationHelper.writeSuccessful();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void close() throws IOException {\n    if (closed.getAndSet(true)) {\n      // already closed\n      LOG.debug(\"Ignoring close() as stream is already closed\");\n      return;\n    }\n    S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n    boolean hasBlock \u003d hasActiveBlock();\n    LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n        this,\n        blockCount,\n        hasBlock ? block : \"(none)\");\n    try {\n      if (multiPartUpload \u003d\u003d null) {\n        if (hasBlock) {\n          // no uploads of data have taken place, put the single block up.\n          // This must happen even if there is no data, so that 0 byte files\n          // are created.\n          putObject();\n        }\n      } else {\n        // there has already been at least one block scheduled for upload;\n        // put up the current then wait\n        if (hasBlock \u0026\u0026 block.hasData()) {\n          //send last part\n          uploadCurrentBlock();\n        }\n        // wait for the partial uploads to finish\n        final List\u003cPartETag\u003e partETags \u003d\n            multiPartUpload.waitForAllPartUploads();\n        // then complete the operation\n        multiPartUpload.complete(partETags);\n      }\n      LOG.debug(\"Upload complete for {}\", writeOperationHelper);\n    } catch (IOException ioe) {\n      writeOperationHelper.writeFailed(ioe);\n      throw ioe;\n    } finally {\n      LOG.debug(\"Closing block and factory\");\n      IOUtils.closeStream(block);\n      IOUtils.closeStream(blockFactory);\n      LOG.debug(\"Statistics: {}\", statistics);\n      IOUtils.closeStream(statistics);\n      clearActiveBlock();\n    }\n    // All end of write operations, including deleting fake parent directories\n    writeOperationHelper.writeSuccessful();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-13560. S3ABlockOutputStream to support huge (many GB) file writes. Contributed by Steve Loughran\n",
          "commitDate": "18/10/16 1:16 PM",
          "commitName": "6c348c56918973fd988b110e79231324a8befe12",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "18/10/16 11:06 AM",
          "commitNameOld": "b733a6f86262522e535cebc972baecbe6a6eab50",
          "commitAuthorOld": "Xuan",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,48 @@\n-  public synchronized void close() throws IOException {\n-    if (closed) {\n+  public void close() throws IOException {\n+    if (closed.getAndSet(true)) {\n+      // already closed\n+      LOG.debug(\"Ignoring close() as stream is already closed\");\n       return;\n     }\n-    closed \u003d true;\n+    S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n+    boolean hasBlock \u003d hasActiveBlock();\n+    LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n+        this,\n+        blockCount,\n+        hasBlock ? block : \"(none)\");\n     try {\n       if (multiPartUpload \u003d\u003d null) {\n-        putObject();\n-      } else {\n-        int size \u003d buffer.size();\n-        if (size \u003e 0) {\n-          fs.incrementPutStartStatistics(size);\n-          //send last part\n-          multiPartUpload.uploadPartAsync(new ByteArrayInputStream(buffer\n-              .toByteArray()), size);\n+        if (hasBlock) {\n+          // no uploads of data have taken place, put the single block up.\n+          // This must happen even if there is no data, so that 0 byte files\n+          // are created.\n+          putObject();\n         }\n-        final List\u003cPartETag\u003e partETags \u003d multiPartUpload\n-            .waitForAllPartUploads();\n+      } else {\n+        // there has already been at least one block scheduled for upload;\n+        // put up the current then wait\n+        if (hasBlock \u0026\u0026 block.hasData()) {\n+          //send last part\n+          uploadCurrentBlock();\n+        }\n+        // wait for the partial uploads to finish\n+        final List\u003cPartETag\u003e partETags \u003d\n+            multiPartUpload.waitForAllPartUploads();\n+        // then complete the operation\n         multiPartUpload.complete(partETags);\n       }\n-      // This will delete unnecessary fake parent directories\n-      fs.finishedWrite(key);\n-      LOG.debug(\"Upload complete for bucket \u0027{}\u0027 key \u0027{}\u0027\", bucket, key);\n+      LOG.debug(\"Upload complete for {}\", writeOperationHelper);\n+    } catch (IOException ioe) {\n+      writeOperationHelper.writeFailed(ioe);\n+      throw ioe;\n     } finally {\n-      buffer \u003d null;\n-      super.close();\n+      LOG.debug(\"Closing block and factory\");\n+      IOUtils.closeStream(block);\n+      IOUtils.closeStream(blockFactory);\n+      LOG.debug(\"Statistics: {}\", statistics);\n+      IOUtils.closeStream(statistics);\n+      clearActiveBlock();\n     }\n+    // All end of write operations, including deleting fake parent directories\n+    writeOperationHelper.writeSuccessful();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void close() throws IOException {\n    if (closed.getAndSet(true)) {\n      // already closed\n      LOG.debug(\"Ignoring close() as stream is already closed\");\n      return;\n    }\n    S3ADataBlocks.DataBlock block \u003d getActiveBlock();\n    boolean hasBlock \u003d hasActiveBlock();\n    LOG.debug(\"{}: Closing block #{}: current block\u003d {}\",\n        this,\n        blockCount,\n        hasBlock ? block : \"(none)\");\n    try {\n      if (multiPartUpload \u003d\u003d null) {\n        if (hasBlock) {\n          // no uploads of data have taken place, put the single block up.\n          // This must happen even if there is no data, so that 0 byte files\n          // are created.\n          putObject();\n        }\n      } else {\n        // there has already been at least one block scheduled for upload;\n        // put up the current then wait\n        if (hasBlock \u0026\u0026 block.hasData()) {\n          //send last part\n          uploadCurrentBlock();\n        }\n        // wait for the partial uploads to finish\n        final List\u003cPartETag\u003e partETags \u003d\n            multiPartUpload.waitForAllPartUploads();\n        // then complete the operation\n        multiPartUpload.complete(partETags);\n      }\n      LOG.debug(\"Upload complete for {}\", writeOperationHelper);\n    } catch (IOException ioe) {\n      writeOperationHelper.writeFailed(ioe);\n      throw ioe;\n    } finally {\n      LOG.debug(\"Closing block and factory\");\n      IOUtils.closeStream(block);\n      IOUtils.closeStream(blockFactory);\n      LOG.debug(\"Statistics: {}\", statistics);\n      IOUtils.closeStream(statistics);\n      clearActiveBlock();\n    }\n    // All end of write operations, including deleting fake parent directories\n    writeOperationHelper.writeSuccessful();\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "c58a59f7081d55dd2108545ebf9ee48cf43ca944": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13171. Add StorageStatistics to S3A; instrument some more operations. Contributed by Steve Loughran.\n",
      "commitDate": "03/06/16 8:55 AM",
      "commitName": "c58a59f7081d55dd2108545ebf9ee48cf43ca944",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "21/05/16 8:39 AM",
      "commitNameOld": "39ec1515a205952eda7e171408a8b83eceb4abde",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 13.01,
      "commitsBetweenForRepo": 75,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,28 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       return;\n     }\n     closed \u003d true;\n     try {\n       if (multiPartUpload \u003d\u003d null) {\n         putObject();\n       } else {\n-        if (buffer.size() \u003e 0) {\n+        int size \u003d buffer.size();\n+        if (size \u003e 0) {\n+          fs.incrementPutStartStatistics(size);\n           //send last part\n           multiPartUpload.uploadPartAsync(new ByteArrayInputStream(buffer\n-              .toByteArray()), buffer.size());\n+              .toByteArray()), size);\n         }\n         final List\u003cPartETag\u003e partETags \u003d multiPartUpload\n             .waitForAllPartUploads();\n         multiPartUpload.complete(partETags);\n       }\n-      statistics.incrementWriteOps(1);\n       // This will delete unnecessary fake parent directories\n       fs.finishedWrite(key);\n       LOG.debug(\"Upload complete for bucket \u0027{}\u0027 key \u0027{}\u0027\", bucket, key);\n     } finally {\n       buffer \u003d null;\n       super.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    closed \u003d true;\n    try {\n      if (multiPartUpload \u003d\u003d null) {\n        putObject();\n      } else {\n        int size \u003d buffer.size();\n        if (size \u003e 0) {\n          fs.incrementPutStartStatistics(size);\n          //send last part\n          multiPartUpload.uploadPartAsync(new ByteArrayInputStream(buffer\n              .toByteArray()), size);\n        }\n        final List\u003cPartETag\u003e partETags \u003d multiPartUpload\n            .waitForAllPartUploads();\n        multiPartUpload.complete(partETags);\n      }\n      // This will delete unnecessary fake parent directories\n      fs.finishedWrite(key);\n      LOG.debug(\"Upload complete for bucket \u0027{}\u0027 key \u0027{}\u0027\", bucket, key);\n    } finally {\n      buffer \u003d null;\n      super.close();\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFastOutputStream.java",
      "extendedDetails": {}
    },
    "39ec1515a205952eda7e171408a8b83eceb4abde": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13130. s3a failures can surface as RTEs, not IOEs. (Steve Loughran)\n",
      "commitDate": "21/05/16 8:39 AM",
      "commitName": "39ec1515a205952eda7e171408a8b83eceb4abde",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "12/05/16 11:24 AM",
      "commitNameOld": "27c4e90efce04e1b1302f668b5eb22412e00d033",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 8.89,
      "commitsBetweenForRepo": 74,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,27 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       return;\n     }\n     closed \u003d true;\n     try {\n       if (multiPartUpload \u003d\u003d null) {\n         putObject();\n       } else {\n         if (buffer.size() \u003e 0) {\n           //send last part\n           multiPartUpload.uploadPartAsync(new ByteArrayInputStream(buffer\n               .toByteArray()), buffer.size());\n         }\n         final List\u003cPartETag\u003e partETags \u003d multiPartUpload\n             .waitForAllPartUploads();\n         multiPartUpload.complete(partETags);\n       }\n       statistics.incrementWriteOps(1);\n       // This will delete unnecessary fake parent directories\n       fs.finishedWrite(key);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Upload complete for bucket \u0027{}\u0027 key \u0027{}\u0027\", bucket, key);\n-      }\n+      LOG.debug(\"Upload complete for bucket \u0027{}\u0027 key \u0027{}\u0027\", bucket, key);\n     } finally {\n       buffer \u003d null;\n       super.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    closed \u003d true;\n    try {\n      if (multiPartUpload \u003d\u003d null) {\n        putObject();\n      } else {\n        if (buffer.size() \u003e 0) {\n          //send last part\n          multiPartUpload.uploadPartAsync(new ByteArrayInputStream(buffer\n              .toByteArray()), buffer.size());\n        }\n        final List\u003cPartETag\u003e partETags \u003d multiPartUpload\n            .waitForAllPartUploads();\n        multiPartUpload.complete(partETags);\n      }\n      statistics.incrementWriteOps(1);\n      // This will delete unnecessary fake parent directories\n      fs.finishedWrite(key);\n      LOG.debug(\"Upload complete for bucket \u0027{}\u0027 key \u0027{}\u0027\", bucket, key);\n    } finally {\n      buffer \u003d null;\n      super.close();\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFastOutputStream.java",
      "extendedDetails": {}
    },
    "15b7076ad5f2ae92d231140b2f8cebc392a92c87": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-11183. Memory-based S3AOutputstream. (Thomas Demoor via stevel)\n",
      "commitDate": "03/03/15 4:18 PM",
      "commitName": "15b7076ad5f2ae92d231140b2f8cebc392a92c87",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,29 @@\n+  public synchronized void close() throws IOException {\n+    if (closed) {\n+      return;\n+    }\n+    closed \u003d true;\n+    try {\n+      if (multiPartUpload \u003d\u003d null) {\n+        putObject();\n+      } else {\n+        if (buffer.size() \u003e 0) {\n+          //send last part\n+          multiPartUpload.uploadPartAsync(new ByteArrayInputStream(buffer\n+              .toByteArray()), buffer.size());\n+        }\n+        final List\u003cPartETag\u003e partETags \u003d multiPartUpload\n+            .waitForAllPartUploads();\n+        multiPartUpload.complete(partETags);\n+      }\n+      statistics.incrementWriteOps(1);\n+      // This will delete unnecessary fake parent directories\n+      fs.finishedWrite(key);\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Upload complete for bucket \u0027{}\u0027 key \u0027{}\u0027\", bucket, key);\n+      }\n+    } finally {\n+      buffer \u003d null;\n+      super.close();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      return;\n    }\n    closed \u003d true;\n    try {\n      if (multiPartUpload \u003d\u003d null) {\n        putObject();\n      } else {\n        if (buffer.size() \u003e 0) {\n          //send last part\n          multiPartUpload.uploadPartAsync(new ByteArrayInputStream(buffer\n              .toByteArray()), buffer.size());\n        }\n        final List\u003cPartETag\u003e partETags \u003d multiPartUpload\n            .waitForAllPartUploads();\n        multiPartUpload.complete(partETags);\n      }\n      statistics.incrementWriteOps(1);\n      // This will delete unnecessary fake parent directories\n      fs.finishedWrite(key);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Upload complete for bucket \u0027{}\u0027 key \u0027{}\u0027\", bucket, key);\n      }\n    } finally {\n      buffer \u003d null;\n      super.close();\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFastOutputStream.java"
    }
  }
}