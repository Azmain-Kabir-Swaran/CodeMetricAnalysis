{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSClient.java",
  "functionName": "inferChecksumTypeByReading",
  "functionId": "inferChecksumTypeByReading___lb-LocatedBlock__dn-DatanodeInfo",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
  "functionStartLine": 1930,
  "functionEndLine": 1950,
  "numCommitsSeen": 464,
  "timeTaken": 10105,
  "changeHistory": [
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
    "4da8490b512a33a255ed27309860859388d7c168",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "67ed59348d638d56e6752ba2c71fdcd69567546d",
    "3b54223c0f32d42a84436c670d80b791a8e9696d",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
    "cfae13306ac0fb3f3c139d5ac511bf78cede1b77"
  ],
  "changeHistoryShort": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": "Ybodychange",
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6": "Ymultichange(Ymodifierchange,Ybodychange)",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Ymultichange(Yfilerename,Ybodychange)",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": "Ybodychange",
    "4da8490b512a33a255ed27309860859388d7c168": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "67ed59348d638d56e6752ba2c71fdcd69567546d": "Ybodychange",
    "3b54223c0f32d42a84436c670d80b791a8e9696d": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": "Ybodychange",
    "cfae13306ac0fb3f3c139d5ac511bf78cede1b77": "Yintroduced"
  },
  "changeHistoryDetails": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13695. Move logging to slf4j in HDFS package. Contributed by Ian Pickering.\n",
      "commitDate": "06/09/18 2:48 PM",
      "commitName": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "03/09/18 10:37 PM",
      "commitNameOld": "6e5ffb74dd678ddc3392ae2f251c80fc5cc8c62f",
      "commitAuthorOld": "Kitti Nanasi",
      "daysBetweenCommits": 2.67,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   protected Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n     IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(),\n         lb.getBlockToken());\n \n     try {\n       new Sender((DataOutputStream) pair.out).readBlock(lb.getBlock(),\n           lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n           BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(pair.in));\n       String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" +\n           dn;\n       DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n       return PBHelperClient.convert(\n           reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n-      IOUtilsClient.cleanup(null, pair.in, pair.out);\n+      IOUtilsClient.cleanupWithLogger(LOG, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(),\n        lb.getBlockToken());\n\n    try {\n      new Sender((DataOutputStream) pair.out).readBlock(lb.getBlock(),\n          lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(pair.in));\n      String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" +\n          dn;\n      DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n      return PBHelperClient.convert(\n          reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtilsClient.cleanupWithLogger(LOG, pair.in, pair.out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-9733. Refactor DFSClient#getFileChecksum and DataXceiver#blockChecksum. Contributed by Kai Zheng\n",
      "commitDate": "29/02/16 9:52 PM",
      "commitName": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
      "commitAuthor": "Uma Maheswara Rao G",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-9733. Refactor DFSClient#getFileChecksum and DataXceiver#blockChecksum. Contributed by Kai Zheng\n",
          "commitDate": "29/02/16 9:52 PM",
          "commitName": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "12/02/16 10:31 AM",
          "commitNameOld": "372d1302c63c6f49f99be5766c5da9647ebd9ca6",
          "commitAuthorOld": "Masatake Iwasaki",
          "daysBetweenCommits": 17.47,
          "commitsBetweenForRepo": 116,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,21 @@\n-  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n+  protected Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n-    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n+    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(),\n+        lb.getBlockToken());\n \n     try {\n-      DataOutputStream out \u003d new DataOutputStream(\n-          new BufferedOutputStream(pair.out, smallBufferSize));\n-      DataInputStream in \u003d new DataInputStream(pair.in);\n-\n-      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n+      new Sender((DataOutputStream) pair.out).readBlock(lb.getBlock(),\n+          lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n-          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n+          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(pair.in));\n       String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" +\n           dn;\n       DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n       return PBHelperClient.convert(\n           reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n       IOUtilsClient.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(),\n        lb.getBlockToken());\n\n    try {\n      new Sender((DataOutputStream) pair.out).readBlock(lb.getBlock(),\n          lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(pair.in));\n      String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" +\n          dn;\n      DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n      return PBHelperClient.convert(\n          reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtilsClient.cleanup(null, pair.in, pair.out);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[protected]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9733. Refactor DFSClient#getFileChecksum and DataXceiver#blockChecksum. Contributed by Kai Zheng\n",
          "commitDate": "29/02/16 9:52 PM",
          "commitName": "307ec80acae3b4a41d21b2d4b3a55032e55fcdc6",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "12/02/16 10:31 AM",
          "commitNameOld": "372d1302c63c6f49f99be5766c5da9647ebd9ca6",
          "commitAuthorOld": "Masatake Iwasaki",
          "daysBetweenCommits": 17.47,
          "commitsBetweenForRepo": 116,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,21 @@\n-  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n+  protected Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n-    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n+    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(),\n+        lb.getBlockToken());\n \n     try {\n-      DataOutputStream out \u003d new DataOutputStream(\n-          new BufferedOutputStream(pair.out, smallBufferSize));\n-      DataInputStream in \u003d new DataInputStream(pair.in);\n-\n-      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n+      new Sender((DataOutputStream) pair.out).readBlock(lb.getBlock(),\n+          lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n-          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n+          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(pair.in));\n       String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" +\n           dn;\n       DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n       return PBHelperClient.convert(\n           reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n       IOUtilsClient.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(),\n        lb.getBlockToken());\n\n    try {\n      new Sender((DataOutputStream) pair.out).readBlock(lb.getBlock(),\n          lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(pair.in));\n      String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" +\n          dn;\n      DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n      return PBHelperClient.convert(\n          reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtilsClient.cleanup(null, pair.in, pair.out);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/09/15 11:08 AM",
          "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "26/09/15 9:06 AM",
          "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n   private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n     IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n \n     try {\n       DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n           smallBufferSize));\n       DataInputStream in \u003d new DataInputStream(pair.in);\n   \n       new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n           BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n       String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n       DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n       return PBHelperClient.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n-      IOUtils.cleanup(null, pair.in, pair.out);\n+      IOUtilsClient.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          smallBufferSize));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n      String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n      DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n      return PBHelperClient.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtilsClient.cleanup(null, pair.in, pair.out);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/09/15 11:08 AM",
          "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "26/09/15 9:06 AM",
          "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n   private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n     IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n \n     try {\n       DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n           smallBufferSize));\n       DataInputStream in \u003d new DataInputStream(pair.in);\n   \n       new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n           BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n       String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n       DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n       return PBHelperClient.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n-      IOUtils.cleanup(null, pair.in, pair.out);\n+      IOUtilsClient.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          smallBufferSize));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n      String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n      DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n      return PBHelperClient.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtilsClient.cleanup(null, pair.in, pair.out);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8934. Move ShortCircuitShm to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "22/08/15 1:31 PM",
      "commitName": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/08/15 11:28 AM",
      "commitNameOld": "3aac4758b007a56e3d66998d457b2156effca528",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.09,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n     IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n \n     try {\n       DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n           smallBufferSize));\n       DataInputStream in \u003d new DataInputStream(pair.in);\n   \n       new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n-          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n+          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n       String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n       DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n-      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n+      return PBHelperClient.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n       IOUtils.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          smallBufferSize));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\n      String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n      DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n      return PBHelperClient.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtils.cleanup(null, pair.in, pair.out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "4da8490b512a33a255ed27309860859388d7c168": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
      "commitDate": "05/05/15 3:41 PM",
      "commitName": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.23,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n     IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n \n     try {\n       DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n-          HdfsServerConstants.SMALL_BUFFER_SIZE));\n+          smallBufferSize));\n       DataInputStream in \u003d new DataInputStream(pair.in);\n   \n       new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n       String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n       DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n       return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n       IOUtils.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          smallBufferSize));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n      DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtils.cleanup(null, pair.in, pair.out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "01/05/15 3:12 PM",
      "commitNameOld": "d3d019c337ecc10e9c6bbefc3a97c6cd1f5283c3",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n     IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n \n     try {\n       DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n-          HdfsConstants.SMALL_BUFFER_SIZE));\n+          HdfsServerConstants.SMALL_BUFFER_SIZE));\n       DataInputStream in \u003d new DataInputStream(pair.in);\n   \n       new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n       String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n       DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n       return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n       IOUtils.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          HdfsServerConstants.SMALL_BUFFER_SIZE));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n      DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtils.cleanup(null, pair.in, pair.out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "10/04/15 11:40 AM",
      "commitNameOld": "7660da95cb67cbfe034aa8fa2a5bf0f8c9fdf41a",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 0.13,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n-    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.socketTimeout, lb);\n+    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n \n     try {\n       DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n           HdfsConstants.SMALL_BUFFER_SIZE));\n       DataInputStream in \u003d new DataInputStream(pair.in);\n   \n       new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n       String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n       DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n \n       return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n       IOUtils.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.getSocketTimeout(), lb);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          HdfsConstants.SMALL_BUFFER_SIZE));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n      DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtils.cleanup(null, pair.in, pair.out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "67ed59348d638d56e6752ba2c71fdcd69567546d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7439. Add BlockOpResponseProto\u0027s message to the exception messages.  Contributed by Takanobu Asanuma\n",
      "commitDate": "01/03/15 11:03 PM",
      "commitName": "67ed59348d638d56e6752ba2c71fdcd69567546d",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "21/02/15 3:38 PM",
      "commitNameOld": "8b465b4b8caed31ca9daeaae108f9a868a30a455",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 8.31,
      "commitsBetweenForRepo": 60,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,21 @@\n   private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n     IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.socketTimeout, lb);\n \n     try {\n       DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n           HdfsConstants.SMALL_BUFFER_SIZE));\n       DataInputStream in \u003d new DataInputStream(pair.in);\n   \n       new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n-      \n-      if (reply.getStatus() !\u003d Status.SUCCESS) {\n-        if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n-          throw new InvalidBlockTokenException();\n-        } else {\n-          throw new IOException(\"Bad response \" + reply + \" trying to read \"\n-              + lb.getBlock() + \" from datanode \" + dn);\n-        }\n-      }\n-      \n+      String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n+      DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n+\n       return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n       IOUtils.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.socketTimeout, lb);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          HdfsConstants.SMALL_BUFFER_SIZE));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      String logInfo \u003d \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\n      DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\n\n      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtils.cleanup(null, pair.in, pair.out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "3b54223c0f32d42a84436c670d80b791a8e9696d": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/07/14 11:10 AM",
      "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthor": "Chris Nauroth",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/07/14 11:10 AM",
          "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthor": "Chris Nauroth",
          "commitDateOld": "30/05/14 5:12 PM",
          "commitNameOld": "880a0c673c74a128a01c72b60695f05327f5e961",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 44.75,
          "commitsBetweenForRepo": 266,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,32 +1,28 @@\n-  private static Type inferChecksumTypeByReading(\n-      String clientName, SocketFactory socketFactory, int socketTimeout,\n-      LocatedBlock lb, DatanodeInfo dn,\n-      DataEncryptionKey encryptionKey, boolean connectToDnViaHostname)\n+  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n-    IOStreamPair pair \u003d connectToDN(socketFactory, connectToDnViaHostname,\n-        encryptionKey, dn, socketTimeout);\n+    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.socketTimeout, lb);\n \n     try {\n       DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n           HdfsConstants.SMALL_BUFFER_SIZE));\n       DataInputStream in \u003d new DataInputStream(pair.in);\n   \n       new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n       \n       if (reply.getStatus() !\u003d Status.SUCCESS) {\n         if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n           throw new InvalidBlockTokenException();\n         } else {\n           throw new IOException(\"Bad response \" + reply + \" trying to read \"\n               + lb.getBlock() + \" from datanode \" + dn);\n         }\n       }\n       \n       return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n       IOUtils.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.socketTimeout, lb);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          HdfsConstants.SMALL_BUFFER_SIZE));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      \n      if (reply.getStatus() !\u003d Status.SUCCESS) {\n        if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n          throw new InvalidBlockTokenException();\n        } else {\n          throw new IOException(\"Bad response \" + reply + \" trying to read \"\n              + lb.getBlock() + \" from datanode \" + dn);\n        }\n      }\n      \n      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtils.cleanup(null, pair.in, pair.out);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldValue": "[clientName-String, socketFactory-SocketFactory, socketTimeout-int, lb-LocatedBlock, dn-DatanodeInfo, encryptionKey-DataEncryptionKey, connectToDnViaHostname-boolean]",
            "newValue": "[lb-LocatedBlock, dn-DatanodeInfo]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/07/14 11:10 AM",
          "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthor": "Chris Nauroth",
          "commitDateOld": "30/05/14 5:12 PM",
          "commitNameOld": "880a0c673c74a128a01c72b60695f05327f5e961",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 44.75,
          "commitsBetweenForRepo": 266,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,32 +1,28 @@\n-  private static Type inferChecksumTypeByReading(\n-      String clientName, SocketFactory socketFactory, int socketTimeout,\n-      LocatedBlock lb, DatanodeInfo dn,\n-      DataEncryptionKey encryptionKey, boolean connectToDnViaHostname)\n+  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n-    IOStreamPair pair \u003d connectToDN(socketFactory, connectToDnViaHostname,\n-        encryptionKey, dn, socketTimeout);\n+    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.socketTimeout, lb);\n \n     try {\n       DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n           HdfsConstants.SMALL_BUFFER_SIZE));\n       DataInputStream in \u003d new DataInputStream(pair.in);\n   \n       new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n       \n       if (reply.getStatus() !\u003d Status.SUCCESS) {\n         if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n           throw new InvalidBlockTokenException();\n         } else {\n           throw new IOException(\"Bad response \" + reply + \" trying to read \"\n               + lb.getBlock() + \" from datanode \" + dn);\n         }\n       }\n       \n       return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n       IOUtils.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.socketTimeout, lb);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          HdfsConstants.SMALL_BUFFER_SIZE));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      \n      if (reply.getStatus() !\u003d Status.SUCCESS) {\n        if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n          throw new InvalidBlockTokenException();\n        } else {\n          throw new IOException(\"Bad response \" + reply + \" trying to read \"\n              + lb.getBlock() + \" from datanode \" + dn);\n        }\n      }\n      \n      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtils.cleanup(null, pair.in, pair.out);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldValue": "[private, static]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/07/14 11:10 AM",
          "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthor": "Chris Nauroth",
          "commitDateOld": "30/05/14 5:12 PM",
          "commitNameOld": "880a0c673c74a128a01c72b60695f05327f5e961",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 44.75,
          "commitsBetweenForRepo": 266,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,32 +1,28 @@\n-  private static Type inferChecksumTypeByReading(\n-      String clientName, SocketFactory socketFactory, int socketTimeout,\n-      LocatedBlock lb, DatanodeInfo dn,\n-      DataEncryptionKey encryptionKey, boolean connectToDnViaHostname)\n+  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n       throws IOException {\n-    IOStreamPair pair \u003d connectToDN(socketFactory, connectToDnViaHostname,\n-        encryptionKey, dn, socketTimeout);\n+    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.socketTimeout, lb);\n \n     try {\n       DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n           HdfsConstants.SMALL_BUFFER_SIZE));\n       DataInputStream in \u003d new DataInputStream(pair.in);\n   \n       new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n           0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n       \n       if (reply.getStatus() !\u003d Status.SUCCESS) {\n         if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n           throw new InvalidBlockTokenException();\n         } else {\n           throw new IOException(\"Bad response \" + reply + \" trying to read \"\n               + lb.getBlock() + \" from datanode \" + dn);\n         }\n       }\n       \n       return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n       IOUtils.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(dn, dfsClientConf.socketTimeout, lb);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          HdfsConstants.SMALL_BUFFER_SIZE));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      \n      if (reply.getStatus() !\u003d Status.SUCCESS) {\n        if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n          throw new InvalidBlockTokenException();\n        } else {\n          throw new IOException(\"Bad response \" + reply + \" trying to read \"\n              + lb.getBlock() + \" from datanode \" + dn);\n        }\n      }\n      \n      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtils.cleanup(null, pair.in, pair.out);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4817.  Make HDFS advisory caching configurable on a per-file basis.  (Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505753 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/13 11:15 AM",
      "commitName": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "17/07/13 5:04 PM",
      "commitNameOld": "68faa67f1b3b681b40ecdc9002d9fb508e529af4",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 4.76,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,32 @@\n   private static Type inferChecksumTypeByReading(\n       String clientName, SocketFactory socketFactory, int socketTimeout,\n       LocatedBlock lb, DatanodeInfo dn,\n       DataEncryptionKey encryptionKey, boolean connectToDnViaHostname)\n       throws IOException {\n     IOStreamPair pair \u003d connectToDN(socketFactory, connectToDnViaHostname,\n         encryptionKey, dn, socketTimeout);\n \n     try {\n       DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n           HdfsConstants.SMALL_BUFFER_SIZE));\n       DataInputStream in \u003d new DataInputStream(pair.in);\n   \n-      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName, 0, 1, true);\n+      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n+          0, 1, true, CachingStrategy.newDefaultStrategy());\n       final BlockOpResponseProto reply \u003d\n           BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n       \n       if (reply.getStatus() !\u003d Status.SUCCESS) {\n         if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n           throw new InvalidBlockTokenException();\n         } else {\n           throw new IOException(\"Bad response \" + reply + \" trying to read \"\n               + lb.getBlock() + \" from datanode \" + dn);\n         }\n       }\n       \n       return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n     } finally {\n       IOUtils.cleanup(null, pair.in, pair.out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static Type inferChecksumTypeByReading(\n      String clientName, SocketFactory socketFactory, int socketTimeout,\n      LocatedBlock lb, DatanodeInfo dn,\n      DataEncryptionKey encryptionKey, boolean connectToDnViaHostname)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(socketFactory, connectToDnViaHostname,\n        encryptionKey, dn, socketTimeout);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          HdfsConstants.SMALL_BUFFER_SIZE));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName,\n          0, 1, true, CachingStrategy.newDefaultStrategy());\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      \n      if (reply.getStatus() !\u003d Status.SUCCESS) {\n        if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n          throw new InvalidBlockTokenException();\n        } else {\n          throw new IOException(\"Bad response \" + reply + \" trying to read \"\n              + lb.getBlock() + \" from datanode \" + dn);\n        }\n      }\n      \n      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtils.cleanup(null, pair.in, pair.out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "cfae13306ac0fb3f3c139d5ac511bf78cede1b77": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4403. DFSClient can infer checksum type when not provided by reading first byte. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1436730 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/01/13 6:59 PM",
      "commitName": "cfae13306ac0fb3f3c139d5ac511bf78cede1b77",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,31 @@\n+  private static Type inferChecksumTypeByReading(\n+      String clientName, SocketFactory socketFactory, int socketTimeout,\n+      LocatedBlock lb, DatanodeInfo dn,\n+      DataEncryptionKey encryptionKey, boolean connectToDnViaHostname)\n+      throws IOException {\n+    IOStreamPair pair \u003d connectToDN(socketFactory, connectToDnViaHostname,\n+        encryptionKey, dn, socketTimeout);\n+\n+    try {\n+      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n+          HdfsConstants.SMALL_BUFFER_SIZE));\n+      DataInputStream in \u003d new DataInputStream(pair.in);\n+  \n+      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName, 0, 1, true);\n+      final BlockOpResponseProto reply \u003d\n+          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n+      \n+      if (reply.getStatus() !\u003d Status.SUCCESS) {\n+        if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n+          throw new InvalidBlockTokenException();\n+        } else {\n+          throw new IOException(\"Bad response \" + reply + \" trying to read \"\n+              + lb.getBlock() + \" from datanode \" + dn);\n+        }\n+      }\n+      \n+      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n+    } finally {\n+      IOUtils.cleanup(null, pair.in, pair.out);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static Type inferChecksumTypeByReading(\n      String clientName, SocketFactory socketFactory, int socketTimeout,\n      LocatedBlock lb, DatanodeInfo dn,\n      DataEncryptionKey encryptionKey, boolean connectToDnViaHostname)\n      throws IOException {\n    IOStreamPair pair \u003d connectToDN(socketFactory, connectToDnViaHostname,\n        encryptionKey, dn, socketTimeout);\n\n    try {\n      DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(pair.out,\n          HdfsConstants.SMALL_BUFFER_SIZE));\n      DataInputStream in \u003d new DataInputStream(pair.in);\n  \n      new Sender(out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName, 0, 1, true);\n      final BlockOpResponseProto reply \u003d\n          BlockOpResponseProto.parseFrom(PBHelper.vintPrefixed(in));\n      \n      if (reply.getStatus() !\u003d Status.SUCCESS) {\n        if (reply.getStatus() \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n          throw new InvalidBlockTokenException();\n        } else {\n          throw new IOException(\"Bad response \" + reply + \" trying to read \"\n              + lb.getBlock() + \" from datanode \" + dn);\n        }\n      }\n      \n      return PBHelper.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\n    } finally {\n      IOUtils.cleanup(null, pair.in, pair.out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java"
    }
  }
}