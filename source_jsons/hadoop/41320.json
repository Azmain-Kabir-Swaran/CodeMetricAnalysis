{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSAppAttempt.java",
  "functionName": "assignContainer",
  "functionId": "assignContainer___node-FSSchedulerNode__reserved-boolean",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
  "functionStartLine": 948,
  "functionEndLine": 1064,
  "numCommitsSeen": 76,
  "timeTaken": 6120,
  "changeHistory": [
    "39b4a37e02e929a698fcf9e32f1f71bb6b977635",
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
    "2977bc6a141041ef7579efc416e93fc55e0c2a1a",
    "10468529a9b858bd945e7ecb063c9c1438efa474",
    "2528bea67ff80fae597f10e26c5f70d601af9fb1",
    "b98fc8249f0576e7b4e230ffc3cec5a20eefc543",
    "b8a30f2f170ffbd590e7366c3c944ab4919e40df",
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
    "bd69ea408f8fdd8293836ce1089fe9b01616f2f7",
    "6a6a59db7f1bfda47c3c14fb49676a7b22d2eb06",
    "b5a22e983832d4843b5df1d07858988e8bbf37e3",
    "7e42088abf230dce9c63497d0937fee4f9a1e4a5",
    "486e718fc1f5befd231494e2ec06bb360484f191"
  ],
  "changeHistoryShort": {
    "39b4a37e02e929a698fcf9e32f1f71bb6b977635": "Ybodychange",
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949": "Ybodychange",
    "2977bc6a141041ef7579efc416e93fc55e0c2a1a": "Ybodychange",
    "10468529a9b858bd945e7ecb063c9c1438efa474": "Ybodychange",
    "2528bea67ff80fae597f10e26c5f70d601af9fb1": "Ybodychange",
    "b98fc8249f0576e7b4e230ffc3cec5a20eefc543": "Ybodychange",
    "b8a30f2f170ffbd590e7366c3c944ab4919e40df": "Ybodychange",
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828": "Ybodychange",
    "bd69ea408f8fdd8293836ce1089fe9b01616f2f7": "Ybodychange",
    "6a6a59db7f1bfda47c3c14fb49676a7b22d2eb06": "Ybodychange",
    "b5a22e983832d4843b5df1d07858988e8bbf37e3": "Ybodychange",
    "7e42088abf230dce9c63497d0937fee4f9a1e4a5": "Ybodychange",
    "486e718fc1f5befd231494e2ec06bb360484f191": "Yintroduced"
  },
  "changeHistoryDetails": {
    "39b4a37e02e929a698fcf9e32f1f71bb6b977635": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9341.  Fixed enentrant lock usage in YARN project.\n            Contributed by Prabhu Joseph\n",
      "commitDate": "07/03/19 1:47 PM",
      "commitName": "39b4a37e02e929a698fcf9e32f1f71bb6b977635",
      "commitAuthor": "Eric Yang",
      "commitDateOld": "04/03/19 9:10 PM",
      "commitNameOld": "e40e2d6ad5cbe782c3a067229270738b501ed27e",
      "commitAuthorOld": "Prabhu Joseph",
      "daysBetweenCommits": 2.69,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,117 +1,117 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n     Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n         Collections.singletonList(\n             node.getReservedContainer().getReservedSchedulerKey()) :\n         getSchedulerKeys();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n+    writeLock.lock();\n     try {\n-      writeLock.lock();\n \n       // TODO (wandga): All logics in this method should be added to\n       // SchedulerPlacement#canDelayTo which is independent from scheduler.\n       // Scheduler can choose to use various/pluggable delay-scheduling\n       // implementation.\n       for (SchedulerRequestKey schedulerKey : keysToTry) {\n         // Skip it for reserved container, since\n         // we already check it in isValidReservation.\n         if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n           continue;\n         }\n \n         addSchedulingOpportunity(schedulerKey);\n \n         PendingAsk rackLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n             node.getRackName());\n         PendingAsk nodeLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n             node.getNodeName());\n \n         if (nodeLocalPendingAsk.getCount() \u003e 0\n             \u0026\u0026 !appSchedulingInfo.canDelayTo(schedulerKey,\n             node.getNodeName())) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n               + nodeLocalPendingAsk);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n           allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n           allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n         if (rackLocalPendingAsk.getCount() \u003e 0\n             \u0026\u0026 nodeLocalPendingAsk.getCount() \u003e 0) {\n           if (LOG.isTraceEnabled()) {\n             LOG.trace(\"Assign container on \" + node.getNodeName()\n                 + \" node, assignType: NODE_LOCAL\" + \", allowedLocality: \"\n                 + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                 + \", app attempt id: \" + this.attemptId);\n           }\n           return assignContainer(node, nodeLocalPendingAsk, NodeType.NODE_LOCAL,\n               reserved, schedulerKey);\n         }\n \n         if (!appSchedulingInfo.canDelayTo(schedulerKey, node.getRackName())) {\n           continue;\n         }\n \n         if (rackLocalPendingAsk.getCount() \u003e 0\n             \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n             .equals(NodeType.OFF_SWITCH))) {\n           if (LOG.isTraceEnabled()) {\n             LOG.trace(\"Assign container on \" + node.getNodeName()\n                 + \" node, assignType: RACK_LOCAL\" + \", allowedLocality: \"\n                 + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                 + \", app attempt id: \" + this.attemptId);\n           }\n           return assignContainer(node, rackLocalPendingAsk, NodeType.RACK_LOCAL,\n               reserved, schedulerKey);\n         }\n \n         PendingAsk offswitchAsk \u003d getPendingAsk(schedulerKey,\n             ResourceRequest.ANY);\n         if (!appSchedulingInfo.canDelayTo(schedulerKey, ResourceRequest.ANY)) {\n           continue;\n         }\n \n         if (offswitchAsk.getCount() \u003e 0) {\n           if (getAppPlacementAllocator(schedulerKey).getUniqueLocationAsks()\n               \u003c\u003d 1 || allowedLocality.equals(NodeType.OFF_SWITCH)) {\n             if (LOG.isTraceEnabled()) {\n               LOG.trace(\"Assign container on \" + node.getNodeName()\n                   + \" node, assignType: OFF_SWITCH\" + \", allowedLocality: \"\n                   + allowedLocality + \", priority: \"\n                   + schedulerKey.getPriority()\n                   + \", app attempt id: \" + this.attemptId);\n             }\n             return assignContainer(node, offswitchAsk, NodeType.OFF_SWITCH,\n                 reserved, schedulerKey);\n           }\n         }\n \n         if (LOG.isTraceEnabled()) {\n           LOG.trace(\"Can\u0027t assign container on \" + node.getNodeName()\n               + \" node, allowedLocality: \" + allowedLocality + \", priority: \"\n               + schedulerKey.getPriority() + \", app attempt id: \"\n               + this.attemptId);\n         }\n       }\n     } finally {\n       writeLock.unlock();\n     }\n \n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n        Collections.singletonList(\n            node.getReservedContainer().getReservedSchedulerKey()) :\n        getSchedulerKeys();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    writeLock.lock();\n    try {\n\n      // TODO (wandga): All logics in this method should be added to\n      // SchedulerPlacement#canDelayTo which is independent from scheduler.\n      // Scheduler can choose to use various/pluggable delay-scheduling\n      // implementation.\n      for (SchedulerRequestKey schedulerKey : keysToTry) {\n        // Skip it for reserved container, since\n        // we already check it in isValidReservation.\n        if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(schedulerKey);\n\n        PendingAsk rackLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n            node.getRackName());\n        PendingAsk nodeLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n            node.getNodeName());\n\n        if (nodeLocalPendingAsk.getCount() \u003e 0\n            \u0026\u0026 !appSchedulingInfo.canDelayTo(schedulerKey,\n            node.getNodeName())) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + nodeLocalPendingAsk);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalPendingAsk.getCount() \u003e 0\n            \u0026\u0026 nodeLocalPendingAsk.getCount() \u003e 0) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Assign container on \" + node.getNodeName()\n                + \" node, assignType: NODE_LOCAL\" + \", allowedLocality: \"\n                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                + \", app attempt id: \" + this.attemptId);\n          }\n          return assignContainer(node, nodeLocalPendingAsk, NodeType.NODE_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        if (!appSchedulingInfo.canDelayTo(schedulerKey, node.getRackName())) {\n          continue;\n        }\n\n        if (rackLocalPendingAsk.getCount() \u003e 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n            .equals(NodeType.OFF_SWITCH))) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Assign container on \" + node.getNodeName()\n                + \" node, assignType: RACK_LOCAL\" + \", allowedLocality: \"\n                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                + \", app attempt id: \" + this.attemptId);\n          }\n          return assignContainer(node, rackLocalPendingAsk, NodeType.RACK_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        PendingAsk offswitchAsk \u003d getPendingAsk(schedulerKey,\n            ResourceRequest.ANY);\n        if (!appSchedulingInfo.canDelayTo(schedulerKey, ResourceRequest.ANY)) {\n          continue;\n        }\n\n        if (offswitchAsk.getCount() \u003e 0) {\n          if (getAppPlacementAllocator(schedulerKey).getUniqueLocationAsks()\n              \u003c\u003d 1 || allowedLocality.equals(NodeType.OFF_SWITCH)) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Assign container on \" + node.getNodeName()\n                  + \" node, assignType: OFF_SWITCH\" + \", allowedLocality: \"\n                  + allowedLocality + \", priority: \"\n                  + schedulerKey.getPriority()\n                  + \", app attempt id: \" + this.attemptId);\n            }\n            return assignContainer(node, offswitchAsk, NodeType.OFF_SWITCH,\n                reserved, schedulerKey);\n          }\n        }\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Can\u0027t assign container on \" + node.getNodeName()\n              + \" node, allowedLocality: \" + allowedLocality + \", priority: \"\n              + schedulerKey.getPriority() + \", app attempt id: \"\n              + this.attemptId);\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949": {
      "type": "Ybodychange",
      "commitMessage": "YARN-7437. Rename PlacementSet and SchedulingPlacementSet. (Wangda Tan via kkaranasos)\n",
      "commitDate": "09/11/17 1:01 PM",
      "commitName": "ac4d2b1081d8836a21bc70e77f4e6cd2071a9949",
      "commitAuthor": "Konstantinos Karanasos",
      "commitDateOld": "24/10/17 10:21 AM",
      "commitNameOld": "025c6565725c1819566377632753e8b9055617a6",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 16.15,
      "commitsBetweenForRepo": 212,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,117 +1,117 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n     Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n         Collections.singletonList(\n             node.getReservedContainer().getReservedSchedulerKey()) :\n         getSchedulerKeys();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n     try {\n       writeLock.lock();\n \n       // TODO (wandga): All logics in this method should be added to\n       // SchedulerPlacement#canDelayTo which is independent from scheduler.\n       // Scheduler can choose to use various/pluggable delay-scheduling\n       // implementation.\n       for (SchedulerRequestKey schedulerKey : keysToTry) {\n         // Skip it for reserved container, since\n         // we already check it in isValidReservation.\n         if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n           continue;\n         }\n \n         addSchedulingOpportunity(schedulerKey);\n \n         PendingAsk rackLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n             node.getRackName());\n         PendingAsk nodeLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n             node.getNodeName());\n \n         if (nodeLocalPendingAsk.getCount() \u003e 0\n             \u0026\u0026 !appSchedulingInfo.canDelayTo(schedulerKey,\n             node.getNodeName())) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n               + nodeLocalPendingAsk);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n           allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n           allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n         if (rackLocalPendingAsk.getCount() \u003e 0\n             \u0026\u0026 nodeLocalPendingAsk.getCount() \u003e 0) {\n           if (LOG.isTraceEnabled()) {\n             LOG.trace(\"Assign container on \" + node.getNodeName()\n                 + \" node, assignType: NODE_LOCAL\" + \", allowedLocality: \"\n                 + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                 + \", app attempt id: \" + this.attemptId);\n           }\n           return assignContainer(node, nodeLocalPendingAsk, NodeType.NODE_LOCAL,\n               reserved, schedulerKey);\n         }\n \n         if (!appSchedulingInfo.canDelayTo(schedulerKey, node.getRackName())) {\n           continue;\n         }\n \n         if (rackLocalPendingAsk.getCount() \u003e 0\n             \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n             .equals(NodeType.OFF_SWITCH))) {\n           if (LOG.isTraceEnabled()) {\n             LOG.trace(\"Assign container on \" + node.getNodeName()\n                 + \" node, assignType: RACK_LOCAL\" + \", allowedLocality: \"\n                 + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                 + \", app attempt id: \" + this.attemptId);\n           }\n           return assignContainer(node, rackLocalPendingAsk, NodeType.RACK_LOCAL,\n               reserved, schedulerKey);\n         }\n \n         PendingAsk offswitchAsk \u003d getPendingAsk(schedulerKey,\n             ResourceRequest.ANY);\n         if (!appSchedulingInfo.canDelayTo(schedulerKey, ResourceRequest.ANY)) {\n           continue;\n         }\n \n         if (offswitchAsk.getCount() \u003e 0) {\n-          if (getSchedulingPlacementSet(schedulerKey).getUniqueLocationAsks()\n+          if (getAppPlacementAllocator(schedulerKey).getUniqueLocationAsks()\n               \u003c\u003d 1 || allowedLocality.equals(NodeType.OFF_SWITCH)) {\n             if (LOG.isTraceEnabled()) {\n               LOG.trace(\"Assign container on \" + node.getNodeName()\n                   + \" node, assignType: OFF_SWITCH\" + \", allowedLocality: \"\n                   + allowedLocality + \", priority: \"\n                   + schedulerKey.getPriority()\n                   + \", app attempt id: \" + this.attemptId);\n             }\n             return assignContainer(node, offswitchAsk, NodeType.OFF_SWITCH,\n                 reserved, schedulerKey);\n           }\n         }\n \n         if (LOG.isTraceEnabled()) {\n           LOG.trace(\"Can\u0027t assign container on \" + node.getNodeName()\n               + \" node, allowedLocality: \" + allowedLocality + \", priority: \"\n               + schedulerKey.getPriority() + \", app attempt id: \"\n               + this.attemptId);\n         }\n       }\n     } finally {\n       writeLock.unlock();\n     }\n \n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n        Collections.singletonList(\n            node.getReservedContainer().getReservedSchedulerKey()) :\n        getSchedulerKeys();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    try {\n      writeLock.lock();\n\n      // TODO (wandga): All logics in this method should be added to\n      // SchedulerPlacement#canDelayTo which is independent from scheduler.\n      // Scheduler can choose to use various/pluggable delay-scheduling\n      // implementation.\n      for (SchedulerRequestKey schedulerKey : keysToTry) {\n        // Skip it for reserved container, since\n        // we already check it in isValidReservation.\n        if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(schedulerKey);\n\n        PendingAsk rackLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n            node.getRackName());\n        PendingAsk nodeLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n            node.getNodeName());\n\n        if (nodeLocalPendingAsk.getCount() \u003e 0\n            \u0026\u0026 !appSchedulingInfo.canDelayTo(schedulerKey,\n            node.getNodeName())) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + nodeLocalPendingAsk);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalPendingAsk.getCount() \u003e 0\n            \u0026\u0026 nodeLocalPendingAsk.getCount() \u003e 0) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Assign container on \" + node.getNodeName()\n                + \" node, assignType: NODE_LOCAL\" + \", allowedLocality: \"\n                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                + \", app attempt id: \" + this.attemptId);\n          }\n          return assignContainer(node, nodeLocalPendingAsk, NodeType.NODE_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        if (!appSchedulingInfo.canDelayTo(schedulerKey, node.getRackName())) {\n          continue;\n        }\n\n        if (rackLocalPendingAsk.getCount() \u003e 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n            .equals(NodeType.OFF_SWITCH))) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Assign container on \" + node.getNodeName()\n                + \" node, assignType: RACK_LOCAL\" + \", allowedLocality: \"\n                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                + \", app attempt id: \" + this.attemptId);\n          }\n          return assignContainer(node, rackLocalPendingAsk, NodeType.RACK_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        PendingAsk offswitchAsk \u003d getPendingAsk(schedulerKey,\n            ResourceRequest.ANY);\n        if (!appSchedulingInfo.canDelayTo(schedulerKey, ResourceRequest.ANY)) {\n          continue;\n        }\n\n        if (offswitchAsk.getCount() \u003e 0) {\n          if (getAppPlacementAllocator(schedulerKey).getUniqueLocationAsks()\n              \u003c\u003d 1 || allowedLocality.equals(NodeType.OFF_SWITCH)) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Assign container on \" + node.getNodeName()\n                  + \" node, assignType: OFF_SWITCH\" + \", allowedLocality: \"\n                  + allowedLocality + \", priority: \"\n                  + schedulerKey.getPriority()\n                  + \", app attempt id: \" + this.attemptId);\n            }\n            return assignContainer(node, offswitchAsk, NodeType.OFF_SWITCH,\n                reserved, schedulerKey);\n          }\n        }\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Can\u0027t assign container on \" + node.getNodeName()\n              + \" node, allowedLocality: \" + allowedLocality + \", priority: \"\n              + schedulerKey.getPriority() + \", app attempt id: \"\n              + this.attemptId);\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "2977bc6a141041ef7579efc416e93fc55e0c2a1a": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6040. Introduce api independent PendingAsk to replace usage of ResourceRequest within Scheduler classes. (Wangda Tan via asuresh)\n",
      "commitDate": "06/01/17 9:59 AM",
      "commitName": "2977bc6a141041ef7579efc416e93fc55e0c2a1a",
      "commitAuthor": "Arun Suresh",
      "commitDateOld": "05/01/17 10:31 AM",
      "commitNameOld": "0a55bd841ec0f2eb89a0383f4c589526e8b138d4",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.98,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,110 +1,116 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n     Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n         Collections.singletonList(\n             node.getReservedContainer().getReservedSchedulerKey()) :\n         getSchedulerKeys();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n     try {\n       writeLock.lock();\n+\n+      // TODO (wandga): All logics in this method should be added to\n+      // SchedulerPlacement#canDelayTo which is independent from scheduler.\n+      // Scheduler can choose to use various/pluggable delay-scheduling\n+      // implementation.\n       for (SchedulerRequestKey schedulerKey : keysToTry) {\n         // Skip it for reserved container, since\n         // we already check it in isValidReservation.\n         if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n           continue;\n         }\n \n         addSchedulingOpportunity(schedulerKey);\n \n-        ResourceRequest rackLocalRequest \u003d getResourceRequest(schedulerKey,\n+        PendingAsk rackLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n             node.getRackName());\n-        ResourceRequest localRequest \u003d getResourceRequest(schedulerKey,\n+        PendingAsk nodeLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n             node.getNodeName());\n \n-        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n+        if (nodeLocalPendingAsk.getCount() \u003e 0\n+            \u0026\u0026 !appSchedulingInfo.canDelayTo(schedulerKey,\n+            node.getNodeName())) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n-              + localRequest);\n+              + nodeLocalPendingAsk);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n           allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n           allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n-        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n-            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n+        if (rackLocalPendingAsk.getCount() \u003e 0\n+            \u0026\u0026 nodeLocalPendingAsk.getCount() \u003e 0) {\n           if (LOG.isTraceEnabled()) {\n             LOG.trace(\"Assign container on \" + node.getNodeName()\n                 + \" node, assignType: NODE_LOCAL\" + \", allowedLocality: \"\n                 + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                 + \", app attempt id: \" + this.attemptId);\n           }\n-          return assignContainer(node, localRequest, NodeType.NODE_LOCAL,\n+          return assignContainer(node, nodeLocalPendingAsk, NodeType.NODE_LOCAL,\n               reserved, schedulerKey);\n         }\n \n-        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n+        if (!appSchedulingInfo.canDelayTo(schedulerKey, node.getRackName())) {\n           continue;\n         }\n \n-        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n+        if (rackLocalPendingAsk.getCount() \u003e 0\n             \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n             .equals(NodeType.OFF_SWITCH))) {\n           if (LOG.isTraceEnabled()) {\n             LOG.trace(\"Assign container on \" + node.getNodeName()\n                 + \" node, assignType: RACK_LOCAL\" + \", allowedLocality: \"\n                 + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                 + \", app attempt id: \" + this.attemptId);\n           }\n-          return assignContainer(node, rackLocalRequest, NodeType.RACK_LOCAL,\n+          return assignContainer(node, rackLocalPendingAsk, NodeType.RACK_LOCAL,\n               reserved, schedulerKey);\n         }\n \n-        ResourceRequest offSwitchRequest \u003d getResourceRequest(schedulerKey,\n+        PendingAsk offswitchAsk \u003d getPendingAsk(schedulerKey,\n             ResourceRequest.ANY);\n-        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n+        if (!appSchedulingInfo.canDelayTo(schedulerKey, ResourceRequest.ANY)) {\n           continue;\n         }\n \n-        if (offSwitchRequest !\u003d null\n-            \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0) {\n-          if (!hasNodeOrRackLocalRequests(schedulerKey) || allowedLocality\n-              .equals(NodeType.OFF_SWITCH)) {\n+        if (offswitchAsk.getCount() \u003e 0) {\n+          if (getSchedulingPlacementSet(schedulerKey).getUniqueLocationAsks()\n+              \u003c\u003d 1 || allowedLocality.equals(NodeType.OFF_SWITCH)) {\n             if (LOG.isTraceEnabled()) {\n               LOG.trace(\"Assign container on \" + node.getNodeName()\n                   + \" node, assignType: OFF_SWITCH\" + \", allowedLocality: \"\n                   + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                   + \", app attempt id: \" + this.attemptId);\n             }\n-            return assignContainer(node, offSwitchRequest, NodeType.OFF_SWITCH,\n+            return assignContainer(node, offswitchAsk, NodeType.OFF_SWITCH,\n                 reserved, schedulerKey);\n           }\n         }\n \n         if (LOG.isTraceEnabled()) {\n           LOG.trace(\"Can\u0027t assign container on \" + node.getNodeName()\n               + \" node, allowedLocality: \" + allowedLocality + \", priority: \"\n               + schedulerKey.getPriority() + \", app attempt id: \"\n               + this.attemptId);\n         }\n       }\n     } finally {\n       writeLock.unlock();\n     }\n \n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n        Collections.singletonList(\n            node.getReservedContainer().getReservedSchedulerKey()) :\n        getSchedulerKeys();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    try {\n      writeLock.lock();\n\n      // TODO (wandga): All logics in this method should be added to\n      // SchedulerPlacement#canDelayTo which is independent from scheduler.\n      // Scheduler can choose to use various/pluggable delay-scheduling\n      // implementation.\n      for (SchedulerRequestKey schedulerKey : keysToTry) {\n        // Skip it for reserved container, since\n        // we already check it in isValidReservation.\n        if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(schedulerKey);\n\n        PendingAsk rackLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n            node.getRackName());\n        PendingAsk nodeLocalPendingAsk \u003d getPendingAsk(schedulerKey,\n            node.getNodeName());\n\n        if (nodeLocalPendingAsk.getCount() \u003e 0\n            \u0026\u0026 !appSchedulingInfo.canDelayTo(schedulerKey,\n            node.getNodeName())) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + nodeLocalPendingAsk);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalPendingAsk.getCount() \u003e 0\n            \u0026\u0026 nodeLocalPendingAsk.getCount() \u003e 0) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Assign container on \" + node.getNodeName()\n                + \" node, assignType: NODE_LOCAL\" + \", allowedLocality: \"\n                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                + \", app attempt id: \" + this.attemptId);\n          }\n          return assignContainer(node, nodeLocalPendingAsk, NodeType.NODE_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        if (!appSchedulingInfo.canDelayTo(schedulerKey, node.getRackName())) {\n          continue;\n        }\n\n        if (rackLocalPendingAsk.getCount() \u003e 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n            .equals(NodeType.OFF_SWITCH))) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Assign container on \" + node.getNodeName()\n                + \" node, assignType: RACK_LOCAL\" + \", allowedLocality: \"\n                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                + \", app attempt id: \" + this.attemptId);\n          }\n          return assignContainer(node, rackLocalPendingAsk, NodeType.RACK_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        PendingAsk offswitchAsk \u003d getPendingAsk(schedulerKey,\n            ResourceRequest.ANY);\n        if (!appSchedulingInfo.canDelayTo(schedulerKey, ResourceRequest.ANY)) {\n          continue;\n        }\n\n        if (offswitchAsk.getCount() \u003e 0) {\n          if (getSchedulingPlacementSet(schedulerKey).getUniqueLocationAsks()\n              \u003c\u003d 1 || allowedLocality.equals(NodeType.OFF_SWITCH)) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Assign container on \" + node.getNodeName()\n                  + \" node, assignType: OFF_SWITCH\" + \", allowedLocality: \"\n                  + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                  + \", app attempt id: \" + this.attemptId);\n            }\n            return assignContainer(node, offswitchAsk, NodeType.OFF_SWITCH,\n                reserved, schedulerKey);\n          }\n        }\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Can\u0027t assign container on \" + node.getNodeName()\n              + \" node, allowedLocality: \" + allowedLocality + \", priority: \"\n              + schedulerKey.getPriority() + \", app attempt id: \"\n              + this.attemptId);\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "10468529a9b858bd945e7ecb063c9c1438efa474": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4752. Improved preemption in FairScheduler. (kasha)\n\nContains:\nYARN-5605. Preempt containers (all on one node) to meet the requirement of starved applications\nYARN-5821. Drop left-over preemption-related code and clean up method visibilities in the Schedulable hierarchy\nYARN-5783. Verify identification of starved applications.\nYARN-5819. Verify fairshare and minshare preemption\nYARN-5885. Cleanup YARN-4752 branch for merge\n\nChange-Id: Iee0962377d019dd64dc69a020725d2eaf360858c\n",
      "commitDate": "23/11/16 9:48 PM",
      "commitName": "10468529a9b858bd945e7ecb063c9c1438efa474",
      "commitAuthor": "Daniel Templeton",
      "commitDateOld": "09/11/16 1:11 PM",
      "commitNameOld": "59ee8b7a88603e94b5661a8d5d088f7aa99fe049",
      "commitAuthorOld": "Daniel Templeton",
      "daysBetweenCommits": 14.36,
      "commitsBetweenForRepo": 89,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,109 +1,110 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n     Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n-        Arrays.asList(node.getReservedContainer().getReservedSchedulerKey()) :\n+        Collections.singletonList(\n+            node.getReservedContainer().getReservedSchedulerKey()) :\n         getSchedulerKeys();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n     try {\n       writeLock.lock();\n       for (SchedulerRequestKey schedulerKey : keysToTry) {\n         // Skip it for reserved container, since\n         // we already check it in isValidReservation.\n         if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n           continue;\n         }\n \n         addSchedulingOpportunity(schedulerKey);\n \n         ResourceRequest rackLocalRequest \u003d getResourceRequest(schedulerKey,\n             node.getRackName());\n         ResourceRequest localRequest \u003d getResourceRequest(schedulerKey,\n             node.getNodeName());\n \n         if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n               + localRequest);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n           allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n           allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n           if (LOG.isTraceEnabled()) {\n             LOG.trace(\"Assign container on \" + node.getNodeName()\n                 + \" node, assignType: NODE_LOCAL\" + \", allowedLocality: \"\n                 + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                 + \", app attempt id: \" + this.attemptId);\n           }\n           return assignContainer(node, localRequest, NodeType.NODE_LOCAL,\n               reserved, schedulerKey);\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n             .equals(NodeType.OFF_SWITCH))) {\n           if (LOG.isTraceEnabled()) {\n             LOG.trace(\"Assign container on \" + node.getNodeName()\n                 + \" node, assignType: RACK_LOCAL\" + \", allowedLocality: \"\n                 + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                 + \", app attempt id: \" + this.attemptId);\n           }\n           return assignContainer(node, rackLocalRequest, NodeType.RACK_LOCAL,\n               reserved, schedulerKey);\n         }\n \n         ResourceRequest offSwitchRequest \u003d getResourceRequest(schedulerKey,\n             ResourceRequest.ANY);\n         if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (offSwitchRequest !\u003d null\n             \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0) {\n           if (!hasNodeOrRackLocalRequests(schedulerKey) || allowedLocality\n               .equals(NodeType.OFF_SWITCH)) {\n             if (LOG.isTraceEnabled()) {\n               LOG.trace(\"Assign container on \" + node.getNodeName()\n                   + \" node, assignType: OFF_SWITCH\" + \", allowedLocality: \"\n                   + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                   + \", app attempt id: \" + this.attemptId);\n             }\n             return assignContainer(node, offSwitchRequest, NodeType.OFF_SWITCH,\n                 reserved, schedulerKey);\n           }\n         }\n \n         if (LOG.isTraceEnabled()) {\n           LOG.trace(\"Can\u0027t assign container on \" + node.getNodeName()\n               + \" node, allowedLocality: \" + allowedLocality + \", priority: \"\n               + schedulerKey.getPriority() + \", app attempt id: \"\n               + this.attemptId);\n         }\n       }\n     } finally {\n       writeLock.unlock();\n     }\n \n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n        Collections.singletonList(\n            node.getReservedContainer().getReservedSchedulerKey()) :\n        getSchedulerKeys();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    try {\n      writeLock.lock();\n      for (SchedulerRequestKey schedulerKey : keysToTry) {\n        // Skip it for reserved container, since\n        // we already check it in isValidReservation.\n        if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(schedulerKey);\n\n        ResourceRequest rackLocalRequest \u003d getResourceRequest(schedulerKey,\n            node.getRackName());\n        ResourceRequest localRequest \u003d getResourceRequest(schedulerKey,\n            node.getNodeName());\n\n        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + localRequest);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Assign container on \" + node.getNodeName()\n                + \" node, assignType: NODE_LOCAL\" + \", allowedLocality: \"\n                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                + \", app attempt id: \" + this.attemptId);\n          }\n          return assignContainer(node, localRequest, NodeType.NODE_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n            .equals(NodeType.OFF_SWITCH))) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Assign container on \" + node.getNodeName()\n                + \" node, assignType: RACK_LOCAL\" + \", allowedLocality: \"\n                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                + \", app attempt id: \" + this.attemptId);\n          }\n          return assignContainer(node, rackLocalRequest, NodeType.RACK_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        ResourceRequest offSwitchRequest \u003d getResourceRequest(schedulerKey,\n            ResourceRequest.ANY);\n        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (offSwitchRequest !\u003d null\n            \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0) {\n          if (!hasNodeOrRackLocalRequests(schedulerKey) || allowedLocality\n              .equals(NodeType.OFF_SWITCH)) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Assign container on \" + node.getNodeName()\n                  + \" node, assignType: OFF_SWITCH\" + \", allowedLocality: \"\n                  + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                  + \", app attempt id: \" + this.attemptId);\n            }\n            return assignContainer(node, offSwitchRequest, NodeType.OFF_SWITCH,\n                reserved, schedulerKey);\n          }\n        }\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Can\u0027t assign container on \" + node.getNodeName()\n              + \" node, allowedLocality: \" + allowedLocality + \", priority: \"\n              + schedulerKey.getPriority() + \", app attempt id: \"\n              + this.attemptId);\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "2528bea67ff80fae597f10e26c5f70d601af9fb1": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4396. Log the trace information on FSAppAttempt#assignContainer (Contributed by Yiqun Li via Daniel Templeton)\n",
      "commitDate": "31/10/16 1:34 PM",
      "commitName": "2528bea67ff80fae597f10e26c5f70d601af9fb1",
      "commitAuthor": "Daniel Templeton",
      "commitDateOld": "27/10/16 2:42 PM",
      "commitNameOld": "b98fc8249f0576e7b4e230ffc3cec5a20eefc543",
      "commitAuthorOld": "Daniel Templeton",
      "daysBetweenCommits": 3.95,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,109 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n     Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n         Arrays.asList(node.getReservedContainer().getReservedSchedulerKey()) :\n         getSchedulerKeys();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n     try {\n       writeLock.lock();\n       for (SchedulerRequestKey schedulerKey : keysToTry) {\n         // Skip it for reserved container, since\n         // we already check it in isValidReservation.\n         if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n           continue;\n         }\n \n         addSchedulingOpportunity(schedulerKey);\n \n         ResourceRequest rackLocalRequest \u003d getResourceRequest(schedulerKey,\n             node.getRackName());\n         ResourceRequest localRequest \u003d getResourceRequest(schedulerKey,\n             node.getNodeName());\n \n         if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n               + localRequest);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n           allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n           allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n+          if (LOG.isTraceEnabled()) {\n+            LOG.trace(\"Assign container on \" + node.getNodeName()\n+                + \" node, assignType: NODE_LOCAL\" + \", allowedLocality: \"\n+                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n+                + \", app attempt id: \" + this.attemptId);\n+          }\n           return assignContainer(node, localRequest, NodeType.NODE_LOCAL,\n               reserved, schedulerKey);\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n             .equals(NodeType.OFF_SWITCH))) {\n+          if (LOG.isTraceEnabled()) {\n+            LOG.trace(\"Assign container on \" + node.getNodeName()\n+                + \" node, assignType: RACK_LOCAL\" + \", allowedLocality: \"\n+                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n+                + \", app attempt id: \" + this.attemptId);\n+          }\n           return assignContainer(node, rackLocalRequest, NodeType.RACK_LOCAL,\n               reserved, schedulerKey);\n         }\n \n         ResourceRequest offSwitchRequest \u003d getResourceRequest(schedulerKey,\n             ResourceRequest.ANY);\n         if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (offSwitchRequest !\u003d null\n             \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0) {\n           if (!hasNodeOrRackLocalRequests(schedulerKey) || allowedLocality\n               .equals(NodeType.OFF_SWITCH)) {\n+            if (LOG.isTraceEnabled()) {\n+              LOG.trace(\"Assign container on \" + node.getNodeName()\n+                  + \" node, assignType: OFF_SWITCH\" + \", allowedLocality: \"\n+                  + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n+                  + \", app attempt id: \" + this.attemptId);\n+            }\n             return assignContainer(node, offSwitchRequest, NodeType.OFF_SWITCH,\n                 reserved, schedulerKey);\n           }\n         }\n+\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"Can\u0027t assign container on \" + node.getNodeName()\n+              + \" node, allowedLocality: \" + allowedLocality + \", priority: \"\n+              + schedulerKey.getPriority() + \", app attempt id: \"\n+              + this.attemptId);\n+        }\n       }\n     } finally {\n       writeLock.unlock();\n     }\n \n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n        Arrays.asList(node.getReservedContainer().getReservedSchedulerKey()) :\n        getSchedulerKeys();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    try {\n      writeLock.lock();\n      for (SchedulerRequestKey schedulerKey : keysToTry) {\n        // Skip it for reserved container, since\n        // we already check it in isValidReservation.\n        if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(schedulerKey);\n\n        ResourceRequest rackLocalRequest \u003d getResourceRequest(schedulerKey,\n            node.getRackName());\n        ResourceRequest localRequest \u003d getResourceRequest(schedulerKey,\n            node.getNodeName());\n\n        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + localRequest);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Assign container on \" + node.getNodeName()\n                + \" node, assignType: NODE_LOCAL\" + \", allowedLocality: \"\n                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                + \", app attempt id: \" + this.attemptId);\n          }\n          return assignContainer(node, localRequest, NodeType.NODE_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n            .equals(NodeType.OFF_SWITCH))) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Assign container on \" + node.getNodeName()\n                + \" node, assignType: RACK_LOCAL\" + \", allowedLocality: \"\n                + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                + \", app attempt id: \" + this.attemptId);\n          }\n          return assignContainer(node, rackLocalRequest, NodeType.RACK_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        ResourceRequest offSwitchRequest \u003d getResourceRequest(schedulerKey,\n            ResourceRequest.ANY);\n        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (offSwitchRequest !\u003d null\n            \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0) {\n          if (!hasNodeOrRackLocalRequests(schedulerKey) || allowedLocality\n              .equals(NodeType.OFF_SWITCH)) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Assign container on \" + node.getNodeName()\n                  + \" node, assignType: OFF_SWITCH\" + \", allowedLocality: \"\n                  + allowedLocality + \", priority: \" + schedulerKey.getPriority()\n                  + \", app attempt id: \" + this.attemptId);\n            }\n            return assignContainer(node, offSwitchRequest, NodeType.OFF_SWITCH,\n                reserved, schedulerKey);\n          }\n        }\n\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Can\u0027t assign container on \" + node.getNodeName()\n              + \" node, allowedLocality: \" + allowedLocality + \", priority: \"\n              + schedulerKey.getPriority() + \", app attempt id: \"\n              + this.attemptId);\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "b98fc8249f0576e7b4e230ffc3cec5a20eefc543": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4710. Reduce logging application reserved debug info in FSAppAttempt#assignContainer (Contributed by Yiqun Lin via Daniel Templeton)\n",
      "commitDate": "27/10/16 2:42 PM",
      "commitName": "b98fc8249f0576e7b4e230ffc3cec5a20eefc543",
      "commitAuthor": "Daniel Templeton",
      "commitDateOld": "19/09/16 2:08 AM",
      "commitNameOld": "b8a30f2f170ffbd590e7366c3c944ab4919e40df",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 38.52,
      "commitsBetweenForRepo": 311,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,84 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n     Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n         Arrays.asList(node.getReservedContainer().getReservedSchedulerKey()) :\n         getSchedulerKeys();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n     try {\n       writeLock.lock();\n       for (SchedulerRequestKey schedulerKey : keysToTry) {\n         // Skip it for reserved container, since\n         // we already check it in isValidReservation.\n         if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n           continue;\n         }\n \n         addSchedulingOpportunity(schedulerKey);\n \n         ResourceRequest rackLocalRequest \u003d getResourceRequest(schedulerKey,\n             node.getRackName());\n         ResourceRequest localRequest \u003d getResourceRequest(schedulerKey,\n             node.getNodeName());\n \n         if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n               + localRequest);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n           allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n           allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n           return assignContainer(node, localRequest, NodeType.NODE_LOCAL,\n               reserved, schedulerKey);\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n             .equals(NodeType.OFF_SWITCH))) {\n           return assignContainer(node, rackLocalRequest, NodeType.RACK_LOCAL,\n               reserved, schedulerKey);\n         }\n \n         ResourceRequest offSwitchRequest \u003d getResourceRequest(schedulerKey,\n             ResourceRequest.ANY);\n         if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (offSwitchRequest !\u003d null\n             \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0) {\n           if (!hasNodeOrRackLocalRequests(schedulerKey) || allowedLocality\n               .equals(NodeType.OFF_SWITCH)) {\n             return assignContainer(node, offSwitchRequest, NodeType.OFF_SWITCH,\n                 reserved, schedulerKey);\n           }\n         }\n       }\n     } finally {\n       writeLock.unlock();\n     }\n \n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n        Arrays.asList(node.getReservedContainer().getReservedSchedulerKey()) :\n        getSchedulerKeys();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    try {\n      writeLock.lock();\n      for (SchedulerRequestKey schedulerKey : keysToTry) {\n        // Skip it for reserved container, since\n        // we already check it in isValidReservation.\n        if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(schedulerKey);\n\n        ResourceRequest rackLocalRequest \u003d getResourceRequest(schedulerKey,\n            node.getRackName());\n        ResourceRequest localRequest \u003d getResourceRequest(schedulerKey,\n            node.getNodeName());\n\n        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + localRequest);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n          return assignContainer(node, localRequest, NodeType.NODE_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n            .equals(NodeType.OFF_SWITCH))) {\n          return assignContainer(node, rackLocalRequest, NodeType.RACK_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        ResourceRequest offSwitchRequest \u003d getResourceRequest(schedulerKey,\n            ResourceRequest.ANY);\n        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (offSwitchRequest !\u003d null\n            \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0) {\n          if (!hasNodeOrRackLocalRequests(schedulerKey) || allowedLocality\n              .equals(NodeType.OFF_SWITCH)) {\n            return assignContainer(node, offSwitchRequest, NodeType.OFF_SWITCH,\n                reserved, schedulerKey);\n          }\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "b8a30f2f170ffbd590e7366c3c944ab4919e40df": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3141. Improve locks in SchedulerApplicationAttempt/FSAppAttempt/FiCaSchedulerApp. Contributed by Wangda Tan\n",
      "commitDate": "19/09/16 2:08 AM",
      "commitName": "b8a30f2f170ffbd590e7366c3c944ab4919e40df",
      "commitAuthor": "Jian He",
      "commitDateOld": "05/08/16 10:43 AM",
      "commitNameOld": "3f100d76ff5df020dbb8ecd4f5b4f9736a0a8270",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 44.64,
      "commitsBetweenForRepo": 264,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,81 +1,84 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n     Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n         Arrays.asList(node.getReservedContainer().getReservedSchedulerKey()) :\n         getSchedulerKeys();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n-    synchronized (this) {\n+    try {\n+      writeLock.lock();\n       for (SchedulerRequestKey schedulerKey : keysToTry) {\n         // Skip it for reserved container, since\n         // we already check it in isValidReservation.\n         if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n           continue;\n         }\n \n         addSchedulingOpportunity(schedulerKey);\n \n         ResourceRequest rackLocalRequest \u003d getResourceRequest(schedulerKey,\n             node.getRackName());\n         ResourceRequest localRequest \u003d getResourceRequest(schedulerKey,\n             node.getNodeName());\n \n         if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n               + localRequest);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n           allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n           allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n-          return assignContainer(node, localRequest,\n-              NodeType.NODE_LOCAL, reserved, schedulerKey);\n+          return assignContainer(node, localRequest, NodeType.NODE_LOCAL,\n+              reserved, schedulerKey);\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n-            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n-            allowedLocality.equals(NodeType.OFF_SWITCH))) {\n-          return assignContainer(node, rackLocalRequest,\n-              NodeType.RACK_LOCAL, reserved, schedulerKey);\n+            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n+            .equals(NodeType.OFF_SWITCH))) {\n+          return assignContainer(node, rackLocalRequest, NodeType.RACK_LOCAL,\n+              reserved, schedulerKey);\n         }\n \n-        ResourceRequest offSwitchRequest \u003d\n-            getResourceRequest(schedulerKey, ResourceRequest.ANY);\n+        ResourceRequest offSwitchRequest \u003d getResourceRequest(schedulerKey,\n+            ResourceRequest.ANY);\n         if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n           continue;\n         }\n \n-        if (offSwitchRequest !\u003d null \u0026\u0026\n-            offSwitchRequest.getNumContainers() !\u003d 0) {\n-          if (!hasNodeOrRackLocalRequests(schedulerKey) ||\n-              allowedLocality.equals(NodeType.OFF_SWITCH)) {\n-            return assignContainer(\n-                node, offSwitchRequest, NodeType.OFF_SWITCH, reserved,\n-                schedulerKey);\n+        if (offSwitchRequest !\u003d null\n+            \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0) {\n+          if (!hasNodeOrRackLocalRequests(schedulerKey) || allowedLocality\n+              .equals(NodeType.OFF_SWITCH)) {\n+            return assignContainer(node, offSwitchRequest, NodeType.OFF_SWITCH,\n+                reserved, schedulerKey);\n           }\n         }\n       }\n+    } finally {\n+      writeLock.unlock();\n     }\n+\n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n        Arrays.asList(node.getReservedContainer().getReservedSchedulerKey()) :\n        getSchedulerKeys();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    try {\n      writeLock.lock();\n      for (SchedulerRequestKey schedulerKey : keysToTry) {\n        // Skip it for reserved container, since\n        // we already check it in isValidReservation.\n        if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(schedulerKey);\n\n        ResourceRequest rackLocalRequest \u003d getResourceRequest(schedulerKey,\n            node.getRackName());\n        ResourceRequest localRequest \u003d getResourceRequest(schedulerKey,\n            node.getNodeName());\n\n        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + localRequest);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n          return assignContainer(node, localRequest, NodeType.NODE_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality\n            .equals(NodeType.OFF_SWITCH))) {\n          return assignContainer(node, rackLocalRequest, NodeType.RACK_LOCAL,\n              reserved, schedulerKey);\n        }\n\n        ResourceRequest offSwitchRequest \u003d getResourceRequest(schedulerKey,\n            ResourceRequest.ANY);\n        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (offSwitchRequest !\u003d null\n            \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0) {\n          if (!hasNodeOrRackLocalRequests(schedulerKey) || allowedLocality\n              .equals(NodeType.OFF_SWITCH)) {\n            return assignContainer(node, offSwitchRequest, NodeType.OFF_SWITCH,\n                reserved, schedulerKey);\n          }\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
      "commitDate": "26/07/16 2:54 PM",
      "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
      "commitAuthor": "Arun Suresh",
      "commitDateOld": "11/07/16 10:36 PM",
      "commitNameOld": "819224dcf9c683aa52f58633ac8e13663f1916d8",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 14.68,
      "commitsBetweenForRepo": 98,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,81 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n-    Collection\u003cPriority\u003e prioritiesToTry \u003d (reserved) ?\n-        Arrays.asList(node.getReservedContainer().getReservedPriority()) :\n-        getPriorities();\n+    Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n+        Arrays.asList(node.getReservedContainer().getReservedSchedulerKey()) :\n+        getSchedulerKeys();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n     synchronized (this) {\n-      for (Priority priority : prioritiesToTry) {\n+      for (SchedulerRequestKey schedulerKey : keysToTry) {\n         // Skip it for reserved container, since\n         // we already check it in isValidReservation.\n-        if (!reserved \u0026\u0026 !hasContainerForNode(priority, node)) {\n+        if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n           continue;\n         }\n \n-        addSchedulingOpportunity(priority);\n+        addSchedulingOpportunity(schedulerKey);\n \n-        ResourceRequest rackLocalRequest \u003d getResourceRequest(priority,\n+        ResourceRequest rackLocalRequest \u003d getResourceRequest(schedulerKey,\n             node.getRackName());\n-        ResourceRequest localRequest \u003d getResourceRequest(priority,\n+        ResourceRequest localRequest \u003d getResourceRequest(schedulerKey,\n             node.getNodeName());\n \n         if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n               + localRequest);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n-          allowedLocality \u003d getAllowedLocalityLevelByTime(priority,\n+          allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n-          allowedLocality \u003d getAllowedLocalityLevel(priority,\n+          allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n           return assignContainer(node, localRequest,\n-              NodeType.NODE_LOCAL, reserved);\n+              NodeType.NODE_LOCAL, reserved, schedulerKey);\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n             allowedLocality.equals(NodeType.OFF_SWITCH))) {\n           return assignContainer(node, rackLocalRequest,\n-              NodeType.RACK_LOCAL, reserved);\n+              NodeType.RACK_LOCAL, reserved, schedulerKey);\n         }\n \n         ResourceRequest offSwitchRequest \u003d\n-            getResourceRequest(priority, ResourceRequest.ANY);\n+            getResourceRequest(schedulerKey, ResourceRequest.ANY);\n         if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (offSwitchRequest !\u003d null \u0026\u0026\n             offSwitchRequest.getNumContainers() !\u003d 0) {\n-          if (!hasNodeOrRackLocalRequests(priority) ||\n+          if (!hasNodeOrRackLocalRequests(schedulerKey) ||\n               allowedLocality.equals(NodeType.OFF_SWITCH)) {\n             return assignContainer(\n-                node, offSwitchRequest, NodeType.OFF_SWITCH, reserved);\n+                node, offSwitchRequest, NodeType.OFF_SWITCH, reserved,\n+                schedulerKey);\n           }\n         }\n       }\n     }\n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cSchedulerRequestKey\u003e keysToTry \u003d (reserved) ?\n        Arrays.asList(node.getReservedContainer().getReservedSchedulerKey()) :\n        getSchedulerKeys();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    synchronized (this) {\n      for (SchedulerRequestKey schedulerKey : keysToTry) {\n        // Skip it for reserved container, since\n        // we already check it in isValidReservation.\n        if (!reserved \u0026\u0026 !hasContainerForNode(schedulerKey, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(schedulerKey);\n\n        ResourceRequest rackLocalRequest \u003d getResourceRequest(schedulerKey,\n            node.getRackName());\n        ResourceRequest localRequest \u003d getResourceRequest(schedulerKey,\n            node.getNodeName());\n\n        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + localRequest);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(schedulerKey,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(schedulerKey,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n          return assignContainer(node, localRequest,\n              NodeType.NODE_LOCAL, reserved, schedulerKey);\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n            allowedLocality.equals(NodeType.OFF_SWITCH))) {\n          return assignContainer(node, rackLocalRequest,\n              NodeType.RACK_LOCAL, reserved, schedulerKey);\n        }\n\n        ResourceRequest offSwitchRequest \u003d\n            getResourceRequest(schedulerKey, ResourceRequest.ANY);\n        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (offSwitchRequest !\u003d null \u0026\u0026\n            offSwitchRequest.getNumContainers() !\u003d 0) {\n          if (!hasNodeOrRackLocalRequests(schedulerKey) ||\n              allowedLocality.equals(NodeType.OFF_SWITCH)) {\n            return assignContainer(\n                node, offSwitchRequest, NodeType.OFF_SWITCH, reserved,\n                schedulerKey);\n          }\n        }\n      }\n    }\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "bd69ea408f8fdd8293836ce1089fe9b01616f2f7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3655. FairScheduler: potential livelock due to maxAMShare limitation and container reservation. (Zhihai Xu via kasha)\n",
      "commitDate": "07/06/15 11:37 AM",
      "commitName": "bd69ea408f8fdd8293836ce1089fe9b01616f2f7",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "28/04/15 9:00 PM",
      "commitNameOld": "8f82970e0c247b37b2bf33aa21f6a39afa07efde",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 39.61,
      "commitsBetweenForRepo": 397,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,80 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n-    // Check the AM resource usage for the leaf queue\n-    if (!isAmRunning() \u0026\u0026 !getUnmanagedAM()) {\n-      List\u003cResourceRequest\u003e ask \u003d appSchedulingInfo.getAllResourceRequests();\n-      if (ask.isEmpty() || !getQueue().canRunAppAM(\n-          ask.get(0).getCapability())) {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Skipping allocation because maxAMShare limit would \" +\n-              \"be exceeded\");\n-        }\n-        return Resources.none();\n-      }\n-    }\n-\n     Collection\u003cPriority\u003e prioritiesToTry \u003d (reserved) ?\n         Arrays.asList(node.getReservedContainer().getReservedPriority()) :\n         getPriorities();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n     synchronized (this) {\n       for (Priority priority : prioritiesToTry) {\n-        if (getTotalRequiredResources(priority) \u003c\u003d 0 ||\n-            !hasContainerForNode(priority, node)) {\n+        // Skip it for reserved container, since\n+        // we already check it in isValidReservation.\n+        if (!reserved \u0026\u0026 !hasContainerForNode(priority, node)) {\n           continue;\n         }\n \n         addSchedulingOpportunity(priority);\n \n         ResourceRequest rackLocalRequest \u003d getResourceRequest(priority,\n             node.getRackName());\n         ResourceRequest localRequest \u003d getResourceRequest(priority,\n             node.getNodeName());\n \n         if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n               + localRequest);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n           allowedLocality \u003d getAllowedLocalityLevelByTime(priority,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n           allowedLocality \u003d getAllowedLocalityLevel(priority,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n           return assignContainer(node, localRequest,\n               NodeType.NODE_LOCAL, reserved);\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n             allowedLocality.equals(NodeType.OFF_SWITCH))) {\n           return assignContainer(node, rackLocalRequest,\n               NodeType.RACK_LOCAL, reserved);\n         }\n \n         ResourceRequest offSwitchRequest \u003d\n             getResourceRequest(priority, ResourceRequest.ANY);\n         if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (offSwitchRequest !\u003d null \u0026\u0026\n             offSwitchRequest.getNumContainers() !\u003d 0) {\n           if (!hasNodeOrRackLocalRequests(priority) ||\n               allowedLocality.equals(NodeType.OFF_SWITCH)) {\n             return assignContainer(\n                 node, offSwitchRequest, NodeType.OFF_SWITCH, reserved);\n           }\n         }\n       }\n     }\n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cPriority\u003e prioritiesToTry \u003d (reserved) ?\n        Arrays.asList(node.getReservedContainer().getReservedPriority()) :\n        getPriorities();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    synchronized (this) {\n      for (Priority priority : prioritiesToTry) {\n        // Skip it for reserved container, since\n        // we already check it in isValidReservation.\n        if (!reserved \u0026\u0026 !hasContainerForNode(priority, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(priority);\n\n        ResourceRequest rackLocalRequest \u003d getResourceRequest(priority,\n            node.getRackName());\n        ResourceRequest localRequest \u003d getResourceRequest(priority,\n            node.getNodeName());\n\n        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + localRequest);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(priority,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(priority,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n          return assignContainer(node, localRequest,\n              NodeType.NODE_LOCAL, reserved);\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n            allowedLocality.equals(NodeType.OFF_SWITCH))) {\n          return assignContainer(node, rackLocalRequest,\n              NodeType.RACK_LOCAL, reserved);\n        }\n\n        ResourceRequest offSwitchRequest \u003d\n            getResourceRequest(priority, ResourceRequest.ANY);\n        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (offSwitchRequest !\u003d null \u0026\u0026\n            offSwitchRequest.getNumContainers() !\u003d 0) {\n          if (!hasNodeOrRackLocalRequests(priority) ||\n              allowedLocality.equals(NodeType.OFF_SWITCH)) {\n            return assignContainer(\n                node, offSwitchRequest, NodeType.OFF_SWITCH, reserved);\n          }\n        }\n      }\n    }\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "6a6a59db7f1bfda47c3c14fb49676a7b22d2eb06": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3415. Non-AM containers can be counted towards amResourceUsage of a fairscheduler queue (Zhihai Xu via Sandy Ryza)\n",
      "commitDate": "02/04/15 1:56 PM",
      "commitName": "6a6a59db7f1bfda47c3c14fb49676a7b22d2eb06",
      "commitAuthor": "Sandy Ryza",
      "commitDateOld": "31/03/15 1:42 AM",
      "commitNameOld": "b5a22e983832d4843b5df1d07858988e8bbf37e3",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 2.51,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,92 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n+    // Check the AM resource usage for the leaf queue\n+    if (!isAmRunning() \u0026\u0026 !getUnmanagedAM()) {\n+      List\u003cResourceRequest\u003e ask \u003d appSchedulingInfo.getAllResourceRequests();\n+      if (ask.isEmpty() || !getQueue().canRunAppAM(\n+          ask.get(0).getCapability())) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Skipping allocation because maxAMShare limit would \" +\n+              \"be exceeded\");\n+        }\n+        return Resources.none();\n+      }\n+    }\n+\n     Collection\u003cPriority\u003e prioritiesToTry \u003d (reserved) ?\n         Arrays.asList(node.getReservedContainer().getReservedPriority()) :\n         getPriorities();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n     synchronized (this) {\n       for (Priority priority : prioritiesToTry) {\n         if (getTotalRequiredResources(priority) \u003c\u003d 0 ||\n             !hasContainerForNode(priority, node)) {\n           continue;\n         }\n \n         addSchedulingOpportunity(priority);\n \n-        // Check the AM resource usage for the leaf queue\n-        if (getLiveContainers().size() \u003d\u003d 0 \u0026\u0026 !getUnmanagedAM()) {\n-          if (!getQueue().canRunAppAM(getAMResource())) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Skipping allocation because maxAMShare limit would \" +\n-                  \"be exceeded\");\n-            }\n-            return Resources.none();\n-          }\n-        }\n-\n         ResourceRequest rackLocalRequest \u003d getResourceRequest(priority,\n             node.getRackName());\n         ResourceRequest localRequest \u003d getResourceRequest(priority,\n             node.getNodeName());\n \n         if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n               + localRequest);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n           allowedLocality \u003d getAllowedLocalityLevelByTime(priority,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n           allowedLocality \u003d getAllowedLocalityLevel(priority,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n           return assignContainer(node, localRequest,\n               NodeType.NODE_LOCAL, reserved);\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n             allowedLocality.equals(NodeType.OFF_SWITCH))) {\n           return assignContainer(node, rackLocalRequest,\n               NodeType.RACK_LOCAL, reserved);\n         }\n \n         ResourceRequest offSwitchRequest \u003d\n             getResourceRequest(priority, ResourceRequest.ANY);\n         if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (offSwitchRequest !\u003d null \u0026\u0026\n             offSwitchRequest.getNumContainers() !\u003d 0) {\n           if (!hasNodeOrRackLocalRequests(priority) ||\n               allowedLocality.equals(NodeType.OFF_SWITCH)) {\n             return assignContainer(\n                 node, offSwitchRequest, NodeType.OFF_SWITCH, reserved);\n           }\n         }\n       }\n     }\n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    // Check the AM resource usage for the leaf queue\n    if (!isAmRunning() \u0026\u0026 !getUnmanagedAM()) {\n      List\u003cResourceRequest\u003e ask \u003d appSchedulingInfo.getAllResourceRequests();\n      if (ask.isEmpty() || !getQueue().canRunAppAM(\n          ask.get(0).getCapability())) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Skipping allocation because maxAMShare limit would \" +\n              \"be exceeded\");\n        }\n        return Resources.none();\n      }\n    }\n\n    Collection\u003cPriority\u003e prioritiesToTry \u003d (reserved) ?\n        Arrays.asList(node.getReservedContainer().getReservedPriority()) :\n        getPriorities();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    synchronized (this) {\n      for (Priority priority : prioritiesToTry) {\n        if (getTotalRequiredResources(priority) \u003c\u003d 0 ||\n            !hasContainerForNode(priority, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(priority);\n\n        ResourceRequest rackLocalRequest \u003d getResourceRequest(priority,\n            node.getRackName());\n        ResourceRequest localRequest \u003d getResourceRequest(priority,\n            node.getNodeName());\n\n        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + localRequest);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(priority,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(priority,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n          return assignContainer(node, localRequest,\n              NodeType.NODE_LOCAL, reserved);\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n            allowedLocality.equals(NodeType.OFF_SWITCH))) {\n          return assignContainer(node, rackLocalRequest,\n              NodeType.RACK_LOCAL, reserved);\n        }\n\n        ResourceRequest offSwitchRequest \u003d\n            getResourceRequest(priority, ResourceRequest.ANY);\n        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (offSwitchRequest !\u003d null \u0026\u0026\n            offSwitchRequest.getNumContainers() !\u003d 0) {\n          if (!hasNodeOrRackLocalRequests(priority) ||\n              allowedLocality.equals(NodeType.OFF_SWITCH)) {\n            return assignContainer(\n                node, offSwitchRequest, NodeType.OFF_SWITCH, reserved);\n          }\n        }\n      }\n    }\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "b5a22e983832d4843b5df1d07858988e8bbf37e3": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3258. FairScheduler: Need to add more logging to investigate allocations. Contributed by Anubhav Dhoot.\n",
      "commitDate": "31/03/15 1:42 AM",
      "commitName": "b5a22e983832d4843b5df1d07858988e8bbf37e3",
      "commitAuthor": "Tsuyoshi Ozawa",
      "commitDateOld": "20/03/15 1:54 PM",
      "commitNameOld": "586348e4cbf197188057d6b843a6701cfffdaff3",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 10.49,
      "commitsBetweenForRepo": 89,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,90 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n     Collection\u003cPriority\u003e prioritiesToTry \u003d (reserved) ?\n         Arrays.asList(node.getReservedContainer().getReservedPriority()) :\n         getPriorities();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n     synchronized (this) {\n       for (Priority priority : prioritiesToTry) {\n         if (getTotalRequiredResources(priority) \u003c\u003d 0 ||\n             !hasContainerForNode(priority, node)) {\n           continue;\n         }\n \n         addSchedulingOpportunity(priority);\n \n         // Check the AM resource usage for the leaf queue\n         if (getLiveContainers().size() \u003d\u003d 0 \u0026\u0026 !getUnmanagedAM()) {\n           if (!getQueue().canRunAppAM(getAMResource())) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Skipping allocation because maxAMShare limit would \" +\n+                  \"be exceeded\");\n+            }\n             return Resources.none();\n           }\n         }\n \n         ResourceRequest rackLocalRequest \u003d getResourceRequest(priority,\n             node.getRackName());\n         ResourceRequest localRequest \u003d getResourceRequest(priority,\n             node.getNodeName());\n \n         if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n               + localRequest);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n           allowedLocality \u003d getAllowedLocalityLevelByTime(priority,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n           allowedLocality \u003d getAllowedLocalityLevel(priority,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n           return assignContainer(node, localRequest,\n               NodeType.NODE_LOCAL, reserved);\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n             allowedLocality.equals(NodeType.OFF_SWITCH))) {\n           return assignContainer(node, rackLocalRequest,\n               NodeType.RACK_LOCAL, reserved);\n         }\n \n         ResourceRequest offSwitchRequest \u003d\n             getResourceRequest(priority, ResourceRequest.ANY);\n         if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (offSwitchRequest !\u003d null \u0026\u0026\n             offSwitchRequest.getNumContainers() !\u003d 0) {\n           if (!hasNodeOrRackLocalRequests(priority) ||\n               allowedLocality.equals(NodeType.OFF_SWITCH)) {\n             return assignContainer(\n                 node, offSwitchRequest, NodeType.OFF_SWITCH, reserved);\n           }\n         }\n       }\n     }\n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cPriority\u003e prioritiesToTry \u003d (reserved) ?\n        Arrays.asList(node.getReservedContainer().getReservedPriority()) :\n        getPriorities();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    synchronized (this) {\n      for (Priority priority : prioritiesToTry) {\n        if (getTotalRequiredResources(priority) \u003c\u003d 0 ||\n            !hasContainerForNode(priority, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(priority);\n\n        // Check the AM resource usage for the leaf queue\n        if (getLiveContainers().size() \u003d\u003d 0 \u0026\u0026 !getUnmanagedAM()) {\n          if (!getQueue().canRunAppAM(getAMResource())) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skipping allocation because maxAMShare limit would \" +\n                  \"be exceeded\");\n            }\n            return Resources.none();\n          }\n        }\n\n        ResourceRequest rackLocalRequest \u003d getResourceRequest(priority,\n            node.getRackName());\n        ResourceRequest localRequest \u003d getResourceRequest(priority,\n            node.getNodeName());\n\n        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + localRequest);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(priority,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(priority,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n          return assignContainer(node, localRequest,\n              NodeType.NODE_LOCAL, reserved);\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n            allowedLocality.equals(NodeType.OFF_SWITCH))) {\n          return assignContainer(node, rackLocalRequest,\n              NodeType.RACK_LOCAL, reserved);\n        }\n\n        ResourceRequest offSwitchRequest \u003d\n            getResourceRequest(priority, ResourceRequest.ANY);\n        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (offSwitchRequest !\u003d null \u0026\u0026\n            offSwitchRequest.getNumContainers() !\u003d 0) {\n          if (!hasNodeOrRackLocalRequests(priority) ||\n              allowedLocality.equals(NodeType.OFF_SWITCH)) {\n            return assignContainer(\n                node, offSwitchRequest, NodeType.OFF_SWITCH, reserved);\n          }\n        }\n      }\n    }\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "7e42088abf230dce9c63497d0937fee4f9a1e4a5": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2990. FairScheduler\u0027s delay-scheduling always waits for node-local and rack-local delays, even for off-rack-only requests. (kasha)\n",
      "commitDate": "08/02/15 10:48 PM",
      "commitName": "7e42088abf230dce9c63497d0937fee4f9a1e4a5",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "05/02/15 9:39 AM",
      "commitNameOld": "b6466deac6d5d6344f693144290b46e2bef83a02",
      "commitAuthorOld": "Sandy Ryza",
      "daysBetweenCommits": 3.55,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,86 @@\n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n     Collection\u003cPriority\u003e prioritiesToTry \u003d (reserved) ?\n         Arrays.asList(node.getReservedContainer().getReservedPriority()) :\n         getPriorities();\n \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n     synchronized (this) {\n       for (Priority priority : prioritiesToTry) {\n         if (getTotalRequiredResources(priority) \u003c\u003d 0 ||\n             !hasContainerForNode(priority, node)) {\n           continue;\n         }\n \n         addSchedulingOpportunity(priority);\n \n         // Check the AM resource usage for the leaf queue\n         if (getLiveContainers().size() \u003d\u003d 0 \u0026\u0026 !getUnmanagedAM()) {\n           if (!getQueue().canRunAppAM(getAMResource())) {\n             return Resources.none();\n           }\n         }\n \n         ResourceRequest rackLocalRequest \u003d getResourceRequest(priority,\n             node.getRackName());\n         ResourceRequest localRequest \u003d getResourceRequest(priority,\n             node.getNodeName());\n \n         if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n           LOG.warn(\"Relax locality off is not supported on local request: \"\n               + localRequest);\n         }\n \n         NodeType allowedLocality;\n         if (scheduler.isContinuousSchedulingEnabled()) {\n           allowedLocality \u003d getAllowedLocalityLevelByTime(priority,\n               scheduler.getNodeLocalityDelayMs(),\n               scheduler.getRackLocalityDelayMs(),\n               scheduler.getClock().getTime());\n         } else {\n           allowedLocality \u003d getAllowedLocalityLevel(priority,\n               scheduler.getNumClusterNodes(),\n               scheduler.getNodeLocalityThreshold(),\n               scheduler.getRackLocalityThreshold());\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n           return assignContainer(node, localRequest,\n               NodeType.NODE_LOCAL, reserved);\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n           continue;\n         }\n \n         if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n             \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n             allowedLocality.equals(NodeType.OFF_SWITCH))) {\n           return assignContainer(node, rackLocalRequest,\n               NodeType.RACK_LOCAL, reserved);\n         }\n \n         ResourceRequest offSwitchRequest \u003d\n             getResourceRequest(priority, ResourceRequest.ANY);\n         if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n           continue;\n         }\n \n-        if (offSwitchRequest !\u003d null \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0\n-            \u0026\u0026 allowedLocality.equals(NodeType.OFF_SWITCH)) {\n-          return assignContainer(node, offSwitchRequest,\n-              NodeType.OFF_SWITCH, reserved);\n+        if (offSwitchRequest !\u003d null \u0026\u0026\n+            offSwitchRequest.getNumContainers() !\u003d 0) {\n+          if (!hasNodeOrRackLocalRequests(priority) ||\n+              allowedLocality.equals(NodeType.OFF_SWITCH)) {\n+            return assignContainer(\n+                node, offSwitchRequest, NodeType.OFF_SWITCH, reserved);\n+          }\n         }\n       }\n     }\n     return Resources.none();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cPriority\u003e prioritiesToTry \u003d (reserved) ?\n        Arrays.asList(node.getReservedContainer().getReservedPriority()) :\n        getPriorities();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    synchronized (this) {\n      for (Priority priority : prioritiesToTry) {\n        if (getTotalRequiredResources(priority) \u003c\u003d 0 ||\n            !hasContainerForNode(priority, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(priority);\n\n        // Check the AM resource usage for the leaf queue\n        if (getLiveContainers().size() \u003d\u003d 0 \u0026\u0026 !getUnmanagedAM()) {\n          if (!getQueue().canRunAppAM(getAMResource())) {\n            return Resources.none();\n          }\n        }\n\n        ResourceRequest rackLocalRequest \u003d getResourceRequest(priority,\n            node.getRackName());\n        ResourceRequest localRequest \u003d getResourceRequest(priority,\n            node.getNodeName());\n\n        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + localRequest);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(priority,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(priority,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n          return assignContainer(node, localRequest,\n              NodeType.NODE_LOCAL, reserved);\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n            allowedLocality.equals(NodeType.OFF_SWITCH))) {\n          return assignContainer(node, rackLocalRequest,\n              NodeType.RACK_LOCAL, reserved);\n        }\n\n        ResourceRequest offSwitchRequest \u003d\n            getResourceRequest(priority, ResourceRequest.ANY);\n        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (offSwitchRequest !\u003d null \u0026\u0026\n            offSwitchRequest.getNumContainers() !\u003d 0) {\n          if (!hasNodeOrRackLocalRequests(priority) ||\n              allowedLocality.equals(NodeType.OFF_SWITCH)) {\n            return assignContainer(\n                node, offSwitchRequest, NodeType.OFF_SWITCH, reserved);\n          }\n        }\n      }\n    }\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "extendedDetails": {}
    },
    "486e718fc1f5befd231494e2ec06bb360484f191": {
      "type": "Yintroduced",
      "commitMessage": "YARN-2399. FairScheduler: Merge AppSchedulable and FSSchedulerApp into FSAppAttempt. (kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1617600 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/08/14 2:43 PM",
      "commitName": "486e718fc1f5befd231494e2ec06bb360484f191",
      "commitAuthor": "Karthik Kambatla",
      "diff": "@@ -0,0 +1,83 @@\n+  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n+    }\n+\n+    Collection\u003cPriority\u003e prioritiesToTry \u003d (reserved) ?\n+        Arrays.asList(node.getReservedContainer().getReservedPriority()) :\n+        getPriorities();\n+\n+    // For each priority, see if we can schedule a node local, rack local\n+    // or off-switch request. Rack of off-switch requests may be delayed\n+    // (not scheduled) in order to promote better locality.\n+    synchronized (this) {\n+      for (Priority priority : prioritiesToTry) {\n+        if (getTotalRequiredResources(priority) \u003c\u003d 0 ||\n+            !hasContainerForNode(priority, node)) {\n+          continue;\n+        }\n+\n+        addSchedulingOpportunity(priority);\n+\n+        // Check the AM resource usage for the leaf queue\n+        if (getLiveContainers().size() \u003d\u003d 0 \u0026\u0026 !getUnmanagedAM()) {\n+          if (!getQueue().canRunAppAM(getAMResource())) {\n+            return Resources.none();\n+          }\n+        }\n+\n+        ResourceRequest rackLocalRequest \u003d getResourceRequest(priority,\n+            node.getRackName());\n+        ResourceRequest localRequest \u003d getResourceRequest(priority,\n+            node.getNodeName());\n+\n+        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n+          LOG.warn(\"Relax locality off is not supported on local request: \"\n+              + localRequest);\n+        }\n+\n+        NodeType allowedLocality;\n+        if (scheduler.isContinuousSchedulingEnabled()) {\n+          allowedLocality \u003d getAllowedLocalityLevelByTime(priority,\n+              scheduler.getNodeLocalityDelayMs(),\n+              scheduler.getRackLocalityDelayMs(),\n+              scheduler.getClock().getTime());\n+        } else {\n+          allowedLocality \u003d getAllowedLocalityLevel(priority,\n+              scheduler.getNumClusterNodes(),\n+              scheduler.getNodeLocalityThreshold(),\n+              scheduler.getRackLocalityThreshold());\n+        }\n+\n+        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n+            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n+          return assignContainer(node, localRequest,\n+              NodeType.NODE_LOCAL, reserved);\n+        }\n+\n+        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n+          continue;\n+        }\n+\n+        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n+            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n+            allowedLocality.equals(NodeType.OFF_SWITCH))) {\n+          return assignContainer(node, rackLocalRequest,\n+              NodeType.RACK_LOCAL, reserved);\n+        }\n+\n+        ResourceRequest offSwitchRequest \u003d\n+            getResourceRequest(priority, ResourceRequest.ANY);\n+        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n+          continue;\n+        }\n+\n+        if (offSwitchRequest !\u003d null \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0\n+            \u0026\u0026 allowedLocality.equals(NodeType.OFF_SWITCH)) {\n+          return assignContainer(node, offSwitchRequest,\n+              NodeType.OFF_SWITCH, reserved);\n+        }\n+      }\n+    }\n+    return Resources.none();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n    }\n\n    Collection\u003cPriority\u003e prioritiesToTry \u003d (reserved) ?\n        Arrays.asList(node.getReservedContainer().getReservedPriority()) :\n        getPriorities();\n\n    // For each priority, see if we can schedule a node local, rack local\n    // or off-switch request. Rack of off-switch requests may be delayed\n    // (not scheduled) in order to promote better locality.\n    synchronized (this) {\n      for (Priority priority : prioritiesToTry) {\n        if (getTotalRequiredResources(priority) \u003c\u003d 0 ||\n            !hasContainerForNode(priority, node)) {\n          continue;\n        }\n\n        addSchedulingOpportunity(priority);\n\n        // Check the AM resource usage for the leaf queue\n        if (getLiveContainers().size() \u003d\u003d 0 \u0026\u0026 !getUnmanagedAM()) {\n          if (!getQueue().canRunAppAM(getAMResource())) {\n            return Resources.none();\n          }\n        }\n\n        ResourceRequest rackLocalRequest \u003d getResourceRequest(priority,\n            node.getRackName());\n        ResourceRequest localRequest \u003d getResourceRequest(priority,\n            node.getNodeName());\n\n        if (localRequest !\u003d null \u0026\u0026 !localRequest.getRelaxLocality()) {\n          LOG.warn(\"Relax locality off is not supported on local request: \"\n              + localRequest);\n        }\n\n        NodeType allowedLocality;\n        if (scheduler.isContinuousSchedulingEnabled()) {\n          allowedLocality \u003d getAllowedLocalityLevelByTime(priority,\n              scheduler.getNodeLocalityDelayMs(),\n              scheduler.getRackLocalityDelayMs(),\n              scheduler.getClock().getTime());\n        } else {\n          allowedLocality \u003d getAllowedLocalityLevel(priority,\n              scheduler.getNumClusterNodes(),\n              scheduler.getNodeLocalityThreshold(),\n              scheduler.getRackLocalityThreshold());\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 localRequest !\u003d null \u0026\u0026 localRequest.getNumContainers() !\u003d 0) {\n          return assignContainer(node, localRequest,\n              NodeType.NODE_LOCAL, reserved);\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 !rackLocalRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (rackLocalRequest !\u003d null \u0026\u0026 rackLocalRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 (allowedLocality.equals(NodeType.RACK_LOCAL) ||\n            allowedLocality.equals(NodeType.OFF_SWITCH))) {\n          return assignContainer(node, rackLocalRequest,\n              NodeType.RACK_LOCAL, reserved);\n        }\n\n        ResourceRequest offSwitchRequest \u003d\n            getResourceRequest(priority, ResourceRequest.ANY);\n        if (offSwitchRequest !\u003d null \u0026\u0026 !offSwitchRequest.getRelaxLocality()) {\n          continue;\n        }\n\n        if (offSwitchRequest !\u003d null \u0026\u0026 offSwitchRequest.getNumContainers() !\u003d 0\n            \u0026\u0026 allowedLocality.equals(NodeType.OFF_SWITCH)) {\n          return assignContainer(node, offSwitchRequest,\n              NodeType.OFF_SWITCH, reserved);\n        }\n      }\n    }\n    return Resources.none();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java"
    }
  }
}