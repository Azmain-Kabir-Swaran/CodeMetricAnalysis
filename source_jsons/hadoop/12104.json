{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataNode.java",
  "functionName": "removeVolumes",
  "functionId": "removeVolumes___storageLocations-Collection__StorageLocation__(modifiers-final)__clearFailure-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
  "functionStartLine": 888,
  "functionEndLine": 921,
  "numCommitsSeen": 358,
  "timeTaken": 2149,
  "changeHistory": [
    "dfcb331ba3516264398121c9f23af3a79c0509cc"
  ],
  "changeHistoryShort": {
    "dfcb331ba3516264398121c9f23af3a79c0509cc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "dfcb331ba3516264398121c9f23af3a79c0509cc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13076: [SPS]: Addendum. Resolve conflicts after rebasing branch to trunk. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "dfcb331ba3516264398121c9f23af3a79c0509cc",
      "commitAuthor": "Rakesh Radhakrishnan",
      "diff": "@@ -0,0 +1,34 @@\n+  private synchronized void removeVolumes(\n+      final Collection\u003cStorageLocation\u003e storageLocations, boolean clearFailure)\n+      throws IOException {\n+    if (storageLocations.isEmpty()) {\n+      return;\n+    }\n+\n+    LOG.info(String.format(\"Deactivating volumes (clear failure\u003d%b): %s\",\n+        clearFailure, Joiner.on(\",\").join(storageLocations)));\n+\n+    IOException ioe \u003d null;\n+    // Remove volumes and block infos from FsDataset.\n+    data.removeVolumes(storageLocations, clearFailure);\n+\n+    // Remove volumes from DataStorage.\n+    try {\n+      storage.removeVolumes(storageLocations);\n+    } catch (IOException e) {\n+      ioe \u003d e;\n+    }\n+\n+    // Set configuration and dataDirs to reflect volume changes.\n+    for (Iterator\u003cStorageLocation\u003e it \u003d dataDirs.iterator(); it.hasNext(); ) {\n+      StorageLocation loc \u003d it.next();\n+      if (storageLocations.contains(loc)) {\n+        it.remove();\n+      }\n+    }\n+    getConf().set(DFS_DATANODE_DATA_DIR_KEY, Joiner.on(\",\").join(dataDirs));\n+\n+    if (ioe !\u003d null) {\n+      throw ioe;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void removeVolumes(\n      final Collection\u003cStorageLocation\u003e storageLocations, boolean clearFailure)\n      throws IOException {\n    if (storageLocations.isEmpty()) {\n      return;\n    }\n\n    LOG.info(String.format(\"Deactivating volumes (clear failure\u003d%b): %s\",\n        clearFailure, Joiner.on(\",\").join(storageLocations)));\n\n    IOException ioe \u003d null;\n    // Remove volumes and block infos from FsDataset.\n    data.removeVolumes(storageLocations, clearFailure);\n\n    // Remove volumes from DataStorage.\n    try {\n      storage.removeVolumes(storageLocations);\n    } catch (IOException e) {\n      ioe \u003d e;\n    }\n\n    // Set configuration and dataDirs to reflect volume changes.\n    for (Iterator\u003cStorageLocation\u003e it \u003d dataDirs.iterator(); it.hasNext(); ) {\n      StorageLocation loc \u003d it.next();\n      if (storageLocations.contains(loc)) {\n        it.remove();\n      }\n    }\n    getConf().set(DFS_DATANODE_DATA_DIR_KEY, Joiner.on(\",\").join(dataDirs));\n\n    if (ioe !\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
    }
  }
}