{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSNamesystem.java",
  "functionName": "startFileInt",
  "functionId": "startFileInt___src-String__permissions-PermissionStatus__holder-String__clientMachine-String__flag-EnumSet__CreateFlag____createParent-boolean__replication-short__blockSize-long__supportedVersions-CryptoProtocolVersion[]__ecPolicyName-String__storagePolicy-String__logRetryCache-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
  "functionStartLine": 2617,
  "functionEndLine": 2735,
  "numCommitsSeen": 3431,
  "timeTaken": 42487,
  "changeHistory": [
    "1824aee9da4056de0fb638906b2172e486bbebe7",
    "4b95c242eca540455a4d5d0899aaf73b6064b5ea",
    "abc8fde4caea0e197568ee28392c46f1ce0d42e1",
    "0d7a5ac5f526801367a9ec963e6d72783b637d55",
    "fba9d7cd746cd7b659d2fd9d2bfa23266be9009b",
    "cf4108313da83e28d07676078a33016ec8856ff6",
    "e2289c8d1496a5eff88e6bcb8776a11d45371ffc",
    "84a1321f6aa0af6895564a7c47f8f264656f0294",
    "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405",
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893",
    "f32e9fc8f7150f0e889c0774b3ad712af26fbd65",
    "796a676d18bd7cd3ed4113d002e0e69cf261d6d1",
    "3fa33b5c2c289ceaced30c6c5451f3569110459d",
    "71a81b6257c0000475ad62eb69292a20d45d269c",
    "f62237bc2f02afe11ce185e13aa51a60b5960037",
    "98d340745be682fb251677bb4830aca76119868f",
    "7817674a3a4d097b647dd77f1345787dd376d5ea",
    "3d734df24cba53ec56074b4d28e3bcdce7d2894e",
    "2b6bcfdafa91223a4116e3e9304579f5f91dccac",
    "5caebbae8c2fc9ba2e32384657aee21641a1a6d0",
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
    "c95b878abf313507666ea018f9e6033c4c166e10",
    "368743140dd076ecd5af309c1ed83c5ae2d59fc8",
    "d3d3d47202a18749eeebec22db759c19dd3e232c",
    "d45e7c7e856c7103752888c0395fa94985cd7670",
    "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
    "20a076bafce548298729bab4fb81d12f829e8f7e",
    "185200e7096d15a5c2c2d59b7c7705362820aebf",
    "6104520369045dfaa4b543cbad21236ed322249b"
  ],
  "changeHistoryShort": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": "Ybodychange",
    "4b95c242eca540455a4d5d0899aaf73b6064b5ea": "Ybodychange",
    "abc8fde4caea0e197568ee28392c46f1ce0d42e1": "Ybodychange",
    "0d7a5ac5f526801367a9ec963e6d72783b637d55": "Ymultichange(Yparameterchange,Ybodychange)",
    "fba9d7cd746cd7b659d2fd9d2bfa23266be9009b": "Ybodychange",
    "cf4108313da83e28d07676078a33016ec8856ff6": "Ybodychange",
    "e2289c8d1496a5eff88e6bcb8776a11d45371ffc": "Ybodychange",
    "84a1321f6aa0af6895564a7c47f8f264656f0294": "Ybodychange",
    "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405": "Ybodychange",
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1": "Ymultichange(Yparameterchange,Ybodychange)",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": "Ybodychange",
    "f32e9fc8f7150f0e889c0774b3ad712af26fbd65": "Ymultichange(Ybodychange,Yparametermetachange)",
    "796a676d18bd7cd3ed4113d002e0e69cf261d6d1": "Ybodychange",
    "3fa33b5c2c289ceaced30c6c5451f3569110459d": "Ybodychange",
    "71a81b6257c0000475ad62eb69292a20d45d269c": "Ybodychange",
    "f62237bc2f02afe11ce185e13aa51a60b5960037": "Ybodychange",
    "98d340745be682fb251677bb4830aca76119868f": "Ybodychange",
    "7817674a3a4d097b647dd77f1345787dd376d5ea": "Ybodychange",
    "3d734df24cba53ec56074b4d28e3bcdce7d2894e": "Ybodychange",
    "2b6bcfdafa91223a4116e3e9304579f5f91dccac": "Ymultichange(Yparameterchange,Yexceptionschange,Ybodychange)",
    "5caebbae8c2fc9ba2e32384657aee21641a1a6d0": "Ybodychange",
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2": "Ybodychange",
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1": "Ybodychange",
    "c95b878abf313507666ea018f9e6033c4c166e10": "Ybodychange",
    "368743140dd076ecd5af309c1ed83c5ae2d59fc8": "Ybodychange",
    "d3d3d47202a18749eeebec22db759c19dd3e232c": "Ybodychange",
    "d45e7c7e856c7103752888c0395fa94985cd7670": "Ybodychange",
    "e96ce6f3e3e549202ce3c48d4733ba34098870ad": "Ymultichange(Yparameterchange,Ybodychange)",
    "20a076bafce548298729bab4fb81d12f829e8f7e": "Ybodychange",
    "185200e7096d15a5c2c2d59b7c7705362820aebf": "Ybodychange",
    "6104520369045dfaa4b543cbad21236ed322249b": "Ybodychange"
  },
  "changeHistoryDetails": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15217 Add more information to longest write/read lock held log\n\n",
      "commitDate": "18/04/20 1:52 PM",
      "commitName": "1824aee9da4056de0fb638906b2172e486bbebe7",
      "commitAuthor": "Toshihiro Suzuki",
      "commitDateOld": "25/03/20 10:28 AM",
      "commitNameOld": "a700803a18fb957d2799001a2ce1dcb70f75c080",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 24.14,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,119 +1,119 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       String ecPolicyName, String storagePolicy, boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n     if (shouldReplicate \u0026\u0026\n         (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n       throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n           \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n     }\n \n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n     FSPermissionChecker.setOperationType(null);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       if (shouldReplicate) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       } else {\n         final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n             .getErasureCodingPolicy(this, ecPolicyName, iip);\n         if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n           checkErasureCodingSupported(\"createWithEC\");\n           if (blockSize \u003c ecPolicy.getCellSize()) {\n             throw new IOException(\"Specified block size (\" + blockSize\n                 + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                 +\") of the erasure coding policy (\" + ecPolicy + \").\");\n           }\n         } else {\n           blockManager.verifyReplication(src, replication, clientMachine);\n         }\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n       if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n             clientMachine, flag, createParent, replication, blockSize, feInfo,\n             toRemoveBlocks, shouldReplicate, ecPolicyName, storagePolicy,\n             logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n-      writeUnlock(\"create\");\n+      writeUnlock(\"create\", getLockReportInfoSupplier(src, null, stat));\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, String storagePolicy, boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n    if (shouldReplicate \u0026\u0026\n        (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n      throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n          \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n    }\n\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    FSPermissionChecker.setOperationType(null);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      if (shouldReplicate) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      } else {\n        final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n            .getErasureCodingPolicy(this, ecPolicyName, iip);\n        if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n          checkErasureCodingSupported(\"createWithEC\");\n          if (blockSize \u003c ecPolicy.getCellSize()) {\n            throw new IOException(\"Specified block size (\" + blockSize\n                + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                +\") of the erasure coding policy (\" + ecPolicy + \").\");\n          }\n        } else {\n          blockManager.verifyReplication(src, replication, clientMachine);\n        }\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, shouldReplicate, ecPolicyName, storagePolicy,\n            logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\", getLockReportInfoSupplier(src, null, stat));\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "4b95c242eca540455a4d5d0899aaf73b6064b5ea": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14743. Enhance INodeAttributeProvider/ AccessControlEnforcer Interface in HDFS to support Authorization of mkdir, rm, rmdir, copy, move etc... (#1829)\n\nReviewed-by: Xiaoyu Yao \u003cxyao@apache.org\u003e",
      "commitDate": "13/03/20 11:29 AM",
      "commitName": "4b95c242eca540455a4d5d0899aaf73b6064b5ea",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "27/02/20 8:49 AM",
      "commitNameOld": "cd2c6b1aac470991b9b90339ce2721ba179e7c48",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 15.07,
      "commitsBetweenForRepo": 51,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,118 +1,119 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       String ecPolicyName, String storagePolicy, boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n     if (shouldReplicate \u0026\u0026\n         (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n       throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n           \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n     }\n \n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n+    FSPermissionChecker.setOperationType(null);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       if (shouldReplicate) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       } else {\n         final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n             .getErasureCodingPolicy(this, ecPolicyName, iip);\n         if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n           checkErasureCodingSupported(\"createWithEC\");\n           if (blockSize \u003c ecPolicy.getCellSize()) {\n             throw new IOException(\"Specified block size (\" + blockSize\n                 + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                 +\") of the erasure coding policy (\" + ecPolicy + \").\");\n           }\n         } else {\n           blockManager.verifyReplication(src, replication, clientMachine);\n         }\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n       if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n             clientMachine, flag, createParent, replication, blockSize, feInfo,\n             toRemoveBlocks, shouldReplicate, ecPolicyName, storagePolicy,\n             logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n       writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, String storagePolicy, boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n    if (shouldReplicate \u0026\u0026\n        (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n      throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n          \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n    }\n\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    FSPermissionChecker.setOperationType(null);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      if (shouldReplicate) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      } else {\n        final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n            .getErasureCodingPolicy(this, ecPolicyName, iip);\n        if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n          checkErasureCodingSupported(\"createWithEC\");\n          if (blockSize \u003c ecPolicy.getCellSize()) {\n            throw new IOException(\"Specified block size (\" + blockSize\n                + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                +\") of the erasure coding policy (\" + ecPolicy + \").\");\n          }\n        } else {\n          blockManager.verifyReplication(src, replication, clientMachine);\n        }\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, shouldReplicate, ecPolicyName, storagePolicy,\n            logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "abc8fde4caea0e197568ee28392c46f1ce0d42e1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13596. NN restart fails after RollingUpgrade from 2.x to 3.x. Contributed by Fei Hui.\n",
      "commitDate": "22/08/19 10:44 PM",
      "commitName": "abc8fde4caea0e197568ee28392c46f1ce0d42e1",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "13/08/19 4:50 PM",
      "commitNameOld": "633b7c1cfecde6166899449efae6326ee03cd8c4",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 9.25,
      "commitsBetweenForRepo": 107,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,117 +1,118 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       String ecPolicyName, String storagePolicy, boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n     if (shouldReplicate \u0026\u0026\n         (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n       throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n           \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n     }\n \n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       if (shouldReplicate) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       } else {\n         final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n             .getErasureCodingPolicy(this, ecPolicyName, iip);\n         if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n+          checkErasureCodingSupported(\"createWithEC\");\n           if (blockSize \u003c ecPolicy.getCellSize()) {\n             throw new IOException(\"Specified block size (\" + blockSize\n                 + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                 +\") of the erasure coding policy (\" + ecPolicy + \").\");\n           }\n         } else {\n           blockManager.verifyReplication(src, replication, clientMachine);\n         }\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n       if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n             clientMachine, flag, createParent, replication, blockSize, feInfo,\n             toRemoveBlocks, shouldReplicate, ecPolicyName, storagePolicy,\n             logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n       writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, String storagePolicy, boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n    if (shouldReplicate \u0026\u0026\n        (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n      throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n          \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n    }\n\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      if (shouldReplicate) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      } else {\n        final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n            .getErasureCodingPolicy(this, ecPolicyName, iip);\n        if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n          checkErasureCodingSupported(\"createWithEC\");\n          if (blockSize \u003c ecPolicy.getCellSize()) {\n            throw new IOException(\"Specified block size (\" + blockSize\n                + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                +\") of the erasure coding policy (\" + ecPolicy + \").\");\n          }\n        } else {\n          blockManager.verifyReplication(src, replication, clientMachine);\n        }\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, shouldReplicate, ecPolicyName, storagePolicy,\n            logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "0d7a5ac5f526801367a9ec963e6d72783b637d55": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13209. DistributedFileSystem.create should allow an option to provide StoragePolicy. Contributed by Ayush Saxena.\n",
      "commitDate": "14/02/19 8:43 AM",
      "commitName": "0d7a5ac5f526801367a9ec963e6d72783b637d55",
      "commitAuthor": "Surendra Singh Lilhore",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13209. DistributedFileSystem.create should allow an option to provide StoragePolicy. Contributed by Ayush Saxena.\n",
          "commitDate": "14/02/19 8:43 AM",
          "commitName": "0d7a5ac5f526801367a9ec963e6d72783b637d55",
          "commitAuthor": "Surendra Singh Lilhore",
          "commitDateOld": "25/01/19 7:37 AM",
          "commitNameOld": "45caeee6cfcf1ae3355cd880402159cf31e94a8a",
          "commitAuthorOld": "Dinesh Chitlangia",
          "daysBetweenCommits": 20.05,
          "commitsBetweenForRepo": 127,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,115 +1,117 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n-      String ecPolicyName, boolean logRetryCache) throws IOException {\n+      String ecPolicyName, String storagePolicy, boolean logRetryCache)\n+      throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n     if (shouldReplicate \u0026\u0026\n         (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n       throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n           \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n     }\n \n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       if (shouldReplicate) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       } else {\n         final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n             .getErasureCodingPolicy(this, ecPolicyName, iip);\n         if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n           if (blockSize \u003c ecPolicy.getCellSize()) {\n             throw new IOException(\"Specified block size (\" + blockSize\n                 + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                 +\") of the erasure coding policy (\" + ecPolicy + \").\");\n           }\n         } else {\n           blockManager.verifyReplication(src, replication, clientMachine);\n         }\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n       if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n             clientMachine, flag, createParent, replication, blockSize, feInfo,\n-            toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n+            toRemoveBlocks, shouldReplicate, ecPolicyName, storagePolicy,\n+            logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n       writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, String storagePolicy, boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n    if (shouldReplicate \u0026\u0026\n        (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n      throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n          \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n    }\n\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      if (shouldReplicate) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      } else {\n        final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n            .getErasureCodingPolicy(this, ecPolicyName, iip);\n        if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n          if (blockSize \u003c ecPolicy.getCellSize()) {\n            throw new IOException(\"Specified block size (\" + blockSize\n                + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                +\") of the erasure coding policy (\" + ecPolicy + \").\");\n          }\n        } else {\n          blockManager.verifyReplication(src, replication, clientMachine);\n        }\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, shouldReplicate, ecPolicyName, storagePolicy,\n            logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[src-String, permissions-PermissionStatus, holder-String, clientMachine-String, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], ecPolicyName-String, logRetryCache-boolean]",
            "newValue": "[src-String, permissions-PermissionStatus, holder-String, clientMachine-String, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], ecPolicyName-String, storagePolicy-String, logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13209. DistributedFileSystem.create should allow an option to provide StoragePolicy. Contributed by Ayush Saxena.\n",
          "commitDate": "14/02/19 8:43 AM",
          "commitName": "0d7a5ac5f526801367a9ec963e6d72783b637d55",
          "commitAuthor": "Surendra Singh Lilhore",
          "commitDateOld": "25/01/19 7:37 AM",
          "commitNameOld": "45caeee6cfcf1ae3355cd880402159cf31e94a8a",
          "commitAuthorOld": "Dinesh Chitlangia",
          "daysBetweenCommits": 20.05,
          "commitsBetweenForRepo": 127,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,115 +1,117 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n-      String ecPolicyName, boolean logRetryCache) throws IOException {\n+      String ecPolicyName, String storagePolicy, boolean logRetryCache)\n+      throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n     if (shouldReplicate \u0026\u0026\n         (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n       throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n           \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n     }\n \n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       if (shouldReplicate) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       } else {\n         final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n             .getErasureCodingPolicy(this, ecPolicyName, iip);\n         if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n           if (blockSize \u003c ecPolicy.getCellSize()) {\n             throw new IOException(\"Specified block size (\" + blockSize\n                 + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                 +\") of the erasure coding policy (\" + ecPolicy + \").\");\n           }\n         } else {\n           blockManager.verifyReplication(src, replication, clientMachine);\n         }\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n       if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n             clientMachine, flag, createParent, replication, blockSize, feInfo,\n-            toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n+            toRemoveBlocks, shouldReplicate, ecPolicyName, storagePolicy,\n+            logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n       writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, String storagePolicy, boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n    if (shouldReplicate \u0026\u0026\n        (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n      throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n          \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n    }\n\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      if (shouldReplicate) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      } else {\n        final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n            .getErasureCodingPolicy(this, ecPolicyName, iip);\n        if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n          if (blockSize \u003c ecPolicy.getCellSize()) {\n            throw new IOException(\"Specified block size (\" + blockSize\n                + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                +\") of the erasure coding policy (\" + ecPolicy + \").\");\n          }\n        } else {\n          blockManager.verifyReplication(src, replication, clientMachine);\n        }\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, shouldReplicate, ecPolicyName, storagePolicy,\n            logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "fba9d7cd746cd7b659d2fd9d2bfa23266be9009b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13621. Upgrade commons-lang version to 3.7 in hadoop-hdfs-project. Contributed by Takanobu Asanuma.\n",
      "commitDate": "18/06/18 10:17 AM",
      "commitName": "fba9d7cd746cd7b659d2fd9d2bfa23266be9009b",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "08/06/18 3:14 PM",
      "commitNameOld": "cf4108313da83e28d07676078a33016ec8856ff6",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 9.79,
      "commitsBetweenForRepo": 58,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,115 +1,115 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       String ecPolicyName, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n     if (shouldReplicate \u0026\u0026\n-        (!org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName))) {\n+        (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n       throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n           \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n     }\n \n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       if (shouldReplicate) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       } else {\n         final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n             .getErasureCodingPolicy(this, ecPolicyName, iip);\n         if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n           if (blockSize \u003c ecPolicy.getCellSize()) {\n             throw new IOException(\"Specified block size (\" + blockSize\n                 + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                 +\") of the erasure coding policy (\" + ecPolicy + \").\");\n           }\n         } else {\n           blockManager.verifyReplication(src, replication, clientMachine);\n         }\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n       if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n             clientMachine, flag, createParent, replication, blockSize, feInfo,\n             toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n       writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n    if (shouldReplicate \u0026\u0026\n        (!org.apache.commons.lang3.StringUtils.isEmpty(ecPolicyName))) {\n      throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n          \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n    }\n\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      if (shouldReplicate) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      } else {\n        final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n            .getErasureCodingPolicy(this, ecPolicyName, iip);\n        if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n          if (blockSize \u003c ecPolicy.getCellSize()) {\n            throw new IOException(\"Specified block size (\" + blockSize\n                + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                +\") of the erasure coding policy (\" + ecPolicy + \").\");\n          }\n        } else {\n          blockManager.verifyReplication(src, replication, clientMachine);\n        }\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cf4108313da83e28d07676078a33016ec8856ff6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13642. Creating a file with block size smaller than EC policy\u0027s cell size should fail.\n",
      "commitDate": "08/06/18 3:14 PM",
      "commitName": "cf4108313da83e28d07676078a33016ec8856ff6",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "04/06/18 7:19 AM",
      "commitNameOld": "e2289c8d1496a5eff88e6bcb8776a11d45371ffc",
      "commitAuthorOld": "Rushabh Shah",
      "daysBetweenCommits": 4.33,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,115 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       String ecPolicyName, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n     if (shouldReplicate \u0026\u0026\n         (!org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName))) {\n       throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n           \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n     }\n \n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n-      if (shouldReplicate ||\n-          (org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName) \u0026\u0026\n-          !FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip))) {\n-        blockManager.verifyReplication(src, replication, clientMachine);\n-      }\n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n+      if (shouldReplicate) {\n+        blockManager.verifyReplication(src, replication, clientMachine);\n+      } else {\n+        final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n+            .getErasureCodingPolicy(this, ecPolicyName, iip);\n+        if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n+          if (blockSize \u003c ecPolicy.getCellSize()) {\n+            throw new IOException(\"Specified block size (\" + blockSize\n+                + \") is less than the cell size (\" + ecPolicy.getCellSize()\n+                +\") of the erasure coding policy (\" + ecPolicy + \").\");\n+          }\n+        } else {\n+          blockManager.verifyReplication(src, replication, clientMachine);\n+        }\n+      }\n+\n       FileEncryptionInfo feInfo \u003d null;\n       if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n             clientMachine, flag, createParent, replication, blockSize, feInfo,\n             toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n       writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n    if (shouldReplicate \u0026\u0026\n        (!org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName))) {\n      throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n          \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n    }\n\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      if (shouldReplicate) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      } else {\n        final ErasureCodingPolicy ecPolicy \u003d FSDirErasureCodingOp\n            .getErasureCodingPolicy(this, ecPolicyName, iip);\n        if (ecPolicy !\u003d null \u0026\u0026 (!ecPolicy.isReplicationPolicy())) {\n          if (blockSize \u003c ecPolicy.getCellSize()) {\n            throw new IOException(\"Specified block size (\" + blockSize\n                + \") is less than the cell size (\" + ecPolicy.getCellSize()\n                +\") of the erasure coding policy (\" + ecPolicy + \").\");\n          }\n        } else {\n          blockManager.verifyReplication(src, replication, clientMachine);\n        }\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "e2289c8d1496a5eff88e6bcb8776a11d45371ffc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13281 Namenode#createFile should be /.reserved/raw/ aware.. Contributed by Rushabh S Shah\n",
      "commitDate": "04/06/18 7:19 AM",
      "commitName": "e2289c8d1496a5eff88e6bcb8776a11d45371ffc",
      "commitAuthor": "Rushabh Shah",
      "commitDateOld": "31/05/18 5:37 PM",
      "commitNameOld": "ff013d2c952272f3176dcf624251b05d610503b5",
      "commitAuthorOld": "Chao Sun",
      "daysBetweenCommits": 3.57,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,104 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       String ecPolicyName, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n     if (shouldReplicate \u0026\u0026\n         (!org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName))) {\n       throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n           \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n     }\n \n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n       if (shouldReplicate ||\n           (org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName) \u0026\u0026\n           !FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip))) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       }\n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n-      if (provider !\u003d null) {\n+      if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n             clientMachine, flag, createParent, replication, blockSize, feInfo,\n             toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n       writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n    if (shouldReplicate \u0026\u0026\n        (!org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName))) {\n      throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n          \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n    }\n\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n      if (shouldReplicate ||\n          (org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName) \u0026\u0026\n          !FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip))) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (!iip.isRaw() \u0026\u0026 provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "84a1321f6aa0af6895564a7c47f8f264656f0294": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
      "commitDate": "22/02/18 11:32 AM",
      "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "15/02/18 1:32 PM",
      "commitNameOld": "47473952e56b0380147d42f4110ad03c2276c961",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 6.92,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,104 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       String ecPolicyName, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n     if (shouldReplicate \u0026\u0026\n         (!org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName))) {\n       throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n           \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n     }\n \n-    FSPermissionChecker pc \u003d getPermissionChecker();\n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n+    final FSPermissionChecker pc \u003d getPermissionChecker();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n       if (shouldReplicate ||\n           (org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName) \u0026\u0026\n           !FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip))) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       }\n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n       if (provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n             clientMachine, flag, createParent, replication, blockSize, feInfo,\n             toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n       writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n    if (shouldReplicate \u0026\u0026\n        (!org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName))) {\n      throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n          \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n    }\n\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n      if (shouldReplicate ||\n          (org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName) \u0026\u0026\n          !FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip))) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11643. Add shouldReplicate option to create builder. Contributed by SammiChen.\n",
      "commitDate": "04/05/17 11:39 AM",
      "commitName": "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/04/17 10:18 PM",
      "commitNameOld": "cb672a45a0bbd8950b9b5e304c2e03f516945903",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 6.56,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,95 +1,104 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       String ecPolicyName, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n+    boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n+    if (shouldReplicate \u0026\u0026\n+        (!org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName))) {\n+      throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n+          \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n+    }\n+\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n-      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip)) {\n+      if (shouldReplicate ||\n+          (org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName) \u0026\u0026\n+          !FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip))) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       }\n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n       if (provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n             clientMachine, flag, createParent, replication, blockSize, feInfo,\n-            toRemoveBlocks, ecPolicyName, logRetryCache);\n+            toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n       writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    boolean shouldReplicate \u003d flag.contains(CreateFlag.SHOULD_REPLICATE);\n    if (shouldReplicate \u0026\u0026\n        (!org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName))) {\n      throw new HadoopIllegalArgumentException(\"SHOULD_REPLICATE flag and \" +\n          \"ecPolicyName are exclusive parameters. Set both is not allowed!\");\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n      if (shouldReplicate ||\n          (org.apache.commons.lang.StringUtils.isEmpty(ecPolicyName) \u0026\u0026\n          !FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip))) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10996. Ability to specify per-file EC policy at create time. Contributed by SammiChen.\n",
      "commitDate": "12/04/17 12:27 PM",
      "commitName": "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10996. Ability to specify per-file EC policy at create time. Contributed by SammiChen.\n",
          "commitDate": "12/04/17 12:27 PM",
          "commitName": "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "11/04/17 10:03 PM",
          "commitNameOld": "23b1a7bdf1b546c1e29d7010cf139b6d700461fc",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.6,
          "commitsBetweenForRepo": 8,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,97 +1,95 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n-      boolean logRetryCache)\n-      throws IOException {\n+      String ecPolicyName, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     FSPermissionChecker pc \u003d getPermissionChecker();\n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n       if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip)) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       }\n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n       if (provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n-                                          clientMachine, flag, createParent,\n-                                          replication, blockSize, feInfo,\n-                                          toRemoveBlocks, logRetryCache);\n+            clientMachine, flag, createParent, replication, blockSize, feInfo,\n+            toRemoveBlocks, ecPolicyName, logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n       writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip)) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, ecPolicyName, logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[src-String, permissions-PermissionStatus, holder-String, clientMachine-String, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], logRetryCache-boolean]",
            "newValue": "[src-String, permissions-PermissionStatus, holder-String, clientMachine-String, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], ecPolicyName-String, logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10996. Ability to specify per-file EC policy at create time. Contributed by SammiChen.\n",
          "commitDate": "12/04/17 12:27 PM",
          "commitName": "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "11/04/17 10:03 PM",
          "commitNameOld": "23b1a7bdf1b546c1e29d7010cf139b6d700461fc",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.6,
          "commitsBetweenForRepo": 8,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,97 +1,95 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n-      boolean logRetryCache)\n-      throws IOException {\n+      String ecPolicyName, boolean logRetryCache) throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     FSPermissionChecker pc \u003d getPermissionChecker();\n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n       if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip)) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       }\n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n       if (provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n-                                          clientMachine, flag, createParent,\n-                                          replication, blockSize, feInfo,\n-                                          toRemoveBlocks, logRetryCache);\n+            clientMachine, flag, createParent, replication, blockSize, feInfo,\n+            toRemoveBlocks, ecPolicyName, logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n       writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      String ecPolicyName, boolean logRetryCache) throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip)) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n            clientMachine, flag, createParent, replication, blockSize, feInfo,\n            toRemoveBlocks, ecPolicyName, logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10872. Add MutableRate metrics for FSNamesystemLock operations. Contributed by Erik Krogen.\n",
      "commitDate": "14/11/16 11:05 AM",
      "commitName": "ff0b99eafeda035ebe0dc82cfe689808047a8893",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "08/11/16 6:17 PM",
      "commitNameOld": "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 5.7,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,97 +1,97 @@\n   private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src) ||\n         FSDirectory.isExactReservedName(src) ||\n         (FSDirectory.isReservedName(src)\n             \u0026\u0026 !FSDirectory.isReservedRawName(src)\n             \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n     FSPermissionChecker pc \u003d getPermissionChecker();\n     INodesInPath iip \u003d null;\n     boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n     checkOperation(OperationCategory.WRITE);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n \n       iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n           dir, pc, src, flag, createParent);\n \n       if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip)) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       }\n \n       if (blockSize \u003c minBlockSize) {\n         throw new IOException(\"Specified block size is less than configured\" +\n             \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n             + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n       }\n \n       FileEncryptionInfo feInfo \u003d null;\n       if (provider !\u003d null) {\n         EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n             this, iip, supportedVersions);\n         // if the path has an encryption zone, the lock was released while\n         // generating the EDEK.  re-resolve the path to ensure the namesystem\n         // and/or EZ has not mutated\n         if (ezInfo !\u003d null) {\n           checkOperation(OperationCategory.WRITE);\n           iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n               dir, pc, iip.getPath(), flag, createParent);\n           feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n               dir, iip, ezInfo);\n         }\n       }\n \n       skipSync \u003d false; // following might generate edits\n       toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n                                           clientMachine, flag, createParent,\n                                           replication, blockSize, feInfo,\n                                           toRemoveBlocks, logRetryCache);\n       } catch (IOException e) {\n         skipSync \u003d e instanceof StandbyException;\n         throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n     } finally {\n-      writeUnlock();\n+      writeUnlock(\"create\");\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip)) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, feInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock(\"create\");\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "f32e9fc8f7150f0e889c0774b3ad712af26fbd65": {
      "type": "Ymultichange(Ybodychange,Yparametermetachange)",
      "commitMessage": "HDFS-10939. Reduce performance penalty of encryption zones. Contributed by Daryn sharp.\n",
      "commitDate": "06/10/16 1:11 PM",
      "commitName": "f32e9fc8f7150f0e889c0774b3ad712af26fbd65",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10939. Reduce performance penalty of encryption zones. Contributed by Daryn sharp.\n",
          "commitDate": "06/10/16 1:11 PM",
          "commitName": "f32e9fc8f7150f0e889c0774b3ad712af26fbd65",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "30/09/16 1:16 PM",
          "commitNameOld": "434c5ea75dc3d87513e49290ac9999148ff5163c",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 6.0,
          "commitsBetweenForRepo": 41,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,110 +1,97 @@\n-  private HdfsFileStatus startFileInt(final String src,\n+  private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n-    if (!DFSUtil.isValidName(src)) {\n+    if (!DFSUtil.isValidName(src) ||\n+        FSDirectory.isExactReservedName(src) ||\n+        (FSDirectory.isReservedName(src)\n+            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n+            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n-    checkOperation(OperationCategory.READ);\n-    readLock();\n-    try {\n-      checkOperation(OperationCategory.READ);\n-      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, src)) {\n-        blockManager.verifyReplication(src, replication, clientMachine);\n-      }\n-    } finally {\n-      readUnlock();\n-    }\n-    \n-    checkOperation(OperationCategory.WRITE);\n-    if (blockSize \u003c minBlockSize) {\n-      throw new IOException(\"Specified block size is less than configured\" +\n-          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n-          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n-    }\n-\n     FSPermissionChecker pc \u003d getPermissionChecker();\n-\n-    /**\n-     * If the file is in an encryption zone, we optimistically create an\n-     * EDEK for the file by calling out to the configured KeyProvider.\n-     * Since this typically involves doing an RPC, we take the readLock\n-     * initially, then drop it to do the RPC.\n-     * \n-     * Since the path can flip-flop between being in an encryption zone and not\n-     * in the meantime, we need to recheck the preconditions when we retake the\n-     * lock to do the create. If the preconditions are not met, we throw a\n-     * special RetryStartFileException to ask the DFSClient to try the create\n-     * again later.\n-     */\n-    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n-\n-    if (provider !\u003d null) {\n-      readLock();\n-      try {\n-        checkOperation(OperationCategory.READ);\n-        ezInfo \u003d FSDirWriteFileOp\n-            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n-      } finally {\n-        readUnlock();\n-      }\n-\n-      // Generate EDEK if necessary while not holding the lock\n-      if (ezInfo !\u003d null) {\n-        ezInfo.edek \u003d FSDirEncryptionZoneOp\n-            .generateEncryptedDataEncryptionKey(dir, ezInfo.ezKeyName);\n-      }\n-      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n-    }\n-\n-    boolean skipSync \u003d false;\n+    INodesInPath iip \u003d null;\n+    boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n+    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n-    // Proceed with the create, using the computed cipher suite and\n-    // generated EDEK\n-    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n+    checkOperation(OperationCategory.WRITE);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n+\n+      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n+          dir, pc, src, flag, createParent);\n+\n+      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip)) {\n+        blockManager.verifyReplication(src, replication, clientMachine);\n+      }\n+\n+      if (blockSize \u003c minBlockSize) {\n+        throw new IOException(\"Specified block size is less than configured\" +\n+            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n+            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n+      }\n+\n+      FileEncryptionInfo feInfo \u003d null;\n+      if (provider !\u003d null) {\n+        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n+            this, iip, supportedVersions);\n+        // if the path has an encryption zone, the lock was released while\n+        // generating the EDEK.  re-resolve the path to ensure the namesystem\n+        // and/or EZ has not mutated\n+        if (ezInfo !\u003d null) {\n+          checkOperation(OperationCategory.WRITE);\n+          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n+              dir, pc, iip.getPath(), flag, createParent);\n+          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n+              dir, iip, ezInfo);\n+        }\n+      }\n+\n+      skipSync \u003d false; // following might generate edits\n+      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n-        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n+        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n                                           clientMachine, flag, createParent,\n-                                          replication, blockSize, ezInfo,\n+                                          replication, blockSize, feInfo,\n                                           toRemoveBlocks, logRetryCache);\n+      } catch (IOException e) {\n+        skipSync \u003d e instanceof StandbyException;\n+        throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n-    } catch (IOException e) {\n-      skipSync \u003d e instanceof StandbyException;\n-      throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n-        removeBlocks(toRemoveBlocks);\n-        toRemoveBlocks.clear();\n+        if (toRemoveBlocks !\u003d null) {\n+          removeBlocks(toRemoveBlocks);\n+          toRemoveBlocks.clear();\n+        }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip)) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, feInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HDFS-10939. Reduce performance penalty of encryption zones. Contributed by Daryn sharp.\n",
          "commitDate": "06/10/16 1:11 PM",
          "commitName": "f32e9fc8f7150f0e889c0774b3ad712af26fbd65",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "30/09/16 1:16 PM",
          "commitNameOld": "434c5ea75dc3d87513e49290ac9999148ff5163c",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 6.0,
          "commitsBetweenForRepo": 41,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,110 +1,97 @@\n-  private HdfsFileStatus startFileInt(final String src,\n+  private HdfsFileStatus startFileInt(String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n-    if (!DFSUtil.isValidName(src)) {\n+    if (!DFSUtil.isValidName(src) ||\n+        FSDirectory.isExactReservedName(src) ||\n+        (FSDirectory.isReservedName(src)\n+            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n+            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n       throw new InvalidPathException(src);\n     }\n \n-    checkOperation(OperationCategory.READ);\n-    readLock();\n-    try {\n-      checkOperation(OperationCategory.READ);\n-      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, src)) {\n-        blockManager.verifyReplication(src, replication, clientMachine);\n-      }\n-    } finally {\n-      readUnlock();\n-    }\n-    \n-    checkOperation(OperationCategory.WRITE);\n-    if (blockSize \u003c minBlockSize) {\n-      throw new IOException(\"Specified block size is less than configured\" +\n-          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n-          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n-    }\n-\n     FSPermissionChecker pc \u003d getPermissionChecker();\n-\n-    /**\n-     * If the file is in an encryption zone, we optimistically create an\n-     * EDEK for the file by calling out to the configured KeyProvider.\n-     * Since this typically involves doing an RPC, we take the readLock\n-     * initially, then drop it to do the RPC.\n-     * \n-     * Since the path can flip-flop between being in an encryption zone and not\n-     * in the meantime, we need to recheck the preconditions when we retake the\n-     * lock to do the create. If the preconditions are not met, we throw a\n-     * special RetryStartFileException to ask the DFSClient to try the create\n-     * again later.\n-     */\n-    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n-\n-    if (provider !\u003d null) {\n-      readLock();\n-      try {\n-        checkOperation(OperationCategory.READ);\n-        ezInfo \u003d FSDirWriteFileOp\n-            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n-      } finally {\n-        readUnlock();\n-      }\n-\n-      // Generate EDEK if necessary while not holding the lock\n-      if (ezInfo !\u003d null) {\n-        ezInfo.edek \u003d FSDirEncryptionZoneOp\n-            .generateEncryptedDataEncryptionKey(dir, ezInfo.ezKeyName);\n-      }\n-      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n-    }\n-\n-    boolean skipSync \u003d false;\n+    INodesInPath iip \u003d null;\n+    boolean skipSync \u003d true; // until we do something that might create edits\n     HdfsFileStatus stat \u003d null;\n+    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n \n-    // Proceed with the create, using the computed cipher suite and\n-    // generated EDEK\n-    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n+    checkOperation(OperationCategory.WRITE);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n+\n+      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n+          dir, pc, src, flag, createParent);\n+\n+      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip)) {\n+        blockManager.verifyReplication(src, replication, clientMachine);\n+      }\n+\n+      if (blockSize \u003c minBlockSize) {\n+        throw new IOException(\"Specified block size is less than configured\" +\n+            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n+            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n+      }\n+\n+      FileEncryptionInfo feInfo \u003d null;\n+      if (provider !\u003d null) {\n+        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n+            this, iip, supportedVersions);\n+        // if the path has an encryption zone, the lock was released while\n+        // generating the EDEK.  re-resolve the path to ensure the namesystem\n+        // and/or EZ has not mutated\n+        if (ezInfo !\u003d null) {\n+          checkOperation(OperationCategory.WRITE);\n+          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n+              dir, pc, iip.getPath(), flag, createParent);\n+          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n+              dir, iip, ezInfo);\n+        }\n+      }\n+\n+      skipSync \u003d false; // following might generate edits\n+      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n       dir.writeLock();\n       try {\n-        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n+        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n                                           clientMachine, flag, createParent,\n-                                          replication, blockSize, ezInfo,\n+                                          replication, blockSize, feInfo,\n                                           toRemoveBlocks, logRetryCache);\n+      } catch (IOException e) {\n+        skipSync \u003d e instanceof StandbyException;\n+        throw e;\n       } finally {\n         dir.writeUnlock();\n       }\n-    } catch (IOException e) {\n-      skipSync \u003d e instanceof StandbyException;\n-      throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n-        removeBlocks(toRemoveBlocks);\n-        toRemoveBlocks.clear();\n+        if (toRemoveBlocks !\u003d null) {\n+          removeBlocks(toRemoveBlocks);\n+          toRemoveBlocks.clear();\n+        }\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private HdfsFileStatus startFileInt(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src) ||\n        FSDirectory.isExactReservedName(src) ||\n        (FSDirectory.isReservedName(src)\n            \u0026\u0026 !FSDirectory.isReservedRawName(src)\n            \u0026\u0026 !FSDirectory.isReservedInodesName(src))) {\n      throw new InvalidPathException(src);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    INodesInPath iip \u003d null;\n    boolean skipSync \u003d true; // until we do something that might create edits\n    HdfsFileStatus stat \u003d null;\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n\n      iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n          dir, pc, src, flag, createParent);\n\n      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, iip)) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n\n      if (blockSize \u003c minBlockSize) {\n        throw new IOException(\"Specified block size is less than configured\" +\n            \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n            + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n      }\n\n      FileEncryptionInfo feInfo \u003d null;\n      if (provider !\u003d null) {\n        EncryptionKeyInfo ezInfo \u003d FSDirEncryptionZoneOp.getEncryptionKeyInfo(\n            this, iip, supportedVersions);\n        // if the path has an encryption zone, the lock was released while\n        // generating the EDEK.  re-resolve the path to ensure the namesystem\n        // and/or EZ has not mutated\n        if (ezInfo !\u003d null) {\n          checkOperation(OperationCategory.WRITE);\n          iip \u003d FSDirWriteFileOp.resolvePathForStartFile(\n              dir, pc, iip.getPath(), flag, createParent);\n          feInfo \u003d FSDirEncryptionZoneOp.getFileEncryptionInfo(\n              dir, iip, ezInfo);\n        }\n      }\n\n      skipSync \u003d false; // following might generate edits\n      toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, iip, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, feInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } catch (IOException e) {\n        skipSync \u003d e instanceof StandbyException;\n        throw e;\n      } finally {\n        dir.writeUnlock();\n      }\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    return stat;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[src-String(modifiers-final), permissions-PermissionStatus, holder-String, clientMachine-String, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], logRetryCache-boolean]",
            "newValue": "[src-String, permissions-PermissionStatus, holder-String, clientMachine-String, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], logRetryCache-boolean]"
          }
        }
      ]
    },
    "796a676d18bd7cd3ed4113d002e0e69cf261d6d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9528. Cleanup namenode audit/log/exception messages. (szetszwo via umamahesh)\n",
      "commitDate": "11/12/15 5:57 PM",
      "commitName": "796a676d18bd7cd3ed4113d002e0e69cf261d6d1",
      "commitAuthor": "Uma Mahesh",
      "commitDateOld": "09/12/15 5:55 PM",
      "commitNameOld": "132478e805ba0f955345217b8ad87c2d17cccb2d",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 2.0,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,111 +1,110 @@\n   private HdfsFileStatus startFileInt(final String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n-          .append(\", createFlag\u003d\").append(flag.toString())\n+          .append(\", createFlag\u003d\").append(flag)\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n-          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n-              (supportedVersions));\n+          .append(Arrays.toString(supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n \n     checkOperation(OperationCategory.READ);\n     readLock();\n     try {\n       checkOperation(OperationCategory.READ);\n       if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, src)) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       }\n     } finally {\n       readUnlock();\n     }\n     \n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n \n     FSPermissionChecker pc \u003d getPermissionChecker();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n         checkOperation(OperationCategory.READ);\n         ezInfo \u003d FSDirWriteFileOp\n             .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n       } finally {\n         readUnlock();\n       }\n \n       // Generate EDEK if necessary while not holding the lock\n       if (ezInfo !\u003d null) {\n         ezInfo.edek \u003d FSDirEncryptionZoneOp\n             .generateEncryptedDataEncryptionKey(dir, ezInfo.ezKeyName);\n       }\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n \n     // Proceed with the create, using the computed cipher suite and\n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                           clientMachine, flag, createParent,\n                                           replication, blockSize, ezInfo,\n                                           toRemoveBlocks, logRetryCache);\n       } finally {\n         dir.writeUnlock();\n       }\n     } catch (IOException e) {\n       skipSync \u003d e instanceof StandbyException;\n       throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         removeBlocks(toRemoveBlocks);\n         toRemoveBlocks.clear();\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag)\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(Arrays.toString(supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, src)) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n    } finally {\n      readUnlock();\n    }\n    \n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        checkOperation(OperationCategory.READ);\n        ezInfo \u003d FSDirWriteFileOp\n            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n      } finally {\n        readUnlock();\n      }\n\n      // Generate EDEK if necessary while not holding the lock\n      if (ezInfo !\u003d null) {\n        ezInfo.edek \u003d FSDirEncryptionZoneOp\n            .generateEncryptedDataEncryptionKey(dir, ezInfo.ezKeyName);\n      }\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n\n    // Proceed with the create, using the computed cipher suite and\n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, ezInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } finally {\n        dir.writeUnlock();\n      }\n    } catch (IOException e) {\n      skipSync \u003d e instanceof StandbyException;\n      throw e;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        removeBlocks(toRemoveBlocks);\n        toRemoveBlocks.clear();\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "3fa33b5c2c289ceaced30c6c5451f3569110459d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9430 Remove waitForLoadingFSImage since checkNNStartup has ensured image loaded and namenode started. (Brahma Reddy Battula via mingma)\n",
      "commitDate": "04/12/15 9:47 AM",
      "commitName": "3fa33b5c2c289ceaced30c6c5451f3569110459d",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "01/12/15 4:09 PM",
      "commitNameOld": "a49cc74b4c72195dee1dfb6f9548e5e411dff553",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.74,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,112 +1,111 @@\n   private HdfsFileStatus startFileInt(final String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag.toString())\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n               (supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n \n     checkOperation(OperationCategory.READ);\n     readLock();\n     try {\n       checkOperation(OperationCategory.READ);\n       if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, src)) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       }\n     } finally {\n       readUnlock();\n     }\n     \n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n \n     FSPermissionChecker pc \u003d getPermissionChecker();\n-    waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n         checkOperation(OperationCategory.READ);\n         ezInfo \u003d FSDirWriteFileOp\n             .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n       } finally {\n         readUnlock();\n       }\n \n       // Generate EDEK if necessary while not holding the lock\n       if (ezInfo !\u003d null) {\n         ezInfo.edek \u003d FSDirEncryptionZoneOp\n             .generateEncryptedDataEncryptionKey(dir, ezInfo.ezKeyName);\n       }\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n \n     // Proceed with the create, using the computed cipher suite and\n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                           clientMachine, flag, createParent,\n                                           replication, blockSize, ezInfo,\n                                           toRemoveBlocks, logRetryCache);\n       } finally {\n         dir.writeUnlock();\n       }\n     } catch (IOException e) {\n       skipSync \u003d e instanceof StandbyException;\n       throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         removeBlocks(toRemoveBlocks);\n         toRemoveBlocks.clear();\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag.toString())\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n              (supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, src)) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n    } finally {\n      readUnlock();\n    }\n    \n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        checkOperation(OperationCategory.READ);\n        ezInfo \u003d FSDirWriteFileOp\n            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n      } finally {\n        readUnlock();\n      }\n\n      // Generate EDEK if necessary while not holding the lock\n      if (ezInfo !\u003d null) {\n        ezInfo.edek \u003d FSDirEncryptionZoneOp\n            .generateEncryptedDataEncryptionKey(dir, ezInfo.ezKeyName);\n      }\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n\n    // Proceed with the create, using the computed cipher suite and\n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, ezInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } finally {\n        dir.writeUnlock();\n      }\n    } catch (IOException e) {\n      skipSync \u003d e instanceof StandbyException;\n      throw e;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        removeBlocks(toRemoveBlocks);\n        toRemoveBlocks.clear();\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "71a81b6257c0000475ad62eb69292a20d45d269c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7529. Consolidate encryption zone related implementation into a single class. Contributed by Rakesh R.\n",
      "commitDate": "24/09/15 8:34 AM",
      "commitName": "71a81b6257c0000475ad62eb69292a20d45d269c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "23/09/15 2:58 AM",
      "commitNameOld": "a2c76e5f26301d4b01e56b347442f3dec171591d",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 1.23,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,99 +1,100 @@\n   private HdfsFileStatus startFileInt(final String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag.toString())\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n               (supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n \n     FSPermissionChecker pc \u003d getPermissionChecker();\n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n         checkOperation(OperationCategory.READ);\n         ezInfo \u003d FSDirWriteFileOp\n             .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n       } finally {\n         readUnlock();\n       }\n \n       // Generate EDEK if necessary while not holding the lock\n       if (ezInfo !\u003d null) {\n-        ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n+        ezInfo.edek \u003d FSDirEncryptionZoneOp\n+            .generateEncryptedDataEncryptionKey(dir, ezInfo.ezKeyName);\n       }\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n \n     // Proceed with the create, using the computed cipher suite and\n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                           clientMachine, flag, createParent,\n                                           replication, blockSize, ezInfo,\n                                           toRemoveBlocks, logRetryCache);\n       } finally {\n         dir.writeUnlock();\n       }\n     } catch (IOException e) {\n       skipSync \u003d e instanceof StandbyException;\n       throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         removeBlocks(toRemoveBlocks);\n         toRemoveBlocks.clear();\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag.toString())\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n              (supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        checkOperation(OperationCategory.READ);\n        ezInfo \u003d FSDirWriteFileOp\n            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n      } finally {\n        readUnlock();\n      }\n\n      // Generate EDEK if necessary while not holding the lock\n      if (ezInfo !\u003d null) {\n        ezInfo.edek \u003d FSDirEncryptionZoneOp\n            .generateEncryptedDataEncryptionKey(dir, ezInfo.ezKeyName);\n      }\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n\n    // Proceed with the create, using the computed cipher suite and\n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, ezInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } finally {\n        dir.writeUnlock();\n      }\n    } catch (IOException e) {\n      skipSync \u003d e instanceof StandbyException;\n      throw e;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        removeBlocks(toRemoveBlocks);\n        toRemoveBlocks.clear();\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "f62237bc2f02afe11ce185e13aa51a60b5960037": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8833. Erasure coding: store EC schema and cell size in INodeFile and eliminate notion of EC zones.\n",
      "commitDate": "09/09/15 11:07 PM",
      "commitName": "f62237bc2f02afe11ce185e13aa51a60b5960037",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "01/09/15 2:30 PM",
      "commitNameOld": "ab56fcdb1219d03713b408dd3a95d7405635254d",
      "commitAuthorOld": "",
      "daysBetweenCommits": 8.36,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,111 +1,111 @@\n   private HdfsFileStatus startFileInt(final String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag.toString())\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n               (supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n \n     checkOperation(OperationCategory.READ);\n     readLock();\n     try {\n       checkOperation(OperationCategory.READ);\n-      if (!FSDirErasureCodingOp.isInErasureCodingZone(this, src)) {\n+      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, src)) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       }\n     } finally {\n       readUnlock();\n     }\n     \n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n \n     FSPermissionChecker pc \u003d getPermissionChecker();\n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n         checkOperation(OperationCategory.READ);\n         ezInfo \u003d FSDirWriteFileOp\n             .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n       } finally {\n         readUnlock();\n       }\n \n       // Generate EDEK if necessary while not holding the lock\n       if (ezInfo !\u003d null) {\n         ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n       }\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n \n     // Proceed with the create, using the computed cipher suite and\n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                           clientMachine, flag, createParent,\n                                           replication, blockSize, ezInfo,\n                                           toRemoveBlocks, logRetryCache);\n       } finally {\n         dir.writeUnlock();\n       }\n     } catch (IOException e) {\n       skipSync \u003d e instanceof StandbyException;\n       throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         removeBlocks(toRemoveBlocks);\n         toRemoveBlocks.clear();\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag.toString())\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n              (supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      if (!FSDirErasureCodingOp.hasErasureCodingPolicy(this, src)) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n    } finally {\n      readUnlock();\n    }\n    \n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        checkOperation(OperationCategory.READ);\n        ezInfo \u003d FSDirWriteFileOp\n            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n      } finally {\n        readUnlock();\n      }\n\n      // Generate EDEK if necessary while not holding the lock\n      if (ezInfo !\u003d null) {\n        ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n      }\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n\n    // Proceed with the create, using the computed cipher suite and\n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, ezInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } finally {\n        dir.writeUnlock();\n      }\n    } catch (IOException e) {\n      skipSync \u003d e instanceof StandbyException;\n      throw e;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        removeBlocks(toRemoveBlocks);\n        toRemoveBlocks.clear();\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "98d340745be682fb251677bb4830aca76119868f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8450. Erasure Coding: Consolidate erasure coding zone related implementation into a single class (Contributed by Rakesh R)\n",
      "commitDate": "10/06/15 10:18 PM",
      "commitName": "98d340745be682fb251677bb4830aca76119868f",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "26/05/15 12:07 PM",
      "commitNameOld": "9a18598e2da8e699ed852ffa30fd7f503902190c",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 15.42,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,111 +1,111 @@\n   private HdfsFileStatus startFileInt(final String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag.toString())\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n               (supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n \n     checkOperation(OperationCategory.READ);\n     readLock();\n     try {\n       checkOperation(OperationCategory.READ);\n-      if (!isInECZone(src)) {\n+      if (!FSDirErasureCodingOp.isInErasureCodingZone(this, src)) {\n         blockManager.verifyReplication(src, replication, clientMachine);\n       }\n     } finally {\n       readUnlock();\n     }\n     \n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n \n     FSPermissionChecker pc \u003d getPermissionChecker();\n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n         checkOperation(OperationCategory.READ);\n         ezInfo \u003d FSDirWriteFileOp\n             .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n       } finally {\n         readUnlock();\n       }\n \n       // Generate EDEK if necessary while not holding the lock\n       if (ezInfo !\u003d null) {\n         ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n       }\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n \n     // Proceed with the create, using the computed cipher suite and\n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                           clientMachine, flag, createParent,\n                                           replication, blockSize, ezInfo,\n                                           toRemoveBlocks, logRetryCache);\n       } finally {\n         dir.writeUnlock();\n       }\n     } catch (IOException e) {\n       skipSync \u003d e instanceof StandbyException;\n       throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         removeBlocks(toRemoveBlocks);\n         toRemoveBlocks.clear();\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag.toString())\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n              (supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      if (!FSDirErasureCodingOp.isInErasureCodingZone(this, src)) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n    } finally {\n      readUnlock();\n    }\n    \n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        checkOperation(OperationCategory.READ);\n        ezInfo \u003d FSDirWriteFileOp\n            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n      } finally {\n        readUnlock();\n      }\n\n      // Generate EDEK if necessary while not holding the lock\n      if (ezInfo !\u003d null) {\n        ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n      }\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n\n    // Proceed with the create, using the computed cipher suite and\n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, ezInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } finally {\n        dir.writeUnlock();\n      }\n    } catch (IOException e) {\n      skipSync \u003d e instanceof StandbyException;\n      throw e;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        removeBlocks(toRemoveBlocks);\n        toRemoveBlocks.clear();\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "7817674a3a4d097b647dd77f1345787dd376d5ea": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7609. Avoid retry cache collision when Standby NameNode loading edits. Contributed by Ming Ma.\n",
      "commitDate": "29/05/15 11:05 AM",
      "commitName": "7817674a3a4d097b647dd77f1345787dd376d5ea",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/05/15 3:42 PM",
      "commitNameOld": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.81,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,100 +1,99 @@\n   private HdfsFileStatus startFileInt(final String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag.toString())\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n               (supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n-    checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n \n     FSPermissionChecker pc \u003d getPermissionChecker();\n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n         checkOperation(OperationCategory.READ);\n         ezInfo \u003d FSDirWriteFileOp\n             .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n       } finally {\n         readUnlock();\n       }\n \n       // Generate EDEK if necessary while not holding the lock\n       if (ezInfo !\u003d null) {\n         ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n       }\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n \n     // Proceed with the create, using the computed cipher suite and\n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                           clientMachine, flag, createParent,\n                                           replication, blockSize, ezInfo,\n                                           toRemoveBlocks, logRetryCache);\n       } finally {\n         dir.writeUnlock();\n       }\n     } catch (IOException e) {\n       skipSync \u003d e instanceof StandbyException;\n       throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         removeBlocks(toRemoveBlocks);\n         toRemoveBlocks.clear();\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag.toString())\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n              (supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        checkOperation(OperationCategory.READ);\n        ezInfo \u003d FSDirWriteFileOp\n            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n      } finally {\n        readUnlock();\n      }\n\n      // Generate EDEK if necessary while not holding the lock\n      if (ezInfo !\u003d null) {\n        ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n      }\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n\n    // Proceed with the create, using the computed cipher suite and\n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, ezInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } finally {\n        dir.writeUnlock();\n      }\n    } catch (IOException e) {\n      skipSync \u003d e instanceof StandbyException;\n      throw e;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        removeBlocks(toRemoveBlocks);\n        toRemoveBlocks.clear();\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "3d734df24cba53ec56074b4d28e3bcdce7d2894e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8441. Erasure Coding: make condition check earlier for setReplication. (waltersu4549)\n",
      "commitDate": "26/05/15 12:07 PM",
      "commitName": "3d734df24cba53ec56074b4d28e3bcdce7d2894e",
      "commitAuthor": "Walter Su",
      "commitDateOld": "26/05/15 12:02 PM",
      "commitNameOld": "e53fa769c97416af69ea567aecd44f67e896688b",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,100 +1,111 @@\n   private HdfsFileStatus startFileInt(final String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n           .append(\", holder\u003d\").append(holder)\n           .append(\", clientMachine\u003d\").append(clientMachine)\n           .append(\", createParent\u003d\").append(createParent)\n           .append(\", replication\u003d\").append(replication)\n           .append(\", createFlag\u003d\").append(flag.toString())\n           .append(\", blockSize\u003d\").append(blockSize)\n           .append(\", supportedVersions\u003d\")\n           .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n               (supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n-    blockManager.verifyReplication(src, replication, clientMachine);\n+\n+    checkOperation(OperationCategory.READ);\n+    readLock();\n+    try {\n+      checkOperation(OperationCategory.READ);\n+      if (!isInECZone(src)) {\n+        blockManager.verifyReplication(src, replication, clientMachine);\n+      }\n+    } finally {\n+      readUnlock();\n+    }\n+    \n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n \n     FSPermissionChecker pc \u003d getPermissionChecker();\n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n         checkOperation(OperationCategory.READ);\n         ezInfo \u003d FSDirWriteFileOp\n             .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n       } finally {\n         readUnlock();\n       }\n \n       // Generate EDEK if necessary while not holding the lock\n       if (ezInfo !\u003d null) {\n         ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n       }\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n \n     // Proceed with the create, using the computed cipher suite and\n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       dir.writeLock();\n       try {\n         stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                           clientMachine, flag, createParent,\n                                           replication, blockSize, ezInfo,\n                                           toRemoveBlocks, logRetryCache);\n       } finally {\n         dir.writeUnlock();\n       }\n     } catch (IOException e) {\n       skipSync \u003d e instanceof StandbyException;\n       throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         removeBlocks(toRemoveBlocks);\n         toRemoveBlocks.clear();\n       }\n     }\n \n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag.toString())\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n              (supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      if (!isInECZone(src)) {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      }\n    } finally {\n      readUnlock();\n    }\n    \n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        checkOperation(OperationCategory.READ);\n        ezInfo \u003d FSDirWriteFileOp\n            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n      } finally {\n        readUnlock();\n      }\n\n      // Generate EDEK if necessary while not holding the lock\n      if (ezInfo !\u003d null) {\n        ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n      }\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n\n    // Proceed with the create, using the computed cipher suite and\n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, ezInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } finally {\n        dir.writeUnlock();\n      }\n    } catch (IOException e) {\n      skipSync \u003d e instanceof StandbyException;\n      throw e;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        removeBlocks(toRemoveBlocks);\n        toRemoveBlocks.clear();\n      }\n    }\n\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "2b6bcfdafa91223a4116e3e9304579f5f91dccac": {
      "type": "Ymultichange(Yparameterchange,Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-8421. Move startFile() and related functions into FSDirWriteFileOp. Contributed by Haohui Mai.\n",
      "commitDate": "21/05/15 8:08 AM",
      "commitName": "2b6bcfdafa91223a4116e3e9304579f5f91dccac",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8421. Move startFile() and related functions into FSDirWriteFileOp. Contributed by Haohui Mai.\n",
          "commitDate": "21/05/15 8:08 AM",
          "commitName": "2b6bcfdafa91223a4116e3e9304579f5f91dccac",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "18/05/15 12:37 PM",
          "commitNameOld": "cdfae446ad285db979a79bf55665363fd943702c",
          "commitAuthorOld": "Ravi Prakash",
          "daysBetweenCommits": 2.81,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,137 +1,100 @@\n-  private HdfsFileStatus startFileInt(final String srcArg,\n+  private HdfsFileStatus startFileInt(final String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n-      throws AccessControlException, SafeModeException,\n-      FileAlreadyExistsException, UnresolvedLinkException,\n-      FileNotFoundException, ParentNotDirectoryException, IOException {\n-    String src \u003d srcArg;\n+      throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n-      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n-              + \", holder\u003d\" + holder\n-              + \", clientMachine\u003d\" + clientMachine\n-              + \", createParent\u003d\" + createParent\n-              + \", replication\u003d\" + replication\n-              + \", createFlag\u003d\" + flag.toString()\n-              + \", blockSize\u003d\" + blockSize);\n-      builder.append(\", supportedVersions\u003d\");\n-      if (supportedVersions !\u003d null) {\n-        builder.append(Arrays.toString(supportedVersions));\n-      } else {\n-        builder.append(\"null\");\n-      }\n+      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n+          .append(\", holder\u003d\").append(holder)\n+          .append(\", clientMachine\u003d\").append(clientMachine)\n+          .append(\", createParent\u003d\").append(createParent)\n+          .append(\", replication\u003d\").append(replication)\n+          .append(\", createFlag\u003d\").append(flag.toString())\n+          .append(\", blockSize\u003d\").append(blockSize)\n+          .append(\", supportedVersions\u003d\")\n+          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n+              (supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n-\n-    boolean skipSync \u003d false;\n-    HdfsFileStatus stat \u003d null;\n-    FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    boolean create \u003d flag.contains(CreateFlag.CREATE);\n-    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n-    boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n \n+    FSPermissionChecker pc \u003d getPermissionChecker();\n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n-    CryptoProtocolVersion protocolVersion \u003d null;\n-    CipherSuite suite \u003d null;\n-    String ezKeyName \u003d null;\n-    EncryptedKeyVersion edek \u003d null;\n+    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n-        src \u003d dir.resolvePath(pc, src, pathComponents);\n-        INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-        // Nothing to do if the path is not within an EZ\n-        final EncryptionZone zone \u003d dir.getEZForPath(iip);\n-        if (zone !\u003d null) {\n-          protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n-          suite \u003d zone.getSuite();\n-          ezKeyName \u003d zone.getKeyName();\n-\n-          Preconditions.checkNotNull(protocolVersion);\n-          Preconditions.checkNotNull(suite);\n-          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n-              \"Chose an UNKNOWN CipherSuite!\");\n-          Preconditions.checkNotNull(ezKeyName);\n-        }\n+        checkOperation(OperationCategory.READ);\n+        ezInfo \u003d FSDirWriteFileOp\n+            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n       } finally {\n         readUnlock();\n       }\n \n-      Preconditions.checkState(\n-          (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n-              (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n-          \"Both suite and ezKeyName should both be null or not null\");\n-\n       // Generate EDEK if necessary while not holding the lock\n-      edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n+      if (ezInfo !\u003d null) {\n+        ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n+      }\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n-    // Proceed with the create, using the computed cipher suite and \n+    boolean skipSync \u003d false;\n+    HdfsFileStatus stat \u003d null;\n+\n+    // Proceed with the create, using the computed cipher suite and\n     // generated EDEK\n-    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n+    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       dir.writeLock();\n       try {\n-        src \u003d dir.resolvePath(pc, src, pathComponents);\n-        final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-        toRemoveBlocks \u003d startFileInternal(\n-            pc, iip, permissions, holder,\n-            clientMachine, create, overwrite,\n-            createParent, replication, blockSize,\n-            isLazyPersist, suite, protocolVersion, edek,\n-            logRetryCache);\n-        stat \u003d FSDirStatAndListingOp.getFileInfo(\n-            dir, src, false, FSDirectory.isReservedRawName(srcArg), true);\n+        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n+                                          clientMachine, flag, createParent,\n+                                          replication, blockSize, ezInfo,\n+                                          toRemoveBlocks, logRetryCache);\n       } finally {\n         dir.writeUnlock();\n       }\n-    } catch (StandbyException se) {\n-      skipSync \u003d true;\n-      throw se;\n+    } catch (IOException e) {\n+      skipSync \u003d e instanceof StandbyException;\n+      throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n-        if (toRemoveBlocks !\u003d null) {\n-          removeBlocks(toRemoveBlocks);\n-          toRemoveBlocks.clear();\n-        }\n+        removeBlocks(toRemoveBlocks);\n+        toRemoveBlocks.clear();\n       }\n     }\n \n-    logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private HdfsFileStatus startFileInt(final String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag.toString())\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n              (supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        checkOperation(OperationCategory.READ);\n        ezInfo \u003d FSDirWriteFileOp\n            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n      } finally {\n        readUnlock();\n      }\n\n      // Generate EDEK if necessary while not holding the lock\n      if (ezInfo !\u003d null) {\n        ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n      }\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n\n    // Proceed with the create, using the computed cipher suite and\n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, ezInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } finally {\n        dir.writeUnlock();\n      }\n    } catch (IOException e) {\n      skipSync \u003d e instanceof StandbyException;\n      throw e;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        removeBlocks(toRemoveBlocks);\n        toRemoveBlocks.clear();\n      }\n    }\n\n    return stat;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[srcArg-String(modifiers-final), permissions-PermissionStatus, holder-String, clientMachine-String, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], logRetryCache-boolean]",
            "newValue": "[src-String(modifiers-final), permissions-PermissionStatus, holder-String, clientMachine-String, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], logRetryCache-boolean]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-8421. Move startFile() and related functions into FSDirWriteFileOp. Contributed by Haohui Mai.\n",
          "commitDate": "21/05/15 8:08 AM",
          "commitName": "2b6bcfdafa91223a4116e3e9304579f5f91dccac",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "18/05/15 12:37 PM",
          "commitNameOld": "cdfae446ad285db979a79bf55665363fd943702c",
          "commitAuthorOld": "Ravi Prakash",
          "daysBetweenCommits": 2.81,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,137 +1,100 @@\n-  private HdfsFileStatus startFileInt(final String srcArg,\n+  private HdfsFileStatus startFileInt(final String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n-      throws AccessControlException, SafeModeException,\n-      FileAlreadyExistsException, UnresolvedLinkException,\n-      FileNotFoundException, ParentNotDirectoryException, IOException {\n-    String src \u003d srcArg;\n+      throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n-      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n-              + \", holder\u003d\" + holder\n-              + \", clientMachine\u003d\" + clientMachine\n-              + \", createParent\u003d\" + createParent\n-              + \", replication\u003d\" + replication\n-              + \", createFlag\u003d\" + flag.toString()\n-              + \", blockSize\u003d\" + blockSize);\n-      builder.append(\", supportedVersions\u003d\");\n-      if (supportedVersions !\u003d null) {\n-        builder.append(Arrays.toString(supportedVersions));\n-      } else {\n-        builder.append(\"null\");\n-      }\n+      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n+          .append(\", holder\u003d\").append(holder)\n+          .append(\", clientMachine\u003d\").append(clientMachine)\n+          .append(\", createParent\u003d\").append(createParent)\n+          .append(\", replication\u003d\").append(replication)\n+          .append(\", createFlag\u003d\").append(flag.toString())\n+          .append(\", blockSize\u003d\").append(blockSize)\n+          .append(\", supportedVersions\u003d\")\n+          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n+              (supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n-\n-    boolean skipSync \u003d false;\n-    HdfsFileStatus stat \u003d null;\n-    FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    boolean create \u003d flag.contains(CreateFlag.CREATE);\n-    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n-    boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n \n+    FSPermissionChecker pc \u003d getPermissionChecker();\n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n-    CryptoProtocolVersion protocolVersion \u003d null;\n-    CipherSuite suite \u003d null;\n-    String ezKeyName \u003d null;\n-    EncryptedKeyVersion edek \u003d null;\n+    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n-        src \u003d dir.resolvePath(pc, src, pathComponents);\n-        INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-        // Nothing to do if the path is not within an EZ\n-        final EncryptionZone zone \u003d dir.getEZForPath(iip);\n-        if (zone !\u003d null) {\n-          protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n-          suite \u003d zone.getSuite();\n-          ezKeyName \u003d zone.getKeyName();\n-\n-          Preconditions.checkNotNull(protocolVersion);\n-          Preconditions.checkNotNull(suite);\n-          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n-              \"Chose an UNKNOWN CipherSuite!\");\n-          Preconditions.checkNotNull(ezKeyName);\n-        }\n+        checkOperation(OperationCategory.READ);\n+        ezInfo \u003d FSDirWriteFileOp\n+            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n       } finally {\n         readUnlock();\n       }\n \n-      Preconditions.checkState(\n-          (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n-              (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n-          \"Both suite and ezKeyName should both be null or not null\");\n-\n       // Generate EDEK if necessary while not holding the lock\n-      edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n+      if (ezInfo !\u003d null) {\n+        ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n+      }\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n-    // Proceed with the create, using the computed cipher suite and \n+    boolean skipSync \u003d false;\n+    HdfsFileStatus stat \u003d null;\n+\n+    // Proceed with the create, using the computed cipher suite and\n     // generated EDEK\n-    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n+    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       dir.writeLock();\n       try {\n-        src \u003d dir.resolvePath(pc, src, pathComponents);\n-        final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-        toRemoveBlocks \u003d startFileInternal(\n-            pc, iip, permissions, holder,\n-            clientMachine, create, overwrite,\n-            createParent, replication, blockSize,\n-            isLazyPersist, suite, protocolVersion, edek,\n-            logRetryCache);\n-        stat \u003d FSDirStatAndListingOp.getFileInfo(\n-            dir, src, false, FSDirectory.isReservedRawName(srcArg), true);\n+        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n+                                          clientMachine, flag, createParent,\n+                                          replication, blockSize, ezInfo,\n+                                          toRemoveBlocks, logRetryCache);\n       } finally {\n         dir.writeUnlock();\n       }\n-    } catch (StandbyException se) {\n-      skipSync \u003d true;\n-      throw se;\n+    } catch (IOException e) {\n+      skipSync \u003d e instanceof StandbyException;\n+      throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n-        if (toRemoveBlocks !\u003d null) {\n-          removeBlocks(toRemoveBlocks);\n-          toRemoveBlocks.clear();\n-        }\n+        removeBlocks(toRemoveBlocks);\n+        toRemoveBlocks.clear();\n       }\n     }\n \n-    logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private HdfsFileStatus startFileInt(final String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag.toString())\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n              (supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        checkOperation(OperationCategory.READ);\n        ezInfo \u003d FSDirWriteFileOp\n            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n      } finally {\n        readUnlock();\n      }\n\n      // Generate EDEK if necessary while not holding the lock\n      if (ezInfo !\u003d null) {\n        ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n      }\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n\n    // Proceed with the create, using the computed cipher suite and\n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, ezInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } finally {\n        dir.writeUnlock();\n      }\n    } catch (IOException e) {\n      skipSync \u003d e instanceof StandbyException;\n      throw e;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        removeBlocks(toRemoveBlocks);\n        toRemoveBlocks.clear();\n      }\n    }\n\n    return stat;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[AccessControlException, SafeModeException, FileAlreadyExistsException, UnresolvedLinkException, FileNotFoundException, ParentNotDirectoryException, IOException]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8421. Move startFile() and related functions into FSDirWriteFileOp. Contributed by Haohui Mai.\n",
          "commitDate": "21/05/15 8:08 AM",
          "commitName": "2b6bcfdafa91223a4116e3e9304579f5f91dccac",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "18/05/15 12:37 PM",
          "commitNameOld": "cdfae446ad285db979a79bf55665363fd943702c",
          "commitAuthorOld": "Ravi Prakash",
          "daysBetweenCommits": 2.81,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,137 +1,100 @@\n-  private HdfsFileStatus startFileInt(final String srcArg,\n+  private HdfsFileStatus startFileInt(final String src,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n-      throws AccessControlException, SafeModeException,\n-      FileAlreadyExistsException, UnresolvedLinkException,\n-      FileNotFoundException, ParentNotDirectoryException, IOException {\n-    String src \u003d srcArg;\n+      throws IOException {\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n-      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n-              + \", holder\u003d\" + holder\n-              + \", clientMachine\u003d\" + clientMachine\n-              + \", createParent\u003d\" + createParent\n-              + \", replication\u003d\" + replication\n-              + \", createFlag\u003d\" + flag.toString()\n-              + \", blockSize\u003d\" + blockSize);\n-      builder.append(\", supportedVersions\u003d\");\n-      if (supportedVersions !\u003d null) {\n-        builder.append(Arrays.toString(supportedVersions));\n-      } else {\n-        builder.append(\"null\");\n-      }\n+      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n+          .append(\", holder\u003d\").append(holder)\n+          .append(\", clientMachine\u003d\").append(clientMachine)\n+          .append(\", createParent\u003d\").append(createParent)\n+          .append(\", replication\u003d\").append(replication)\n+          .append(\", createFlag\u003d\").append(flag.toString())\n+          .append(\", blockSize\u003d\").append(blockSize)\n+          .append(\", supportedVersions\u003d\")\n+          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n+              (supportedVersions));\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n-\n-    boolean skipSync \u003d false;\n-    HdfsFileStatus stat \u003d null;\n-    FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    boolean create \u003d flag.contains(CreateFlag.CREATE);\n-    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n-    boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n \n+    FSPermissionChecker pc \u003d getPermissionChecker();\n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n-    CryptoProtocolVersion protocolVersion \u003d null;\n-    CipherSuite suite \u003d null;\n-    String ezKeyName \u003d null;\n-    EncryptedKeyVersion edek \u003d null;\n+    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n-        src \u003d dir.resolvePath(pc, src, pathComponents);\n-        INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-        // Nothing to do if the path is not within an EZ\n-        final EncryptionZone zone \u003d dir.getEZForPath(iip);\n-        if (zone !\u003d null) {\n-          protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n-          suite \u003d zone.getSuite();\n-          ezKeyName \u003d zone.getKeyName();\n-\n-          Preconditions.checkNotNull(protocolVersion);\n-          Preconditions.checkNotNull(suite);\n-          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n-              \"Chose an UNKNOWN CipherSuite!\");\n-          Preconditions.checkNotNull(ezKeyName);\n-        }\n+        checkOperation(OperationCategory.READ);\n+        ezInfo \u003d FSDirWriteFileOp\n+            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n       } finally {\n         readUnlock();\n       }\n \n-      Preconditions.checkState(\n-          (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n-              (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n-          \"Both suite and ezKeyName should both be null or not null\");\n-\n       // Generate EDEK if necessary while not holding the lock\n-      edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n+      if (ezInfo !\u003d null) {\n+        ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n+      }\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n-    // Proceed with the create, using the computed cipher suite and \n+    boolean skipSync \u003d false;\n+    HdfsFileStatus stat \u003d null;\n+\n+    // Proceed with the create, using the computed cipher suite and\n     // generated EDEK\n-    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n+    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       dir.writeLock();\n       try {\n-        src \u003d dir.resolvePath(pc, src, pathComponents);\n-        final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-        toRemoveBlocks \u003d startFileInternal(\n-            pc, iip, permissions, holder,\n-            clientMachine, create, overwrite,\n-            createParent, replication, blockSize,\n-            isLazyPersist, suite, protocolVersion, edek,\n-            logRetryCache);\n-        stat \u003d FSDirStatAndListingOp.getFileInfo(\n-            dir, src, false, FSDirectory.isReservedRawName(srcArg), true);\n+        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n+                                          clientMachine, flag, createParent,\n+                                          replication, blockSize, ezInfo,\n+                                          toRemoveBlocks, logRetryCache);\n       } finally {\n         dir.writeUnlock();\n       }\n-    } catch (StandbyException se) {\n-      skipSync \u003d true;\n-      throw se;\n+    } catch (IOException e) {\n+      skipSync \u003d e instanceof StandbyException;\n+      throw e;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n-        if (toRemoveBlocks !\u003d null) {\n-          removeBlocks(toRemoveBlocks);\n-          toRemoveBlocks.clear();\n-        }\n+        removeBlocks(toRemoveBlocks);\n+        toRemoveBlocks.clear();\n       }\n     }\n \n-    logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private HdfsFileStatus startFileInt(final String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\").append(src)\n          .append(\", holder\u003d\").append(holder)\n          .append(\", clientMachine\u003d\").append(clientMachine)\n          .append(\", createParent\u003d\").append(createParent)\n          .append(\", replication\u003d\").append(replication)\n          .append(\", createFlag\u003d\").append(flag.toString())\n          .append(\", blockSize\u003d\").append(blockSize)\n          .append(\", supportedVersions\u003d\")\n          .append(supportedVersions \u003d\u003d null ? null : Arrays.toString\n              (supportedVersions));\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    FSDirWriteFileOp.EncryptionKeyInfo ezInfo \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        checkOperation(OperationCategory.READ);\n        ezInfo \u003d FSDirWriteFileOp\n            .getEncryptionKeyInfo(this, pc, src, supportedVersions);\n      } finally {\n        readUnlock();\n      }\n\n      // Generate EDEK if necessary while not holding the lock\n      if (ezInfo !\u003d null) {\n        ezInfo.edek \u003d generateEncryptedDataEncryptionKey(ezInfo.ezKeyName);\n      }\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n\n    // Proceed with the create, using the computed cipher suite and\n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d new BlocksMapUpdateInfo();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      dir.writeLock();\n      try {\n        stat \u003d FSDirWriteFileOp.startFile(this, pc, src, permissions, holder,\n                                          clientMachine, flag, createParent,\n                                          replication, blockSize, ezInfo,\n                                          toRemoveBlocks, logRetryCache);\n      } finally {\n        dir.writeUnlock();\n      }\n    } catch (IOException e) {\n      skipSync \u003d e instanceof StandbyException;\n      throw e;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        removeBlocks(toRemoveBlocks);\n        toRemoveBlocks.clear();\n      }\n    }\n\n    return stat;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "5caebbae8c2fc9ba2e32384657aee21641a1a6d0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7484. Make FSDirectory#addINode take existing INodes as its parameter. Contributed by Jing Zhao.\n",
      "commitDate": "22/12/14 11:19 PM",
      "commitName": "5caebbae8c2fc9ba2e32384657aee21641a1a6d0",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "18/12/14 11:25 AM",
      "commitNameOld": "65f2a4ee600dfffa5203450261da3c1989de25a9",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 4.5,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,129 +1,137 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n       builder.append(\", supportedVersions\u003d\");\n       if (supportedVersions !\u003d null) {\n         builder.append(Arrays.toString(supportedVersions));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n     boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n \n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     CryptoProtocolVersion protocolVersion \u003d null;\n     CipherSuite suite \u003d null;\n     String ezKeyName \u003d null;\n     EncryptedKeyVersion edek \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n         src \u003d dir.resolvePath(pc, src, pathComponents);\n         INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n         // Nothing to do if the path is not within an EZ\n         final EncryptionZone zone \u003d dir.getEZForPath(iip);\n         if (zone !\u003d null) {\n           protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n           suite \u003d zone.getSuite();\n           ezKeyName \u003d zone.getKeyName();\n \n           Preconditions.checkNotNull(protocolVersion);\n           Preconditions.checkNotNull(suite);\n           Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n               \"Chose an UNKNOWN CipherSuite!\");\n           Preconditions.checkNotNull(ezKeyName);\n         }\n       } finally {\n         readUnlock();\n       }\n \n       Preconditions.checkState(\n           (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n               (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n           \"Both suite and ezKeyName should both be null or not null\");\n \n       // Generate EDEK if necessary while not holding the lock\n       edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     // Proceed with the create, using the computed cipher suite and \n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n-      src \u003d dir.resolvePath(pc, src, pathComponents);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-      toRemoveBlocks \u003d startFileInternal(pc, iip, permissions, holder,\n-          clientMachine, create, overwrite, createParent, replication, \n-          blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n-      stat \u003d FSDirStatAndListingOp.getFileInfo(dir, src, false,\n-          FSDirectory.isReservedRawName(srcArg), true);\n+      dir.writeLock();\n+      try {\n+        src \u003d dir.resolvePath(pc, src, pathComponents);\n+        final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n+        toRemoveBlocks \u003d startFileInternal(\n+            pc, iip, permissions, holder,\n+            clientMachine, create, overwrite,\n+            createParent, replication, blockSize,\n+            isLazyPersist, suite, protocolVersion, edek,\n+            logRetryCache);\n+        stat \u003d FSDirStatAndListingOp.getFileInfo(\n+            dir, src, false, FSDirectory.isReservedRawName(srcArg), true);\n+      } finally {\n+        dir.writeUnlock();\n+      }\n     } catch (StandbyException se) {\n       skipSync \u003d true;\n       throw se;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", supportedVersions\u003d\");\n      if (supportedVersions !\u003d null) {\n        builder.append(Arrays.toString(supportedVersions));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n    boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    CryptoProtocolVersion protocolVersion \u003d null;\n    CipherSuite suite \u003d null;\n    String ezKeyName \u003d null;\n    EncryptedKeyVersion edek \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        src \u003d dir.resolvePath(pc, src, pathComponents);\n        INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n        // Nothing to do if the path is not within an EZ\n        final EncryptionZone zone \u003d dir.getEZForPath(iip);\n        if (zone !\u003d null) {\n          protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n          suite \u003d zone.getSuite();\n          ezKeyName \u003d zone.getKeyName();\n\n          Preconditions.checkNotNull(protocolVersion);\n          Preconditions.checkNotNull(suite);\n          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n              \"Chose an UNKNOWN CipherSuite!\");\n          Preconditions.checkNotNull(ezKeyName);\n        }\n      } finally {\n        readUnlock();\n      }\n\n      Preconditions.checkState(\n          (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n              (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n          \"Both suite and ezKeyName should both be null or not null\");\n\n      // Generate EDEK if necessary while not holding the lock\n      edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    // Proceed with the create, using the computed cipher suite and \n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      dir.writeLock();\n      try {\n        src \u003d dir.resolvePath(pc, src, pathComponents);\n        final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n        toRemoveBlocks \u003d startFileInternal(\n            pc, iip, permissions, holder,\n            clientMachine, create, overwrite,\n            createParent, replication, blockSize,\n            isLazyPersist, suite, protocolVersion, edek,\n            logRetryCache);\n        stat \u003d FSDirStatAndListingOp.getFileInfo(\n            dir, src, false, FSDirectory.isReservedRawName(srcArg), true);\n      } finally {\n        dir.writeUnlock();\n      }\n    } catch (StandbyException se) {\n      skipSync \u003d true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7474. Avoid resolving path in FSPermissionChecker. Contributed by Jing Zhao.\n",
      "commitDate": "05/12/14 2:17 PM",
      "commitName": "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "05/12/14 10:55 AM",
      "commitNameOld": "6a5596e3b4443462fc86f800b3c2eb839d44c3bd",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.14,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,128 +1,129 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n       builder.append(\", supportedVersions\u003d\");\n       if (supportedVersions !\u003d null) {\n         builder.append(Arrays.toString(supportedVersions));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n     boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n \n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     CryptoProtocolVersion protocolVersion \u003d null;\n     CipherSuite suite \u003d null;\n     String ezKeyName \u003d null;\n     EncryptedKeyVersion edek \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n         src \u003d dir.resolvePath(pc, src, pathComponents);\n         INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n         // Nothing to do if the path is not within an EZ\n         final EncryptionZone zone \u003d dir.getEZForPath(iip);\n         if (zone !\u003d null) {\n           protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n           suite \u003d zone.getSuite();\n           ezKeyName \u003d zone.getKeyName();\n \n           Preconditions.checkNotNull(protocolVersion);\n           Preconditions.checkNotNull(suite);\n           Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n               \"Chose an UNKNOWN CipherSuite!\");\n           Preconditions.checkNotNull(ezKeyName);\n         }\n       } finally {\n         readUnlock();\n       }\n \n       Preconditions.checkState(\n           (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n               (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n           \"Both suite and ezKeyName should both be null or not null\");\n \n       // Generate EDEK if necessary while not holding the lock\n       edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     // Proceed with the create, using the computed cipher suite and \n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       src \u003d dir.resolvePath(pc, src, pathComponents);\n-      toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n+      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n+      toRemoveBlocks \u003d startFileInternal(pc, iip, permissions, holder,\n           clientMachine, create, overwrite, createParent, replication, \n           blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n       stat \u003d FSDirStatAndListingOp.getFileInfo(dir, src, false,\n           FSDirectory.isReservedRawName(srcArg), true);\n     } catch (StandbyException se) {\n       skipSync \u003d true;\n       throw se;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", supportedVersions\u003d\");\n      if (supportedVersions !\u003d null) {\n        builder.append(Arrays.toString(supportedVersions));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n    boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    CryptoProtocolVersion protocolVersion \u003d null;\n    CipherSuite suite \u003d null;\n    String ezKeyName \u003d null;\n    EncryptedKeyVersion edek \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        src \u003d dir.resolvePath(pc, src, pathComponents);\n        INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n        // Nothing to do if the path is not within an EZ\n        final EncryptionZone zone \u003d dir.getEZForPath(iip);\n        if (zone !\u003d null) {\n          protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n          suite \u003d zone.getSuite();\n          ezKeyName \u003d zone.getKeyName();\n\n          Preconditions.checkNotNull(protocolVersion);\n          Preconditions.checkNotNull(suite);\n          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n              \"Chose an UNKNOWN CipherSuite!\");\n          Preconditions.checkNotNull(ezKeyName);\n        }\n      } finally {\n        readUnlock();\n      }\n\n      Preconditions.checkState(\n          (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n              (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n          \"Both suite and ezKeyName should both be null or not null\");\n\n      // Generate EDEK if necessary while not holding the lock\n      edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    // Proceed with the create, using the computed cipher suite and \n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      src \u003d dir.resolvePath(pc, src, pathComponents);\n      final INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n      toRemoveBlocks \u003d startFileInternal(pc, iip, permissions, holder,\n          clientMachine, create, overwrite, createParent, replication, \n          blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n      stat \u003d FSDirStatAndListingOp.getFileInfo(dir, src, false,\n          FSDirectory.isReservedRawName(srcArg), true);\n    } catch (StandbyException se) {\n      skipSync \u003d true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "01/12/14 9:36 PM",
      "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/11/14 7:39 AM",
      "commitNameOld": "1556f86a31a54733d6550363aa0e027acca7823b",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 3.58,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,128 +1,128 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n       builder.append(\", supportedVersions\u003d\");\n       if (supportedVersions !\u003d null) {\n         builder.append(Arrays.toString(supportedVersions));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n     boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n \n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     CryptoProtocolVersion protocolVersion \u003d null;\n     CipherSuite suite \u003d null;\n     String ezKeyName \u003d null;\n     EncryptedKeyVersion edek \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n         src \u003d dir.resolvePath(pc, src, pathComponents);\n         INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n         // Nothing to do if the path is not within an EZ\n         final EncryptionZone zone \u003d dir.getEZForPath(iip);\n         if (zone !\u003d null) {\n           protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n           suite \u003d zone.getSuite();\n           ezKeyName \u003d zone.getKeyName();\n \n           Preconditions.checkNotNull(protocolVersion);\n           Preconditions.checkNotNull(suite);\n           Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n               \"Chose an UNKNOWN CipherSuite!\");\n           Preconditions.checkNotNull(ezKeyName);\n         }\n       } finally {\n         readUnlock();\n       }\n \n       Preconditions.checkState(\n           (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n               (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n           \"Both suite and ezKeyName should both be null or not null\");\n \n       // Generate EDEK if necessary while not holding the lock\n       edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     // Proceed with the create, using the computed cipher suite and \n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       src \u003d dir.resolvePath(pc, src, pathComponents);\n       toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n           clientMachine, create, overwrite, createParent, replication, \n           blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n-      stat \u003d dir.getFileInfo(src, false,\n+      stat \u003d FSDirStatAndListingOp.getFileInfo(dir, src, false,\n           FSDirectory.isReservedRawName(srcArg), true);\n     } catch (StandbyException se) {\n       skipSync \u003d true;\n       throw se;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", supportedVersions\u003d\");\n      if (supportedVersions !\u003d null) {\n        builder.append(Arrays.toString(supportedVersions));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n    boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    CryptoProtocolVersion protocolVersion \u003d null;\n    CipherSuite suite \u003d null;\n    String ezKeyName \u003d null;\n    EncryptedKeyVersion edek \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        src \u003d dir.resolvePath(pc, src, pathComponents);\n        INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n        // Nothing to do if the path is not within an EZ\n        final EncryptionZone zone \u003d dir.getEZForPath(iip);\n        if (zone !\u003d null) {\n          protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n          suite \u003d zone.getSuite();\n          ezKeyName \u003d zone.getKeyName();\n\n          Preconditions.checkNotNull(protocolVersion);\n          Preconditions.checkNotNull(suite);\n          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n              \"Chose an UNKNOWN CipherSuite!\");\n          Preconditions.checkNotNull(ezKeyName);\n        }\n      } finally {\n        readUnlock();\n      }\n\n      Preconditions.checkState(\n          (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n              (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n          \"Both suite and ezKeyName should both be null or not null\");\n\n      // Generate EDEK if necessary while not holding the lock\n      edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    // Proceed with the create, using the computed cipher suite and \n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      src \u003d dir.resolvePath(pc, src, pathComponents);\n      toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n          clientMachine, create, overwrite, createParent, replication, \n          blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n      stat \u003d FSDirStatAndListingOp.getFileInfo(dir, src, false,\n          FSDirectory.isReservedRawName(srcArg), true);\n    } catch (StandbyException se) {\n      skipSync \u003d true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "c95b878abf313507666ea018f9e6033c4c166e10": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7415. Move FSNameSystem.resolvePath() to FSDirectory. Contributed by Haohui Mai.\n",
      "commitDate": "20/11/14 7:23 PM",
      "commitName": "c95b878abf313507666ea018f9e6033c4c166e10",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "17/11/14 5:33 PM",
      "commitNameOld": "dcb8e24427b02e2f3ff9a12d2eb1eb878e3443bb",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 3.08,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,128 +1,128 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n       builder.append(\", supportedVersions\u003d\");\n       if (supportedVersions !\u003d null) {\n         builder.append(Arrays.toString(supportedVersions));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n     boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n \n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     CryptoProtocolVersion protocolVersion \u003d null;\n     CipherSuite suite \u003d null;\n     String ezKeyName \u003d null;\n     EncryptedKeyVersion edek \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n-        src \u003d resolvePath(src, pathComponents);\n+        src \u003d dir.resolvePath(pc, src, pathComponents);\n         INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n         // Nothing to do if the path is not within an EZ\n         final EncryptionZone zone \u003d dir.getEZForPath(iip);\n         if (zone !\u003d null) {\n           protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n           suite \u003d zone.getSuite();\n           ezKeyName \u003d zone.getKeyName();\n \n           Preconditions.checkNotNull(protocolVersion);\n           Preconditions.checkNotNull(suite);\n           Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n               \"Chose an UNKNOWN CipherSuite!\");\n           Preconditions.checkNotNull(ezKeyName);\n         }\n       } finally {\n         readUnlock();\n       }\n \n       Preconditions.checkState(\n           (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n               (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n           \"Both suite and ezKeyName should both be null or not null\");\n \n       // Generate EDEK if necessary while not holding the lock\n       edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     // Proceed with the create, using the computed cipher suite and \n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n-      src \u003d resolvePath(src, pathComponents);\n+      src \u003d dir.resolvePath(pc, src, pathComponents);\n       toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n           clientMachine, create, overwrite, createParent, replication, \n           blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n       stat \u003d dir.getFileInfo(src, false,\n           FSDirectory.isReservedRawName(srcArg), true);\n     } catch (StandbyException se) {\n       skipSync \u003d true;\n       throw se;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", supportedVersions\u003d\");\n      if (supportedVersions !\u003d null) {\n        builder.append(Arrays.toString(supportedVersions));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n    boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    CryptoProtocolVersion protocolVersion \u003d null;\n    CipherSuite suite \u003d null;\n    String ezKeyName \u003d null;\n    EncryptedKeyVersion edek \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        src \u003d dir.resolvePath(pc, src, pathComponents);\n        INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n        // Nothing to do if the path is not within an EZ\n        final EncryptionZone zone \u003d dir.getEZForPath(iip);\n        if (zone !\u003d null) {\n          protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n          suite \u003d zone.getSuite();\n          ezKeyName \u003d zone.getKeyName();\n\n          Preconditions.checkNotNull(protocolVersion);\n          Preconditions.checkNotNull(suite);\n          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n              \"Chose an UNKNOWN CipherSuite!\");\n          Preconditions.checkNotNull(ezKeyName);\n        }\n      } finally {\n        readUnlock();\n      }\n\n      Preconditions.checkState(\n          (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n              (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n          \"Both suite and ezKeyName should both be null or not null\");\n\n      // Generate EDEK if necessary while not holding the lock\n      edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    // Proceed with the create, using the computed cipher suite and \n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      src \u003d dir.resolvePath(pc, src, pathComponents);\n      toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n          clientMachine, create, overwrite, createParent, replication, \n          blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n      stat \u003d dir.getFileInfo(src, false,\n          FSDirectory.isReservedRawName(srcArg), true);\n    } catch (StandbyException se) {\n      skipSync \u003d true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "368743140dd076ecd5af309c1ed83c5ae2d59fc8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7252. small refinement to the use of isInAnEZ in FSNamesystem. (Yi Liu via vinayakumarb)\n",
      "commitDate": "17/10/14 5:33 AM",
      "commitName": "368743140dd076ecd5af309c1ed83c5ae2d59fc8",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "17/10/14 5:18 AM",
      "commitNameOld": "1c3ff0b7c892b9d70737c375fb6f4a6fc6dd6d81",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,128 +1,128 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n       builder.append(\", supportedVersions\u003d\");\n       if (supportedVersions !\u003d null) {\n         builder.append(Arrays.toString(supportedVersions));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n     boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n \n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     CryptoProtocolVersion protocolVersion \u003d null;\n     CipherSuite suite \u003d null;\n     String ezKeyName \u003d null;\n     EncryptedKeyVersion edek \u003d null;\n \n     if (provider !\u003d null) {\n       readLock();\n       try {\n         src \u003d resolvePath(src, pathComponents);\n         INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n         // Nothing to do if the path is not within an EZ\n-        if (dir.isInAnEZ(iip)) {\n-          EncryptionZone zone \u003d dir.getEZForPath(iip);\n+        final EncryptionZone zone \u003d dir.getEZForPath(iip);\n+        if (zone !\u003d null) {\n           protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n           suite \u003d zone.getSuite();\n-          ezKeyName \u003d dir.getKeyName(iip);\n+          ezKeyName \u003d zone.getKeyName();\n \n           Preconditions.checkNotNull(protocolVersion);\n           Preconditions.checkNotNull(suite);\n           Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n               \"Chose an UNKNOWN CipherSuite!\");\n           Preconditions.checkNotNull(ezKeyName);\n         }\n       } finally {\n         readUnlock();\n       }\n \n       Preconditions.checkState(\n           (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n               (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n           \"Both suite and ezKeyName should both be null or not null\");\n \n       // Generate EDEK if necessary while not holding the lock\n       edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n       EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n     // Proceed with the create, using the computed cipher suite and \n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       src \u003d resolvePath(src, pathComponents);\n       toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n           clientMachine, create, overwrite, createParent, replication, \n           blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n       stat \u003d dir.getFileInfo(src, false,\n           FSDirectory.isReservedRawName(srcArg), true);\n     } catch (StandbyException se) {\n       skipSync \u003d true;\n       throw se;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", supportedVersions\u003d\");\n      if (supportedVersions !\u003d null) {\n        builder.append(Arrays.toString(supportedVersions));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n    boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    CryptoProtocolVersion protocolVersion \u003d null;\n    CipherSuite suite \u003d null;\n    String ezKeyName \u003d null;\n    EncryptedKeyVersion edek \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        src \u003d resolvePath(src, pathComponents);\n        INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n        // Nothing to do if the path is not within an EZ\n        final EncryptionZone zone \u003d dir.getEZForPath(iip);\n        if (zone !\u003d null) {\n          protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n          suite \u003d zone.getSuite();\n          ezKeyName \u003d zone.getKeyName();\n\n          Preconditions.checkNotNull(protocolVersion);\n          Preconditions.checkNotNull(suite);\n          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n              \"Chose an UNKNOWN CipherSuite!\");\n          Preconditions.checkNotNull(ezKeyName);\n        }\n      } finally {\n        readUnlock();\n      }\n\n      Preconditions.checkState(\n          (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n              (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n          \"Both suite and ezKeyName should both be null or not null\");\n\n      // Generate EDEK if necessary while not holding the lock\n      edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    // Proceed with the create, using the computed cipher suite and \n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      src \u003d resolvePath(src, pathComponents);\n      toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n          clientMachine, create, overwrite, createParent, replication, \n          blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n      stat \u003d dir.getFileInfo(src, false,\n          FSDirectory.isReservedRawName(srcArg), true);\n    } catch (StandbyException se) {\n      skipSync \u003d true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "d3d3d47202a18749eeebec22db759c19dd3e232c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7209. Populate EDEK cache when creating encryption zone. (Yi Liu via wang)\n",
      "commitDate": "10/10/14 1:40 PM",
      "commitName": "d3d3d47202a18749eeebec22db759c19dd3e232c",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "06/10/14 2:01 AM",
      "commitNameOld": "ed841dd9a96e54cb84d9cae5507e47ff1c8cdf6e",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 4.49,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,125 +1,128 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n       builder.append(\", supportedVersions\u003d\");\n       if (supportedVersions !\u003d null) {\n         builder.append(Arrays.toString(supportedVersions));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n     boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n \n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     CryptoProtocolVersion protocolVersion \u003d null;\n     CipherSuite suite \u003d null;\n     String ezKeyName \u003d null;\n-    readLock();\n-    try {\n-      src \u003d resolvePath(src, pathComponents);\n-      INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-      // Nothing to do if the path is not within an EZ\n-      if (dir.isInAnEZ(iip)) {\n-        EncryptionZone zone \u003d dir.getEZForPath(iip);\n-        protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n-        suite \u003d zone.getSuite();\n-        ezKeyName \u003d dir.getKeyName(iip);\n+    EncryptedKeyVersion edek \u003d null;\n \n-        Preconditions.checkNotNull(protocolVersion);\n-        Preconditions.checkNotNull(suite);\n-        Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n-            \"Chose an UNKNOWN CipherSuite!\");\n-        Preconditions.checkNotNull(ezKeyName);\n+    if (provider !\u003d null) {\n+      readLock();\n+      try {\n+        src \u003d resolvePath(src, pathComponents);\n+        INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n+        // Nothing to do if the path is not within an EZ\n+        if (dir.isInAnEZ(iip)) {\n+          EncryptionZone zone \u003d dir.getEZForPath(iip);\n+          protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n+          suite \u003d zone.getSuite();\n+          ezKeyName \u003d dir.getKeyName(iip);\n+\n+          Preconditions.checkNotNull(protocolVersion);\n+          Preconditions.checkNotNull(suite);\n+          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n+              \"Chose an UNKNOWN CipherSuite!\");\n+          Preconditions.checkNotNull(ezKeyName);\n+        }\n+      } finally {\n+        readUnlock();\n       }\n-    } finally {\n-      readUnlock();\n+\n+      Preconditions.checkState(\n+          (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n+              (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n+          \"Both suite and ezKeyName should both be null or not null\");\n+\n+      // Generate EDEK if necessary while not holding the lock\n+      edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n+      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n     }\n \n-    Preconditions.checkState(\n-        (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n-            (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n-        \"Both suite and ezKeyName should both be null or not null\");\n-\n-    // Generate EDEK if necessary while not holding the lock\n-    EncryptedKeyVersion edek \u003d\n-        generateEncryptedDataEncryptionKey(ezKeyName);\n-    EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n-\n     // Proceed with the create, using the computed cipher suite and \n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       src \u003d resolvePath(src, pathComponents);\n       toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n           clientMachine, create, overwrite, createParent, replication, \n           blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n       stat \u003d dir.getFileInfo(src, false,\n           FSDirectory.isReservedRawName(srcArg), true);\n     } catch (StandbyException se) {\n       skipSync \u003d true;\n       throw se;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", supportedVersions\u003d\");\n      if (supportedVersions !\u003d null) {\n        builder.append(Arrays.toString(supportedVersions));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n    boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    CryptoProtocolVersion protocolVersion \u003d null;\n    CipherSuite suite \u003d null;\n    String ezKeyName \u003d null;\n    EncryptedKeyVersion edek \u003d null;\n\n    if (provider !\u003d null) {\n      readLock();\n      try {\n        src \u003d resolvePath(src, pathComponents);\n        INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n        // Nothing to do if the path is not within an EZ\n        if (dir.isInAnEZ(iip)) {\n          EncryptionZone zone \u003d dir.getEZForPath(iip);\n          protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n          suite \u003d zone.getSuite();\n          ezKeyName \u003d dir.getKeyName(iip);\n\n          Preconditions.checkNotNull(protocolVersion);\n          Preconditions.checkNotNull(suite);\n          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n              \"Chose an UNKNOWN CipherSuite!\");\n          Preconditions.checkNotNull(ezKeyName);\n        }\n      } finally {\n        readUnlock();\n      }\n\n      Preconditions.checkState(\n          (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n              (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n          \"Both suite and ezKeyName should both be null or not null\");\n\n      // Generate EDEK if necessary while not holding the lock\n      edek \u003d generateEncryptedDataEncryptionKey(ezKeyName);\n      EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n    }\n\n    // Proceed with the create, using the computed cipher suite and \n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      src \u003d resolvePath(src, pathComponents);\n      toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n          clientMachine, create, overwrite, createParent, replication, \n          blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n      stat \u003d dir.getFileInfo(src, false,\n          FSDirectory.isReservedRawName(srcArg), true);\n    } catch (StandbyException se) {\n      skipSync \u003d true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "d45e7c7e856c7103752888c0395fa94985cd7670": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7153. Add storagePolicy to NN edit log during file creation. (Arpit Agarwal)\n",
      "commitDate": "29/09/14 12:36 PM",
      "commitName": "d45e7c7e856c7103752888c0395fa94985cd7670",
      "commitAuthor": "arp",
      "commitDateOld": "28/09/14 8:42 PM",
      "commitNameOld": "9a53c3699b52a0bbeb0f723ac409ac189aa00f7f",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.66,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,125 +1,125 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, CryptoProtocolVersion[] supportedVersions,\n       boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n       builder.append(\", supportedVersions\u003d\");\n       if (supportedVersions !\u003d null) {\n         builder.append(Arrays.toString(supportedVersions));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n     boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n \n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n     CryptoProtocolVersion protocolVersion \u003d null;\n     CipherSuite suite \u003d null;\n     String ezKeyName \u003d null;\n     readLock();\n     try {\n       src \u003d resolvePath(src, pathComponents);\n       INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n       // Nothing to do if the path is not within an EZ\n       if (dir.isInAnEZ(iip)) {\n         EncryptionZone zone \u003d dir.getEZForPath(iip);\n         protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n         suite \u003d zone.getSuite();\n         ezKeyName \u003d dir.getKeyName(iip);\n \n         Preconditions.checkNotNull(protocolVersion);\n         Preconditions.checkNotNull(suite);\n         Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n             \"Chose an UNKNOWN CipherSuite!\");\n         Preconditions.checkNotNull(ezKeyName);\n       }\n     } finally {\n       readUnlock();\n     }\n \n     Preconditions.checkState(\n         (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n             (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n         \"Both suite and ezKeyName should both be null or not null\");\n \n     // Generate EDEK if necessary while not holding the lock\n     EncryptedKeyVersion edek \u003d\n         generateEncryptedDataEncryptionKey(ezKeyName);\n     EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n \n     // Proceed with the create, using the computed cipher suite and \n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       src \u003d resolvePath(src, pathComponents);\n       toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n           clientMachine, create, overwrite, createParent, replication, \n           blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n       stat \u003d dir.getFileInfo(src, false,\n-          FSDirectory.isReservedRawName(srcArg), false);\n+          FSDirectory.isReservedRawName(srcArg), true);\n     } catch (StandbyException se) {\n       skipSync \u003d true;\n       throw se;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", supportedVersions\u003d\");\n      if (supportedVersions !\u003d null) {\n        builder.append(Arrays.toString(supportedVersions));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n    boolean isLazyPersist \u003d flag.contains(CreateFlag.LAZY_PERSIST);\n\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    CryptoProtocolVersion protocolVersion \u003d null;\n    CipherSuite suite \u003d null;\n    String ezKeyName \u003d null;\n    readLock();\n    try {\n      src \u003d resolvePath(src, pathComponents);\n      INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n      // Nothing to do if the path is not within an EZ\n      if (dir.isInAnEZ(iip)) {\n        EncryptionZone zone \u003d dir.getEZForPath(iip);\n        protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n        suite \u003d zone.getSuite();\n        ezKeyName \u003d dir.getKeyName(iip);\n\n        Preconditions.checkNotNull(protocolVersion);\n        Preconditions.checkNotNull(suite);\n        Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n            \"Chose an UNKNOWN CipherSuite!\");\n        Preconditions.checkNotNull(ezKeyName);\n      }\n    } finally {\n      readUnlock();\n    }\n\n    Preconditions.checkState(\n        (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n            (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n        \"Both suite and ezKeyName should both be null or not null\");\n\n    // Generate EDEK if necessary while not holding the lock\n    EncryptedKeyVersion edek \u003d\n        generateEncryptedDataEncryptionKey(ezKeyName);\n    EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n\n    // Proceed with the create, using the computed cipher suite and \n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      src \u003d resolvePath(src, pathComponents);\n      toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n          clientMachine, create, overwrite, createParent, replication, \n          blockSize, isLazyPersist, suite, protocolVersion, edek, logRetryCache);\n      stat \u003d dir.getFileInfo(src, false,\n          FSDirectory.isReservedRawName(srcArg), true);\n    } catch (StandbyException se) {\n      skipSync \u003d true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "e96ce6f3e3e549202ce3c48d4733ba34098870ad": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7077. Separate CipherSuite from crypto protocol version. (wang)\n",
      "commitDate": "25/09/14 6:40 PM",
      "commitName": "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7077. Separate CipherSuite from crypto protocol version. (wang)\n",
          "commitDate": "25/09/14 6:40 PM",
          "commitName": "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "24/09/14 7:11 PM",
          "commitNameOld": "428a76663a0de5d0d74cc9525273ddc470760e44",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.98,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,119 +1,124 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n-      long blockSize, List\u003cCipherSuite\u003e cipherSuites, boolean logRetryCache)\n+      long blockSize, CryptoProtocolVersion[] supportedVersions,\n+      boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n-      builder.append(\", cipherSuites\u003d\");\n-      if (cipherSuites !\u003d null) {\n-        builder.append(Arrays.toString(cipherSuites.toArray()));\n+      builder.append(\", supportedVersions\u003d\");\n+      if (supportedVersions !\u003d null) {\n+        builder.append(Arrays.toString(supportedVersions));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n \n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n+    CryptoProtocolVersion protocolVersion \u003d null;\n     CipherSuite suite \u003d null;\n     String ezKeyName \u003d null;\n     readLock();\n     try {\n       src \u003d resolvePath(src, pathComponents);\n       INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n       // Nothing to do if the path is not within an EZ\n       if (dir.isInAnEZ(iip)) {\n-        suite \u003d chooseCipherSuite(iip, cipherSuites);\n-        if (suite !\u003d null) {\n-          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n-              \"Chose an UNKNOWN CipherSuite!\");\n-        }\n+        EncryptionZone zone \u003d dir.getEZForPath(iip);\n+        protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n+        suite \u003d zone.getSuite();\n         ezKeyName \u003d dir.getKeyName(iip);\n-        Preconditions.checkState(ezKeyName !\u003d null);\n+\n+        Preconditions.checkNotNull(protocolVersion);\n+        Preconditions.checkNotNull(suite);\n+        Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n+            \"Chose an UNKNOWN CipherSuite!\");\n+        Preconditions.checkNotNull(ezKeyName);\n       }\n     } finally {\n       readUnlock();\n     }\n \n     Preconditions.checkState(\n         (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n             (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n         \"Both suite and ezKeyName should both be null or not null\");\n \n     // Generate EDEK if necessary while not holding the lock\n     EncryptedKeyVersion edek \u003d\n         generateEncryptedDataEncryptionKey(ezKeyName);\n     EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n \n     // Proceed with the create, using the computed cipher suite and \n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       src \u003d resolvePath(src, pathComponents);\n       toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n           clientMachine, create, overwrite, createParent, replication, \n-          blockSize, suite, edek, logRetryCache);\n+          blockSize, suite, protocolVersion, edek, logRetryCache);\n       stat \u003d dir.getFileInfo(src, false,\n           FSDirectory.isReservedRawName(srcArg), false);\n     } catch (StandbyException se) {\n       skipSync \u003d true;\n       throw se;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", supportedVersions\u003d\");\n      if (supportedVersions !\u003d null) {\n        builder.append(Arrays.toString(supportedVersions));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    CryptoProtocolVersion protocolVersion \u003d null;\n    CipherSuite suite \u003d null;\n    String ezKeyName \u003d null;\n    readLock();\n    try {\n      src \u003d resolvePath(src, pathComponents);\n      INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n      // Nothing to do if the path is not within an EZ\n      if (dir.isInAnEZ(iip)) {\n        EncryptionZone zone \u003d dir.getEZForPath(iip);\n        protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n        suite \u003d zone.getSuite();\n        ezKeyName \u003d dir.getKeyName(iip);\n\n        Preconditions.checkNotNull(protocolVersion);\n        Preconditions.checkNotNull(suite);\n        Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n            \"Chose an UNKNOWN CipherSuite!\");\n        Preconditions.checkNotNull(ezKeyName);\n      }\n    } finally {\n      readUnlock();\n    }\n\n    Preconditions.checkState(\n        (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n            (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n        \"Both suite and ezKeyName should both be null or not null\");\n\n    // Generate EDEK if necessary while not holding the lock\n    EncryptedKeyVersion edek \u003d\n        generateEncryptedDataEncryptionKey(ezKeyName);\n    EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n\n    // Proceed with the create, using the computed cipher suite and \n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      src \u003d resolvePath(src, pathComponents);\n      toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n          clientMachine, create, overwrite, createParent, replication, \n          blockSize, suite, protocolVersion, edek, logRetryCache);\n      stat \u003d dir.getFileInfo(src, false,\n          FSDirectory.isReservedRawName(srcArg), false);\n    } catch (StandbyException se) {\n      skipSync \u003d true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[srcArg-String(modifiers-final), permissions-PermissionStatus, holder-String, clientMachine-String, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, cipherSuites-List\u003cCipherSuite\u003e, logRetryCache-boolean]",
            "newValue": "[srcArg-String(modifiers-final), permissions-PermissionStatus, holder-String, clientMachine-String, flag-EnumSet\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7077. Separate CipherSuite from crypto protocol version. (wang)\n",
          "commitDate": "25/09/14 6:40 PM",
          "commitName": "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "24/09/14 7:11 PM",
          "commitNameOld": "428a76663a0de5d0d74cc9525273ddc470760e44",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.98,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,119 +1,124 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n-      long blockSize, List\u003cCipherSuite\u003e cipherSuites, boolean logRetryCache)\n+      long blockSize, CryptoProtocolVersion[] supportedVersions,\n+      boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n-      builder.append(\", cipherSuites\u003d\");\n-      if (cipherSuites !\u003d null) {\n-        builder.append(Arrays.toString(cipherSuites.toArray()));\n+      builder.append(\", supportedVersions\u003d\");\n+      if (supportedVersions !\u003d null) {\n+        builder.append(Arrays.toString(supportedVersions));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n \n     waitForLoadingFSImage();\n \n     /**\n      * If the file is in an encryption zone, we optimistically create an\n      * EDEK for the file by calling out to the configured KeyProvider.\n      * Since this typically involves doing an RPC, we take the readLock\n      * initially, then drop it to do the RPC.\n      * \n      * Since the path can flip-flop between being in an encryption zone and not\n      * in the meantime, we need to recheck the preconditions when we retake the\n      * lock to do the create. If the preconditions are not met, we throw a\n      * special RetryStartFileException to ask the DFSClient to try the create\n      * again later.\n      */\n+    CryptoProtocolVersion protocolVersion \u003d null;\n     CipherSuite suite \u003d null;\n     String ezKeyName \u003d null;\n     readLock();\n     try {\n       src \u003d resolvePath(src, pathComponents);\n       INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n       // Nothing to do if the path is not within an EZ\n       if (dir.isInAnEZ(iip)) {\n-        suite \u003d chooseCipherSuite(iip, cipherSuites);\n-        if (suite !\u003d null) {\n-          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n-              \"Chose an UNKNOWN CipherSuite!\");\n-        }\n+        EncryptionZone zone \u003d dir.getEZForPath(iip);\n+        protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n+        suite \u003d zone.getSuite();\n         ezKeyName \u003d dir.getKeyName(iip);\n-        Preconditions.checkState(ezKeyName !\u003d null);\n+\n+        Preconditions.checkNotNull(protocolVersion);\n+        Preconditions.checkNotNull(suite);\n+        Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n+            \"Chose an UNKNOWN CipherSuite!\");\n+        Preconditions.checkNotNull(ezKeyName);\n       }\n     } finally {\n       readUnlock();\n     }\n \n     Preconditions.checkState(\n         (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n             (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n         \"Both suite and ezKeyName should both be null or not null\");\n \n     // Generate EDEK if necessary while not holding the lock\n     EncryptedKeyVersion edek \u003d\n         generateEncryptedDataEncryptionKey(ezKeyName);\n     EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n \n     // Proceed with the create, using the computed cipher suite and \n     // generated EDEK\n     BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot create file\" + src);\n       src \u003d resolvePath(src, pathComponents);\n       toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n           clientMachine, create, overwrite, createParent, replication, \n-          blockSize, suite, edek, logRetryCache);\n+          blockSize, suite, protocolVersion, edek, logRetryCache);\n       stat \u003d dir.getFileInfo(src, false,\n           FSDirectory.isReservedRawName(srcArg), false);\n     } catch (StandbyException se) {\n       skipSync \u003d true;\n       throw se;\n     } finally {\n       writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, CryptoProtocolVersion[] supportedVersions,\n      boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", supportedVersions\u003d\");\n      if (supportedVersions !\u003d null) {\n        builder.append(Arrays.toString(supportedVersions));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    CryptoProtocolVersion protocolVersion \u003d null;\n    CipherSuite suite \u003d null;\n    String ezKeyName \u003d null;\n    readLock();\n    try {\n      src \u003d resolvePath(src, pathComponents);\n      INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n      // Nothing to do if the path is not within an EZ\n      if (dir.isInAnEZ(iip)) {\n        EncryptionZone zone \u003d dir.getEZForPath(iip);\n        protocolVersion \u003d chooseProtocolVersion(zone, supportedVersions);\n        suite \u003d zone.getSuite();\n        ezKeyName \u003d dir.getKeyName(iip);\n\n        Preconditions.checkNotNull(protocolVersion);\n        Preconditions.checkNotNull(suite);\n        Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n            \"Chose an UNKNOWN CipherSuite!\");\n        Preconditions.checkNotNull(ezKeyName);\n      }\n    } finally {\n      readUnlock();\n    }\n\n    Preconditions.checkState(\n        (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n            (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n        \"Both suite and ezKeyName should both be null or not null\");\n\n    // Generate EDEK if necessary while not holding the lock\n    EncryptedKeyVersion edek \u003d\n        generateEncryptedDataEncryptionKey(ezKeyName);\n    EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n\n    // Proceed with the create, using the computed cipher suite and \n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      src \u003d resolvePath(src, pathComponents);\n      toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n          clientMachine, create, overwrite, createParent, replication, \n          blockSize, suite, protocolVersion, edek, logRetryCache);\n      stat \u003d dir.getFileInfo(src, false,\n          FSDirectory.isReservedRawName(srcArg), false);\n    } catch (StandbyException se) {\n      skipSync \u003d true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "20a076bafce548298729bab4fb81d12f829e8f7e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6970. Move startFile EDEK retries to the DFSClient. (wang)\n",
      "commitDate": "18/09/14 5:35 PM",
      "commitName": "20a076bafce548298729bab4fb81d12f829e8f7e",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "17/09/14 1:12 PM",
      "commitNameOld": "911979c8ab2e6dd4fe82023ae022a1582c8590c2",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.18,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,137 +1,119 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, List\u003cCipherSuite\u003e cipherSuites, boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n       builder.append(\", cipherSuites\u003d\");\n       if (cipherSuites !\u003d null) {\n         builder.append(Arrays.toString(cipherSuites.toArray()));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n \n     waitForLoadingFSImage();\n \n-    /*\n-     * We want to avoid holding any locks while doing KeyProvider operations,\n-     * since they can be very slow. Since the path can\n-     * flip flop between being in an encryption zone and not in the meantime,\n-     * we need to recheck the preconditions and redo KeyProvider operations\n-     * in some situations.\n-     *\n-     * A special RetryStartFileException is used to indicate that we should\n-     * retry creation of a FileEncryptionInfo.\n+    /**\n+     * If the file is in an encryption zone, we optimistically create an\n+     * EDEK for the file by calling out to the configured KeyProvider.\n+     * Since this typically involves doing an RPC, we take the readLock\n+     * initially, then drop it to do the RPC.\n+     * \n+     * Since the path can flip-flop between being in an encryption zone and not\n+     * in the meantime, we need to recheck the preconditions when we retake the\n+     * lock to do the create. If the preconditions are not met, we throw a\n+     * special RetryStartFileException to ask the DFSClient to try the create\n+     * again later.\n      */\n-    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n+    CipherSuite suite \u003d null;\n+    String ezKeyName \u003d null;\n+    readLock();\n     try {\n-      boolean shouldContinue \u003d true;\n-      int iters \u003d 0;\n-      while (shouldContinue) {\n-        skipSync \u003d false;\n-        if (iters \u003e\u003d 10) {\n-          throw new IOException(\"Too many retries because of encryption zone \" +\n-              \"operations, something might be broken!\");\n+      src \u003d resolvePath(src, pathComponents);\n+      INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n+      // Nothing to do if the path is not within an EZ\n+      if (dir.isInAnEZ(iip)) {\n+        suite \u003d chooseCipherSuite(iip, cipherSuites);\n+        if (suite !\u003d null) {\n+          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n+              \"Chose an UNKNOWN CipherSuite!\");\n         }\n-        shouldContinue \u003d false;\n-        iters++;\n-\n-        // Optimistically determine CipherSuite and ezKeyName if the path is\n-        // currently within an encryption zone\n-        CipherSuite suite \u003d null;\n-        String ezKeyName \u003d null;\n-        readLock();\n-        try {\n-          src \u003d resolvePath(src, pathComponents);\n-          INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n-          // Nothing to do if the path is not within an EZ\n-          if (dir.isInAnEZ(iip)) {\n-            suite \u003d chooseCipherSuite(iip, cipherSuites);\n-            if (suite !\u003d null) {\n-              Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n-                  \"Chose an UNKNOWN CipherSuite!\");\n-            }\n-            ezKeyName \u003d dir.getKeyName(iip);\n-            Preconditions.checkState(ezKeyName !\u003d null);\n-          }\n-        } finally {\n-          readUnlock();\n-        }\n-\n-        Preconditions.checkState(\n-            (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n-            (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n-            \"Both suite and ezKeyName should both be null or not null\");\n-        // Generate EDEK if necessary while not holding the lock\n-        EncryptedKeyVersion edek \u003d\n-            generateEncryptedDataEncryptionKey(ezKeyName);\n-        EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n-        // Try to create the file with the computed cipher suite and EDEK\n-        writeLock();\n-        try {\n-          checkOperation(OperationCategory.WRITE);\n-          checkNameNodeSafeMode(\"Cannot create file\" + src);\n-          src \u003d resolvePath(src, pathComponents);\n-          toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n-              clientMachine, create, overwrite, createParent, replication, \n-              blockSize, suite, edek, logRetryCache);\n-          stat \u003d dir.getFileInfo(src, false,\n-              FSDirectory.isReservedRawName(srcArg), false);\n-        } catch (StandbyException se) {\n-          skipSync \u003d true;\n-          throw se;\n-        } catch (RetryStartFileException e) {\n-          shouldContinue \u003d true;\n-          if (LOG.isTraceEnabled()) {\n-            LOG.trace(\"Preconditions failed, retrying creation of \" +\n-                    \"FileEncryptionInfo\", e);\n-          }\n-        } finally {\n-          writeUnlock();\n-        }\n+        ezKeyName \u003d dir.getKeyName(iip);\n+        Preconditions.checkState(ezKeyName !\u003d null);\n       }\n     } finally {\n+      readUnlock();\n+    }\n+\n+    Preconditions.checkState(\n+        (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n+            (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n+        \"Both suite and ezKeyName should both be null or not null\");\n+\n+    // Generate EDEK if necessary while not holding the lock\n+    EncryptedKeyVersion edek \u003d\n+        generateEncryptedDataEncryptionKey(ezKeyName);\n+    EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n+\n+    // Proceed with the create, using the computed cipher suite and \n+    // generated EDEK\n+    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n+    writeLock();\n+    try {\n+      checkOperation(OperationCategory.WRITE);\n+      checkNameNodeSafeMode(\"Cannot create file\" + src);\n+      src \u003d resolvePath(src, pathComponents);\n+      toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n+          clientMachine, create, overwrite, createParent, replication, \n+          blockSize, suite, edek, logRetryCache);\n+      stat \u003d dir.getFileInfo(src, false,\n+          FSDirectory.isReservedRawName(srcArg), false);\n+    } catch (StandbyException se) {\n+      skipSync \u003d true;\n+      throw se;\n+    } finally {\n+      writeUnlock();\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n         if (toRemoveBlocks !\u003d null) {\n           removeBlocks(toRemoveBlocks);\n           toRemoveBlocks.clear();\n         }\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, List\u003cCipherSuite\u003e cipherSuites, boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", cipherSuites\u003d\");\n      if (cipherSuites !\u003d null) {\n        builder.append(Arrays.toString(cipherSuites.toArray()));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n\n    waitForLoadingFSImage();\n\n    /**\n     * If the file is in an encryption zone, we optimistically create an\n     * EDEK for the file by calling out to the configured KeyProvider.\n     * Since this typically involves doing an RPC, we take the readLock\n     * initially, then drop it to do the RPC.\n     * \n     * Since the path can flip-flop between being in an encryption zone and not\n     * in the meantime, we need to recheck the preconditions when we retake the\n     * lock to do the create. If the preconditions are not met, we throw a\n     * special RetryStartFileException to ask the DFSClient to try the create\n     * again later.\n     */\n    CipherSuite suite \u003d null;\n    String ezKeyName \u003d null;\n    readLock();\n    try {\n      src \u003d resolvePath(src, pathComponents);\n      INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n      // Nothing to do if the path is not within an EZ\n      if (dir.isInAnEZ(iip)) {\n        suite \u003d chooseCipherSuite(iip, cipherSuites);\n        if (suite !\u003d null) {\n          Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n              \"Chose an UNKNOWN CipherSuite!\");\n        }\n        ezKeyName \u003d dir.getKeyName(iip);\n        Preconditions.checkState(ezKeyName !\u003d null);\n      }\n    } finally {\n      readUnlock();\n    }\n\n    Preconditions.checkState(\n        (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n            (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n        \"Both suite and ezKeyName should both be null or not null\");\n\n    // Generate EDEK if necessary while not holding the lock\n    EncryptedKeyVersion edek \u003d\n        generateEncryptedDataEncryptionKey(ezKeyName);\n    EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n\n    // Proceed with the create, using the computed cipher suite and \n    // generated EDEK\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      src \u003d resolvePath(src, pathComponents);\n      toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n          clientMachine, create, overwrite, createParent, replication, \n          blockSize, suite, edek, logRetryCache);\n      stat \u003d dir.getFileInfo(src, false,\n          FSDirectory.isReservedRawName(srcArg), false);\n    } catch (StandbyException se) {\n      skipSync \u003d true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "185200e7096d15a5c2c2d59b7c7705362820aebf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6876. Archival Storage: support set/get storage policy in DFSAdmin. Contributed by Jing Zhao.\n",
      "commitDate": "04/09/14 8:14 PM",
      "commitName": "185200e7096d15a5c2c2d59b7c7705362820aebf",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "03/09/14 3:36 PM",
      "commitNameOld": "45d5b132562990d91724c03358096c3a5dd97146",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.19,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,132 +1,132 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, List\u003cCipherSuite\u003e cipherSuites, boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n       builder.append(\", cipherSuites\u003d\");\n       if (cipherSuites !\u003d null) {\n         builder.append(Arrays.toString(cipherSuites.toArray()));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n \n     waitForLoadingFSImage();\n \n     /*\n      * We want to avoid holding any locks while doing KeyProvider operations,\n      * since they can be very slow. Since the path can\n      * flip flop between being in an encryption zone and not in the meantime,\n      * we need to recheck the preconditions and redo KeyProvider operations\n      * in some situations.\n      *\n      * A special RetryStartFileException is used to indicate that we should\n      * retry creation of a FileEncryptionInfo.\n      */\n     try {\n       boolean shouldContinue \u003d true;\n       int iters \u003d 0;\n       while (shouldContinue) {\n         skipSync \u003d false;\n         if (iters \u003e\u003d 10) {\n           throw new IOException(\"Too many retries because of encryption zone \" +\n               \"operations, something might be broken!\");\n         }\n         shouldContinue \u003d false;\n         iters++;\n \n         // Optimistically determine CipherSuite and ezKeyName if the path is\n         // currently within an encryption zone\n         CipherSuite suite \u003d null;\n         String ezKeyName \u003d null;\n         readLock();\n         try {\n           src \u003d resolvePath(src, pathComponents);\n           INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n           // Nothing to do if the path is not within an EZ\n           if (dir.isInAnEZ(iip)) {\n             suite \u003d chooseCipherSuite(iip, cipherSuites);\n             if (suite !\u003d null) {\n               Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n                   \"Chose an UNKNOWN CipherSuite!\");\n             }\n             ezKeyName \u003d dir.getKeyName(iip);\n             Preconditions.checkState(ezKeyName !\u003d null);\n           }\n         } finally {\n           readUnlock();\n         }\n \n         Preconditions.checkState(\n             (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n             (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n             \"Both suite and ezKeyName should both be null or not null\");\n         // Generate EDEK if necessary while not holding the lock\n         EncryptedKeyVersion edek \u003d\n             generateEncryptedDataEncryptionKey(ezKeyName);\n         EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n         // Try to create the file with the computed cipher suite and EDEK\n         writeLock();\n         try {\n           checkOperation(OperationCategory.WRITE);\n           checkNameNodeSafeMode(\"Cannot create file\" + src);\n           src \u003d resolvePath(src, pathComponents);\n           startFileInternal(pc, src, permissions, holder, clientMachine, create,\n               overwrite, createParent, replication, blockSize, suite, edek,\n               logRetryCache);\n           stat \u003d dir.getFileInfo(src, false,\n-              FSDirectory.isReservedRawName(srcArg));\n+              FSDirectory.isReservedRawName(srcArg), false);\n         } catch (StandbyException se) {\n           skipSync \u003d true;\n           throw se;\n         } catch (RetryStartFileException e) {\n           shouldContinue \u003d true;\n           if (LOG.isTraceEnabled()) {\n             LOG.trace(\"Preconditions failed, retrying creation of \" +\n                     \"FileEncryptionInfo\", e);\n           }\n         } finally {\n           writeUnlock();\n         }\n       }\n     } finally {\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, List\u003cCipherSuite\u003e cipherSuites, boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", cipherSuites\u003d\");\n      if (cipherSuites !\u003d null) {\n        builder.append(Arrays.toString(cipherSuites.toArray()));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n\n    waitForLoadingFSImage();\n\n    /*\n     * We want to avoid holding any locks while doing KeyProvider operations,\n     * since they can be very slow. Since the path can\n     * flip flop between being in an encryption zone and not in the meantime,\n     * we need to recheck the preconditions and redo KeyProvider operations\n     * in some situations.\n     *\n     * A special RetryStartFileException is used to indicate that we should\n     * retry creation of a FileEncryptionInfo.\n     */\n    try {\n      boolean shouldContinue \u003d true;\n      int iters \u003d 0;\n      while (shouldContinue) {\n        skipSync \u003d false;\n        if (iters \u003e\u003d 10) {\n          throw new IOException(\"Too many retries because of encryption zone \" +\n              \"operations, something might be broken!\");\n        }\n        shouldContinue \u003d false;\n        iters++;\n\n        // Optimistically determine CipherSuite and ezKeyName if the path is\n        // currently within an encryption zone\n        CipherSuite suite \u003d null;\n        String ezKeyName \u003d null;\n        readLock();\n        try {\n          src \u003d resolvePath(src, pathComponents);\n          INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n          // Nothing to do if the path is not within an EZ\n          if (dir.isInAnEZ(iip)) {\n            suite \u003d chooseCipherSuite(iip, cipherSuites);\n            if (suite !\u003d null) {\n              Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n                  \"Chose an UNKNOWN CipherSuite!\");\n            }\n            ezKeyName \u003d dir.getKeyName(iip);\n            Preconditions.checkState(ezKeyName !\u003d null);\n          }\n        } finally {\n          readUnlock();\n        }\n\n        Preconditions.checkState(\n            (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n            (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n            \"Both suite and ezKeyName should both be null or not null\");\n        // Generate EDEK if necessary while not holding the lock\n        EncryptedKeyVersion edek \u003d\n            generateEncryptedDataEncryptionKey(ezKeyName);\n        EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n        // Try to create the file with the computed cipher suite and EDEK\n        writeLock();\n        try {\n          checkOperation(OperationCategory.WRITE);\n          checkNameNodeSafeMode(\"Cannot create file\" + src);\n          src \u003d resolvePath(src, pathComponents);\n          startFileInternal(pc, src, permissions, holder, clientMachine, create,\n              overwrite, createParent, replication, blockSize, suite, edek,\n              logRetryCache);\n          stat \u003d dir.getFileInfo(src, false,\n              FSDirectory.isReservedRawName(srcArg), false);\n        } catch (StandbyException se) {\n          skipSync \u003d true;\n          throw se;\n        } catch (RetryStartFileException e) {\n          shouldContinue \u003d true;\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Preconditions failed, retrying creation of \" +\n                    \"FileEncryptionInfo\", e);\n          }\n        } finally {\n          writeUnlock();\n        }\n      }\n    } finally {\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "6104520369045dfaa4b543cbad21236ed322249b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6886. Use single editlog record for creating file + overwrite. Contributed by Yi Liu.\n",
      "commitDate": "04/09/14 6:54 PM",
      "commitName": "6104520369045dfaa4b543cbad21236ed322249b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "03/09/14 6:23 AM",
      "commitNameOld": "3425ae5d7eaa27b2526d0e0c07bdfea9440359f8",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 1.52,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,132 +1,137 @@\n   private HdfsFileStatus startFileInt(final String srcArg,\n       PermissionStatus permissions, String holder, String clientMachine,\n       EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n       long blockSize, List\u003cCipherSuite\u003e cipherSuites, boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       FileAlreadyExistsException, UnresolvedLinkException,\n       FileNotFoundException, ParentNotDirectoryException, IOException {\n     String src \u003d srcArg;\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       StringBuilder builder \u003d new StringBuilder();\n       builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n               + \", holder\u003d\" + holder\n               + \", clientMachine\u003d\" + clientMachine\n               + \", createParent\u003d\" + createParent\n               + \", replication\u003d\" + replication\n               + \", createFlag\u003d\" + flag.toString()\n               + \", blockSize\u003d\" + blockSize);\n       builder.append(\", cipherSuites\u003d\");\n       if (cipherSuites !\u003d null) {\n         builder.append(Arrays.toString(cipherSuites.toArray()));\n       } else {\n         builder.append(\"null\");\n       }\n       NameNode.stateChangeLog.debug(builder.toString());\n     }\n     if (!DFSUtil.isValidName(src)) {\n       throw new InvalidPathException(src);\n     }\n     blockManager.verifyReplication(src, replication, clientMachine);\n \n     boolean skipSync \u003d false;\n     HdfsFileStatus stat \u003d null;\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     if (blockSize \u003c minBlockSize) {\n       throw new IOException(\"Specified block size is less than configured\" +\n           \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n           + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n     }\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean create \u003d flag.contains(CreateFlag.CREATE);\n     boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n \n     waitForLoadingFSImage();\n \n     /*\n      * We want to avoid holding any locks while doing KeyProvider operations,\n      * since they can be very slow. Since the path can\n      * flip flop between being in an encryption zone and not in the meantime,\n      * we need to recheck the preconditions and redo KeyProvider operations\n      * in some situations.\n      *\n      * A special RetryStartFileException is used to indicate that we should\n      * retry creation of a FileEncryptionInfo.\n      */\n+    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n     try {\n       boolean shouldContinue \u003d true;\n       int iters \u003d 0;\n       while (shouldContinue) {\n         skipSync \u003d false;\n         if (iters \u003e\u003d 10) {\n           throw new IOException(\"Too many retries because of encryption zone \" +\n               \"operations, something might be broken!\");\n         }\n         shouldContinue \u003d false;\n         iters++;\n \n         // Optimistically determine CipherSuite and ezKeyName if the path is\n         // currently within an encryption zone\n         CipherSuite suite \u003d null;\n         String ezKeyName \u003d null;\n         readLock();\n         try {\n           src \u003d resolvePath(src, pathComponents);\n           INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n           // Nothing to do if the path is not within an EZ\n           if (dir.isInAnEZ(iip)) {\n             suite \u003d chooseCipherSuite(iip, cipherSuites);\n             if (suite !\u003d null) {\n               Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n                   \"Chose an UNKNOWN CipherSuite!\");\n             }\n             ezKeyName \u003d dir.getKeyName(iip);\n             Preconditions.checkState(ezKeyName !\u003d null);\n           }\n         } finally {\n           readUnlock();\n         }\n \n         Preconditions.checkState(\n             (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n             (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n             \"Both suite and ezKeyName should both be null or not null\");\n         // Generate EDEK if necessary while not holding the lock\n         EncryptedKeyVersion edek \u003d\n             generateEncryptedDataEncryptionKey(ezKeyName);\n         EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n         // Try to create the file with the computed cipher suite and EDEK\n         writeLock();\n         try {\n           checkOperation(OperationCategory.WRITE);\n           checkNameNodeSafeMode(\"Cannot create file\" + src);\n           src \u003d resolvePath(src, pathComponents);\n-          startFileInternal(pc, src, permissions, holder, clientMachine, create,\n-              overwrite, createParent, replication, blockSize, suite, edek,\n-              logRetryCache);\n+          toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n+              clientMachine, create, overwrite, createParent, replication, \n+              blockSize, suite, edek, logRetryCache);\n           stat \u003d dir.getFileInfo(src, false,\n               FSDirectory.isReservedRawName(srcArg));\n         } catch (StandbyException se) {\n           skipSync \u003d true;\n           throw se;\n         } catch (RetryStartFileException e) {\n           shouldContinue \u003d true;\n           if (LOG.isTraceEnabled()) {\n             LOG.trace(\"Preconditions failed, retrying creation of \" +\n                     \"FileEncryptionInfo\", e);\n           }\n         } finally {\n           writeUnlock();\n         }\n       }\n     } finally {\n       // There might be transactions logged while trying to recover the lease.\n       // They need to be sync\u0027ed even when an exception was thrown.\n       if (!skipSync) {\n         getEditLog().logSync();\n+        if (toRemoveBlocks !\u003d null) {\n+          removeBlocks(toRemoveBlocks);\n+          toRemoveBlocks.clear();\n+        }\n       }\n     }\n \n     logAuditEvent(true, \"create\", srcArg, null, stat);\n     return stat;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private HdfsFileStatus startFileInt(final String srcArg,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet\u003cCreateFlag\u003e flag, boolean createParent, short replication,\n      long blockSize, List\u003cCipherSuite\u003e cipherSuites, boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    String src \u003d srcArg;\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      StringBuilder builder \u003d new StringBuilder();\n      builder.append(\"DIR* NameSystem.startFile: src\u003d\" + src\n              + \", holder\u003d\" + holder\n              + \", clientMachine\u003d\" + clientMachine\n              + \", createParent\u003d\" + createParent\n              + \", replication\u003d\" + replication\n              + \", createFlag\u003d\" + flag.toString()\n              + \", blockSize\u003d\" + blockSize);\n      builder.append(\", cipherSuites\u003d\");\n      if (cipherSuites !\u003d null) {\n        builder.append(Arrays.toString(cipherSuites.toArray()));\n      } else {\n        builder.append(\"null\");\n      }\n      NameNode.stateChangeLog.debug(builder.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync \u003d false;\n    HdfsFileStatus stat \u003d null;\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize \u003c minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" \u003c \" + minBlockSize);\n    }\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create \u003d flag.contains(CreateFlag.CREATE);\n    boolean overwrite \u003d flag.contains(CreateFlag.OVERWRITE);\n\n    waitForLoadingFSImage();\n\n    /*\n     * We want to avoid holding any locks while doing KeyProvider operations,\n     * since they can be very slow. Since the path can\n     * flip flop between being in an encryption zone and not in the meantime,\n     * we need to recheck the preconditions and redo KeyProvider operations\n     * in some situations.\n     *\n     * A special RetryStartFileException is used to indicate that we should\n     * retry creation of a FileEncryptionInfo.\n     */\n    BlocksMapUpdateInfo toRemoveBlocks \u003d null;\n    try {\n      boolean shouldContinue \u003d true;\n      int iters \u003d 0;\n      while (shouldContinue) {\n        skipSync \u003d false;\n        if (iters \u003e\u003d 10) {\n          throw new IOException(\"Too many retries because of encryption zone \" +\n              \"operations, something might be broken!\");\n        }\n        shouldContinue \u003d false;\n        iters++;\n\n        // Optimistically determine CipherSuite and ezKeyName if the path is\n        // currently within an encryption zone\n        CipherSuite suite \u003d null;\n        String ezKeyName \u003d null;\n        readLock();\n        try {\n          src \u003d resolvePath(src, pathComponents);\n          INodesInPath iip \u003d dir.getINodesInPath4Write(src);\n          // Nothing to do if the path is not within an EZ\n          if (dir.isInAnEZ(iip)) {\n            suite \u003d chooseCipherSuite(iip, cipherSuites);\n            if (suite !\u003d null) {\n              Preconditions.checkArgument(!suite.equals(CipherSuite.UNKNOWN),\n                  \"Chose an UNKNOWN CipherSuite!\");\n            }\n            ezKeyName \u003d dir.getKeyName(iip);\n            Preconditions.checkState(ezKeyName !\u003d null);\n          }\n        } finally {\n          readUnlock();\n        }\n\n        Preconditions.checkState(\n            (suite \u003d\u003d null \u0026\u0026 ezKeyName \u003d\u003d null) ||\n            (suite !\u003d null \u0026\u0026 ezKeyName !\u003d null),\n            \"Both suite and ezKeyName should both be null or not null\");\n        // Generate EDEK if necessary while not holding the lock\n        EncryptedKeyVersion edek \u003d\n            generateEncryptedDataEncryptionKey(ezKeyName);\n        EncryptionFaultInjector.getInstance().startFileAfterGenerateKey();\n        // Try to create the file with the computed cipher suite and EDEK\n        writeLock();\n        try {\n          checkOperation(OperationCategory.WRITE);\n          checkNameNodeSafeMode(\"Cannot create file\" + src);\n          src \u003d resolvePath(src, pathComponents);\n          toRemoveBlocks \u003d startFileInternal(pc, src, permissions, holder, \n              clientMachine, create, overwrite, createParent, replication, \n              blockSize, suite, edek, logRetryCache);\n          stat \u003d dir.getFileInfo(src, false,\n              FSDirectory.isReservedRawName(srcArg));\n        } catch (StandbyException se) {\n          skipSync \u003d true;\n          throw se;\n        } catch (RetryStartFileException e) {\n          shouldContinue \u003d true;\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Preconditions failed, retrying creation of \" +\n                    \"FileEncryptionInfo\", e);\n          }\n        } finally {\n          writeUnlock();\n        }\n      }\n    } finally {\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync\u0027ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n        if (toRemoveBlocks !\u003d null) {\n          removeBlocks(toRemoveBlocks);\n          toRemoveBlocks.clear();\n        }\n      }\n    }\n\n    logAuditEvent(true, \"create\", srcArg, null, stat);\n    return stat;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    }
  }
}