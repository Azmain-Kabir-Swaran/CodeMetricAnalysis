{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FileSystemAccessService.java",
  "functionName": "postInit",
  "functionId": "postInit",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.java",
  "functionStartLine": 228,
  "functionEndLine": 251,
  "numCommitsSeen": 14,
  "timeTaken": 707,
  "changeHistory": [
    "fe17d871d0ad4a71aa009d10499ac5d8174e7788",
    "3334306512b5dc932814fded31a89ba1ee97cd9f"
  ],
  "changeHistoryShort": {
    "fe17d871d0ad4a71aa009d10499ac5d8174e7788": "Ybodychange",
    "3334306512b5dc932814fded31a89ba1ee97cd9f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "fe17d871d0ad4a71aa009d10499ac5d8174e7788": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3513. HttpFS should cache filesystems. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1368304 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/12 4:09 PM",
      "commitName": "fe17d871d0ad4a71aa009d10499ac5d8174e7788",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "18/04/12 11:51 AM",
      "commitNameOld": "8bda086d046b12e8efed834f39a775e710ca0962",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 105.18,
      "commitsBetweenForRepo": 575,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,24 @@\n   public void postInit() throws ServiceException {\n     super.postInit();\n     Instrumentation instrumentation \u003d getServer().get(Instrumentation.class);\n     instrumentation.addVariable(INSTRUMENTATION_GROUP, \"unmanaged.fs\", new Instrumentation.Variable\u003cInteger\u003e() {\n       @Override\n       public Integer getValue() {\n         return unmanagedFileSystems.get();\n       }\n     });\n     instrumentation.addSampler(INSTRUMENTATION_GROUP, \"unmanaged.fs\", 60, new Instrumentation.Variable\u003cLong\u003e() {\n       @Override\n       public Long getValue() {\n         return (long) unmanagedFileSystems.get();\n       }\n     });\n+    Scheduler scheduler \u003d getServer().get(Scheduler.class);\n+    int purgeInterval \u003d getServiceConfig().getInt(FS_CACHE_PURGE_FREQUENCY, 60);\n+    purgeTimeout \u003d getServiceConfig().getLong(FS_CACHE_PURGE_TIMEOUT, 60);\n+    purgeTimeout \u003d (purgeTimeout \u003e 0) ? purgeTimeout : 0;\n+    if (purgeTimeout \u003e 0) {\n+      scheduler.schedule(new FileSystemCachePurger(),\n+                         purgeInterval, purgeInterval, TimeUnit.SECONDS);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void postInit() throws ServiceException {\n    super.postInit();\n    Instrumentation instrumentation \u003d getServer().get(Instrumentation.class);\n    instrumentation.addVariable(INSTRUMENTATION_GROUP, \"unmanaged.fs\", new Instrumentation.Variable\u003cInteger\u003e() {\n      @Override\n      public Integer getValue() {\n        return unmanagedFileSystems.get();\n      }\n    });\n    instrumentation.addSampler(INSTRUMENTATION_GROUP, \"unmanaged.fs\", 60, new Instrumentation.Variable\u003cLong\u003e() {\n      @Override\n      public Long getValue() {\n        return (long) unmanagedFileSystems.get();\n      }\n    });\n    Scheduler scheduler \u003d getServer().get(Scheduler.class);\n    int purgeInterval \u003d getServiceConfig().getInt(FS_CACHE_PURGE_FREQUENCY, 60);\n    purgeTimeout \u003d getServiceConfig().getLong(FS_CACHE_PURGE_TIMEOUT, 60);\n    purgeTimeout \u003d (purgeTimeout \u003e 0) ? purgeTimeout : 0;\n    if (purgeTimeout \u003e 0) {\n      scheduler.schedule(new FileSystemCachePurger(),\n                         purgeInterval, purgeInterval, TimeUnit.SECONDS);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.java",
      "extendedDetails": {}
    },
    "3334306512b5dc932814fded31a89ba1ee97cd9f": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2178. Contributing Hoop to HDFS, replacement for HDFS proxy with read/write capabilities. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212060 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/12/11 11:25 AM",
      "commitName": "3334306512b5dc932814fded31a89ba1ee97cd9f",
      "commitAuthor": "Alejandro Abdelnur",
      "diff": "@@ -0,0 +1,16 @@\n+  public void postInit() throws ServiceException {\n+    super.postInit();\n+    Instrumentation instrumentation \u003d getServer().get(Instrumentation.class);\n+    instrumentation.addVariable(INSTRUMENTATION_GROUP, \"unmanaged.fs\", new Instrumentation.Variable\u003cInteger\u003e() {\n+      @Override\n+      public Integer getValue() {\n+        return unmanagedFileSystems.get();\n+      }\n+    });\n+    instrumentation.addSampler(INSTRUMENTATION_GROUP, \"unmanaged.fs\", 60, new Instrumentation.Variable\u003cLong\u003e() {\n+      @Override\n+      public Long getValue() {\n+        return (long) unmanagedFileSystems.get();\n+      }\n+    });\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void postInit() throws ServiceException {\n    super.postInit();\n    Instrumentation instrumentation \u003d getServer().get(Instrumentation.class);\n    instrumentation.addVariable(INSTRUMENTATION_GROUP, \"unmanaged.fs\", new Instrumentation.Variable\u003cInteger\u003e() {\n      @Override\n      public Integer getValue() {\n        return unmanagedFileSystems.get();\n      }\n    });\n    instrumentation.addSampler(INSTRUMENTATION_GROUP, \"unmanaged.fs\", 60, new Instrumentation.Variable\u003cLong\u003e() {\n      @Override\n      public Long getValue() {\n        return (long) unmanagedFileSystems.get();\n      }\n    });\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.java"
    }
  }
}