{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSEditLog.java",
  "functionName": "startLogSegment",
  "functionId": "startLogSegment___segmentTxId-long(modifiers-final)__layoutVersion-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
  "functionStartLine": 1387,
  "functionEndLine": 1425,
  "numCommitsSeen": 287,
  "timeTaken": 6611,
  "changeHistory": [
    "042c8ef593ced1915a688e99aa9a6a52fdf66734",
    "e620530301fd3e62537d4b7bc3d8ed296bda1ffc",
    "71de367c5e80ea76d1e8d21f0216cd6b879dcee5",
    "9dab514b22f49322738935cfd915c2b4eba50b88",
    "841fdc5628fbba341efe0bfc6763fe12e7fca7f4",
    "706394d03992b394e9f907aff2155df493e4ea4e",
    "6be13332db5342465c2f279a5984b4b8a33420fc",
    "1ae5b5e338ef383c5642e2f04b927871c7b184f6",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63"
  ],
  "changeHistoryShort": {
    "042c8ef593ced1915a688e99aa9a6a52fdf66734": "Ybodychange",
    "e620530301fd3e62537d4b7bc3d8ed296bda1ffc": "Ybodychange",
    "71de367c5e80ea76d1e8d21f0216cd6b879dcee5": "Ymultichange(Yparameterchange,Ybodychange)",
    "9dab514b22f49322738935cfd915c2b4eba50b88": "Ybodychange",
    "841fdc5628fbba341efe0bfc6763fe12e7fca7f4": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "706394d03992b394e9f907aff2155df493e4ea4e": "Ybodychange",
    "6be13332db5342465c2f279a5984b4b8a33420fc": "Ybodychange",
    "1ae5b5e338ef383c5642e2f04b927871c7b184f6": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": "Yintroduced"
  },
  "changeHistoryDetails": {
    "042c8ef593ced1915a688e99aa9a6a52fdf66734": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14075. Terminate the namenode when failed to start log segment. Contributed by Ayush Saxena.\n",
      "commitDate": "01/12/18 11:01 PM",
      "commitName": "042c8ef593ced1915a688e99aa9a6a52fdf66734",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "23/08/18 7:44 PM",
      "commitNameOld": "96c4575d7373079becfa3e3db29ba98e6fb86388",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 100.18,
      "commitsBetweenForRepo": 885,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,39 @@\n   private void startLogSegment(final long segmentTxId, int layoutVersion)\n       throws IOException {\n     assert Thread.holdsLock(this);\n \n     LOG.info(\"Starting log segment at \" + segmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003e 0,\n         \"Bad txid: %s\", segmentTxId);\n     Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n         \"Bad state: %s\", state);\n     Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n         \"Cannot start writing to log segment \" + segmentTxId +\n         \" when previous log segment started at \" + curSegmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n         \"Cannot start log segment at txid %s when next expected \" +\n         \"txid is %s\", segmentTxId, txid + 1);\n     \n     numTransactions \u003d 0;\n     totalTimeTransactions \u003d 0;\n     numTransactionsBatchedInSync.set(0L);\n \n     // TODO no need to link this back to storage anymore!\n     // See HDFS-2174.\n     storage.attemptRestoreRemovedStorage();\n     \n     try {\n       editLogStream \u003d journalSet.startLogSegment(segmentTxId, layoutVersion);\n     } catch (IOException ex) {\n-      throw new IOException(\"Unable to start log segment \" +\n-          segmentTxId + \": too few journals successfully started.\", ex);\n+      final String msg \u003d \"Unable to start log segment \" + segmentTxId\n+          + \": too few journals successfully started.\";\n+      LOG.error(msg, ex);\n+      synchronized (journalSetLock) {\n+        IOUtils.cleanupWithLogger(LOG, journalSet);\n+      }\n+      terminate(1, msg);\n     }\n-    \n+\n     curSegmentTxId \u003d segmentTxId;\n     state \u003d State.IN_SEGMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void startLogSegment(final long segmentTxId, int layoutVersion)\n      throws IOException {\n    assert Thread.holdsLock(this);\n\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d 0;\n    totalTimeTransactions \u003d 0;\n    numTransactionsBatchedInSync.set(0L);\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    try {\n      editLogStream \u003d journalSet.startLogSegment(segmentTxId, layoutVersion);\n    } catch (IOException ex) {\n      final String msg \u003d \"Unable to start log segment \" + segmentTxId\n          + \": too few journals successfully started.\";\n      LOG.error(msg, ex);\n      synchronized (journalSetLock) {\n        IOUtils.cleanupWithLogger(LOG, journalSet);\n      }\n      terminate(1, msg);\n    }\n\n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {}
    },
    "e620530301fd3e62537d4b7bc3d8ed296bda1ffc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10485. Fix findbugs warning in FSEditLog.java. (aajisaka)\n",
      "commitDate": "07/06/16 1:52 AM",
      "commitName": "e620530301fd3e62537d4b7bc3d8ed296bda1ffc",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "29/02/16 3:34 PM",
      "commitNameOld": "2151716832ad14932dd65b1a4e47e64d8d6cd767",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 98.39,
      "commitsBetweenForRepo": 619,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,34 @@\n   private void startLogSegment(final long segmentTxId, int layoutVersion)\n       throws IOException {\n     assert Thread.holdsLock(this);\n \n     LOG.info(\"Starting log segment at \" + segmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003e 0,\n         \"Bad txid: %s\", segmentTxId);\n     Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n         \"Bad state: %s\", state);\n     Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n         \"Cannot start writing to log segment \" + segmentTxId +\n         \" when previous log segment started at \" + curSegmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n         \"Cannot start log segment at txid %s when next expected \" +\n         \"txid is %s\", segmentTxId, txid + 1);\n     \n-    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n+    numTransactions \u003d 0;\n+    totalTimeTransactions \u003d 0;\n+    numTransactionsBatchedInSync.set(0L);\n \n     // TODO no need to link this back to storage anymore!\n     // See HDFS-2174.\n     storage.attemptRestoreRemovedStorage();\n     \n     try {\n       editLogStream \u003d journalSet.startLogSegment(segmentTxId, layoutVersion);\n     } catch (IOException ex) {\n       throw new IOException(\"Unable to start log segment \" +\n           segmentTxId + \": too few journals successfully started.\", ex);\n     }\n     \n     curSegmentTxId \u003d segmentTxId;\n     state \u003d State.IN_SEGMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void startLogSegment(final long segmentTxId, int layoutVersion)\n      throws IOException {\n    assert Thread.holdsLock(this);\n\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d 0;\n    totalTimeTransactions \u003d 0;\n    numTransactionsBatchedInSync.set(0L);\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    try {\n      editLogStream \u003d journalSet.startLogSegment(segmentTxId, layoutVersion);\n    } catch (IOException ex) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": too few journals successfully started.\", ex);\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {}
    },
    "71de367c5e80ea76d1e8d21f0216cd6b879dcee5": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8432. Introduce a minimum compatible layout version to allow downgrade in more rolling upgrade use cases. Contributed by Chris Nauroth.\n",
      "commitDate": "06/06/15 9:43 AM",
      "commitName": "71de367c5e80ea76d1e8d21f0216cd6b879dcee5",
      "commitAuthor": "cnauroth",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8432. Introduce a minimum compatible layout version to allow downgrade in more rolling upgrade use cases. Contributed by Chris Nauroth.\n",
          "commitDate": "06/06/15 9:43 AM",
          "commitName": "71de367c5e80ea76d1e8d21f0216cd6b879dcee5",
          "commitAuthor": "cnauroth",
          "commitDateOld": "27/05/15 3:42 PM",
          "commitNameOld": "4928f5473394981829e5ffd4b16ea0801baf5c45",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 9.75,
          "commitsBetweenForRepo": 88,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,32 +1,32 @@\n-  private void startLogSegment(final long segmentTxId) throws IOException {\n+  private void startLogSegment(final long segmentTxId, int layoutVersion)\n+      throws IOException {\n     assert Thread.holdsLock(this);\n \n     LOG.info(\"Starting log segment at \" + segmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003e 0,\n         \"Bad txid: %s\", segmentTxId);\n     Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n         \"Bad state: %s\", state);\n     Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n         \"Cannot start writing to log segment \" + segmentTxId +\n         \" when previous log segment started at \" + curSegmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n         \"Cannot start log segment at txid %s when next expected \" +\n         \"txid is %s\", segmentTxId, txid + 1);\n     \n     numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n \n     // TODO no need to link this back to storage anymore!\n     // See HDFS-2174.\n     storage.attemptRestoreRemovedStorage();\n     \n     try {\n-      editLogStream \u003d journalSet.startLogSegment(segmentTxId,\n-          NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);\n+      editLogStream \u003d journalSet.startLogSegment(segmentTxId, layoutVersion);\n     } catch (IOException ex) {\n       throw new IOException(\"Unable to start log segment \" +\n           segmentTxId + \": too few journals successfully started.\", ex);\n     }\n     \n     curSegmentTxId \u003d segmentTxId;\n     state \u003d State.IN_SEGMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void startLogSegment(final long segmentTxId, int layoutVersion)\n      throws IOException {\n    assert Thread.holdsLock(this);\n\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    try {\n      editLogStream \u003d journalSet.startLogSegment(segmentTxId, layoutVersion);\n    } catch (IOException ex) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": too few journals successfully started.\", ex);\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {
            "oldValue": "[segmentTxId-long(modifiers-final)]",
            "newValue": "[segmentTxId-long(modifiers-final), layoutVersion-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8432. Introduce a minimum compatible layout version to allow downgrade in more rolling upgrade use cases. Contributed by Chris Nauroth.\n",
          "commitDate": "06/06/15 9:43 AM",
          "commitName": "71de367c5e80ea76d1e8d21f0216cd6b879dcee5",
          "commitAuthor": "cnauroth",
          "commitDateOld": "27/05/15 3:42 PM",
          "commitNameOld": "4928f5473394981829e5ffd4b16ea0801baf5c45",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 9.75,
          "commitsBetweenForRepo": 88,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,32 +1,32 @@\n-  private void startLogSegment(final long segmentTxId) throws IOException {\n+  private void startLogSegment(final long segmentTxId, int layoutVersion)\n+      throws IOException {\n     assert Thread.holdsLock(this);\n \n     LOG.info(\"Starting log segment at \" + segmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003e 0,\n         \"Bad txid: %s\", segmentTxId);\n     Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n         \"Bad state: %s\", state);\n     Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n         \"Cannot start writing to log segment \" + segmentTxId +\n         \" when previous log segment started at \" + curSegmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n         \"Cannot start log segment at txid %s when next expected \" +\n         \"txid is %s\", segmentTxId, txid + 1);\n     \n     numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n \n     // TODO no need to link this back to storage anymore!\n     // See HDFS-2174.\n     storage.attemptRestoreRemovedStorage();\n     \n     try {\n-      editLogStream \u003d journalSet.startLogSegment(segmentTxId,\n-          NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);\n+      editLogStream \u003d journalSet.startLogSegment(segmentTxId, layoutVersion);\n     } catch (IOException ex) {\n       throw new IOException(\"Unable to start log segment \" +\n           segmentTxId + \": too few journals successfully started.\", ex);\n     }\n     \n     curSegmentTxId \u003d segmentTxId;\n     state \u003d State.IN_SEGMENT;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void startLogSegment(final long segmentTxId, int layoutVersion)\n      throws IOException {\n    assert Thread.holdsLock(this);\n\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    try {\n      editLogStream \u003d journalSet.startLogSegment(segmentTxId, layoutVersion);\n    } catch (IOException ex) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": too few journals successfully started.\", ex);\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {}
        }
      ]
    },
    "9dab514b22f49322738935cfd915c2b4eba50b88": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6038. Allow JournalNode to handle editlog produced by new release with future layoutversion. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1579813 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/03/14 4:06 PM",
      "commitName": "9dab514b22f49322738935cfd915c2b4eba50b88",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "19/02/14 11:57 AM",
      "commitNameOld": "72c214c89bed15e1c4b97c1f922d564f54491fed",
      "commitAuthorOld": "",
      "daysBetweenCommits": 29.13,
      "commitsBetweenForRepo": 272,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,32 @@\n   private void startLogSegment(final long segmentTxId) throws IOException {\n     assert Thread.holdsLock(this);\n \n     LOG.info(\"Starting log segment at \" + segmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003e 0,\n         \"Bad txid: %s\", segmentTxId);\n     Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n         \"Bad state: %s\", state);\n     Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n         \"Cannot start writing to log segment \" + segmentTxId +\n         \" when previous log segment started at \" + curSegmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n         \"Cannot start log segment at txid %s when next expected \" +\n         \"txid is %s\", segmentTxId, txid + 1);\n     \n     numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n \n     // TODO no need to link this back to storage anymore!\n     // See HDFS-2174.\n     storage.attemptRestoreRemovedStorage();\n     \n     try {\n-      editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n+      editLogStream \u003d journalSet.startLogSegment(segmentTxId,\n+          NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);\n     } catch (IOException ex) {\n       throw new IOException(\"Unable to start log segment \" +\n           segmentTxId + \": too few journals successfully started.\", ex);\n     }\n     \n     curSegmentTxId \u003d segmentTxId;\n     state \u003d State.IN_SEGMENT;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void startLogSegment(final long segmentTxId) throws IOException {\n    assert Thread.holdsLock(this);\n\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    try {\n      editLogStream \u003d journalSet.startLogSegment(segmentTxId,\n          NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);\n    } catch (IOException ex) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": too few journals successfully started.\", ex);\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {}
    },
    "841fdc5628fbba341efe0bfc6763fe12e7fca7f4": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-3273. Refactor BackupImage and FSEditLog, and rename JournalListener.rollLogs(..) to startLogSegment(..). \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1326016 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/04/12 4:41 PM",
      "commitName": "841fdc5628fbba341efe0bfc6763fe12e7fca7f4",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-3273. Refactor BackupImage and FSEditLog, and rename JournalListener.rollLogs(..) to startLogSegment(..). \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1326016 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/04/12 4:41 PM",
          "commitName": "841fdc5628fbba341efe0bfc6763fe12e7fca7f4",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "12/04/12 2:11 PM",
          "commitNameOld": "07a436744588d131d8ef31abab3093aa59b4d531",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 1.1,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,31 @@\n-  synchronized void startLogSegment(final long segmentTxId,\n-      boolean writeHeaderTxn) throws IOException {\n+  private void startLogSegment(final long segmentTxId) throws IOException {\n+    assert Thread.holdsLock(this);\n+\n     LOG.info(\"Starting log segment at \" + segmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003e 0,\n         \"Bad txid: %s\", segmentTxId);\n     Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n         \"Bad state: %s\", state);\n     Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n         \"Cannot start writing to log segment \" + segmentTxId +\n         \" when previous log segment started at \" + curSegmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n         \"Cannot start log segment at txid %s when next expected \" +\n         \"txid is %s\", segmentTxId, txid + 1);\n     \n     numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n \n     // TODO no need to link this back to storage anymore!\n     // See HDFS-2174.\n     storage.attemptRestoreRemovedStorage();\n     \n     try {\n       editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n     } catch (IOException ex) {\n       throw new IOException(\"Unable to start log segment \" +\n           segmentTxId + \": too few journals successfully started.\", ex);\n     }\n     \n     curSegmentTxId \u003d segmentTxId;\n     state \u003d State.IN_SEGMENT;\n-\n-    if (writeHeaderTxn) {\n-      logEdit(LogSegmentOp.getInstance(cache.get(),\n-          FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n-      logSync();\n-    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void startLogSegment(final long segmentTxId) throws IOException {\n    assert Thread.holdsLock(this);\n\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    try {\n      editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n    } catch (IOException ex) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": too few journals successfully started.\", ex);\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {
            "oldValue": "[segmentTxId-long(modifiers-final), writeHeaderTxn-boolean]",
            "newValue": "[segmentTxId-long(modifiers-final)]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-3273. Refactor BackupImage and FSEditLog, and rename JournalListener.rollLogs(..) to startLogSegment(..). \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1326016 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/04/12 4:41 PM",
          "commitName": "841fdc5628fbba341efe0bfc6763fe12e7fca7f4",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "12/04/12 2:11 PM",
          "commitNameOld": "07a436744588d131d8ef31abab3093aa59b4d531",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 1.1,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,31 @@\n-  synchronized void startLogSegment(final long segmentTxId,\n-      boolean writeHeaderTxn) throws IOException {\n+  private void startLogSegment(final long segmentTxId) throws IOException {\n+    assert Thread.holdsLock(this);\n+\n     LOG.info(\"Starting log segment at \" + segmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003e 0,\n         \"Bad txid: %s\", segmentTxId);\n     Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n         \"Bad state: %s\", state);\n     Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n         \"Cannot start writing to log segment \" + segmentTxId +\n         \" when previous log segment started at \" + curSegmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n         \"Cannot start log segment at txid %s when next expected \" +\n         \"txid is %s\", segmentTxId, txid + 1);\n     \n     numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n \n     // TODO no need to link this back to storage anymore!\n     // See HDFS-2174.\n     storage.attemptRestoreRemovedStorage();\n     \n     try {\n       editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n     } catch (IOException ex) {\n       throw new IOException(\"Unable to start log segment \" +\n           segmentTxId + \": too few journals successfully started.\", ex);\n     }\n     \n     curSegmentTxId \u003d segmentTxId;\n     state \u003d State.IN_SEGMENT;\n-\n-    if (writeHeaderTxn) {\n-      logEdit(LogSegmentOp.getInstance(cache.get(),\n-          FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n-      logSync();\n-    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void startLogSegment(final long segmentTxId) throws IOException {\n    assert Thread.holdsLock(this);\n\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    try {\n      editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n    } catch (IOException ex) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": too few journals successfully started.\", ex);\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {
            "oldValue": "[synchronized]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3273. Refactor BackupImage and FSEditLog, and rename JournalListener.rollLogs(..) to startLogSegment(..). \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1326016 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/04/12 4:41 PM",
          "commitName": "841fdc5628fbba341efe0bfc6763fe12e7fca7f4",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "12/04/12 2:11 PM",
          "commitNameOld": "07a436744588d131d8ef31abab3093aa59b4d531",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 1.1,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,31 @@\n-  synchronized void startLogSegment(final long segmentTxId,\n-      boolean writeHeaderTxn) throws IOException {\n+  private void startLogSegment(final long segmentTxId) throws IOException {\n+    assert Thread.holdsLock(this);\n+\n     LOG.info(\"Starting log segment at \" + segmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003e 0,\n         \"Bad txid: %s\", segmentTxId);\n     Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n         \"Bad state: %s\", state);\n     Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n         \"Cannot start writing to log segment \" + segmentTxId +\n         \" when previous log segment started at \" + curSegmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n         \"Cannot start log segment at txid %s when next expected \" +\n         \"txid is %s\", segmentTxId, txid + 1);\n     \n     numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n \n     // TODO no need to link this back to storage anymore!\n     // See HDFS-2174.\n     storage.attemptRestoreRemovedStorage();\n     \n     try {\n       editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n     } catch (IOException ex) {\n       throw new IOException(\"Unable to start log segment \" +\n           segmentTxId + \": too few journals successfully started.\", ex);\n     }\n     \n     curSegmentTxId \u003d segmentTxId;\n     state \u003d State.IN_SEGMENT;\n-\n-    if (writeHeaderTxn) {\n-      logEdit(LogSegmentOp.getInstance(cache.get(),\n-          FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n-      logSync();\n-    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void startLogSegment(final long segmentTxId) throws IOException {\n    assert Thread.holdsLock(this);\n\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    try {\n      editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n    } catch (IOException ex) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": too few journals successfully started.\", ex);\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
          "extendedDetails": {}
        }
      ]
    },
    "706394d03992b394e9f907aff2155df493e4ea4e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3004. Implement Recovery Mode. Contributed by Colin Patrick McCabe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1311394 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/04/12 12:39 PM",
      "commitName": "706394d03992b394e9f907aff2155df493e4ea4e",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "02/04/12 5:55 PM",
      "commitNameOld": "e449de0526ce0aa58bdd0f513b0e2a744a4bbda1",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 6.78,
      "commitsBetweenForRepo": 66,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   synchronized void startLogSegment(final long segmentTxId,\n       boolean writeHeaderTxn) throws IOException {\n     LOG.info(\"Starting log segment at \" + segmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003e 0,\n         \"Bad txid: %s\", segmentTxId);\n     Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n         \"Bad state: %s\", state);\n     Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n         \"Cannot start writing to log segment \" + segmentTxId +\n         \" when previous log segment started at \" + curSegmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n         \"Cannot start log segment at txid %s when next expected \" +\n         \"txid is %s\", segmentTxId, txid + 1);\n     \n     numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n \n     // TODO no need to link this back to storage anymore!\n     // See HDFS-2174.\n     storage.attemptRestoreRemovedStorage();\n     \n     try {\n       editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n     } catch (IOException ex) {\n       throw new IOException(\"Unable to start log segment \" +\n           segmentTxId + \": too few journals successfully started.\", ex);\n     }\n     \n     curSegmentTxId \u003d segmentTxId;\n     state \u003d State.IN_SEGMENT;\n \n     if (writeHeaderTxn) {\n-      logEdit(LogSegmentOp.getInstance(\n+      logEdit(LogSegmentOp.getInstance(cache.get(),\n           FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n       logSync();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void startLogSegment(final long segmentTxId,\n      boolean writeHeaderTxn) throws IOException {\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    try {\n      editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n    } catch (IOException ex) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": too few journals successfully started.\", ex);\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n\n    if (writeHeaderTxn) {\n      logEdit(LogSegmentOp.getInstance(cache.get(),\n          FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n      logSync();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {}
    },
    "6be13332db5342465c2f279a5984b4b8a33420fc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2853. HA: NN fails to start if the shared edits dir is marked required. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1238134 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/01/12 6:17 PM",
      "commitName": "6be13332db5342465c2f279a5984b4b8a33420fc",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "11/01/12 12:32 AM",
      "commitNameOld": "4f1bf2fe23e53ff4b8550882d19f2cf1dd477926",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 19.74,
      "commitsBetweenForRepo": 121,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   synchronized void startLogSegment(final long segmentTxId,\n       boolean writeHeaderTxn) throws IOException {\n     LOG.info(\"Starting log segment at \" + segmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003e 0,\n         \"Bad txid: %s\", segmentTxId);\n     Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n         \"Bad state: %s\", state);\n     Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n         \"Cannot start writing to log segment \" + segmentTxId +\n         \" when previous log segment started at \" + curSegmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n         \"Cannot start log segment at txid %s when next expected \" +\n         \"txid is %s\", segmentTxId, txid + 1);\n     \n     numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n \n     // TODO no need to link this back to storage anymore!\n     // See HDFS-2174.\n     storage.attemptRestoreRemovedStorage();\n     \n     try {\n       editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n     } catch (IOException ex) {\n       throw new IOException(\"Unable to start log segment \" +\n-          segmentTxId + \": no journals successfully started.\");\n+          segmentTxId + \": too few journals successfully started.\", ex);\n     }\n     \n     curSegmentTxId \u003d segmentTxId;\n     state \u003d State.IN_SEGMENT;\n \n     if (writeHeaderTxn) {\n       logEdit(LogSegmentOp.getInstance(\n           FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n       logSync();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void startLogSegment(final long segmentTxId,\n      boolean writeHeaderTxn) throws IOException {\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    try {\n      editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n    } catch (IOException ex) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": too few journals successfully started.\", ex);\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n\n    if (writeHeaderTxn) {\n      logEdit(LogSegmentOp.getInstance(\n          FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n      logSync();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {}
    },
    "1ae5b5e338ef383c5642e2f04b927871c7b184f6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2158. Add JournalSet to manage the set of journals.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1177473 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/09/11 6:14 PM",
      "commitName": "1ae5b5e338ef383c5642e2f04b927871c7b184f6",
      "commitAuthor": "Jitendra Nath Pandey",
      "commitDateOld": "06/09/11 1:27 PM",
      "commitNameOld": "bdc3720d5b67a1c8fc2dfb29be16e4155c0e7f15",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 23.2,
      "commitsBetweenForRepo": 169,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,36 @@\n   synchronized void startLogSegment(final long segmentTxId,\n       boolean writeHeaderTxn) throws IOException {\n     LOG.info(\"Starting log segment at \" + segmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003e 0,\n         \"Bad txid: %s\", segmentTxId);\n     Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n         \"Bad state: %s\", state);\n     Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n         \"Cannot start writing to log segment \" + segmentTxId +\n         \" when previous log segment started at \" + curSegmentTxId);\n     Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n         \"Cannot start log segment at txid %s when next expected \" +\n         \"txid is %s\", segmentTxId, txid + 1);\n     \n     numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n \n     // TODO no need to link this back to storage anymore!\n     // See HDFS-2174.\n     storage.attemptRestoreRemovedStorage();\n     \n-    mapJournalsAndReportErrors(new JournalClosure() {\n-      @Override\n-      public void apply(JournalAndStream jas) throws IOException {\n-        jas.startLogSegment(segmentTxId);\n-      }\n-    }, \"starting log segment \" + segmentTxId);\n-\n-    if (countActiveJournals() \u003d\u003d 0) {\n+    try {\n+      editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n+    } catch (IOException ex) {\n       throw new IOException(\"Unable to start log segment \" +\n           segmentTxId + \": no journals successfully started.\");\n     }\n     \n     curSegmentTxId \u003d segmentTxId;\n     state \u003d State.IN_SEGMENT;\n \n     if (writeHeaderTxn) {\n       logEdit(LogSegmentOp.getInstance(\n           FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n       logSync();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void startLogSegment(final long segmentTxId,\n      boolean writeHeaderTxn) throws IOException {\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    try {\n      editLogStream \u003d journalSet.startLogSegment(segmentTxId);\n    } catch (IOException ex) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": no journals successfully started.\");\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n\n    if (writeHeaderTxn) {\n      logEdit(LogSegmentOp.getInstance(\n          FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n      logSync();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  synchronized void startLogSegment(final long segmentTxId,\n      boolean writeHeaderTxn) throws IOException {\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    mapJournalsAndReportErrors(new JournalClosure() {\n      @Override\n      public void apply(JournalAndStream jas) throws IOException {\n        jas.startLogSegment(segmentTxId);\n      }\n    }, \"starting log segment \" + segmentTxId);\n\n    if (countActiveJournals() \u003d\u003d 0) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": no journals successfully started.\");\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n\n    if (writeHeaderTxn) {\n      logEdit(LogSegmentOp.getInstance(\n          FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n      logSync();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  synchronized void startLogSegment(final long segmentTxId,\n      boolean writeHeaderTxn) throws IOException {\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    mapJournalsAndReportErrors(new JournalClosure() {\n      @Override\n      public void apply(JournalAndStream jas) throws IOException {\n        jas.startLogSegment(segmentTxId);\n      }\n    }, \"starting log segment \" + segmentTxId);\n\n    if (countActiveJournals() \u003d\u003d 0) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": no journals successfully started.\");\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n\n    if (writeHeaderTxn) {\n      logEdit(LogSegmentOp.getInstance(\n          FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n      logSync();\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java"
      }
    },
    "28e6a4e44a3e920dcaf858f9a74a6358226b3a63": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-1073. Redesign the NameNode\u0027s storage layout for image checkpoints and edit logs to introduce transaction IDs and be more robust. Contributed by Todd Lipcon and Ivan Kelly.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1152295 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/07/11 9:28 AM",
      "commitName": "28e6a4e44a3e920dcaf858f9a74a6358226b3a63",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,41 @@\n+  synchronized void startLogSegment(final long segmentTxId,\n+      boolean writeHeaderTxn) throws IOException {\n+    LOG.info(\"Starting log segment at \" + segmentTxId);\n+    Preconditions.checkArgument(segmentTxId \u003e 0,\n+        \"Bad txid: %s\", segmentTxId);\n+    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n+        \"Bad state: %s\", state);\n+    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n+        \"Cannot start writing to log segment \" + segmentTxId +\n+        \" when previous log segment started at \" + curSegmentTxId);\n+    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n+        \"Cannot start log segment at txid %s when next expected \" +\n+        \"txid is %s\", segmentTxId, txid + 1);\n+    \n+    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n+\n+    // TODO no need to link this back to storage anymore!\n+    // See HDFS-2174.\n+    storage.attemptRestoreRemovedStorage();\n+    \n+    mapJournalsAndReportErrors(new JournalClosure() {\n+      @Override\n+      public void apply(JournalAndStream jas) throws IOException {\n+        jas.startLogSegment(segmentTxId);\n+      }\n+    }, \"starting log segment \" + segmentTxId);\n+\n+    if (countActiveJournals() \u003d\u003d 0) {\n+      throw new IOException(\"Unable to start log segment \" +\n+          segmentTxId + \": no journals successfully started.\");\n+    }\n+    \n+    curSegmentTxId \u003d segmentTxId;\n+    state \u003d State.IN_SEGMENT;\n+\n+    if (writeHeaderTxn) {\n+      logEdit(LogSegmentOp.getInstance(\n+          FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n+      logSync();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void startLogSegment(final long segmentTxId,\n      boolean writeHeaderTxn) throws IOException {\n    LOG.info(\"Starting log segment at \" + segmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003e 0,\n        \"Bad txid: %s\", segmentTxId);\n    Preconditions.checkState(state \u003d\u003d State.BETWEEN_LOG_SEGMENTS,\n        \"Bad state: %s\", state);\n    Preconditions.checkState(segmentTxId \u003e curSegmentTxId,\n        \"Cannot start writing to log segment \" + segmentTxId +\n        \" when previous log segment started at \" + curSegmentTxId);\n    Preconditions.checkArgument(segmentTxId \u003d\u003d txid + 1,\n        \"Cannot start log segment at txid %s when next expected \" +\n        \"txid is %s\", segmentTxId, txid + 1);\n    \n    numTransactions \u003d totalTimeTransactions \u003d numTransactionsBatchedInSync \u003d 0;\n\n    // TODO no need to link this back to storage anymore!\n    // See HDFS-2174.\n    storage.attemptRestoreRemovedStorage();\n    \n    mapJournalsAndReportErrors(new JournalClosure() {\n      @Override\n      public void apply(JournalAndStream jas) throws IOException {\n        jas.startLogSegment(segmentTxId);\n      }\n    }, \"starting log segment \" + segmentTxId);\n\n    if (countActiveJournals() \u003d\u003d 0) {\n      throw new IOException(\"Unable to start log segment \" +\n          segmentTxId + \": no journals successfully started.\");\n    }\n    \n    curSegmentTxId \u003d segmentTxId;\n    state \u003d State.IN_SEGMENT;\n\n    if (writeHeaderTxn) {\n      logEdit(LogSegmentOp.getInstance(\n          FSEditLogOpCodes.OP_START_LOG_SEGMENT));\n      logSync();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java"
    }
  }
}