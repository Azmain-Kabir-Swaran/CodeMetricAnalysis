{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirDeleteOp.java",
  "functionName": "deleteInternal",
  "functionId": "deleteInternal___fsn-FSNamesystem__iip-INodesInPath__logRetryCache-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
  "functionStartLine": 170,
  "functionEndLine": 200,
  "numCommitsSeen": 786,
  "timeTaken": 41520,
  "changeHistory": [
    "3565c9af17ab05bf9e7f68b71b6c6850df772bb9",
    "3dadf369d550c2ae393b751cb5a184dbfe2814df",
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
    "c79e7f7d997596e0c38ae4cddff2bd0910581c16",
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
    "c95b878abf313507666ea018f9e6033c4c166e10",
    "407bb3d3e452c8277c498dd14e0cc5b7762a7091",
    "8044a12ac02cc4495935a3afded8c1d4369c1445",
    "c38665282884122d3c82b6f68376cce036aee748",
    "2a3bccddd939ee0d6941aa2d22edc67dea85fe35",
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d",
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
    "e9c6840b24bab7d6c21243baa9b2353119b0f976",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
    "1fe1942328856dd832e9f94fb56a40ab3d810870",
    "6431192c0ee00ecfe578b270889b0c7a0a9cb8c8",
    "8c7a7e619699386f9e6991842558d78aa0c8053d",
    "1b531c1dbb452a6192fad411605d2baaa3831bcd",
    "4cd605413a4e82375a2b905075cb067b93ae1a2e",
    "6521b5ee423ef489d7b7f85e74dd5f8d91bd06aa",
    "92e0416ced279a910616985bf11fa3f8b1b1de9b",
    "0b101bd7e875ee5597ddb8f0d887159076310ffa",
    "980e6c54bab4ffc87e168cd5c217fef44c72a878",
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c",
    "cdb292f44caff9763631d9e9bcd69c375a7cddea",
    "1734215a10fd93e38849ed0235b5e026b7f50f83",
    "0e796b61e829c4bf763caf13b0f53cb1bcefdeee",
    "ff91453227488902d8c730d69c90fb1a97b35d75",
    "a70bc6c6a268e774306acb71550e84e85a989fc6",
    "36d1c49486587c2dbb193e8538b1d4510c462fa6",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "3565c9af17ab05bf9e7f68b71b6c6850df772bb9": "Ymultichange(Yparameterchange,Ybodychange)",
    "3dadf369d550c2ae393b751cb5a184dbfe2814df": "Ybodychange",
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63": "Ybodychange",
    "c79e7f7d997596e0c38ae4cddff2bd0910581c16": "Ybodychange",
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Yexceptionschange,Ybodychange,Yparameterchange)",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": "Ybodychange",
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2": "Ybodychange",
    "c95b878abf313507666ea018f9e6033c4c166e10": "Ybodychange",
    "407bb3d3e452c8277c498dd14e0cc5b7762a7091": "Ybodychange",
    "8044a12ac02cc4495935a3afded8c1d4369c1445": "Ybodychange",
    "c38665282884122d3c82b6f68376cce036aee748": "Ybodychange",
    "2a3bccddd939ee0d6941aa2d22edc67dea85fe35": "Ybodychange",
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d": "Ybodychange",
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec": "Ybodychange",
    "e9c6840b24bab7d6c21243baa9b2353119b0f976": "Ybodychange",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": "Ybodychange",
    "1fe1942328856dd832e9f94fb56a40ab3d810870": "Ybodychange",
    "6431192c0ee00ecfe578b270889b0c7a0a9cb8c8": "Ybodychange",
    "8c7a7e619699386f9e6991842558d78aa0c8053d": "Ymultichange(Yparameterchange,Ybodychange)",
    "1b531c1dbb452a6192fad411605d2baaa3831bcd": "Ybodychange",
    "4cd605413a4e82375a2b905075cb067b93ae1a2e": "Ybodychange",
    "6521b5ee423ef489d7b7f85e74dd5f8d91bd06aa": "Ybodychange",
    "92e0416ced279a910616985bf11fa3f8b1b1de9b": "Ybodychange",
    "0b101bd7e875ee5597ddb8f0d887159076310ffa": "Ybodychange",
    "980e6c54bab4ffc87e168cd5c217fef44c72a878": "Ybodychange",
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c": "Ybodychange",
    "cdb292f44caff9763631d9e9bcd69c375a7cddea": "Ybodychange",
    "1734215a10fd93e38849ed0235b5e026b7f50f83": "Ybodychange",
    "0e796b61e829c4bf763caf13b0f53cb1bcefdeee": "Ybodychange",
    "ff91453227488902d8c730d69c90fb1a97b35d75": "Ybodychange",
    "a70bc6c6a268e774306acb71550e84e85a989fc6": "Ybodychange",
    "36d1c49486587c2dbb193e8538b1d4510c462fa6": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3565c9af17ab05bf9e7f68b71b6c6850df772bb9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10979. Pass IIP for FSDirDeleteOp methods. Contributed by Daryn Sharp.\n",
      "commitDate": "07/10/16 12:15 PM",
      "commitName": "3565c9af17ab05bf9e7f68b71b6c6850df772bb9",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10979. Pass IIP for FSDirDeleteOp methods. Contributed by Daryn Sharp.\n",
          "commitDate": "07/10/16 12:15 PM",
          "commitName": "3565c9af17ab05bf9e7f68b71b6c6850df772bb9",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "04/10/16 1:05 PM",
          "commitNameOld": "44f48ee96ee6b2a3909911c37bfddb0c963d5ffc",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 2.97,
          "commitsBetweenForRepo": 30,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,35 +1,31 @@\n   static BlocksMapUpdateInfo deleteInternal(\n-      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n+      FSNamesystem fsn, INodesInPath iip, boolean logRetryCache)\n       throws IOException {\n     assert fsn.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n-    }\n-\n-    if (FSDirectory.isExactReservedName(src)) {\n-      throw new InvalidPathException(src);\n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + iip.getPath());\n     }\n \n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n     List\u003cLong\u003e removedUCFiles \u003d new ChunkedArrayList\u003c\u003e();\n \n     long mtime \u003d now();\n     // Unlink the target directory from directory tree\n     long filesRemoved \u003d delete(\n         fsd, iip, collectedBlocks, removedINodes, removedUCFiles, mtime);\n     if (filesRemoved \u003c 0) {\n       return null;\n     }\n-    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n+    fsd.getEditLog().logDelete(iip.getPath(), mtime, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n \n     fsn.removeLeasesAndINodes(removedUCFiles, removedINodes, true);\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n-                                        + src +\" is removed\");\n+      NameNode.stateChangeLog.debug(\n+          \"DIR* Namesystem.delete: \" + iip.getPath() +\" is removed\");\n     }\n     return collectedBlocks;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo deleteInternal(\n      FSNamesystem fsn, INodesInPath iip, boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + iip.getPath());\n    }\n\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n    List\u003cLong\u003e removedUCFiles \u003d new ChunkedArrayList\u003c\u003e();\n\n    long mtime \u003d now();\n    // Unlink the target directory from directory tree\n    long filesRemoved \u003d delete(\n        fsd, iip, collectedBlocks, removedINodes, removedUCFiles, mtime);\n    if (filesRemoved \u003c 0) {\n      return null;\n    }\n    fsd.getEditLog().logDelete(iip.getPath(), mtime, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n\n    fsn.removeLeasesAndINodes(removedUCFiles, removedINodes, true);\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\n          \"DIR* Namesystem.delete: \" + iip.getPath() +\" is removed\");\n    }\n    return collectedBlocks;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "[fsn-FSNamesystem, src-String, iip-INodesInPath, logRetryCache-boolean]",
            "newValue": "[fsn-FSNamesystem, iip-INodesInPath, logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10979. Pass IIP for FSDirDeleteOp methods. Contributed by Daryn Sharp.\n",
          "commitDate": "07/10/16 12:15 PM",
          "commitName": "3565c9af17ab05bf9e7f68b71b6c6850df772bb9",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "04/10/16 1:05 PM",
          "commitNameOld": "44f48ee96ee6b2a3909911c37bfddb0c963d5ffc",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 2.97,
          "commitsBetweenForRepo": 30,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,35 +1,31 @@\n   static BlocksMapUpdateInfo deleteInternal(\n-      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n+      FSNamesystem fsn, INodesInPath iip, boolean logRetryCache)\n       throws IOException {\n     assert fsn.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n-    }\n-\n-    if (FSDirectory.isExactReservedName(src)) {\n-      throw new InvalidPathException(src);\n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + iip.getPath());\n     }\n \n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n     List\u003cLong\u003e removedUCFiles \u003d new ChunkedArrayList\u003c\u003e();\n \n     long mtime \u003d now();\n     // Unlink the target directory from directory tree\n     long filesRemoved \u003d delete(\n         fsd, iip, collectedBlocks, removedINodes, removedUCFiles, mtime);\n     if (filesRemoved \u003c 0) {\n       return null;\n     }\n-    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n+    fsd.getEditLog().logDelete(iip.getPath(), mtime, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n \n     fsn.removeLeasesAndINodes(removedUCFiles, removedINodes, true);\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n-                                        + src +\" is removed\");\n+      NameNode.stateChangeLog.debug(\n+          \"DIR* Namesystem.delete: \" + iip.getPath() +\" is removed\");\n     }\n     return collectedBlocks;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo deleteInternal(\n      FSNamesystem fsn, INodesInPath iip, boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + iip.getPath());\n    }\n\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n    List\u003cLong\u003e removedUCFiles \u003d new ChunkedArrayList\u003c\u003e();\n\n    long mtime \u003d now();\n    // Unlink the target directory from directory tree\n    long filesRemoved \u003d delete(\n        fsd, iip, collectedBlocks, removedINodes, removedUCFiles, mtime);\n    if (filesRemoved \u003c 0) {\n      return null;\n    }\n    fsd.getEditLog().logDelete(iip.getPath(), mtime, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n\n    fsn.removeLeasesAndINodes(removedUCFiles, removedINodes, true);\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\n          \"DIR* Namesystem.delete: \" + iip.getPath() +\" is removed\");\n    }\n    return collectedBlocks;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "3dadf369d550c2ae393b751cb5a184dbfe2814df": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7087. Ability to list /.reserved. Contributed by Xiao Chen.\n",
      "commitDate": "21/10/15 4:58 PM",
      "commitName": "3dadf369d550c2ae393b751cb5a184dbfe2814df",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "29/08/15 9:52 AM",
      "commitNameOld": "bdbe53c676dd4ff135ea2f64d3b9193fe43d7c8e",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 53.3,
      "commitsBetweenForRepo": 392,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,35 @@\n   static BlocksMapUpdateInfo deleteInternal(\n       FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n       throws IOException {\n     assert fsn.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n     }\n \n+    if (FSDirectory.isExactReservedName(src)) {\n+      throw new InvalidPathException(src);\n+    }\n+\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n     List\u003cLong\u003e removedUCFiles \u003d new ChunkedArrayList\u003c\u003e();\n \n     long mtime \u003d now();\n     // Unlink the target directory from directory tree\n     long filesRemoved \u003d delete(\n         fsd, iip, collectedBlocks, removedINodes, removedUCFiles, mtime);\n     if (filesRemoved \u003c 0) {\n       return null;\n     }\n     fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n \n     fsn.removeLeasesAndINodes(removedUCFiles, removedINodes, true);\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                         + src +\" is removed\");\n     }\n     return collectedBlocks;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlocksMapUpdateInfo deleteInternal(\n      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n    }\n\n    if (FSDirectory.isExactReservedName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n    List\u003cLong\u003e removedUCFiles \u003d new ChunkedArrayList\u003c\u003e();\n\n    long mtime \u003d now();\n    // Unlink the target directory from directory tree\n    long filesRemoved \u003d delete(\n        fsd, iip, collectedBlocks, removedINodes, removedUCFiles, mtime);\n    if (filesRemoved \u003c 0) {\n      return null;\n    }\n    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n\n    fsn.removeLeasesAndINodes(removedUCFiles, removedINodes, true);\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                        + src +\" is removed\");\n    }\n    return collectedBlocks;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6757. Simplify lease manager with INodeID. Contributed by Haohui Mai.\n",
      "commitDate": "08/05/15 11:04 PM",
      "commitName": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/04/15 6:05 PM",
      "commitNameOld": "c79e7f7d997596e0c38ae4cddff2bd0910581c16",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 10.21,
      "commitsBetweenForRepo": 163,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,31 @@\n   static BlocksMapUpdateInfo deleteInternal(\n       FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n       throws IOException {\n     assert fsn.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n     }\n \n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n+    List\u003cLong\u003e removedUCFiles \u003d new ChunkedArrayList\u003c\u003e();\n \n     long mtime \u003d now();\n     // Unlink the target directory from directory tree\n     long filesRemoved \u003d delete(\n-        fsd, iip, collectedBlocks, removedINodes, mtime);\n+        fsd, iip, collectedBlocks, removedINodes, removedUCFiles, mtime);\n     if (filesRemoved \u003c 0) {\n       return null;\n     }\n     fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n \n-    fsn.removeLeasesAndINodes(src, removedINodes, true);\n+    fsn.removeLeasesAndINodes(removedUCFiles, removedINodes, true);\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                         + src +\" is removed\");\n     }\n     return collectedBlocks;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlocksMapUpdateInfo deleteInternal(\n      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n    }\n\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n    List\u003cLong\u003e removedUCFiles \u003d new ChunkedArrayList\u003c\u003e();\n\n    long mtime \u003d now();\n    // Unlink the target directory from directory tree\n    long filesRemoved \u003d delete(\n        fsd, iip, collectedBlocks, removedINodes, removedUCFiles, mtime);\n    if (filesRemoved \u003c 0) {\n      return null;\n    }\n    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n\n    fsn.removeLeasesAndINodes(removedUCFiles, removedINodes, true);\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                        + src +\" is removed\");\n    }\n    return collectedBlocks;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "c79e7f7d997596e0c38ae4cddff2bd0910581c16": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8273. FSNamesystem#Delete() should not call logSync() when holding the lock. Contributed by Haohui Mai.\n",
      "commitDate": "28/04/15 6:05 PM",
      "commitName": "c79e7f7d997596e0c38ae4cddff2bd0910581c16",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/04/15 2:06 AM",
      "commitNameOld": "5112477d9e1f1ebc7d91757924c4bdc6eabc35a9",
      "commitAuthorOld": "Tsuyoshi Ozawa",
      "daysBetweenCommits": 9.67,
      "commitsBetweenForRepo": 95,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,30 @@\n   static BlocksMapUpdateInfo deleteInternal(\n       FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n       throws IOException {\n     assert fsn.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n     }\n \n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n \n     long mtime \u003d now();\n     // Unlink the target directory from directory tree\n     long filesRemoved \u003d delete(\n         fsd, iip, collectedBlocks, removedINodes, mtime);\n     if (filesRemoved \u003c 0) {\n       return null;\n     }\n     fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n     incrDeletedFileCount(filesRemoved);\n \n     fsn.removeLeasesAndINodes(src, removedINodes, true);\n-    fsd.getEditLog().logSync();\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                         + src +\" is removed\");\n     }\n     return collectedBlocks;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlocksMapUpdateInfo deleteInternal(\n      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n    }\n\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n\n    long mtime \u003d now();\n    // Unlink the target directory from directory tree\n    long filesRemoved \u003d delete(\n        fsd, iip, collectedBlocks, removedINodes, mtime);\n    if (filesRemoved \u003c 0) {\n      return null;\n    }\n    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n\n    fsn.removeLeasesAndINodes(src, removedINodes, true);\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                        + src +\" is removed\");\n    }\n    return collectedBlocks;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
      "extendedDetails": {}
    },
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Yexceptionschange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "17/01/15 12:56 PM",
      "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,31 @@\n-  private boolean deleteInternal(String src, boolean recursive,\n-      boolean enforcePermission, boolean logRetryCache)\n-      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n-             IOException {\n-    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n-    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n-    FSPermissionChecker pc \u003d getPermissionChecker();\n-    checkOperation(OperationCategory.WRITE);\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    boolean ret \u003d false;\n-\n-    waitForLoadingFSImage();\n-    writeLock();\n-    try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot delete \" + src);\n-      src \u003d dir.resolvePath(pc, src, pathComponents);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src, false);\n-      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(iip)) {\n-        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n-      }\n-      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n-        dir.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n-            FsAction.ALL, true);\n-      }\n-\n-      long mtime \u003d now();\n-      // Unlink the target directory from directory tree\n-      long filesRemoved \u003d dir.delete(iip, collectedBlocks, removedINodes,\n-              mtime);\n-      if (filesRemoved \u003c 0) {\n-        return false;\n-      }\n-      getEditLog().logDelete(src, mtime, logRetryCache);\n-      incrDeletedFileCount(filesRemoved);\n-      // Blocks/INodes will be handled later\n-      removePathAndBlocks(src, null, removedINodes, true);\n-      ret \u003d true;\n-    } finally {\n-      writeUnlock();\n+  static BlocksMapUpdateInfo deleteInternal(\n+      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n+      throws IOException {\n+    assert fsn.hasWriteLock();\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n     }\n-    getEditLog().logSync(); \n-    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n-    collectedBlocks.clear();\n+\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n+    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n+\n+    long mtime \u003d now();\n+    // Unlink the target directory from directory tree\n+    long filesRemoved \u003d delete(\n+        fsd, iip, collectedBlocks, removedINodes, mtime);\n+    if (filesRemoved \u003c 0) {\n+      return null;\n+    }\n+    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n+    incrDeletedFileCount(filesRemoved);\n+\n+    fsn.removeLeasesAndINodes(src, removedINodes, true);\n+    fsd.getEditLog().logSync();\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n-        + src +\" is removed\");\n+                                        + src +\" is removed\");\n     }\n-    return ret;\n+    return collectedBlocks;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo deleteInternal(\n      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n    }\n\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n\n    long mtime \u003d now();\n    // Unlink the target directory from directory tree\n    long filesRemoved \u003d delete(\n        fsd, iip, collectedBlocks, removedINodes, mtime);\n    if (filesRemoved \u003c 0) {\n      return null;\n    }\n    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n\n    fsn.removeLeasesAndINodes(src, removedINodes, true);\n    fsd.getEditLog().logSync();\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                        + src +\" is removed\");\n    }\n    return collectedBlocks;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
            "oldMethodName": "deleteInternal",
            "newMethodName": "deleteInternal"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,31 @@\n-  private boolean deleteInternal(String src, boolean recursive,\n-      boolean enforcePermission, boolean logRetryCache)\n-      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n-             IOException {\n-    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n-    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n-    FSPermissionChecker pc \u003d getPermissionChecker();\n-    checkOperation(OperationCategory.WRITE);\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    boolean ret \u003d false;\n-\n-    waitForLoadingFSImage();\n-    writeLock();\n-    try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot delete \" + src);\n-      src \u003d dir.resolvePath(pc, src, pathComponents);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src, false);\n-      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(iip)) {\n-        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n-      }\n-      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n-        dir.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n-            FsAction.ALL, true);\n-      }\n-\n-      long mtime \u003d now();\n-      // Unlink the target directory from directory tree\n-      long filesRemoved \u003d dir.delete(iip, collectedBlocks, removedINodes,\n-              mtime);\n-      if (filesRemoved \u003c 0) {\n-        return false;\n-      }\n-      getEditLog().logDelete(src, mtime, logRetryCache);\n-      incrDeletedFileCount(filesRemoved);\n-      // Blocks/INodes will be handled later\n-      removePathAndBlocks(src, null, removedINodes, true);\n-      ret \u003d true;\n-    } finally {\n-      writeUnlock();\n+  static BlocksMapUpdateInfo deleteInternal(\n+      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n+      throws IOException {\n+    assert fsn.hasWriteLock();\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n     }\n-    getEditLog().logSync(); \n-    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n-    collectedBlocks.clear();\n+\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n+    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n+\n+    long mtime \u003d now();\n+    // Unlink the target directory from directory tree\n+    long filesRemoved \u003d delete(\n+        fsd, iip, collectedBlocks, removedINodes, mtime);\n+    if (filesRemoved \u003c 0) {\n+      return null;\n+    }\n+    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n+    incrDeletedFileCount(filesRemoved);\n+\n+    fsn.removeLeasesAndINodes(src, removedINodes, true);\n+    fsd.getEditLog().logSync();\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n-        + src +\" is removed\");\n+                                        + src +\" is removed\");\n     }\n-    return ret;\n+    return collectedBlocks;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo deleteInternal(\n      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n    }\n\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n\n    long mtime \u003d now();\n    // Unlink the target directory from directory tree\n    long filesRemoved \u003d delete(\n        fsd, iip, collectedBlocks, removedINodes, mtime);\n    if (filesRemoved \u003c 0) {\n      return null;\n    }\n    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n\n    fsn.removeLeasesAndINodes(src, removedINodes, true);\n    fsd.getEditLog().logSync();\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                        + src +\" is removed\");\n    }\n    return collectedBlocks;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "boolean",
            "newValue": "BlocksMapUpdateInfo"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,31 @@\n-  private boolean deleteInternal(String src, boolean recursive,\n-      boolean enforcePermission, boolean logRetryCache)\n-      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n-             IOException {\n-    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n-    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n-    FSPermissionChecker pc \u003d getPermissionChecker();\n-    checkOperation(OperationCategory.WRITE);\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    boolean ret \u003d false;\n-\n-    waitForLoadingFSImage();\n-    writeLock();\n-    try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot delete \" + src);\n-      src \u003d dir.resolvePath(pc, src, pathComponents);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src, false);\n-      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(iip)) {\n-        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n-      }\n-      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n-        dir.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n-            FsAction.ALL, true);\n-      }\n-\n-      long mtime \u003d now();\n-      // Unlink the target directory from directory tree\n-      long filesRemoved \u003d dir.delete(iip, collectedBlocks, removedINodes,\n-              mtime);\n-      if (filesRemoved \u003c 0) {\n-        return false;\n-      }\n-      getEditLog().logDelete(src, mtime, logRetryCache);\n-      incrDeletedFileCount(filesRemoved);\n-      // Blocks/INodes will be handled later\n-      removePathAndBlocks(src, null, removedINodes, true);\n-      ret \u003d true;\n-    } finally {\n-      writeUnlock();\n+  static BlocksMapUpdateInfo deleteInternal(\n+      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n+      throws IOException {\n+    assert fsn.hasWriteLock();\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n     }\n-    getEditLog().logSync(); \n-    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n-    collectedBlocks.clear();\n+\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n+    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n+\n+    long mtime \u003d now();\n+    // Unlink the target directory from directory tree\n+    long filesRemoved \u003d delete(\n+        fsd, iip, collectedBlocks, removedINodes, mtime);\n+    if (filesRemoved \u003c 0) {\n+      return null;\n+    }\n+    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n+    incrDeletedFileCount(filesRemoved);\n+\n+    fsn.removeLeasesAndINodes(src, removedINodes, true);\n+    fsd.getEditLog().logSync();\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n-        + src +\" is removed\");\n+                                        + src +\" is removed\");\n     }\n-    return ret;\n+    return collectedBlocks;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo deleteInternal(\n      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n    }\n\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n\n    long mtime \u003d now();\n    // Unlink the target directory from directory tree\n    long filesRemoved \u003d delete(\n        fsd, iip, collectedBlocks, removedINodes, mtime);\n    if (filesRemoved \u003c 0) {\n      return null;\n    }\n    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n\n    fsn.removeLeasesAndINodes(src, removedINodes, true);\n    fsd.getEditLog().logSync();\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                        + src +\" is removed\");\n    }\n    return collectedBlocks;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,31 @@\n-  private boolean deleteInternal(String src, boolean recursive,\n-      boolean enforcePermission, boolean logRetryCache)\n-      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n-             IOException {\n-    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n-    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n-    FSPermissionChecker pc \u003d getPermissionChecker();\n-    checkOperation(OperationCategory.WRITE);\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    boolean ret \u003d false;\n-\n-    waitForLoadingFSImage();\n-    writeLock();\n-    try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot delete \" + src);\n-      src \u003d dir.resolvePath(pc, src, pathComponents);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src, false);\n-      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(iip)) {\n-        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n-      }\n-      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n-        dir.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n-            FsAction.ALL, true);\n-      }\n-\n-      long mtime \u003d now();\n-      // Unlink the target directory from directory tree\n-      long filesRemoved \u003d dir.delete(iip, collectedBlocks, removedINodes,\n-              mtime);\n-      if (filesRemoved \u003c 0) {\n-        return false;\n-      }\n-      getEditLog().logDelete(src, mtime, logRetryCache);\n-      incrDeletedFileCount(filesRemoved);\n-      // Blocks/INodes will be handled later\n-      removePathAndBlocks(src, null, removedINodes, true);\n-      ret \u003d true;\n-    } finally {\n-      writeUnlock();\n+  static BlocksMapUpdateInfo deleteInternal(\n+      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n+      throws IOException {\n+    assert fsn.hasWriteLock();\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n     }\n-    getEditLog().logSync(); \n-    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n-    collectedBlocks.clear();\n+\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n+    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n+\n+    long mtime \u003d now();\n+    // Unlink the target directory from directory tree\n+    long filesRemoved \u003d delete(\n+        fsd, iip, collectedBlocks, removedINodes, mtime);\n+    if (filesRemoved \u003c 0) {\n+      return null;\n+    }\n+    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n+    incrDeletedFileCount(filesRemoved);\n+\n+    fsn.removeLeasesAndINodes(src, removedINodes, true);\n+    fsd.getEditLog().logSync();\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n-        + src +\" is removed\");\n+                                        + src +\" is removed\");\n     }\n-    return ret;\n+    return collectedBlocks;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo deleteInternal(\n      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n    }\n\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n\n    long mtime \u003d now();\n    // Unlink the target directory from directory tree\n    long filesRemoved \u003d delete(\n        fsd, iip, collectedBlocks, removedINodes, mtime);\n    if (filesRemoved \u003c 0) {\n      return null;\n    }\n    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n\n    fsn.removeLeasesAndINodes(src, removedINodes, true);\n    fsd.getEditLog().logSync();\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                        + src +\" is removed\");\n    }\n    return collectedBlocks;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "[AccessControlException, SafeModeException, UnresolvedLinkException, IOException]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,31 @@\n-  private boolean deleteInternal(String src, boolean recursive,\n-      boolean enforcePermission, boolean logRetryCache)\n-      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n-             IOException {\n-    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n-    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n-    FSPermissionChecker pc \u003d getPermissionChecker();\n-    checkOperation(OperationCategory.WRITE);\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    boolean ret \u003d false;\n-\n-    waitForLoadingFSImage();\n-    writeLock();\n-    try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot delete \" + src);\n-      src \u003d dir.resolvePath(pc, src, pathComponents);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src, false);\n-      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(iip)) {\n-        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n-      }\n-      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n-        dir.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n-            FsAction.ALL, true);\n-      }\n-\n-      long mtime \u003d now();\n-      // Unlink the target directory from directory tree\n-      long filesRemoved \u003d dir.delete(iip, collectedBlocks, removedINodes,\n-              mtime);\n-      if (filesRemoved \u003c 0) {\n-        return false;\n-      }\n-      getEditLog().logDelete(src, mtime, logRetryCache);\n-      incrDeletedFileCount(filesRemoved);\n-      // Blocks/INodes will be handled later\n-      removePathAndBlocks(src, null, removedINodes, true);\n-      ret \u003d true;\n-    } finally {\n-      writeUnlock();\n+  static BlocksMapUpdateInfo deleteInternal(\n+      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n+      throws IOException {\n+    assert fsn.hasWriteLock();\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n     }\n-    getEditLog().logSync(); \n-    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n-    collectedBlocks.clear();\n+\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n+    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n+\n+    long mtime \u003d now();\n+    // Unlink the target directory from directory tree\n+    long filesRemoved \u003d delete(\n+        fsd, iip, collectedBlocks, removedINodes, mtime);\n+    if (filesRemoved \u003c 0) {\n+      return null;\n+    }\n+    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n+    incrDeletedFileCount(filesRemoved);\n+\n+    fsn.removeLeasesAndINodes(src, removedINodes, true);\n+    fsd.getEditLog().logSync();\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n-        + src +\" is removed\");\n+                                        + src +\" is removed\");\n     }\n-    return ret;\n+    return collectedBlocks;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo deleteInternal(\n      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n    }\n\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n\n    long mtime \u003d now();\n    // Unlink the target directory from directory tree\n    long filesRemoved \u003d delete(\n        fsd, iip, collectedBlocks, removedINodes, mtime);\n    if (filesRemoved \u003c 0) {\n      return null;\n    }\n    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n\n    fsn.removeLeasesAndINodes(src, removedINodes, true);\n    fsd.getEditLog().logSync();\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                        + src +\" is removed\");\n    }\n    return collectedBlocks;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/01/15 10:40 AM",
          "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,31 @@\n-  private boolean deleteInternal(String src, boolean recursive,\n-      boolean enforcePermission, boolean logRetryCache)\n-      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n-             IOException {\n-    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n-    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n-    FSPermissionChecker pc \u003d getPermissionChecker();\n-    checkOperation(OperationCategory.WRITE);\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n-    boolean ret \u003d false;\n-\n-    waitForLoadingFSImage();\n-    writeLock();\n-    try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot delete \" + src);\n-      src \u003d dir.resolvePath(pc, src, pathComponents);\n-      final INodesInPath iip \u003d dir.getINodesInPath4Write(src, false);\n-      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(iip)) {\n-        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n-      }\n-      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n-        dir.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n-            FsAction.ALL, true);\n-      }\n-\n-      long mtime \u003d now();\n-      // Unlink the target directory from directory tree\n-      long filesRemoved \u003d dir.delete(iip, collectedBlocks, removedINodes,\n-              mtime);\n-      if (filesRemoved \u003c 0) {\n-        return false;\n-      }\n-      getEditLog().logDelete(src, mtime, logRetryCache);\n-      incrDeletedFileCount(filesRemoved);\n-      // Blocks/INodes will be handled later\n-      removePathAndBlocks(src, null, removedINodes, true);\n-      ret \u003d true;\n-    } finally {\n-      writeUnlock();\n+  static BlocksMapUpdateInfo deleteInternal(\n+      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n+      throws IOException {\n+    assert fsn.hasWriteLock();\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n     }\n-    getEditLog().logSync(); \n-    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n-    collectedBlocks.clear();\n+\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n+    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n+\n+    long mtime \u003d now();\n+    // Unlink the target directory from directory tree\n+    long filesRemoved \u003d delete(\n+        fsd, iip, collectedBlocks, removedINodes, mtime);\n+    if (filesRemoved \u003c 0) {\n+      return null;\n+    }\n+    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n+    incrDeletedFileCount(filesRemoved);\n+\n+    fsn.removeLeasesAndINodes(src, removedINodes, true);\n+    fsd.getEditLog().logSync();\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n-        + src +\" is removed\");\n+                                        + src +\" is removed\");\n     }\n-    return ret;\n+    return collectedBlocks;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlocksMapUpdateInfo deleteInternal(\n      FSNamesystem fsn, String src, INodesInPath iip, boolean logRetryCache)\n      throws IOException {\n    assert fsn.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n    }\n\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003c\u003e();\n\n    long mtime \u003d now();\n    // Unlink the target directory from directory tree\n    long filesRemoved \u003d delete(\n        fsd, iip, collectedBlocks, removedINodes, mtime);\n    if (filesRemoved \u003c 0) {\n      return null;\n    }\n    fsd.getEditLog().logDelete(src, mtime, logRetryCache);\n    incrDeletedFileCount(filesRemoved);\n\n    fsn.removeLeasesAndINodes(src, removedINodes, true);\n    fsd.getEditLog().logSync();\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n                                        + src +\" is removed\");\n    }\n    return collectedBlocks;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java",
          "extendedDetails": {
            "oldValue": "[src-String, recursive-boolean, enforcePermission-boolean, logRetryCache-boolean]",
            "newValue": "[fsn-FSNamesystem, src-String, iip-INodesInPath, logRetryCache-boolean]"
          }
        }
      ]
    },
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
      "commitDate": "12/12/14 3:13 PM",
      "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/12/14 11:51 AM",
      "commitNameOld": "46612c7a5135d20b20403780b47dd00654aab057",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.14,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,51 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n \n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       src \u003d dir.resolvePath(pc, src, pathComponents);\n       final INodesInPath iip \u003d dir.getINodesInPath4Write(src, false);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(iip)) {\n         throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         dir.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n             FsAction.ALL, true);\n       }\n \n       long mtime \u003d now();\n       // Unlink the target directory from directory tree\n-      long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n+      long filesRemoved \u003d dir.delete(iip, collectedBlocks, removedINodes,\n               mtime);\n       if (filesRemoved \u003c 0) {\n         return false;\n       }\n       getEditLog().logDelete(src, mtime, logRetryCache);\n       incrDeletedFileCount(filesRemoved);\n       // Blocks/INodes will be handled later\n       removePathAndBlocks(src, null, removedINodes, true);\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d dir.resolvePath(pc, src, pathComponents);\n      final INodesInPath iip \u003d dir.getINodesInPath4Write(src, false);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(iip)) {\n        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        dir.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n            FsAction.ALL, true);\n      }\n\n      long mtime \u003d now();\n      // Unlink the target directory from directory tree\n      long filesRemoved \u003d dir.delete(iip, collectedBlocks, removedINodes,\n              mtime);\n      if (filesRemoved \u003c 0) {\n        return false;\n      }\n      getEditLog().logDelete(src, mtime, logRetryCache);\n      incrDeletedFileCount(filesRemoved);\n      // Blocks/INodes will be handled later\n      removePathAndBlocks(src, null, removedINodes, true);\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7474. Avoid resolving path in FSPermissionChecker. Contributed by Jing Zhao.\n",
      "commitDate": "05/12/14 2:17 PM",
      "commitName": "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "05/12/14 10:55 AM",
      "commitNameOld": "6a5596e3b4443462fc86f800b3c2eb839d44c3bd",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.14,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,51 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n \n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       src \u003d dir.resolvePath(pc, src, pathComponents);\n-      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n+      final INodesInPath iip \u003d dir.getINodesInPath4Write(src, false);\n+      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(iip)) {\n         throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n-        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n-            FsAction.ALL, true, false);\n+        dir.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n+            FsAction.ALL, true);\n       }\n \n       long mtime \u003d now();\n       // Unlink the target directory from directory tree\n       long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n               mtime);\n       if (filesRemoved \u003c 0) {\n         return false;\n       }\n       getEditLog().logDelete(src, mtime, logRetryCache);\n       incrDeletedFileCount(filesRemoved);\n       // Blocks/INodes will be handled later\n       removePathAndBlocks(src, null, removedINodes, true);\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d dir.resolvePath(pc, src, pathComponents);\n      final INodesInPath iip \u003d dir.getINodesInPath4Write(src, false);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(iip)) {\n        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        dir.checkPermission(pc, iip, false, null, FsAction.WRITE, null,\n            FsAction.ALL, true);\n      }\n\n      long mtime \u003d now();\n      // Unlink the target directory from directory tree\n      long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n              mtime);\n      if (filesRemoved \u003c 0) {\n        return false;\n      }\n      getEditLog().logDelete(src, mtime, logRetryCache);\n      incrDeletedFileCount(filesRemoved);\n      // Blocks/INodes will be handled later\n      removePathAndBlocks(src, null, removedINodes, true);\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "c95b878abf313507666ea018f9e6033c4c166e10": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7415. Move FSNameSystem.resolvePath() to FSDirectory. Contributed by Haohui Mai.\n",
      "commitDate": "20/11/14 7:23 PM",
      "commitName": "c95b878abf313507666ea018f9e6033c4c166e10",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "17/11/14 5:33 PM",
      "commitNameOld": "dcb8e24427b02e2f3ff9a12d2eb1eb878e3443bb",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 3.08,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,50 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n \n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n-      src \u003d resolvePath(src, pathComponents);\n+      src \u003d dir.resolvePath(pc, src, pathComponents);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, true, false);\n       }\n \n       long mtime \u003d now();\n       // Unlink the target directory from directory tree\n       long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n               mtime);\n       if (filesRemoved \u003c 0) {\n         return false;\n       }\n       getEditLog().logDelete(src, mtime, logRetryCache);\n       incrDeletedFileCount(filesRemoved);\n       // Blocks/INodes will be handled later\n       removePathAndBlocks(src, null, removedINodes, true);\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d dir.resolvePath(pc, src, pathComponents);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, true, false);\n      }\n\n      long mtime \u003d now();\n      // Unlink the target directory from directory tree\n      long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n              mtime);\n      if (filesRemoved \u003c 0) {\n        return false;\n      }\n      getEditLog().logDelete(src, mtime, logRetryCache);\n      incrDeletedFileCount(filesRemoved);\n      // Blocks/INodes will be handled later\n      removePathAndBlocks(src, null, removedINodes, true);\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "407bb3d3e452c8277c498dd14e0cc5b7762a7091": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6509. Create a special /.reserved/raw directory for raw access to encrypted data. Contributed by Charles Lamb.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1614490 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/07/14 2:11 PM",
      "commitName": "407bb3d3e452c8277c498dd14e0cc5b7762a7091",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/07/14 6:32 AM",
      "commitNameOld": "1d3e9ec935de0e5bcb6fda0b88fa69d9e9ce6595",
      "commitAuthorOld": "",
      "daysBetweenCommits": 2.32,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,50 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n \n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n-      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n+      src \u003d resolvePath(src, pathComponents);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, true, false);\n       }\n \n       long mtime \u003d now();\n       // Unlink the target directory from directory tree\n       long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n               mtime);\n       if (filesRemoved \u003c 0) {\n         return false;\n       }\n       getEditLog().logDelete(src, mtime, logRetryCache);\n       incrDeletedFileCount(filesRemoved);\n       // Blocks/INodes will be handled later\n       removePathAndBlocks(src, null, removedINodes, true);\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n \n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d resolvePath(src, pathComponents);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, true, false);\n      }\n\n      long mtime \u003d now();\n      // Unlink the target directory from directory tree\n      long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n              mtime);\n      if (filesRemoved \u003c 0) {\n        return false;\n      }\n      getEditLog().logDelete(src, mtime, logRetryCache);\n      incrDeletedFileCount(filesRemoved);\n      // Blocks/INodes will be handled later\n      removePathAndBlocks(src, null, removedINodes, true);\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "8044a12ac02cc4495935a3afded8c1d4369c1445": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6618. FSNamesystem#delete drops the FSN lock between removing INodes from the tree and deleting them from the inode map (kihwal via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1609380 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/07/14 8:49 PM",
      "commitName": "8044a12ac02cc4495935a3afded8c1d4369c1445",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "07/07/14 5:08 PM",
      "commitNameOld": "76a621ffd2d66bf012a554f4400091a92a5b473e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 2.15,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,49 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n \n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, true, false);\n       }\n       long mtime \u003d now();\n       // Unlink the target directory from directory tree\n       long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n               mtime);\n       if (filesRemoved \u003c 0) {\n         return false;\n       }\n       getEditLog().logDelete(src, mtime, logRetryCache);\n       incrDeletedFileCount(filesRemoved);\n       // Blocks/INodes will be handled later\n-      removePathAndBlocks(src, null, null);\n+      removePathAndBlocks(src, null, removedINodes, true);\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n \n-    dir.writeLock();\n-    try {\n-      dir.removeFromInodeMap(removedINodes);\n-    } finally {\n-      dir.writeUnlock();\n-    }\n-    removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, true, false);\n      }\n      long mtime \u003d now();\n      // Unlink the target directory from directory tree\n      long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n              mtime);\n      if (filesRemoved \u003c 0) {\n        return false;\n      }\n      getEditLog().logDelete(src, mtime, logRetryCache);\n      incrDeletedFileCount(filesRemoved);\n      // Blocks/INodes will be handled later\n      removePathAndBlocks(src, null, removedINodes, true);\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "c38665282884122d3c82b6f68376cce036aee748": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6516. List of Encryption Zones should be based on inodes (clamb)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1607770 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/07/14 5:24 PM",
      "commitName": "c38665282884122d3c82b6f68376cce036aee748",
      "commitAuthor": "Charles Lamb",
      "commitDateOld": "02/07/14 5:58 PM",
      "commitNameOld": "2a3bccddd939ee0d6941aa2d22edc67dea85fe35",
      "commitAuthorOld": "Charles Lamb",
      "daysBetweenCommits": 0.98,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,57 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n \n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, true, false);\n       }\n \n-      final EncryptionZone ez \u003d getEncryptionZoneForPath(src);\n-      if (ez !\u003d null) {\n-        encryptionZones.remove(src);\n-      }\n-\n       long mtime \u003d now();\n       // Unlink the target directory from directory tree\n       long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n               mtime);\n       if (filesRemoved \u003c 0) {\n         return false;\n       }\n       getEditLog().logDelete(src, mtime, logRetryCache);\n       incrDeletedFileCount(filesRemoved);\n       // Blocks/INodes will be handled later\n       removePathAndBlocks(src, null, null);\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n \n     dir.writeLock();\n     try {\n       dir.removeFromInodeMap(removedINodes);\n     } finally {\n       dir.writeUnlock();\n     }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, true, false);\n      }\n\n      long mtime \u003d now();\n      // Unlink the target directory from directory tree\n      long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n              mtime);\n      if (filesRemoved \u003c 0) {\n        return false;\n      }\n      getEditLog().logDelete(src, mtime, logRetryCache);\n      incrDeletedFileCount(filesRemoved);\n      // Blocks/INodes will be handled later\n      removePathAndBlocks(src, null, null);\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "2a3bccddd939ee0d6941aa2d22edc67dea85fe35": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6625. Remove the Delete Encryption Zone function (clamb)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1607507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/07/14 5:58 PM",
      "commitName": "2a3bccddd939ee0d6941aa2d22edc67dea85fe35",
      "commitAuthor": "Charles Lamb",
      "commitDateOld": "02/07/14 4:08 PM",
      "commitNameOld": "51b97a1396a4cb32aaa08b451985a6af236c0c4b",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,62 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n \n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, true, false);\n       }\n+\n+      final EncryptionZone ez \u003d getEncryptionZoneForPath(src);\n+      if (ez !\u003d null) {\n+        encryptionZones.remove(src);\n+      }\n+\n       long mtime \u003d now();\n       // Unlink the target directory from directory tree\n       long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n               mtime);\n       if (filesRemoved \u003c 0) {\n         return false;\n       }\n       getEditLog().logDelete(src, mtime, logRetryCache);\n       incrDeletedFileCount(filesRemoved);\n       // Blocks/INodes will be handled later\n       removePathAndBlocks(src, null, null);\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n \n     dir.writeLock();\n     try {\n       dir.removeFromInodeMap(removedINodes);\n     } finally {\n       dir.writeUnlock();\n     }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, true, false);\n      }\n\n      final EncryptionZone ez \u003d getEncryptionZoneForPath(src);\n      if (ez !\u003d null) {\n        encryptionZones.remove(src);\n      }\n\n      long mtime \u003d now();\n      // Unlink the target directory from directory tree\n      long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n              mtime);\n      if (filesRemoved \u003c 0) {\n        return false;\n      }\n      getEditLog().logDelete(src, mtime, logRetryCache);\n      incrDeletedFileCount(filesRemoved);\n      // Blocks/INodes will be handled later\n      removePathAndBlocks(src, null, null);\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6480. Move waitForReady() from FSDirectory to FSNamesystem. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603705 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/06/14 9:13 PM",
      "commitName": "a4e0ff5e052abad498595ee198b49c5310c9ec0d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "17/06/14 6:00 PM",
      "commitNameOld": "8e8a769e7f5ce806ffdf584f017512ab58cd84e8",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.13,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,56 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n+\n+    waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, true, false);\n       }\n       long mtime \u003d now();\n       // Unlink the target directory from directory tree\n       long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n               mtime);\n       if (filesRemoved \u003c 0) {\n         return false;\n       }\n       getEditLog().logDelete(src, mtime, logRetryCache);\n       incrDeletedFileCount(filesRemoved);\n       // Blocks/INodes will be handled later\n       removePathAndBlocks(src, null, null);\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n \n     dir.writeLock();\n     try {\n       dir.removeFromInodeMap(removedINodes);\n     } finally {\n       dir.writeUnlock();\n     }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, true, false);\n      }\n      long mtime \u003d now();\n      // Unlink the target directory from directory tree\n      long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n              mtime);\n      if (filesRemoved \u003c 0) {\n        return false;\n      }\n      getEditLog().logDelete(src, mtime, logRetryCache);\n      incrDeletedFileCount(filesRemoved);\n      // Blocks/INodes will be handled later\n      removePathAndBlocks(src, null, null);\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6315. Decouple recording edit logs from FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601960 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/06/14 10:22 AM",
      "commitName": "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "03/06/14 11:33 AM",
      "commitNameOld": "02fcb6b6bae7c3fe2a10b00b2a563e4098ff225e",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 7.95,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,53 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, true, false);\n       }\n+      long mtime \u003d now();\n       // Unlink the target directory from directory tree\n-      if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n+      long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n+              mtime);\n+      if (filesRemoved \u003c 0) {\n         return false;\n       }\n+      getEditLog().logDelete(src, mtime, logRetryCache);\n+      incrDeletedFileCount(filesRemoved);\n+      // Blocks/INodes will be handled later\n+      removePathAndBlocks(src, null, null);\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     dir.writeLock();\n     try {\n       dir.removeFromInodeMap(removedINodes);\n     } finally {\n       dir.writeUnlock();\n     }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, true, false);\n      }\n      long mtime \u003d now();\n      // Unlink the target directory from directory tree\n      long filesRemoved \u003d dir.delete(src, collectedBlocks, removedINodes,\n              mtime);\n      if (filesRemoved \u003c 0) {\n        return false;\n      }\n      getEditLog().logDelete(src, mtime, logRetryCache);\n      incrDeletedFileCount(filesRemoved);\n      // Blocks/INodes will be handled later\n      removePathAndBlocks(src, null, null);\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "e9c6840b24bab7d6c21243baa9b2353119b0f976": {
      "type": "Ybodychange",
      "commitMessage": "Command hdfs dfs -rm -r can\u0027t remove empty directory. Contributed by Yongjun Zhang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594036 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/05/14 10:54 AM",
      "commitName": "e9c6840b24bab7d6c21243baa9b2353119b0f976",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "09/05/14 3:36 PM",
      "commitNameOld": "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 2.8,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n-        throw new IOException(src + \" is non empty\");\n+        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n-            FsAction.ALL, false);\n+            FsAction.ALL, true, false);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n         return false;\n       }\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     dir.writeLock();\n     try {\n       dir.removeFromInodeMap(removedINodes);\n     } finally {\n       dir.writeUnlock();\n     }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new PathIsNotEmptyDirectoryException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, true, false);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n        return false;\n      }\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": {
      "type": "Ybodychange",
      "commitMessage": "merge trunk to branch HDFS-4949\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532952 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 7:14 PM",
      "commitName": "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "16/10/13 3:15 PM",
      "commitNameOld": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 0.17,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,46 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n-      if (isInSafeMode()) {\n-        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n-      }\n+      checkNameNodeSafeMode(\"Cannot delete \" + src);\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, false);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n         return false;\n       }\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     dir.writeLock();\n     try {\n       dir.removeFromInodeMap(removedINodes);\n     } finally {\n       dir.writeUnlock();\n     }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, false);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n        return false;\n      }\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "1fe1942328856dd832e9f94fb56a40ab3d810870": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5291. Standby namenode after transition to active goes into safemode. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1530112 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/10/13 4:58 PM",
      "commitName": "1fe1942328856dd832e9f94fb56a40ab3d810870",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "06/10/13 11:39 AM",
      "commitNameOld": "7317e97bd72ca30f5db37fa94389dbdb52ae079e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.22,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,46 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n-      if (isInSafeMode()) {\n-        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n-      }\n+      checkNameNodeSafeMode(\"Cannot delete \" + src);\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, false);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n         return false;\n       }\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     dir.writeLock();\n     try {\n       dir.removeFromInodeMap(removedINodes);\n     } finally {\n       dir.writeUnlock();\n     }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, false);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n        return false;\n      }\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "6431192c0ee00ecfe578b270889b0c7a0a9cb8c8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4879. Add BlockedArrayList collection to avoid CMS full GCs (Contributed by Todd Lipcon)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1520667 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/09/13 12:05 PM",
      "commitName": "6431192c0ee00ecfe578b270889b0c7a0a9cb8c8",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "05/09/13 10:23 PM",
      "commitNameOld": "a62839195548a632b1b6197456b9adf95127427c",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 0.57,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,48 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n-    List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n+    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, false);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n         return false;\n       }\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     dir.writeLock();\n     try {\n       dir.removeFromInodeMap(removedINodes);\n     } finally {\n       dir.writeUnlock();\n     }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ChunkedArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, false);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n        return false;\n      }\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "8c7a7e619699386f9e6991842558d78aa0c8053d": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/07/13 12:51 AM",
      "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/07/13 12:51 AM",
          "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "26/07/13 4:59 PM",
          "commitNameOld": "dc17bda4b677e30c02c2a9a053895a43e41f7a12",
          "commitAuthorOld": "Konstantin Boudnik",
          "daysBetweenCommits": 3.33,
          "commitsBetweenForRepo": 18,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,48 @@\n   private boolean deleteInternal(String src, boolean recursive,\n-      boolean enforcePermission)\n+      boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, false);\n       }\n       // Unlink the target directory from directory tree\n-      if (!dir.delete(src, collectedBlocks, removedINodes)) {\n+      if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n         return false;\n       }\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     dir.writeLock();\n     try {\n       dir.removeFromInodeMap(removedINodes);\n     } finally {\n       dir.writeUnlock();\n     }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, false);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n        return false;\n      }\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[src-String, recursive-boolean, enforcePermission-boolean]",
            "newValue": "[src-String, recursive-boolean, enforcePermission-boolean, logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/07/13 12:51 AM",
          "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "26/07/13 4:59 PM",
          "commitNameOld": "dc17bda4b677e30c02c2a9a053895a43e41f7a12",
          "commitAuthorOld": "Konstantin Boudnik",
          "daysBetweenCommits": 3.33,
          "commitsBetweenForRepo": 18,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,48 @@\n   private boolean deleteInternal(String src, boolean recursive,\n-      boolean enforcePermission)\n+      boolean enforcePermission, boolean logRetryCache)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     boolean ret \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, false);\n       }\n       // Unlink the target directory from directory tree\n-      if (!dir.delete(src, collectedBlocks, removedINodes)) {\n+      if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n         return false;\n       }\n       ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     dir.writeLock();\n     try {\n       dir.removeFromInodeMap(removedINodes);\n     } finally {\n       dir.writeUnlock();\n     }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, false);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n        return false;\n      }\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "1b531c1dbb452a6192fad411605d2baaa3831bcd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4979. Implement retry cache on Namenode. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1507170 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/07/13 6:09 PM",
      "commitName": "1b531c1dbb452a6192fad411605d2baaa3831bcd",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "24/07/13 5:32 PM",
      "commitNameOld": "f138ae68f9be0ae072a6a4ee50e94a1608c90edb",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.03,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,48 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n+    boolean ret \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, false);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks, removedINodes)) {\n         return false;\n       }\n+      ret \u003d true;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     dir.writeLock();\n     try {\n       dir.removeFromInodeMap(removedINodes);\n     } finally {\n       dir.writeUnlock();\n     }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n-    return true;\n+    return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, false);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes)) {\n        return false;\n      }\n      ret \u003d true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "4cd605413a4e82375a2b905075cb067b93ae1a2e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4845. FSNamesystem.deleteInternal should acquire write-lock before changing the inode map.  Contributed by Arpit Agarwal\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1492941 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/06/13 8:12 PM",
      "commitName": "4cd605413a4e82375a2b905075cb067b93ae1a2e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "11/06/13 2:03 PM",
      "commitNameOld": "390eca2cdea5a83ea02e4c795d65a506e6a00039",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.26,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,46 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null,\n             FsAction.ALL, false);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks, removedINodes)) {\n         return false;\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n-    dir.removeFromInodeMap(removedINodes);\n+    dir.writeLock();\n+    try {\n+      dir.removeFromInodeMap(removedINodes);\n+    } finally {\n+      dir.writeUnlock();\n+    }\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, false);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes)) {\n        return false;\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "6521b5ee423ef489d7b7f85e74dd5f8d91bd06aa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4765. Permission check of symlink deletion incorrectly throws UnresolvedLinkException. Contributed by Andrew Wang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1481976 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/05/13 10:22 AM",
      "commitName": "6521b5ee423ef489d7b7f85e74dd5f8d91bd06aa",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "30/04/13 4:02 PM",
      "commitNameOld": "faca77f227a52907ed278014c3a6f65f0a3e0ea1",
      "commitAuthorOld": "",
      "daysBetweenCommits": 12.76,
      "commitsBetweenForRepo": 70,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,41 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n-        checkPermission(pc, src, false, null, FsAction.WRITE, null, FsAction.ALL);\n+        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n+            FsAction.ALL, false);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks, removedINodes)) {\n         return false;\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     dir.removeFromInodeMap(removedINodes);\n     removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, false);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes)) {\n        return false;\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.removeFromInodeMap(removedINodes);\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "92e0416ced279a910616985bf11fa3f8b1b1de9b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4727. Update inodeMap after deleting files/directories/snapshots.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1470756 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/04/13 5:00 PM",
      "commitName": "92e0416ced279a910616985bf11fa3f8b1b1de9b",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "20/04/13 1:22 PM",
      "commitNameOld": "27b3f84fe58530caa2eac18e924da7de4c859578",
      "commitAuthorOld": "",
      "daysBetweenCommits": 2.15,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,40 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n+    List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null, FsAction.ALL);\n       }\n       // Unlink the target directory from directory tree\n-      if (!dir.delete(src, collectedBlocks)) {\n+      if (!dir.delete(src, collectedBlocks, removedINodes)) {\n         return false;\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n+    dir.removeFromInodeMap(removedINodes);\n+    removedINodes.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    List\u003cINode\u003e removedINodes \u003d new ArrayList\u003cINode\u003e();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes)) {\n        return false;\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.removeFromInodeMap(removedINodes);\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "0b101bd7e875ee5597ddb8f0d887159076310ffa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4434. Reverting change r1470089 that merges trunk to HDFS-2802.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1470194 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/04/13 9:57 AM",
      "commitName": "0b101bd7e875ee5597ddb8f0d887159076310ffa",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "19/04/13 5:02 PM",
      "commitNameOld": "9af0babe7ef9c4bc956b77aac250f8eee6c8450f",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.7,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,35 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n-    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n-      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null, FsAction.ALL);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks)) {\n         return false;\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "980e6c54bab4ffc87e168cd5c217fef44c72a878": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4434. Provide a mapping from INodeId to INode. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1469644 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/04/13 5:10 PM",
      "commitName": "980e6c54bab4ffc87e168cd5c217fef44c72a878",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "12/04/13 6:35 PM",
      "commitNameOld": "242028a3fb887708dea5ef557c0ded22e014ac7d",
      "commitAuthorOld": "Konstantin Shvachko",
      "daysBetweenCommits": 5.94,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,37 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n+    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n+      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null, FsAction.ALL);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks)) {\n         return false;\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      src \u003d FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4591. HA clients can fail to fail over while Standby NN is performing long checkpoint. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1456107 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/03/13 12:51 PM",
      "commitName": "3bf09c51501a23b7fa28fd0a0c4c0965858d026c",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "12/03/13 7:32 PM",
      "commitNameOld": "86a940f7adc5bd9c9eaea2283df5e014e5079ab6",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.72,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,35 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n     FSPermissionChecker pc \u003d getPermissionChecker();\n+    checkOperation(OperationCategory.WRITE);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(pc, src, false, null, FsAction.WRITE, null, FsAction.ALL);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks)) {\n         return false;\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cdb292f44caff9763631d9e9bcd69c375a7cddea": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4222. NN is unresponsive and loses heartbeats from DNs when configured to use LDAP and LDAP has issues. Contributed by Xiaobo Peng and Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448801 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/02/13 1:02 PM",
      "commitName": "cdb292f44caff9763631d9e9bcd69c375a7cddea",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "11/02/13 4:50 PM",
      "commitNameOld": "969e84decbc976bd98f1050aead695d15a024ab6",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 9.84,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n-\n+    FSPermissionChecker pc \u003d getPermissionChecker();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n-        checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n+        checkPermission(pc, src, false, null, FsAction.WRITE, null, FsAction.ALL);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks)) {\n         return false;\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n    FSPermissionChecker pc \u003d getPermissionChecker();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "1734215a10fd93e38849ed0235b5e026b7f50f83": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4152. Add a new class BlocksMapUpdateInfo for the parameter in INode.collectSubtreeBlocksAndClear(..). Contributed by Jing Zhao \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1406326 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/11/12 1:04 PM",
      "commitName": "1734215a10fd93e38849ed0235b5e026b7f50f83",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "05/11/12 3:26 PM",
      "commitNameOld": "7ee5ce3176a74d217551b5981f809a56c719424b",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.9,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n-    ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n+    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n \n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks)) {\n         return false;\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks \u003d new BlocksMapUpdateInfo();\n\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "0e796b61e829c4bf763caf13b0f53cb1bcefdeee": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4112. A few improvements on INodeDirectory include adding a utility method for casting; avoiding creation of new empty lists; cleaning up some code and rewriting some javadoc.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1402599 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/10/12 11:08 AM",
      "commitName": "0e796b61e829c4bf763caf13b0f53cb1bcefdeee",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "25/10/12 11:44 AM",
      "commitNameOld": "ba2ee1d7fb91462c861169224d250d2d90bec3a6",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.97,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n \n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n-      if (!recursive \u0026\u0026 !dir.isDirEmpty(src)) {\n+      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks)) {\n         return false;\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync(); \n     removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      if (!recursive \u0026\u0026 dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ff91453227488902d8c730d69c90fb1a97b35d75": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2938. Recursive delete of a large directory make namenode unresponsive. Contributed by Hari Mankude.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1244752 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/02/12 2:00 PM",
      "commitName": "ff91453227488902d8c730d69c90fb1a97b35d75",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/02/12 12:27 AM",
      "commitNameOld": "a87328dfab96a335535e8952e548534b73c00b7c",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 1.56,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,33 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n \n     writeLock();\n     try {\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       if (!recursive \u0026\u0026 !dir.isDirEmpty(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks)) {\n         return false;\n       }\n     } finally {\n       writeUnlock();\n     }\n-\n-    getEditLog().logSync();\n-\n-    writeLock();\n-    try {\n-      removeBlocks(collectedBlocks); // Incremental deletion of blocks\n-    } finally {\n-      writeUnlock();\n-    }\n+    getEditLog().logSync(); \n+    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     collectedBlocks.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      if (!recursive \u0026\u0026 !dir.isDirEmpty(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "a70bc6c6a268e774306acb71550e84e85a989fc6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2815. Namenode sometimes oes not come out of safemode during NN crash + restart. Contributed by Uma Maheswara Rao.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1243673 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/02/12 12:11 PM",
      "commitName": "a70bc6c6a268e774306acb71550e84e85a989fc6",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "01/02/12 9:31 PM",
      "commitNameOld": "c909aedbc14690ec35ac26e8ffa8aefa42c8435b",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 11.61,
      "commitsBetweenForRepo": 94,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,40 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n-    boolean deleteNow \u003d false;\n     ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n \n     writeLock();\n     try {\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       if (!recursive \u0026\u0026 !dir.isDirEmpty(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks)) {\n         return false;\n       }\n-      deleteNow \u003d collectedBlocks.size() \u003c\u003d BLOCK_DELETION_INCREMENT;\n-      if (deleteNow) { // Perform small deletes right away\n-        removeBlocks(collectedBlocks);\n-      }\n     } finally {\n       writeUnlock();\n     }\n \n     getEditLog().logSync();\n \n     writeLock();\n     try {\n-      if (!deleteNow) {\n-        removeBlocks(collectedBlocks); // Incremental deletion of blocks\n-      }\n+      removeBlocks(collectedBlocks); // Incremental deletion of blocks\n     } finally {\n       writeUnlock();\n     }\n     collectedBlocks.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      if (!recursive \u0026\u0026 !dir.isDirEmpty(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n\n    writeLock();\n    try {\n      removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    } finally {\n      writeUnlock();\n    }\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "36d1c49486587c2dbb193e8538b1d4510c462fa6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2693. Fix synchronization issues around state transition. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1221582 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/11 7:03 PM",
      "commitName": "36d1c49486587c2dbb193e8538b1d4510c462fa6",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "16/12/11 10:36 AM",
      "commitNameOld": "371f4228e86f5ebffb3d8647fb30b8bdc2b777c4",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.35,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,48 @@\n   private boolean deleteInternal(String src, boolean recursive,\n       boolean enforcePermission)\n       throws AccessControlException, SafeModeException, UnresolvedLinkException,\n              IOException {\n     boolean deleteNow \u003d false;\n     ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n \n     writeLock();\n     try {\n+      checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n       }\n       if (!recursive \u0026\u0026 !dir.isDirEmpty(src)) {\n         throw new IOException(src + \" is non empty\");\n       }\n       if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n         checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n       }\n       // Unlink the target directory from directory tree\n       if (!dir.delete(src, collectedBlocks)) {\n         return false;\n       }\n       deleteNow \u003d collectedBlocks.size() \u003c\u003d BLOCK_DELETION_INCREMENT;\n       if (deleteNow) { // Perform small deletes right away\n         removeBlocks(collectedBlocks);\n       }\n     } finally {\n       writeUnlock();\n     }\n \n     getEditLog().logSync();\n \n     writeLock();\n     try {\n       if (!deleteNow) {\n         removeBlocks(collectedBlocks); // Incremental deletion of blocks\n       }\n     } finally {\n       writeUnlock();\n     }\n     collectedBlocks.clear();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n         + src +\" is removed\");\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    boolean deleteNow \u003d false;\n    ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      if (!recursive \u0026\u0026 !dir.isDirEmpty(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n      deleteNow \u003d collectedBlocks.size() \u003c\u003d BLOCK_DELETION_INCREMENT;\n      if (deleteNow) { // Perform small deletes right away\n        removeBlocks(collectedBlocks);\n      }\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n\n    writeLock();\n    try {\n      if (!deleteNow) {\n        removeBlocks(collectedBlocks); // Incremental deletion of blocks\n      }\n    } finally {\n      writeUnlock();\n    }\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    boolean deleteNow \u003d false;\n    ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      if (!recursive \u0026\u0026 !dir.isDirEmpty(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n      deleteNow \u003d collectedBlocks.size() \u003c\u003d BLOCK_DELETION_INCREMENT;\n      if (deleteNow) { // Perform small deletes right away\n        removeBlocks(collectedBlocks);\n      }\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n\n    writeLock();\n    try {\n      if (!deleteNow) {\n        removeBlocks(collectedBlocks); // Incremental deletion of blocks\n      }\n    } finally {\n      writeUnlock();\n    }\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    boolean deleteNow \u003d false;\n    ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      if (!recursive \u0026\u0026 !dir.isDirEmpty(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n      deleteNow \u003d collectedBlocks.size() \u003c\u003d BLOCK_DELETION_INCREMENT;\n      if (deleteNow) { // Perform small deletes right away\n        removeBlocks(collectedBlocks);\n      }\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n\n    writeLock();\n    try {\n      if (!deleteNow) {\n        removeBlocks(collectedBlocks); // Incremental deletion of blocks\n      }\n    } finally {\n      writeUnlock();\n    }\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,47 @@\n+  private boolean deleteInternal(String src, boolean recursive,\n+      boolean enforcePermission)\n+      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n+             IOException {\n+    boolean deleteNow \u003d false;\n+    ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n+\n+    writeLock();\n+    try {\n+      if (isInSafeMode()) {\n+        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n+      }\n+      if (!recursive \u0026\u0026 !dir.isDirEmpty(src)) {\n+        throw new IOException(src + \" is non empty\");\n+      }\n+      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n+        checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n+      }\n+      // Unlink the target directory from directory tree\n+      if (!dir.delete(src, collectedBlocks)) {\n+        return false;\n+      }\n+      deleteNow \u003d collectedBlocks.size() \u003c\u003d BLOCK_DELETION_INCREMENT;\n+      if (deleteNow) { // Perform small deletes right away\n+        removeBlocks(collectedBlocks);\n+      }\n+    } finally {\n+      writeUnlock();\n+    }\n+\n+    getEditLog().logSync();\n+\n+    writeLock();\n+    try {\n+      if (!deleteNow) {\n+        removeBlocks(collectedBlocks); // Incremental deletion of blocks\n+      }\n+    } finally {\n+      writeUnlock();\n+    }\n+    collectedBlocks.clear();\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n+        + src +\" is removed\");\n+    }\n+    return true;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    boolean deleteNow \u003d false;\n    ArrayList\u003cBlock\u003e collectedBlocks \u003d new ArrayList\u003cBlock\u003e();\n\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      if (!recursive \u0026\u0026 !dir.isDirEmpty(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission \u0026\u0026 isPermissionEnabled) {\n        checkPermission(src, false, null, FsAction.WRITE, null, FsAction.ALL);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks)) {\n        return false;\n      }\n      deleteNow \u003d collectedBlocks.size() \u003c\u003d BLOCK_DELETION_INCREMENT;\n      if (deleteNow) { // Perform small deletes right away\n        removeBlocks(collectedBlocks);\n      }\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n\n    writeLock();\n    try {\n      if (!deleteNow) {\n        removeBlocks(collectedBlocks); // Incremental deletion of blocks\n      }\n    } finally {\n      writeUnlock();\n    }\n    collectedBlocks.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return true;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}