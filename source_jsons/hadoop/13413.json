{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "InvalidateBlocks.java",
  "functionName": "dumpBlockSet",
  "functionId": "dumpBlockSet___nodeToBlocksMap-Map__DatanodeInfo,LightWeightHashSet__Block____(modifiers-final)__out-PrintWriter(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
  "functionStartLine": 215,
  "functionEndLine": 225,
  "numCommitsSeen": 43,
  "timeTaken": 3520,
  "changeHistory": [
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
    "4d3af47f2765f6f57936d316ef2a4150b787cc97",
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e",
    "be7dd8333a7e56e732171db0781786987de03195",
    "9a3f147fdd5421460889b266ead3a2300323cda2",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "513f17d115564e49124bb744cecf36d16a144ffc"
  ],
  "changeHistoryShort": {
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
    "4d3af47f2765f6f57936d316ef2a4150b787cc97": "Ybodychange",
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "9a3f147fdd5421460889b266ead3a2300323cda2": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "513f17d115564e49124bb744cecf36d16a144ffc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-10999. Introduce separate stats for Replicated and Erasure Coded Blocks apart from the current Aggregated stats. (Manoj Govindassamy via lei)\n",
      "commitDate": "14/06/17 10:44 AM",
      "commitName": "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
      "commitAuthor": "Lei Xu",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-10999. Introduce separate stats for Replicated and Erasure Coded Blocks apart from the current Aggregated stats. (Manoj Govindassamy via lei)\n",
          "commitDate": "14/06/17 10:44 AM",
          "commitName": "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "29/05/17 1:30 AM",
          "commitNameOld": "a7f085d6bf499edf23e650a4f7211c53a442da0e",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 16.38,
          "commitsBetweenForRepo": 71,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,11 @@\n-  synchronized void dump(final PrintWriter out) {\n-    final int size \u003d node2blocks.values().size();\n-    out.println(\"Metasave: Blocks \" + numBlocks \n-        + \" waiting deletion from \" + size + \" datanodes.\");\n-    if (size \u003d\u003d 0) {\n-      return;\n-    }\n-\n-    for(Map.Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n+  private void dumpBlockSet(final Map\u003cDatanodeInfo,\n+      LightWeightHashSet\u003cBlock\u003e\u003e nodeToBlocksMap, final PrintWriter out) {\n+    for(Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry :\n+        nodeToBlocksMap.entrySet()) {\n       final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n-      if (blocks.size() \u003e 0) {\n+      if (blocks !\u003d null \u0026\u0026 blocks.size() \u003e 0) {\n         out.println(entry.getKey());\n         out.println(StringUtils.join(\u0027,\u0027, blocks));\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void dumpBlockSet(final Map\u003cDatanodeInfo,\n      LightWeightHashSet\u003cBlock\u003e\u003e nodeToBlocksMap, final PrintWriter out) {\n    for(Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry :\n        nodeToBlocksMap.entrySet()) {\n      final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n      if (blocks !\u003d null \u0026\u0026 blocks.size() \u003e 0) {\n        out.println(entry.getKey());\n        out.println(StringUtils.join(\u0027,\u0027, blocks));\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
          "extendedDetails": {
            "oldValue": "dump",
            "newValue": "dumpBlockSet"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10999. Introduce separate stats for Replicated and Erasure Coded Blocks apart from the current Aggregated stats. (Manoj Govindassamy via lei)\n",
          "commitDate": "14/06/17 10:44 AM",
          "commitName": "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "29/05/17 1:30 AM",
          "commitNameOld": "a7f085d6bf499edf23e650a4f7211c53a442da0e",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 16.38,
          "commitsBetweenForRepo": 71,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,11 @@\n-  synchronized void dump(final PrintWriter out) {\n-    final int size \u003d node2blocks.values().size();\n-    out.println(\"Metasave: Blocks \" + numBlocks \n-        + \" waiting deletion from \" + size + \" datanodes.\");\n-    if (size \u003d\u003d 0) {\n-      return;\n-    }\n-\n-    for(Map.Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n+  private void dumpBlockSet(final Map\u003cDatanodeInfo,\n+      LightWeightHashSet\u003cBlock\u003e\u003e nodeToBlocksMap, final PrintWriter out) {\n+    for(Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry :\n+        nodeToBlocksMap.entrySet()) {\n       final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n-      if (blocks.size() \u003e 0) {\n+      if (blocks !\u003d null \u0026\u0026 blocks.size() \u003e 0) {\n         out.println(entry.getKey());\n         out.println(StringUtils.join(\u0027,\u0027, blocks));\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void dumpBlockSet(final Map\u003cDatanodeInfo,\n      LightWeightHashSet\u003cBlock\u003e\u003e nodeToBlocksMap, final PrintWriter out) {\n    for(Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry :\n        nodeToBlocksMap.entrySet()) {\n      final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n      if (blocks !\u003d null \u0026\u0026 blocks.size() \u003e 0) {\n        out.println(entry.getKey());\n        out.println(StringUtils.join(\u0027,\u0027, blocks));\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
          "extendedDetails": {
            "oldValue": "[out-PrintWriter(modifiers-final)]",
            "newValue": "[nodeToBlocksMap-Map\u003cDatanodeInfo,LightWeightHashSet\u003cBlock\u003e\u003e(modifiers-final), out-PrintWriter(modifiers-final)]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-10999. Introduce separate stats for Replicated and Erasure Coded Blocks apart from the current Aggregated stats. (Manoj Govindassamy via lei)\n",
          "commitDate": "14/06/17 10:44 AM",
          "commitName": "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "29/05/17 1:30 AM",
          "commitNameOld": "a7f085d6bf499edf23e650a4f7211c53a442da0e",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 16.38,
          "commitsBetweenForRepo": 71,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,11 @@\n-  synchronized void dump(final PrintWriter out) {\n-    final int size \u003d node2blocks.values().size();\n-    out.println(\"Metasave: Blocks \" + numBlocks \n-        + \" waiting deletion from \" + size + \" datanodes.\");\n-    if (size \u003d\u003d 0) {\n-      return;\n-    }\n-\n-    for(Map.Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n+  private void dumpBlockSet(final Map\u003cDatanodeInfo,\n+      LightWeightHashSet\u003cBlock\u003e\u003e nodeToBlocksMap, final PrintWriter out) {\n+    for(Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry :\n+        nodeToBlocksMap.entrySet()) {\n       final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n-      if (blocks.size() \u003e 0) {\n+      if (blocks !\u003d null \u0026\u0026 blocks.size() \u003e 0) {\n         out.println(entry.getKey());\n         out.println(StringUtils.join(\u0027,\u0027, blocks));\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void dumpBlockSet(final Map\u003cDatanodeInfo,\n      LightWeightHashSet\u003cBlock\u003e\u003e nodeToBlocksMap, final PrintWriter out) {\n    for(Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry :\n        nodeToBlocksMap.entrySet()) {\n      final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n      if (blocks !\u003d null \u0026\u0026 blocks.size() \u003e 0) {\n        out.println(entry.getKey());\n        out.println(StringUtils.join(\u0027,\u0027, blocks));\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
          "extendedDetails": {
            "oldValue": "[synchronized]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10999. Introduce separate stats for Replicated and Erasure Coded Blocks apart from the current Aggregated stats. (Manoj Govindassamy via lei)\n",
          "commitDate": "14/06/17 10:44 AM",
          "commitName": "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "29/05/17 1:30 AM",
          "commitNameOld": "a7f085d6bf499edf23e650a4f7211c53a442da0e",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 16.38,
          "commitsBetweenForRepo": 71,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,11 @@\n-  synchronized void dump(final PrintWriter out) {\n-    final int size \u003d node2blocks.values().size();\n-    out.println(\"Metasave: Blocks \" + numBlocks \n-        + \" waiting deletion from \" + size + \" datanodes.\");\n-    if (size \u003d\u003d 0) {\n-      return;\n-    }\n-\n-    for(Map.Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n+  private void dumpBlockSet(final Map\u003cDatanodeInfo,\n+      LightWeightHashSet\u003cBlock\u003e\u003e nodeToBlocksMap, final PrintWriter out) {\n+    for(Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry :\n+        nodeToBlocksMap.entrySet()) {\n       final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n-      if (blocks.size() \u003e 0) {\n+      if (blocks !\u003d null \u0026\u0026 blocks.size() \u003e 0) {\n         out.println(entry.getKey());\n         out.println(StringUtils.join(\u0027,\u0027, blocks));\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void dumpBlockSet(final Map\u003cDatanodeInfo,\n      LightWeightHashSet\u003cBlock\u003e\u003e nodeToBlocksMap, final PrintWriter out) {\n    for(Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry :\n        nodeToBlocksMap.entrySet()) {\n      final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n      if (blocks !\u003d null \u0026\u0026 blocks.size() \u003e 0) {\n        out.println(entry.getKey());\n        out.println(StringUtils.join(\u0027,\u0027, blocks));\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
          "extendedDetails": {}
        }
      ]
    },
    "4d3af47f2765f6f57936d316ef2a4150b787cc97": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10693. metaSave should print blocks, not LightWeightHashSet. Contributed by Yuanbo Liu.",
      "commitDate": "07/08/16 2:29 PM",
      "commitName": "4d3af47f2765f6f57936d316ef2a4150b787cc97",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "31/07/15 4:15 PM",
      "commitNameOld": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 372.93,
      "commitsBetweenForRepo": 2572,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   synchronized void dump(final PrintWriter out) {\n     final int size \u003d node2blocks.values().size();\n     out.println(\"Metasave: Blocks \" + numBlocks \n         + \" waiting deletion from \" + size + \" datanodes.\");\n     if (size \u003d\u003d 0) {\n       return;\n     }\n \n     for(Map.Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n       final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n       if (blocks.size() \u003e 0) {\n         out.println(entry.getKey());\n-        out.println(blocks);\n+        out.println(StringUtils.join(\u0027,\u0027, blocks));\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void dump(final PrintWriter out) {\n    final int size \u003d node2blocks.values().size();\n    out.println(\"Metasave: Blocks \" + numBlocks \n        + \" waiting deletion from \" + size + \" datanodes.\");\n    if (size \u003d\u003d 0) {\n      return;\n    }\n\n    for(Map.Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n      final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n      if (blocks.size() \u003e 0) {\n        out.println(entry.getKey());\n        out.println(StringUtils.join(\u0027,\u0027, blocks));\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "9a0fcae5bc9e481201e101c3c98e23b6e827774e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6362. InvalidateBlocks is inconsistent in usage of DatanodeUuid and StorageID. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595056 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/05/14 2:30 PM",
      "commitName": "9a0fcae5bc9e481201e101c3c98e23b6e827774e",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "13/05/14 11:22 AM",
      "commitNameOld": "8e5b5165c14486af6d5d73e7b4e591d4787ad8f2",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.13,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   synchronized void dump(final PrintWriter out) {\n     final int size \u003d node2blocks.values().size();\n     out.println(\"Metasave: Blocks \" + numBlocks \n         + \" waiting deletion from \" + size + \" datanodes.\");\n     if (size \u003d\u003d 0) {\n       return;\n     }\n \n-    for(Map.Entry\u003cString,LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n+    for(Map.Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n       final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n       if (blocks.size() \u003e 0) {\n-        out.println(datanodeManager.getDatanode(entry.getKey()));\n+        out.println(entry.getKey());\n         out.println(blocks);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void dump(final PrintWriter out) {\n    final int size \u003d node2blocks.values().size();\n    out.println(\"Metasave: Blocks \" + numBlocks \n        + \" waiting deletion from \" + size + \" datanodes.\");\n    if (size \u003d\u003d 0) {\n      return;\n    }\n\n    for(Map.Entry\u003cDatanodeInfo, LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n      final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n      if (blocks.size() \u003e 0) {\n        out.println(entry.getKey());\n        out.println(blocks);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "20/12/11 8:32 PM",
      "commitNameOld": "31c91706f7d17da006ef2d6c541f8dd092fae077",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 102.74,
      "commitsBetweenForRepo": 694,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,16 @@\n   synchronized void dump(final PrintWriter out) {\n     final int size \u003d node2blocks.values().size();\n     out.println(\"Metasave: Blocks \" + numBlocks \n         + \" waiting deletion from \" + size + \" datanodes.\");\n     if (size \u003d\u003d 0) {\n       return;\n     }\n \n     for(Map.Entry\u003cString,LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n       final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n       if (blocks.size() \u003e 0) {\n-        out.println(datanodeManager.getDatanode(entry.getKey()).getName() + blocks);\n+        out.println(datanodeManager.getDatanode(entry.getKey()));\n+        out.println(blocks);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void dump(final PrintWriter out) {\n    final int size \u003d node2blocks.values().size();\n    out.println(\"Metasave: Blocks \" + numBlocks \n        + \" waiting deletion from \" + size + \" datanodes.\");\n    if (size \u003d\u003d 0) {\n      return;\n    }\n\n    for(Map.Entry\u003cString,LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n      final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n      if (blocks.size() \u003e 0) {\n        out.println(datanodeManager.getDatanode(entry.getKey()));\n        out.println(blocks);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "9a3f147fdd5421460889b266ead3a2300323cda2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2476. More CPU efficient data structure for under-replicated, over-replicated, and invalidated blocks. Contributed by Tomasz Nykiel.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1201991 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/11/11 5:13 PM",
      "commitName": "9a3f147fdd5421460889b266ead3a2300323cda2",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 82.04,
      "commitsBetweenForRepo": 586,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   synchronized void dump(final PrintWriter out) {\n     final int size \u003d node2blocks.values().size();\n     out.println(\"Metasave: Blocks \" + numBlocks \n         + \" waiting deletion from \" + size + \" datanodes.\");\n     if (size \u003d\u003d 0) {\n       return;\n     }\n \n-    for(Map.Entry\u003cString,Collection\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n-      final Collection\u003cBlock\u003e blocks \u003d entry.getValue();\n+    for(Map.Entry\u003cString,LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n+      final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n       if (blocks.size() \u003e 0) {\n         out.println(datanodeManager.getDatanode(entry.getKey()).getName() + blocks);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void dump(final PrintWriter out) {\n    final int size \u003d node2blocks.values().size();\n    out.println(\"Metasave: Blocks \" + numBlocks \n        + \" waiting deletion from \" + size + \" datanodes.\");\n    if (size \u003d\u003d 0) {\n      return;\n    }\n\n    for(Map.Entry\u003cString,LightWeightHashSet\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n      final LightWeightHashSet\u003cBlock\u003e blocks \u003d entry.getValue();\n      if (blocks.size() \u003e 0) {\n        out.println(datanodeManager.getDatanode(entry.getKey()).getName() + blocks);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  synchronized void dump(final PrintWriter out) {\n    final int size \u003d node2blocks.values().size();\n    out.println(\"Metasave: Blocks \" + numBlocks \n        + \" waiting deletion from \" + size + \" datanodes.\");\n    if (size \u003d\u003d 0) {\n      return;\n    }\n\n    for(Map.Entry\u003cString,Collection\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n      final Collection\u003cBlock\u003e blocks \u003d entry.getValue();\n      if (blocks.size() \u003e 0) {\n        out.println(datanodeManager.getDatanode(entry.getKey()).getName() + blocks);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java"
      }
    },
    "513f17d115564e49124bb744cecf36d16a144ffc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2273.  Refactor BlockManager.recentInvalidateSets to a new class.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1160475 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/08/11 3:28 PM",
      "commitName": "513f17d115564e49124bb744cecf36d16a144ffc",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,15 @@\n+  synchronized void dump(final PrintWriter out) {\n+    final int size \u003d node2blocks.values().size();\n+    out.println(\"Metasave: Blocks \" + numBlocks \n+        + \" waiting deletion from \" + size + \" datanodes.\");\n+    if (size \u003d\u003d 0) {\n+      return;\n+    }\n+\n+    for(Map.Entry\u003cString,Collection\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n+      final Collection\u003cBlock\u003e blocks \u003d entry.getValue();\n+      if (blocks.size() \u003e 0) {\n+        out.println(datanodeManager.getDatanode(entry.getKey()).getName() + blocks);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void dump(final PrintWriter out) {\n    final int size \u003d node2blocks.values().size();\n    out.println(\"Metasave: Blocks \" + numBlocks \n        + \" waiting deletion from \" + size + \" datanodes.\");\n    if (size \u003d\u003d 0) {\n      return;\n    }\n\n    for(Map.Entry\u003cString,Collection\u003cBlock\u003e\u003e entry : node2blocks.entrySet()) {\n      final Collection\u003cBlock\u003e blocks \u003d entry.getValue();\n      if (blocks.size() \u003e 0) {\n        out.println(datanodeManager.getDatanode(entry.getKey()).getName() + blocks);\n      }\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java"
    }
  }
}