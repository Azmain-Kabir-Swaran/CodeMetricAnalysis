{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReceiver.java",
  "functionName": "finalizeBlock",
  "functionId": "finalizeBlock___startTime-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
  "functionStartLine": 1519,
  "functionEndLine": 1546,
  "numCommitsSeen": 106,
  "timeTaken": 4180,
  "changeHistory": [
    "1543d0f5be6a02ad00e7a33e35d78af8516043e3",
    "d1d4e16690cc85f7f22fbead9cf596260819b561",
    "023133cef9a7ca05364cefbcead57c921589eda7",
    "085b1e293ff53f7a86aa21406cfd4bfa0f3bf33b",
    "01f37e42f050207b7659bf74e2484cf8bdae2d89",
    "4551da302d94cffea0313eac79479ab6f9b7cb34",
    "7e56bfe40589a1aa9b5ef20b342e421823cd0592"
  ],
  "changeHistoryShort": {
    "1543d0f5be6a02ad00e7a33e35d78af8516043e3": "Ybodychange",
    "d1d4e16690cc85f7f22fbead9cf596260819b561": "Ybodychange",
    "023133cef9a7ca05364cefbcead57c921589eda7": "Ybodychange",
    "085b1e293ff53f7a86aa21406cfd4bfa0f3bf33b": "Ybodychange",
    "01f37e42f050207b7659bf74e2484cf8bdae2d89": "Ybodychange",
    "4551da302d94cffea0313eac79479ab6f9b7cb34": "Ybodychange",
    "7e56bfe40589a1aa9b5ef20b342e421823cd0592": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1543d0f5be6a02ad00e7a33e35d78af8516043e3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5042. Completed files lost after power failure. Contributed by Vinayakumar B.\n",
      "commitDate": "31/05/17 8:55 AM",
      "commitName": "1543d0f5be6a02ad00e7a33e35d78af8516043e3",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "31/05/17 8:09 AM",
      "commitNameOld": "13de636b4079b077890ad10389ff350dcf8086a2",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n     private void finalizeBlock(long startTime) throws IOException {\n       long endTime \u003d 0;\n       // Hold a volume reference to finalize block.\n       try (ReplicaHandler handler \u003d BlockReceiver.this.claimReplicaHandler()) {\n         BlockReceiver.this.close();\n         endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n         block.setNumBytes(replicaInfo.getNumBytes());\n-        datanode.data.finalizeBlock(block);\n+        datanode.data.finalizeBlock(block, dirSyncOnFinalize);\n       }\n \n       if (pinning) {\n         datanode.data.setPinning(block);\n       }\n       \n       datanode.closeBlock(block, null, replicaInfo.getStorageUuid(),\n           replicaInfo.isOnTransientStorage());\n       if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n         long offset \u003d 0;\n         DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n             .getBlockPoolId());\n         ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n             myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n             dnR.getDatanodeUuid(), block, endTime - startTime));\n       } else {\n         LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n             + \" from \" + inAddr);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void finalizeBlock(long startTime) throws IOException {\n      long endTime \u003d 0;\n      // Hold a volume reference to finalize block.\n      try (ReplicaHandler handler \u003d BlockReceiver.this.claimReplicaHandler()) {\n        BlockReceiver.this.close();\n        endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n        block.setNumBytes(replicaInfo.getNumBytes());\n        datanode.data.finalizeBlock(block, dirSyncOnFinalize);\n      }\n\n      if (pinning) {\n        datanode.data.setPinning(block);\n      }\n      \n      datanode.closeBlock(block, null, replicaInfo.getStorageUuid(),\n          replicaInfo.isOnTransientStorage());\n      if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n        long offset \u003d 0;\n        DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n            .getBlockPoolId());\n        ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n            myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n            dnR.getDatanodeUuid(), block, endTime - startTime));\n      } else {\n        LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n            + \" from \" + inAddr);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
      "extendedDetails": {}
    },
    "d1d4e16690cc85f7f22fbead9cf596260819b561": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9710. DN can be configured to send block receipt IBRs in batches.\n",
      "commitDate": "26/02/16 3:32 PM",
      "commitName": "d1d4e16690cc85f7f22fbead9cf596260819b561",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "28/01/16 11:04 PM",
      "commitNameOld": "8ee060311c89b7faa71dd039481a97ba15e2413d",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 28.69,
      "commitsBetweenForRepo": 200,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n     private void finalizeBlock(long startTime) throws IOException {\n       long endTime \u003d 0;\n       // Hold a volume reference to finalize block.\n       try (ReplicaHandler handler \u003d BlockReceiver.this.claimReplicaHandler()) {\n         BlockReceiver.this.close();\n         endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n         block.setNumBytes(replicaInfo.getNumBytes());\n         datanode.data.finalizeBlock(block);\n       }\n \n       if (pinning) {\n         datanode.data.setPinning(block);\n       }\n       \n-      datanode.closeBlock(\n-          block, DataNode.EMPTY_DEL_HINT, replicaInfo.getStorageUuid());\n+      datanode.closeBlock(block, null, replicaInfo.getStorageUuid(),\n+          replicaInfo.isOnTransientStorage());\n       if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n         long offset \u003d 0;\n         DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n             .getBlockPoolId());\n         ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n             myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n             dnR.getDatanodeUuid(), block, endTime - startTime));\n       } else {\n         LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n             + \" from \" + inAddr);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void finalizeBlock(long startTime) throws IOException {\n      long endTime \u003d 0;\n      // Hold a volume reference to finalize block.\n      try (ReplicaHandler handler \u003d BlockReceiver.this.claimReplicaHandler()) {\n        BlockReceiver.this.close();\n        endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n        block.setNumBytes(replicaInfo.getNumBytes());\n        datanode.data.finalizeBlock(block);\n      }\n\n      if (pinning) {\n        datanode.data.setPinning(block);\n      }\n      \n      datanode.closeBlock(block, null, replicaInfo.getStorageUuid(),\n          replicaInfo.isOnTransientStorage());\n      if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n        long offset \u003d 0;\n        DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n            .getBlockPoolId());\n        ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n            myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n            dnR.getDatanodeUuid(), block, endTime - startTime));\n      } else {\n        LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n            + \" from \" + inAddr);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
      "extendedDetails": {}
    },
    "023133cef9a7ca05364cefbcead57c921589eda7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7996. After swapping a volume, BlockReceiver reports ReplicaNotFoundException (Lei (Eddy) Xu via Colin P. McCabe)\n",
      "commitDate": "03/04/15 2:19 PM",
      "commitName": "023133cef9a7ca05364cefbcead57c921589eda7",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "30/03/15 11:59 AM",
      "commitNameOld": "b80457158daf0dc712fbe5695625cc17d70d4bb4",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 4.1,
      "commitsBetweenForRepo": 49,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,28 @@\n     private void finalizeBlock(long startTime) throws IOException {\n-      BlockReceiver.this.close();\n-      final long endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n-          : 0;\n-      block.setNumBytes(replicaInfo.getNumBytes());\n-      datanode.data.finalizeBlock(block);\n-      \n+      long endTime \u003d 0;\n+      // Hold a volume reference to finalize block.\n+      try (ReplicaHandler handler \u003d BlockReceiver.this.claimReplicaHandler()) {\n+        BlockReceiver.this.close();\n+        endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n+        block.setNumBytes(replicaInfo.getNumBytes());\n+        datanode.data.finalizeBlock(block);\n+      }\n+\n       if (pinning) {\n         datanode.data.setPinning(block);\n       }\n       \n       datanode.closeBlock(\n           block, DataNode.EMPTY_DEL_HINT, replicaInfo.getStorageUuid());\n       if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n         long offset \u003d 0;\n         DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n             .getBlockPoolId());\n         ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n             myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n             dnR.getDatanodeUuid(), block, endTime - startTime));\n       } else {\n         LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n             + \" from \" + inAddr);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void finalizeBlock(long startTime) throws IOException {\n      long endTime \u003d 0;\n      // Hold a volume reference to finalize block.\n      try (ReplicaHandler handler \u003d BlockReceiver.this.claimReplicaHandler()) {\n        BlockReceiver.this.close();\n        endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n        block.setNumBytes(replicaInfo.getNumBytes());\n        datanode.data.finalizeBlock(block);\n      }\n\n      if (pinning) {\n        datanode.data.setPinning(block);\n      }\n      \n      datanode.closeBlock(\n          block, DataNode.EMPTY_DEL_HINT, replicaInfo.getStorageUuid());\n      if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n        long offset \u003d 0;\n        DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n            .getBlockPoolId());\n        ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n            myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n            dnR.getDatanodeUuid(), block, endTime - startTime));\n      } else {\n        LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n            + \" from \" + inAddr);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
      "extendedDetails": {}
    },
    "085b1e293ff53f7a86aa21406cfd4bfa0f3bf33b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6133. Add a feature for replica pinning so that a pinned replica will not be moved by Balancer/Mover.  Contributed by zhaoyunjiong\n",
      "commitDate": "11/02/15 3:12 PM",
      "commitName": "085b1e293ff53f7a86aa21406cfd4bfa0f3bf33b",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "05/02/15 10:58 AM",
      "commitNameOld": "c4980a2f343778544ca20ebea1338651793ea0d9",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.18,
      "commitsBetweenForRepo": 66,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,25 @@\n     private void finalizeBlock(long startTime) throws IOException {\n       BlockReceiver.this.close();\n       final long endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n           : 0;\n       block.setNumBytes(replicaInfo.getNumBytes());\n       datanode.data.finalizeBlock(block);\n+      \n+      if (pinning) {\n+        datanode.data.setPinning(block);\n+      }\n+      \n       datanode.closeBlock(\n           block, DataNode.EMPTY_DEL_HINT, replicaInfo.getStorageUuid());\n       if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n         long offset \u003d 0;\n         DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n             .getBlockPoolId());\n         ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n             myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n             dnR.getDatanodeUuid(), block, endTime - startTime));\n       } else {\n         LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n             + \" from \" + inAddr);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void finalizeBlock(long startTime) throws IOException {\n      BlockReceiver.this.close();\n      final long endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n          : 0;\n      block.setNumBytes(replicaInfo.getNumBytes());\n      datanode.data.finalizeBlock(block);\n      \n      if (pinning) {\n        datanode.data.setPinning(block);\n      }\n      \n      datanode.closeBlock(\n          block, DataNode.EMPTY_DEL_HINT, replicaInfo.getStorageUuid());\n      if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n        long offset \u003d 0;\n        DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n            .getBlockPoolId());\n        ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n            myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n            dnR.getDatanodeUuid(), block, endTime - startTime));\n      } else {\n        LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n            + \" from \" + inAddr);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
      "extendedDetails": {}
    },
    "01f37e42f050207b7659bf74e2484cf8bdae2d89": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5390. Send one incremental block report per storage directory.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1534891 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/10/13 6:28 PM",
      "commitName": "01f37e42f050207b7659bf74e2484cf8bdae2d89",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/09/13 11:03 AM",
      "commitNameOld": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 30.31,
      "commitsBetweenForRepo": 225,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,20 @@\n     private void finalizeBlock(long startTime) throws IOException {\n       BlockReceiver.this.close();\n       final long endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n           : 0;\n       block.setNumBytes(replicaInfo.getNumBytes());\n       datanode.data.finalizeBlock(block);\n-      datanode.closeBlock(block, DataNode.EMPTY_DEL_HINT);\n+      datanode.closeBlock(\n+          block, DataNode.EMPTY_DEL_HINT, replicaInfo.getStorageUuid());\n       if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n         long offset \u003d 0;\n         DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n             .getBlockPoolId());\n         ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n             myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n             dnR.getDatanodeUuid(), block, endTime - startTime));\n       } else {\n         LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n             + \" from \" + inAddr);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void finalizeBlock(long startTime) throws IOException {\n      BlockReceiver.this.close();\n      final long endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n          : 0;\n      block.setNumBytes(replicaInfo.getNumBytes());\n      datanode.data.finalizeBlock(block);\n      datanode.closeBlock(\n          block, DataNode.EMPTY_DEL_HINT, replicaInfo.getStorageUuid());\n      if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n        long offset \u003d 0;\n        DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n            .getBlockPoolId());\n        ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n            myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n            dnR.getDatanodeUuid(), block, endTime - startTime));\n      } else {\n        LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n            + \" from \" + inAddr);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
      "extendedDetails": {}
    },
    "4551da302d94cffea0313eac79479ab6f9b7cb34": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5233. Use Datanode UUID to identify Datanodes.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1525407 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/09/13 11:03 AM",
      "commitName": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "25/07/13 9:42 PM",
      "commitNameOld": "7723b139d55fc2c3954939559cb4914046a0f81c",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 58.56,
      "commitsBetweenForRepo": 325,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n     private void finalizeBlock(long startTime) throws IOException {\n       BlockReceiver.this.close();\n       final long endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n           : 0;\n       block.setNumBytes(replicaInfo.getNumBytes());\n       datanode.data.finalizeBlock(block);\n       datanode.closeBlock(block, DataNode.EMPTY_DEL_HINT);\n       if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n         long offset \u003d 0;\n         DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n             .getBlockPoolId());\n         ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n             myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n-            dnR.getStorageID(), block, endTime - startTime));\n+            dnR.getDatanodeUuid(), block, endTime - startTime));\n       } else {\n         LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n             + \" from \" + inAddr);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void finalizeBlock(long startTime) throws IOException {\n      BlockReceiver.this.close();\n      final long endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n          : 0;\n      block.setNumBytes(replicaInfo.getNumBytes());\n      datanode.data.finalizeBlock(block);\n      datanode.closeBlock(block, DataNode.EMPTY_DEL_HINT);\n      if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n        long offset \u003d 0;\n        DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n            .getBlockPoolId());\n        ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n            myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n            dnR.getDatanodeUuid(), block, endTime - startTime));\n      } else {\n        LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n            + \" from \" + inAddr);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
      "extendedDetails": {}
    },
    "7e56bfe40589a1aa9b5ef20b342e421823cd0592": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4200. Reduce the size of synchronized sections in PacketResponder. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1413826 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/11/12 12:47 PM",
      "commitName": "7e56bfe40589a1aa9b5ef20b342e421823cd0592",
      "commitAuthor": "Suresh Srinivas",
      "diff": "@@ -0,0 +1,19 @@\n+    private void finalizeBlock(long startTime) throws IOException {\n+      BlockReceiver.this.close();\n+      final long endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n+          : 0;\n+      block.setNumBytes(replicaInfo.getNumBytes());\n+      datanode.data.finalizeBlock(block);\n+      datanode.closeBlock(block, DataNode.EMPTY_DEL_HINT);\n+      if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n+        long offset \u003d 0;\n+        DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n+            .getBlockPoolId());\n+        ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n+            myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n+            dnR.getStorageID(), block, endTime - startTime));\n+      } else {\n+        LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n+            + \" from \" + inAddr);\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void finalizeBlock(long startTime) throws IOException {\n      BlockReceiver.this.close();\n      final long endTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n          : 0;\n      block.setNumBytes(replicaInfo.getNumBytes());\n      datanode.data.finalizeBlock(block);\n      datanode.closeBlock(block, DataNode.EMPTY_DEL_HINT);\n      if (ClientTraceLog.isInfoEnabled() \u0026\u0026 isClient) {\n        long offset \u003d 0;\n        DatanodeRegistration dnR \u003d datanode.getDNRegistrationForBP(block\n            .getBlockPoolId());\n        ClientTraceLog.info(String.format(DN_CLIENTTRACE_FORMAT, inAddr,\n            myAddr, block.getNumBytes(), \"HDFS_WRITE\", clientname, offset,\n            dnR.getStorageID(), block, endTime - startTime));\n      } else {\n        LOG.info(\"Received \" + block + \" size \" + block.getNumBytes()\n            + \" from \" + inAddr);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java"
    }
  }
}