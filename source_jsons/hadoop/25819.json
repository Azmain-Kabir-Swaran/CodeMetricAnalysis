{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RMContainerAllocator.java",
  "functionName": "handle",
  "functionId": "handle___event-ContainerAllocatorEvent",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
  "functionStartLine": 358,
  "functionEndLine": 373,
  "numCommitsSeen": 83,
  "timeTaken": 8368,
  "changeHistory": [
    "a83fb61ac07c0468cbc7a38526e92683883dd932",
    "08f8abf5639d39167952dc5120b44fe35c63ff7a",
    "fffdf661e30afd10331d2153ff052c141b7ebe4b",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517"
  ],
  "changeHistoryShort": {
    "a83fb61ac07c0468cbc7a38526e92683883dd932": "Ybodychange",
    "08f8abf5639d39167952dc5120b44fe35c63ff7a": "Ymultichange(Ymodifierchange,Ybodychange)",
    "fffdf661e30afd10331d2153ff052c141b7ebe4b": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a83fb61ac07c0468cbc7a38526e92683883dd932": {
      "type": "Ybodychange",
      "commitMessage": "YARN-635. Renamed YarnRemoteException to YarnException. Contributed by Siddharth Seth.\nMAPREDUCE-5301. Updated MR code to work with YARN-635 changes of renaming YarnRemoteException to YarnException. Contributed by Siddharth Seth\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489283 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/06/13 9:05 PM",
      "commitName": "a83fb61ac07c0468cbc7a38526e92683883dd932",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "15/04/13 4:54 PM",
      "commitNameOld": "0e01f26821caa3bf8554afe422bb080abcfe1e83",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 49.17,
      "commitsBetweenForRepo": 314,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   public void handle(ContainerAllocatorEvent event) {\n     int qSize \u003d eventQueue.size();\n     if (qSize !\u003d 0 \u0026\u0026 qSize % 1000 \u003d\u003d 0) {\n       LOG.info(\"Size of event-queue in RMContainerAllocator is \" + qSize);\n     }\n     int remCapacity \u003d eventQueue.remainingCapacity();\n     if (remCapacity \u003c 1000) {\n       LOG.warn(\"Very low remaining capacity in the event-queue \"\n           + \"of RMContainerAllocator: \" + remCapacity);\n     }\n     try {\n       eventQueue.put(event);\n     } catch (InterruptedException e) {\n-      throw new YarnException(e);\n+      throw new YarnRuntimeException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void handle(ContainerAllocatorEvent event) {\n    int qSize \u003d eventQueue.size();\n    if (qSize !\u003d 0 \u0026\u0026 qSize % 1000 \u003d\u003d 0) {\n      LOG.info(\"Size of event-queue in RMContainerAllocator is \" + qSize);\n    }\n    int remCapacity \u003d eventQueue.remainingCapacity();\n    if (remCapacity \u003c 1000) {\n      LOG.warn(\"Very low remaining capacity in the event-queue \"\n          + \"of RMContainerAllocator: \" + remCapacity);\n    }\n    try {\n      eventQueue.put(event);\n    } catch (InterruptedException e) {\n      throw new YarnRuntimeException(e);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "08f8abf5639d39167952dc5120b44fe35c63ff7a": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-3572. Moved AM event dispatcher to a separate thread for performance reasons. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227426 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/01/12 5:37 PM",
      "commitName": "08f8abf5639d39167952dc5120b44fe35c63ff7a",
      "commitAuthor": "Arun Murthy",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "MAPREDUCE-3572. Moved AM event dispatcher to a separate thread for performance reasons. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227426 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:37 PM",
          "commitName": "08f8abf5639d39167952dc5120b44fe35c63ff7a",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "04/01/12 9:10 AM",
          "commitNameOld": "55e94dc5ef4171c4e7b57942f22ead9a01dd9012",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 0.35,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,91 +1,16 @@\n-  public synchronized void handle(ContainerAllocatorEvent event) {\n-    LOG.info(\"Processing the event \" + event.toString());\n-    recalculateReduceSchedule \u003d true;\n-    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n-      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n-      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n-        if (mapResourceReqt \u003d\u003d 0) {\n-          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n-          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n-          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n-              * minSlotMemSize;\n-          JobID id \u003d TypeConverter.fromYarn(applicationId);\n-          JobId jobId \u003d TypeConverter.toYarn(id);\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n-              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n-              mapResourceReqt)));\n-          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n-          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n-            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n-            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n-            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n-            LOG.info(diagMsg);\n-            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n-                getJob().getID(), diagMsg));\n-            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n-          }\n-        }\n-        //set the rounded off memory\n-        reqEvent.getCapability().setMemory(mapResourceReqt);\n-        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n-      } else {\n-        if (reduceResourceReqt \u003d\u003d 0) {\n-          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n-          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n-          //round off on slotsize\n-          reduceResourceReqt \u003d (int) Math.ceil((float) \n-              reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n-          JobID id \u003d TypeConverter.fromYarn(applicationId);\n-          JobId jobId \u003d TypeConverter.toYarn(id);\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n-              new NormalizedResourceEvent(\n-                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n-              reduceResourceReqt)));\n-          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n-          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n-            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n-            \t\t\"supported max container capability in the cluster. Killing the \" +\n-            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n-            \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n-            LOG.info(diagMsg);\n-            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n-                getJob().getID(), diagMsg));\n-            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n-          }\n-        }\n-        //set the rounded off memory\n-        reqEvent.getCapability().setMemory(reduceResourceReqt);\n-        if (reqEvent.getEarlierAttemptFailed()) {\n-          //add to the front of queue for fail fast\n-          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n-        } else {\n-          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n-          //reduces are added to pending and are slowly ramped up\n-        }\n-      }\n-      \n-    } else if (\n-        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n-      TaskAttemptId aId \u003d event.getAttemptID();\n-      \n-      boolean removed \u003d scheduledRequests.remove(aId);\n-      if (!removed) {\n-        ContainerId containerId \u003d assignedRequests.get(aId);\n-        if (containerId !\u003d null) {\n-          removed \u003d true;\n-          assignedRequests.remove(aId);\n-          containersReleased++;\n-          release(containerId);\n-        }\n-      }\n-      if (!removed) {\n-        LOG.error(\"Could not deallocate container for task attemptId \" + \n-            aId);\n-      }\n-    } else if (\n-        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n-      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n-      String host \u003d getHost(fEv.getContMgrAddress());\n-      containerFailedOnHost(host);\n+  public void handle(ContainerAllocatorEvent event) {\n+    int qSize \u003d eventQueue.size();\n+    if (qSize !\u003d 0 \u0026\u0026 qSize % 1000 \u003d\u003d 0) {\n+      LOG.info(\"Size of event-queue in RMContainerAllocator is \" + qSize);\n+    }\n+    int remCapacity \u003d eventQueue.remainingCapacity();\n+    if (remCapacity \u003c 1000) {\n+      LOG.warn(\"Very low remaining capacity in the event-queue \"\n+          + \"of RMContainerAllocator: \" + remCapacity);\n+    }\n+    try {\n+      eventQueue.put(event);\n+    } catch (InterruptedException e) {\n+      throw new YarnException(e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void handle(ContainerAllocatorEvent event) {\n    int qSize \u003d eventQueue.size();\n    if (qSize !\u003d 0 \u0026\u0026 qSize % 1000 \u003d\u003d 0) {\n      LOG.info(\"Size of event-queue in RMContainerAllocator is \" + qSize);\n    }\n    int remCapacity \u003d eventQueue.remainingCapacity();\n    if (remCapacity \u003c 1000) {\n      LOG.warn(\"Very low remaining capacity in the event-queue \"\n          + \"of RMContainerAllocator: \" + remCapacity);\n    }\n    try {\n      eventQueue.put(event);\n    } catch (InterruptedException e) {\n      throw new YarnException(e);\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-3572. Moved AM event dispatcher to a separate thread for performance reasons. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227426 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:37 PM",
          "commitName": "08f8abf5639d39167952dc5120b44fe35c63ff7a",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "04/01/12 9:10 AM",
          "commitNameOld": "55e94dc5ef4171c4e7b57942f22ead9a01dd9012",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 0.35,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,91 +1,16 @@\n-  public synchronized void handle(ContainerAllocatorEvent event) {\n-    LOG.info(\"Processing the event \" + event.toString());\n-    recalculateReduceSchedule \u003d true;\n-    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n-      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n-      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n-        if (mapResourceReqt \u003d\u003d 0) {\n-          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n-          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n-          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n-              * minSlotMemSize;\n-          JobID id \u003d TypeConverter.fromYarn(applicationId);\n-          JobId jobId \u003d TypeConverter.toYarn(id);\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n-              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n-              mapResourceReqt)));\n-          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n-          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n-            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n-            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n-            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n-            LOG.info(diagMsg);\n-            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n-                getJob().getID(), diagMsg));\n-            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n-          }\n-        }\n-        //set the rounded off memory\n-        reqEvent.getCapability().setMemory(mapResourceReqt);\n-        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n-      } else {\n-        if (reduceResourceReqt \u003d\u003d 0) {\n-          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n-          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n-          //round off on slotsize\n-          reduceResourceReqt \u003d (int) Math.ceil((float) \n-              reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n-          JobID id \u003d TypeConverter.fromYarn(applicationId);\n-          JobId jobId \u003d TypeConverter.toYarn(id);\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n-              new NormalizedResourceEvent(\n-                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n-              reduceResourceReqt)));\n-          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n-          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n-            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n-            \t\t\"supported max container capability in the cluster. Killing the \" +\n-            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n-            \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n-            LOG.info(diagMsg);\n-            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n-                getJob().getID(), diagMsg));\n-            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n-          }\n-        }\n-        //set the rounded off memory\n-        reqEvent.getCapability().setMemory(reduceResourceReqt);\n-        if (reqEvent.getEarlierAttemptFailed()) {\n-          //add to the front of queue for fail fast\n-          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n-        } else {\n-          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n-          //reduces are added to pending and are slowly ramped up\n-        }\n-      }\n-      \n-    } else if (\n-        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n-      TaskAttemptId aId \u003d event.getAttemptID();\n-      \n-      boolean removed \u003d scheduledRequests.remove(aId);\n-      if (!removed) {\n-        ContainerId containerId \u003d assignedRequests.get(aId);\n-        if (containerId !\u003d null) {\n-          removed \u003d true;\n-          assignedRequests.remove(aId);\n-          containersReleased++;\n-          release(containerId);\n-        }\n-      }\n-      if (!removed) {\n-        LOG.error(\"Could not deallocate container for task attemptId \" + \n-            aId);\n-      }\n-    } else if (\n-        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n-      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n-      String host \u003d getHost(fEv.getContMgrAddress());\n-      containerFailedOnHost(host);\n+  public void handle(ContainerAllocatorEvent event) {\n+    int qSize \u003d eventQueue.size();\n+    if (qSize !\u003d 0 \u0026\u0026 qSize % 1000 \u003d\u003d 0) {\n+      LOG.info(\"Size of event-queue in RMContainerAllocator is \" + qSize);\n+    }\n+    int remCapacity \u003d eventQueue.remainingCapacity();\n+    if (remCapacity \u003c 1000) {\n+      LOG.warn(\"Very low remaining capacity in the event-queue \"\n+          + \"of RMContainerAllocator: \" + remCapacity);\n+    }\n+    try {\n+      eventQueue.put(event);\n+    } catch (InterruptedException e) {\n+      throw new YarnException(e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void handle(ContainerAllocatorEvent event) {\n    int qSize \u003d eventQueue.size();\n    if (qSize !\u003d 0 \u0026\u0026 qSize % 1000 \u003d\u003d 0) {\n      LOG.info(\"Size of event-queue in RMContainerAllocator is \" + qSize);\n    }\n    int remCapacity \u003d eventQueue.remainingCapacity();\n    if (remCapacity \u003c 1000) {\n      LOG.warn(\"Very low remaining capacity in the event-queue \"\n          + \"of RMContainerAllocator: \" + remCapacity);\n    }\n    try {\n      eventQueue.put(event);\n    } catch (InterruptedException e) {\n      throw new YarnException(e);\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "fffdf661e30afd10331d2153ff052c141b7ebe4b": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2821. Added missing fields (resourcePerMap \u0026 resourcePerReduce) to JobSummary logs. Contributed by Mahadev Konar.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1188528 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/10/11 11:26 PM",
      "commitName": "fffdf661e30afd10331d2153ff052c141b7ebe4b",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "20/10/11 4:45 AM",
      "commitNameOld": "df2991c0cbc3f35c2640b93680667507c4f810dd",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 4.78,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,91 @@\n   public synchronized void handle(ContainerAllocatorEvent event) {\n     LOG.info(\"Processing the event \" + event.toString());\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceReqt \u003d\u003d 0) {\n           mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n-          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize) * minSlotMemSize;\n+          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n+              * minSlotMemSize;\n+          JobID id \u003d TypeConverter.fromYarn(applicationId);\n+          JobId jobId \u003d TypeConverter.toYarn(id);\n+          eventHandler.handle(new JobHistoryEvent(jobId, \n+              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n+              mapResourceReqt)));\n           LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n           if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n             String diagMsg \u003d \"MAP capability required is more than the supported \" +\n             \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n             mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 getJob().getID(), diagMsg));\n             eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(mapResourceReqt);\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceReqt \u003d\u003d 0) {\n           reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n           //round off on slotsize\n-          reduceResourceReqt \u003d (int) Math.ceil((float) reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n+          reduceResourceReqt \u003d (int) Math.ceil((float) \n+              reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n+          JobID id \u003d TypeConverter.fromYarn(applicationId);\n+          JobId jobId \u003d TypeConverter.toYarn(id);\n+          eventHandler.handle(new JobHistoryEvent(jobId, \n+              new NormalizedResourceEvent(\n+                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n+              reduceResourceReqt)));\n           LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n           if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n-            String diagMsg \u003d \"REDUCE capability required is more than the supported \" +\n-            \"max container capability in the cluster. Killing the Job. reduceResourceReqt: \" + \n-            reduceResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n+            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n+            \t\t\"supported max container capability in the cluster. Killing the \" +\n+            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n+            \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 getJob().getID(), diagMsg));\n             eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(reduceResourceReqt);\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n         } else {\n-          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));//reduces are added to pending and are slowly ramped up\n+          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n+          //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void handle(ContainerAllocatorEvent event) {\n    LOG.info(\"Processing the event \" + event.toString());\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n              * minSlotMemSize;\n          JobID id \u003d TypeConverter.fromYarn(applicationId);\n          JobId jobId \u003d TypeConverter.toYarn(id);\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n              mapResourceReqt)));\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          //round off on slotsize\n          reduceResourceReqt \u003d (int) Math.ceil((float) \n              reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n          JobID id \u003d TypeConverter.fromYarn(applicationId);\n          JobId jobId \u003d TypeConverter.toYarn(id);\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(\n                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceReqt)));\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n            \t\t\"supported max container capability in the cluster. Killing the \" +\n            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n            \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void handle(ContainerAllocatorEvent event) {\n    LOG.info(\"Processing the event \" + event.toString());\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize) * minSlotMemSize;\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          //round off on slotsize\n          reduceResourceReqt \u003d (int) Math.ceil((float) reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"REDUCE capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. reduceResourceReqt: \" + \n            reduceResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));//reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,76 @@\n+  public synchronized void handle(ContainerAllocatorEvent event) {\n+    LOG.info(\"Processing the event \" + event.toString());\n+    recalculateReduceSchedule \u003d true;\n+    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n+      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n+      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n+        if (mapResourceReqt \u003d\u003d 0) {\n+          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n+          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n+          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize) * minSlotMemSize;\n+          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n+          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n+            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n+            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n+            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n+            LOG.info(diagMsg);\n+            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n+                getJob().getID(), diagMsg));\n+            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n+          }\n+        }\n+        //set the rounded off memory\n+        reqEvent.getCapability().setMemory(mapResourceReqt);\n+        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n+      } else {\n+        if (reduceResourceReqt \u003d\u003d 0) {\n+          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n+          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n+          //round off on slotsize\n+          reduceResourceReqt \u003d (int) Math.ceil((float) reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n+          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n+          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n+            String diagMsg \u003d \"REDUCE capability required is more than the supported \" +\n+            \"max container capability in the cluster. Killing the Job. reduceResourceReqt: \" + \n+            reduceResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n+            LOG.info(diagMsg);\n+            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n+                getJob().getID(), diagMsg));\n+            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n+          }\n+        }\n+        //set the rounded off memory\n+        reqEvent.getCapability().setMemory(reduceResourceReqt);\n+        if (reqEvent.getEarlierAttemptFailed()) {\n+          //add to the front of queue for fail fast\n+          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n+        } else {\n+          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));//reduces are added to pending and are slowly ramped up\n+        }\n+      }\n+      \n+    } else if (\n+        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n+      TaskAttemptId aId \u003d event.getAttemptID();\n+      \n+      boolean removed \u003d scheduledRequests.remove(aId);\n+      if (!removed) {\n+        ContainerId containerId \u003d assignedRequests.get(aId);\n+        if (containerId !\u003d null) {\n+          removed \u003d true;\n+          assignedRequests.remove(aId);\n+          containersReleased++;\n+          release(containerId);\n+        }\n+      }\n+      if (!removed) {\n+        LOG.error(\"Could not deallocate container for task attemptId \" + \n+            aId);\n+      }\n+    } else if (\n+        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n+      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n+      String host \u003d getHost(fEv.getContMgrAddress());\n+      containerFailedOnHost(host);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void handle(ContainerAllocatorEvent event) {\n    LOG.info(\"Processing the event \" + event.toString());\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize) * minSlotMemSize;\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          //round off on slotsize\n          reduceResourceReqt \u003d (int) Math.ceil((float) reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"REDUCE capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. reduceResourceReqt: \" + \n            reduceResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));//reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java"
    }
  }
}