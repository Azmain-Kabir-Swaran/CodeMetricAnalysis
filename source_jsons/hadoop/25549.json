{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "TaskAttemptListenerImpl.java",
  "functionName": "getMapCompletionEvents",
  "functionId": "getMapCompletionEvents___jobIdentifier-JobID__startIndex-int__maxEvents-int__taskAttemptID-TaskAttemptID",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
  "functionStartLine": 335,
  "functionEndLine": 352,
  "numCommitsSeen": 51,
  "timeTaken": 10435,
  "changeHistory": [
    "74ffc7a74dc6cdd5a615cd2267400873f3c65ceb",
    "1195f844a9a74de6709ba7d8aaf70c21f27cd2b3",
    "bb74427da27ab90ade868c4fd89ed8ac3310aea2",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "74ffc7a74dc6cdd5a615cd2267400873f3c65ceb": "Ybodychange",
    "1195f844a9a74de6709ba7d8aaf70c21f27cd2b3": "Ymultichange(Yparameterchange,Ybodychange)",
    "bb74427da27ab90ade868c4fd89ed8ac3310aea2": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "74ffc7a74dc6cdd5a615cd2267400873f3c65ceb": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4946. Fix a performance problem for large jobs by reducing the number of map completion event type conversions. Contributed by Jason Lowe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1437103 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/01/13 11:07 AM",
      "commitName": "74ffc7a74dc6cdd5a615cd2267400873f3c65ceb",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "04/01/13 11:15 AM",
      "commitNameOld": "78ab699fe93cafbaff8f496be53d26aff40a68b1",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 17.99,
      "commitsBetweenForRepo": 109,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,18 @@\n   public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n       JobID jobIdentifier, int startIndex, int maxEvents,\n       TaskAttemptID taskAttemptID) throws IOException {\n     LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n         + \". startIndex \" + startIndex + \" maxEvents \" + maxEvents);\n \n     // TODO: shouldReset is never used. See TT. Ask for Removal.\n     boolean shouldReset \u003d false;\n     org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n       TypeConverter.toYarn(taskAttemptID);\n-    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n+    TaskCompletionEvent[] events \u003d\n         context.getJob(attemptID.getTaskId().getJobId()).getMapAttemptCompletionEvents(\n             startIndex, maxEvents);\n \n     taskHeartbeatHandler.progressing(attemptID);\n     \n-    return new MapTaskCompletionEventsUpdate(\n-        TypeConverter.fromYarn(events), shouldReset);\n+    return new MapTaskCompletionEventsUpdate(events, shouldReset);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n      JobID jobIdentifier, int startIndex, int maxEvents,\n      TaskAttemptID taskAttemptID) throws IOException {\n    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n        + \". startIndex \" + startIndex + \" maxEvents \" + maxEvents);\n\n    // TODO: shouldReset is never used. See TT. Ask for Removal.\n    boolean shouldReset \u003d false;\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n      TypeConverter.toYarn(taskAttemptID);\n    TaskCompletionEvent[] events \u003d\n        context.getJob(attemptID.getTaskId().getJobId()).getMapAttemptCompletionEvents(\n            startIndex, maxEvents);\n\n    taskHeartbeatHandler.progressing(attemptID);\n    \n    return new MapTaskCompletionEventsUpdate(events, shouldReset);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
      "extendedDetails": {}
    },
    "1195f844a9a74de6709ba7d8aaf70c21f27cd2b3": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-4733. Reducer can fail to make progress during shuffle if too many reducers complete consecutively. Contributed by Jason Lowe via.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1400264 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/10/12 1:18 PM",
      "commitName": "1195f844a9a74de6709ba7d8aaf70c21f27cd2b3",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-4733. Reducer can fail to make progress during shuffle if too many reducers complete consecutively. Contributed by Jason Lowe via.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1400264 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "19/10/12 1:18 PM",
          "commitName": "1195f844a9a74de6709ba7d8aaf70c21f27cd2b3",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "03/05/12 11:35 AM",
          "commitNameOld": "48414b08277b86cdbc34ae36d7c4d204fd838294",
          "commitAuthorOld": "Robert Joseph Evans",
          "daysBetweenCommits": 169.07,
          "commitsBetweenForRepo": 965,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,19 @@\n   public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n-      JobID jobIdentifier, int fromEventId, int maxEvents,\n+      JobID jobIdentifier, int startIndex, int maxEvents,\n       TaskAttemptID taskAttemptID) throws IOException {\n     LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n-        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n+        + \". startIndex \" + startIndex + \" maxEvents \" + maxEvents);\n \n     // TODO: shouldReset is never used. See TT. Ask for Removal.\n     boolean shouldReset \u003d false;\n     org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n       TypeConverter.toYarn(taskAttemptID);\n     org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n-        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n-            fromEventId, maxEvents);\n+        context.getJob(attemptID.getTaskId().getJobId()).getMapAttemptCompletionEvents(\n+            startIndex, maxEvents);\n \n     taskHeartbeatHandler.progressing(attemptID);\n-\n-    // filter the events to return only map completion events in old format\n-    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n-    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n-      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n-        mapEvents.add(TypeConverter.fromYarn(event));\n-      }\n-    }\n     \n     return new MapTaskCompletionEventsUpdate(\n-        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n+        TypeConverter.fromYarn(events), shouldReset);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n      JobID jobIdentifier, int startIndex, int maxEvents,\n      TaskAttemptID taskAttemptID) throws IOException {\n    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n        + \". startIndex \" + startIndex + \" maxEvents \" + maxEvents);\n\n    // TODO: shouldReset is never used. See TT. Ask for Removal.\n    boolean shouldReset \u003d false;\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n      TypeConverter.toYarn(taskAttemptID);\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n        context.getJob(attemptID.getTaskId().getJobId()).getMapAttemptCompletionEvents(\n            startIndex, maxEvents);\n\n    taskHeartbeatHandler.progressing(attemptID);\n    \n    return new MapTaskCompletionEventsUpdate(\n        TypeConverter.fromYarn(events), shouldReset);\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
          "extendedDetails": {
            "oldValue": "[jobIdentifier-JobID, fromEventId-int, maxEvents-int, taskAttemptID-TaskAttemptID]",
            "newValue": "[jobIdentifier-JobID, startIndex-int, maxEvents-int, taskAttemptID-TaskAttemptID]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-4733. Reducer can fail to make progress during shuffle if too many reducers complete consecutively. Contributed by Jason Lowe via.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1400264 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "19/10/12 1:18 PM",
          "commitName": "1195f844a9a74de6709ba7d8aaf70c21f27cd2b3",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "03/05/12 11:35 AM",
          "commitNameOld": "48414b08277b86cdbc34ae36d7c4d204fd838294",
          "commitAuthorOld": "Robert Joseph Evans",
          "daysBetweenCommits": 169.07,
          "commitsBetweenForRepo": 965,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,19 @@\n   public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n-      JobID jobIdentifier, int fromEventId, int maxEvents,\n+      JobID jobIdentifier, int startIndex, int maxEvents,\n       TaskAttemptID taskAttemptID) throws IOException {\n     LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n-        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n+        + \". startIndex \" + startIndex + \" maxEvents \" + maxEvents);\n \n     // TODO: shouldReset is never used. See TT. Ask for Removal.\n     boolean shouldReset \u003d false;\n     org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n       TypeConverter.toYarn(taskAttemptID);\n     org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n-        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n-            fromEventId, maxEvents);\n+        context.getJob(attemptID.getTaskId().getJobId()).getMapAttemptCompletionEvents(\n+            startIndex, maxEvents);\n \n     taskHeartbeatHandler.progressing(attemptID);\n-\n-    // filter the events to return only map completion events in old format\n-    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n-    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n-      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n-        mapEvents.add(TypeConverter.fromYarn(event));\n-      }\n-    }\n     \n     return new MapTaskCompletionEventsUpdate(\n-        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n+        TypeConverter.fromYarn(events), shouldReset);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n      JobID jobIdentifier, int startIndex, int maxEvents,\n      TaskAttemptID taskAttemptID) throws IOException {\n    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n        + \". startIndex \" + startIndex + \" maxEvents \" + maxEvents);\n\n    // TODO: shouldReset is never used. See TT. Ask for Removal.\n    boolean shouldReset \u003d false;\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n      TypeConverter.toYarn(taskAttemptID);\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n        context.getJob(attemptID.getTaskId().getJobId()).getMapAttemptCompletionEvents(\n            startIndex, maxEvents);\n\n    taskHeartbeatHandler.progressing(attemptID);\n    \n    return new MapTaskCompletionEventsUpdate(\n        TypeConverter.fromYarn(events), shouldReset);\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "bb74427da27ab90ade868c4fd89ed8ac3310aea2": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4089. Hung Tasks never time out. (Robert Evans via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308531 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/04/12 1:20 PM",
      "commitName": "bb74427da27ab90ade868c4fd89ed8ac3310aea2",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "13/01/12 1:31 PM",
      "commitNameOld": "0c278b0f636a01c81aba9e46fe7658fcdfb0f33c",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 79.95,
      "commitsBetweenForRepo": 599,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n       JobID jobIdentifier, int fromEventId, int maxEvents,\n       TaskAttemptID taskAttemptID) throws IOException {\n     LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n         + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n \n     // TODO: shouldReset is never used. See TT. Ask for Removal.\n     boolean shouldReset \u003d false;\n     org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n       TypeConverter.toYarn(taskAttemptID);\n     org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n         context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n             fromEventId, maxEvents);\n \n-    taskHeartbeatHandler.receivedPing(attemptID);\n+    taskHeartbeatHandler.progressing(attemptID);\n \n     // filter the events to return only map completion events in old format\n     List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n     for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n       if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n         mapEvents.add(TypeConverter.fromYarn(event));\n       }\n     }\n     \n     return new MapTaskCompletionEventsUpdate(\n         mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n      JobID jobIdentifier, int fromEventId, int maxEvents,\n      TaskAttemptID taskAttemptID) throws IOException {\n    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n\n    // TODO: shouldReset is never used. See TT. Ask for Removal.\n    boolean shouldReset \u003d false;\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n      TypeConverter.toYarn(taskAttemptID);\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n            fromEventId, maxEvents);\n\n    taskHeartbeatHandler.progressing(attemptID);\n\n    // filter the events to return only map completion events in old format\n    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n        mapEvents.add(TypeConverter.fromYarn(event));\n      }\n    }\n    \n    return new MapTaskCompletionEventsUpdate(\n        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n      JobID jobIdentifier, int fromEventId, int maxEvents,\n      TaskAttemptID taskAttemptID) throws IOException {\n    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n\n    // TODO: shouldReset is never used. See TT. Ask for Removal.\n    boolean shouldReset \u003d false;\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n      TypeConverter.toYarn(taskAttemptID);\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n            fromEventId, maxEvents);\n\n    taskHeartbeatHandler.receivedPing(attemptID);\n\n    // filter the events to return only map completion events in old format\n    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n        mapEvents.add(TypeConverter.fromYarn(event));\n      }\n    }\n    \n    return new MapTaskCompletionEventsUpdate(\n        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,27 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n+      JobID jobIdentifier, int fromEventId, int maxEvents,\n+      TaskAttemptID taskAttemptID) throws IOException {\n+    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n+        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n+\n+    // TODO: shouldReset is never used. See TT. Ask for Removal.\n+    boolean shouldReset \u003d false;\n+    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n+      TypeConverter.toYarn(taskAttemptID);\n+    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n+        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n+            fromEventId, maxEvents);\n+\n+    taskHeartbeatHandler.receivedPing(attemptID);\n+\n+    // filter the events to return only map completion events in old format\n+    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n+    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n+      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n+        mapEvents.add(TypeConverter.fromYarn(event));\n       }\n     }\n-    return numNewMaps;\n+    \n+    return new MapTaskCompletionEventsUpdate(\n+        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n      JobID jobIdentifier, int fromEventId, int maxEvents,\n      TaskAttemptID taskAttemptID) throws IOException {\n    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n\n    // TODO: shouldReset is never used. See TT. Ask for Removal.\n    boolean shouldReset \u003d false;\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n      TypeConverter.toYarn(taskAttemptID);\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n            fromEventId, maxEvents);\n\n    taskHeartbeatHandler.receivedPing(attemptID);\n\n    // filter the events to return only map completion events in old format\n    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n        mapEvents.add(TypeConverter.fromYarn(event));\n      }\n    }\n    \n    return new MapTaskCompletionEventsUpdate(\n        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
          "extendedDetails": {
            "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
            "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
            "oldMethodName": "getMapCompletionEvents",
            "newMethodName": "getMapCompletionEvents"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,27 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n+      JobID jobIdentifier, int fromEventId, int maxEvents,\n+      TaskAttemptID taskAttemptID) throws IOException {\n+    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n+        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n+\n+    // TODO: shouldReset is never used. See TT. Ask for Removal.\n+    boolean shouldReset \u003d false;\n+    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n+      TypeConverter.toYarn(taskAttemptID);\n+    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n+        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n+            fromEventId, maxEvents);\n+\n+    taskHeartbeatHandler.receivedPing(attemptID);\n+\n+    // filter the events to return only map completion events in old format\n+    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n+    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n+      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n+        mapEvents.add(TypeConverter.fromYarn(event));\n       }\n     }\n-    return numNewMaps;\n+    \n+    return new MapTaskCompletionEventsUpdate(\n+        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n      JobID jobIdentifier, int fromEventId, int maxEvents,\n      TaskAttemptID taskAttemptID) throws IOException {\n    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n\n    // TODO: shouldReset is never used. See TT. Ask for Removal.\n    boolean shouldReset \u003d false;\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n      TypeConverter.toYarn(taskAttemptID);\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n            fromEventId, maxEvents);\n\n    taskHeartbeatHandler.receivedPing(attemptID);\n\n    // filter the events to return only map completion events in old format\n    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n        mapEvents.add(TypeConverter.fromYarn(event));\n      }\n    }\n    \n    return new MapTaskCompletionEventsUpdate(\n        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
          "extendedDetails": {
            "oldValue": "int",
            "newValue": "MapTaskCompletionEventsUpdate"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,27 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n+      JobID jobIdentifier, int fromEventId, int maxEvents,\n+      TaskAttemptID taskAttemptID) throws IOException {\n+    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n+        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n+\n+    // TODO: shouldReset is never used. See TT. Ask for Removal.\n+    boolean shouldReset \u003d false;\n+    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n+      TypeConverter.toYarn(taskAttemptID);\n+    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n+        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n+            fromEventId, maxEvents);\n+\n+    taskHeartbeatHandler.receivedPing(attemptID);\n+\n+    // filter the events to return only map completion events in old format\n+    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n+    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n+      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n+        mapEvents.add(TypeConverter.fromYarn(event));\n       }\n     }\n-    return numNewMaps;\n+    \n+    return new MapTaskCompletionEventsUpdate(\n+        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n      JobID jobIdentifier, int fromEventId, int maxEvents,\n      TaskAttemptID taskAttemptID) throws IOException {\n    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n\n    // TODO: shouldReset is never used. See TT. Ask for Removal.\n    boolean shouldReset \u003d false;\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n      TypeConverter.toYarn(taskAttemptID);\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n            fromEventId, maxEvents);\n\n    taskHeartbeatHandler.receivedPing(attemptID);\n\n    // filter the events to return only map completion events in old format\n    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n        mapEvents.add(TypeConverter.fromYarn(event));\n      }\n    }\n    \n    return new MapTaskCompletionEventsUpdate(\n        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,27 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n+      JobID jobIdentifier, int fromEventId, int maxEvents,\n+      TaskAttemptID taskAttemptID) throws IOException {\n+    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n+        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n+\n+    // TODO: shouldReset is never used. See TT. Ask for Removal.\n+    boolean shouldReset \u003d false;\n+    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n+      TypeConverter.toYarn(taskAttemptID);\n+    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n+        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n+            fromEventId, maxEvents);\n+\n+    taskHeartbeatHandler.receivedPing(attemptID);\n+\n+    // filter the events to return only map completion events in old format\n+    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n+    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n+      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n+        mapEvents.add(TypeConverter.fromYarn(event));\n       }\n     }\n-    return numNewMaps;\n+    \n+    return new MapTaskCompletionEventsUpdate(\n+        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n      JobID jobIdentifier, int fromEventId, int maxEvents,\n      TaskAttemptID taskAttemptID) throws IOException {\n    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n\n    // TODO: shouldReset is never used. See TT. Ask for Removal.\n    boolean shouldReset \u003d false;\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n      TypeConverter.toYarn(taskAttemptID);\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n            fromEventId, maxEvents);\n\n    taskHeartbeatHandler.receivedPing(attemptID);\n\n    // filter the events to return only map completion events in old format\n    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n        mapEvents.add(TypeConverter.fromYarn(event));\n      }\n    }\n    \n    return new MapTaskCompletionEventsUpdate(\n        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,27 @@\n-  private int getMapCompletionEvents() throws IOException {\n-    \n-    int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n+      JobID jobIdentifier, int fromEventId, int maxEvents,\n+      TaskAttemptID taskAttemptID) throws IOException {\n+    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n+        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n+\n+    // TODO: shouldReset is never used. See TT. Ask for Removal.\n+    boolean shouldReset \u003d false;\n+    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n+      TypeConverter.toYarn(taskAttemptID);\n+    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n+        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n+            fromEventId, maxEvents);\n+\n+    taskHeartbeatHandler.receivedPing(attemptID);\n+\n+    // filter the events to return only map completion events in old format\n+    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n+    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n+      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n+        mapEvents.add(TypeConverter.fromYarn(event));\n       }\n     }\n-    return numNewMaps;\n+    \n+    return new MapTaskCompletionEventsUpdate(\n+        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public MapTaskCompletionEventsUpdate getMapCompletionEvents(\n      JobID jobIdentifier, int fromEventId, int maxEvents,\n      TaskAttemptID taskAttemptID) throws IOException {\n    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString()\n        + \". fromEventID \" + fromEventId + \" maxEvents \" + maxEvents);\n\n    // TODO: shouldReset is never used. See TT. Ask for Removal.\n    boolean shouldReset \u003d false;\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID \u003d\n      TypeConverter.toYarn(taskAttemptID);\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[] events \u003d\n        context.getJob(attemptID.getTaskId().getJobId()).getTaskAttemptCompletionEvents(\n            fromEventId, maxEvents);\n\n    taskHeartbeatHandler.receivedPing(attemptID);\n\n    // filter the events to return only map completion events in old format\n    List\u003cTaskCompletionEvent\u003e mapEvents \u003d new ArrayList\u003cTaskCompletionEvent\u003e();\n    for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent event : events) {\n      if (TaskType.MAP.equals(event.getAttemptId().getTaskId().getTaskType())) {\n        mapEvents.add(TypeConverter.fromYarn(event));\n      }\n    }\n    \n    return new MapTaskCompletionEventsUpdate(\n        mapEvents.toArray(new TaskCompletionEvent[0]), shouldReset);\n  }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[jobIdentifier-JobID, fromEventId-int, maxEvents-int, taskAttemptID-TaskAttemptID]"
          }
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,63 @@\n+  private int getMapCompletionEvents() throws IOException {\n+    \n+    int numNewMaps \u003d 0;\n+    \n+    MapTaskCompletionEventsUpdate update \u003d \n+      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n+                                       reduce.getJobID(), \n+                                       fromEventId, \n+                                       MAX_EVENTS_TO_FETCH,\n+                                       (org.apache.hadoop.mapred.TaskAttemptID)\n+                                         reduce);\n+    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n+    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n+             fromEventId);\n+      \n+    // Check if the reset is required.\n+    // Since there is no ordering of the task completion events at the \n+    // reducer, the only option to sync with the new jobtracker is to reset \n+    // the events index\n+    if (update.shouldReset()) {\n+      fromEventId \u003d 0;\n+      scheduler.resetKnownMaps();\n+    }\n+    \n+    // Update the last seen event ID\n+    fromEventId +\u003d events.length;\n+    \n+    // Process the TaskCompletionEvents:\n+    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n+    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n+    //    fetching from those maps.\n+    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n+    //    outputs at all.\n+    for (TaskCompletionEvent event : events) {\n+      switch (event.getTaskStatus()) {\n+        case SUCCEEDED:\n+          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n+          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n+                                      u.toString(),\n+                                      event.getTaskAttemptId());\n+          numNewMaps ++;\n+          int duration \u003d event.getTaskRunTime();\n+          if (duration \u003e maxMapRuntime) {\n+            maxMapRuntime \u003d duration;\n+            scheduler.informMaxMapRunTime(maxMapRuntime);\n+          }\n+          break;\n+        case FAILED:\n+        case KILLED:\n+        case OBSOLETE:\n+          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n+          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n+                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n+          break;\n+        case TIPFAILED:\n+          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n+          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n+               event.getTaskAttemptId() + \"\u0027\");\n+          break;\n+      }\n+    }\n+    return numNewMaps;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private int getMapCompletionEvents() throws IOException {\n    \n    int numNewMaps \u003d 0;\n    \n    MapTaskCompletionEventsUpdate update \u003d \n      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n                                       reduce.getJobID(), \n                                       fromEventId, \n                                       MAX_EVENTS_TO_FETCH,\n                                       (org.apache.hadoop.mapred.TaskAttemptID)\n                                         reduce);\n    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n             fromEventId);\n      \n    // Check if the reset is required.\n    // Since there is no ordering of the task completion events at the \n    // reducer, the only option to sync with the new jobtracker is to reset \n    // the events index\n    if (update.shouldReset()) {\n      fromEventId \u003d 0;\n      scheduler.resetKnownMaps();\n    }\n    \n    // Update the last seen event ID\n    fromEventId +\u003d events.length;\n    \n    // Process the TaskCompletionEvents:\n    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n    //    fetching from those maps.\n    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n    //    outputs at all.\n    for (TaskCompletionEvent event : events) {\n      switch (event.getTaskStatus()) {\n        case SUCCEEDED:\n          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n                                      u.toString(),\n                                      event.getTaskAttemptId());\n          numNewMaps ++;\n          int duration \u003d event.getTaskRunTime();\n          if (duration \u003e maxMapRuntime) {\n            maxMapRuntime \u003d duration;\n            scheduler.informMaxMapRunTime(maxMapRuntime);\n          }\n          break;\n        case FAILED:\n        case KILLED:\n        case OBSOLETE:\n          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n          break;\n        case TIPFAILED:\n          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n               event.getTaskAttemptId() + \"\u0027\");\n          break;\n      }\n    }\n    return numNewMaps;\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java"
    }
  }
}