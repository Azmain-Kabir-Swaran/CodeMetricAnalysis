{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "adjustSrcNodesAndIndices",
  "functionId": "adjustSrcNodesAndIndices___block-BlockInfoStriped__srcNodes-DatanodeDescriptor[]__indices-List__Byte____newSrcNodes-DatanodeDescriptor[]__newIndices-byte[]",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 2174,
  "functionEndLine": 2193,
  "numCommitsSeen": 477,
  "timeTaken": 1719,
  "changeHistory": [
    "2ffec347eb4303ad78643431cd2e517d54bc3282"
  ],
  "changeHistoryShort": {
    "2ffec347eb4303ad78643431cd2e517d54bc3282": "Yintroduced"
  },
  "changeHistoryDetails": {
    "2ffec347eb4303ad78643431cd2e517d54bc3282": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-14946. Erasure Coding: Block recovery failed during decommissioning. Contributed by Fei Hui.\n",
      "commitDate": "04/11/19 12:07 PM",
      "commitName": "2ffec347eb4303ad78643431cd2e517d54bc3282",
      "commitAuthor": "Ayush Saxena",
      "diff": "@@ -0,0 +1,20 @@\n+  private void adjustSrcNodesAndIndices(BlockInfoStriped block,\n+      DatanodeDescriptor[] srcNodes, List\u003cByte\u003e indices,\n+      DatanodeDescriptor[] newSrcNodes, byte[] newIndices) {\n+    BitSet bitSet \u003d new BitSet(block.getRealTotalBlockNum());\n+    List\u003cInteger\u003e skipIndexList \u003d new ArrayList\u003c\u003e();\n+    for (int i \u003d 0, j \u003d 0; i \u003c srcNodes.length; i++) {\n+      if (!bitSet.get(indices.get(i))) {\n+        bitSet.set(indices.get(i));\n+        newSrcNodes[j] \u003d srcNodes[i];\n+        newIndices[j++] \u003d indices.get(i);\n+      } else {\n+        skipIndexList.add(i);\n+      }\n+    }\n+    for(int i \u003d srcNodes.length - skipIndexList.size(), j \u003d 0;\n+        i \u003c srcNodes.length; i++, j++) {\n+      newSrcNodes[i] \u003d srcNodes[skipIndexList.get(j)];\n+      newIndices[i] \u003d indices.get(skipIndexList.get(j));\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void adjustSrcNodesAndIndices(BlockInfoStriped block,\n      DatanodeDescriptor[] srcNodes, List\u003cByte\u003e indices,\n      DatanodeDescriptor[] newSrcNodes, byte[] newIndices) {\n    BitSet bitSet \u003d new BitSet(block.getRealTotalBlockNum());\n    List\u003cInteger\u003e skipIndexList \u003d new ArrayList\u003c\u003e();\n    for (int i \u003d 0, j \u003d 0; i \u003c srcNodes.length; i++) {\n      if (!bitSet.get(indices.get(i))) {\n        bitSet.set(indices.get(i));\n        newSrcNodes[j] \u003d srcNodes[i];\n        newIndices[j++] \u003d indices.get(i);\n      } else {\n        skipIndexList.add(i);\n      }\n    }\n    for(int i \u003d srcNodes.length - skipIndexList.size(), j \u003d 0;\n        i \u003c srcNodes.length; i++, j++) {\n      newSrcNodes[i] \u003d srcNodes[skipIndexList.get(j)];\n      newIndices[i] \u003d indices.get(skipIndexList.get(j));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    }
  }
}