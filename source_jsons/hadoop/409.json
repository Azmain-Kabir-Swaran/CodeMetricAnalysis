{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReaderRemote.java",
  "functionName": "read",
  "functionId": "read___buf-byte[]__off-int__len-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
  "functionStartLine": 134,
  "functionEndLine": 162,
  "numCommitsSeen": 46,
  "timeTaken": 7052,
  "changeHistory": [
    "344f324710522ffb27852c1a673c4f7d3d6eac4b",
    "5d748bd056a32f2c6922514cd0c5b31d866a9919",
    "8b281bce85474501868d68f8d5590a6086abb7b7",
    "f308561f1d885491b88db73ac63003202056d661",
    "826ae1c26d31f87d88efc920b271bec7eec2e17a",
    "e89fc53a1d264fde407dd2c36defab5241cd0b52",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
    "837e17b2eac1471d93e2eff395272063b265fee7",
    "239b2742d0e80d13c970fd062af4930e672fe903",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "dd86860633d2ed64705b669a75bf318442ed6225",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "344f324710522ffb27852c1a673c4f7d3d6eac4b": "Ybodychange",
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": "Ybodychange",
    "8b281bce85474501868d68f8d5590a6086abb7b7": "Ybodychange",
    "f308561f1d885491b88db73ac63003202056d661": "Yfilerename",
    "826ae1c26d31f87d88efc920b271bec7eec2e17a": "Yfilerename",
    "e89fc53a1d264fde407dd2c36defab5241cd0b52": "Ybodychange",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": "Ybodychange",
    "837e17b2eac1471d93e2eff395272063b265fee7": "Ybodychange",
    "239b2742d0e80d13c970fd062af4930e672fe903": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "dd86860633d2ed64705b669a75bf318442ed6225": "Ymovefromfile",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "344f324710522ffb27852c1a673c4f7d3d6eac4b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13712. BlockReaderRemote.read() logging improvement. Contributed by Gergo Repas.\n",
      "commitDate": "03/07/18 2:07 AM",
      "commitName": "344f324710522ffb27852c1a673c4f7d3d6eac4b",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "02/07/18 3:11 AM",
      "commitNameOld": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.96,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,29 @@\n   public synchronized int read(byte[] buf, int off, int len)\n       throws IOException {\n-    UUID randomId \u003d (LOG.isTraceEnabled() ? UUID.randomUUID() : null);\n-    LOG.trace(\"Starting read #{} file {} from datanode {}\",\n-        randomId, filename, datanodeID.getHostName());\n+    boolean logTraceEnabled \u003d LOG.isTraceEnabled();\n+    UUID randomId \u003d null;\n+    if (logTraceEnabled) {\n+      randomId \u003d UUID.randomUUID();\n+      LOG.trace(\"Starting read #{} file {} from datanode {}\",\n+          randomId, filename, datanodeID.getHostName());\n+    }\n \n     if (curDataSlice \u003d\u003d null ||\n         curDataSlice.remaining() \u003d\u003d 0 \u0026\u0026 bytesNeededToFinish \u003e 0) {\n       readNextPacket();\n     }\n \n-    LOG.trace(\"Finishing read #{}\", randomId);\n+    if (logTraceEnabled) {\n+      LOG.trace(\"Finishing read #{}\", randomId);\n+    }\n \n     if (curDataSlice.remaining() \u003d\u003d 0) {\n       // we\u0027re at EOF now\n       return -1;\n     }\n \n     int nRead \u003d Math.min(curDataSlice.remaining(), len);\n     curDataSlice.get(buf, off, nRead);\n \n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len)\n      throws IOException {\n    boolean logTraceEnabled \u003d LOG.isTraceEnabled();\n    UUID randomId \u003d null;\n    if (logTraceEnabled) {\n      randomId \u003d UUID.randomUUID();\n      LOG.trace(\"Starting read #{} file {} from datanode {}\",\n          randomId, filename, datanodeID.getHostName());\n    }\n\n    if (curDataSlice \u003d\u003d null ||\n        curDataSlice.remaining() \u003d\u003d 0 \u0026\u0026 bytesNeededToFinish \u003e 0) {\n      readNextPacket();\n    }\n\n    if (logTraceEnabled) {\n      LOG.trace(\"Finishing read #{}\", randomId);\n    }\n\n    if (curDataSlice.remaining() \u003d\u003d 0) {\n      // we\u0027re at EOF now\n      return -1;\n    }\n\n    int nRead \u003d Math.min(curDataSlice.remaining(), len);\n    curDataSlice.get(buf, off, nRead);\n\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
      "extendedDetails": {}
    },
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13702. Remove HTrace hooks from DFSClient to reduce CPU usage. Contributed by Todd Lipcon.\n",
      "commitDate": "02/07/18 3:11 AM",
      "commitName": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "02/07/16 8:56 PM",
      "commitNameOld": "8b281bce85474501868d68f8d5590a6086abb7b7",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 729.26,
      "commitsBetweenForRepo": 5537,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,23 @@\n   public synchronized int read(byte[] buf, int off, int len)\n       throws IOException {\n     UUID randomId \u003d (LOG.isTraceEnabled() ? UUID.randomUUID() : null);\n     LOG.trace(\"Starting read #{} file {} from datanode {}\",\n         randomId, filename, datanodeID.getHostName());\n \n     if (curDataSlice \u003d\u003d null ||\n         curDataSlice.remaining() \u003d\u003d 0 \u0026\u0026 bytesNeededToFinish \u003e 0) {\n-      try (TraceScope ignored \u003d tracer.newScope(\n-          \"BlockReaderRemote2#readNextPacket(\" + blockId + \")\")) {\n-        readNextPacket();\n-      }\n+      readNextPacket();\n     }\n \n     LOG.trace(\"Finishing read #{}\", randomId);\n \n     if (curDataSlice.remaining() \u003d\u003d 0) {\n       // we\u0027re at EOF now\n       return -1;\n     }\n \n     int nRead \u003d Math.min(curDataSlice.remaining(), len);\n     curDataSlice.get(buf, off, nRead);\n \n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len)\n      throws IOException {\n    UUID randomId \u003d (LOG.isTraceEnabled() ? UUID.randomUUID() : null);\n    LOG.trace(\"Starting read #{} file {} from datanode {}\",\n        randomId, filename, datanodeID.getHostName());\n\n    if (curDataSlice \u003d\u003d null ||\n        curDataSlice.remaining() \u003d\u003d 0 \u0026\u0026 bytesNeededToFinish \u003e 0) {\n      readNextPacket();\n    }\n\n    LOG.trace(\"Finishing read #{}\", randomId);\n\n    if (curDataSlice.remaining() \u003d\u003d 0) {\n      // we\u0027re at EOF now\n      return -1;\n    }\n\n    int nRead \u003d Math.min(curDataSlice.remaining(), len);\n    curDataSlice.get(buf, off, nRead);\n\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
      "extendedDetails": {}
    },
    "8b281bce85474501868d68f8d5590a6086abb7b7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10548. Remove the long deprecated BlockReaderRemote. Contributed by Kai Zheng\n",
      "commitDate": "02/07/16 8:56 PM",
      "commitName": "8b281bce85474501868d68f8d5590a6086abb7b7",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "25/04/16 12:01 PM",
      "commitNameOld": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 68.37,
      "commitsBetweenForRepo": 478,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,26 @@\n   public synchronized int read(byte[] buf, int off, int len)\n       throws IOException {\n+    UUID randomId \u003d (LOG.isTraceEnabled() ? UUID.randomUUID() : null);\n+    LOG.trace(\"Starting read #{} file {} from datanode {}\",\n+        randomId, filename, datanodeID.getHostName());\n \n-    // This has to be set here, *before* the skip, since we can\n-    // hit EOS during the skip, in the case that our entire read\n-    // is smaller than the checksum chunk.\n-    boolean eosBefore \u003d eos;\n-\n-    //for the first read, skip the extra bytes at the front.\n-    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n-      // Skip these bytes. But don\u0027t call this.skip()!\n-      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n-      if ( super.readAndDiscard(toSkip) !\u003d toSkip ) {\n-        // should never happen\n-        throw new IOException(\"Could not skip required number of bytes\");\n+    if (curDataSlice \u003d\u003d null ||\n+        curDataSlice.remaining() \u003d\u003d 0 \u0026\u0026 bytesNeededToFinish \u003e 0) {\n+      try (TraceScope ignored \u003d tracer.newScope(\n+          \"BlockReaderRemote2#readNextPacket(\" + blockId + \")\")) {\n+        readNextPacket();\n       }\n     }\n \n-    int nRead \u003d super.read(buf, off, len);\n+    LOG.trace(\"Finishing read #{}\", randomId);\n \n-    // if eos was set in the previous read, send a status code to the DN\n-    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n-      if (needChecksum()) {\n-        sendReadResult(peer, Status.CHECKSUM_OK);\n-      } else {\n-        sendReadResult(peer, Status.SUCCESS);\n-      }\n+    if (curDataSlice.remaining() \u003d\u003d 0) {\n+      // we\u0027re at EOF now\n+      return -1;\n     }\n+\n+    int nRead \u003d Math.min(curDataSlice.remaining(), len);\n+    curDataSlice.get(buf, off, nRead);\n+\n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len)\n      throws IOException {\n    UUID randomId \u003d (LOG.isTraceEnabled() ? UUID.randomUUID() : null);\n    LOG.trace(\"Starting read #{} file {} from datanode {}\",\n        randomId, filename, datanodeID.getHostName());\n\n    if (curDataSlice \u003d\u003d null ||\n        curDataSlice.remaining() \u003d\u003d 0 \u0026\u0026 bytesNeededToFinish \u003e 0) {\n      try (TraceScope ignored \u003d tracer.newScope(\n          \"BlockReaderRemote2#readNextPacket(\" + blockId + \")\")) {\n        readNextPacket();\n      }\n    }\n\n    LOG.trace(\"Finishing read #{}\", randomId);\n\n    if (curDataSlice.remaining() \u003d\u003d 0) {\n      // we\u0027re at EOF now\n      return -1;\n    }\n\n    int nRead \u003d Math.min(curDataSlice.remaining(), len);\n    curDataSlice.get(buf, off, nRead);\n\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
      "extendedDetails": {}
    },
    "f308561f1d885491b88db73ac63003202056d661": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
      "commitDate": "25/04/16 12:01 PM",
      "commitName": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "25/04/16 9:38 AM",
      "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len)\n      throws IOException {\n\n    // This has to be set here, *before* the skip, since we can\n    // hit EOS during the skip, in the case that our entire read\n    // is smaller than the checksum chunk.\n    boolean eosBefore \u003d eos;\n\n    //for the first read, skip the extra bytes at the front.\n    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n      // Skip these bytes. But don\u0027t call this.skip()!\n      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n      if ( super.readAndDiscard(toSkip) !\u003d toSkip ) {\n        // should never happen\n        throw new IOException(\"Could not skip required number of bytes\");\n      }\n    }\n\n    int nRead \u003d super.read(buf, off, len);\n\n    // if eos was set in the previous read, send a status code to the DN\n    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n      if (needChecksum()) {\n        sendReadResult(peer, Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(peer, Status.SUCCESS);\n      }\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java"
      }
    },
    "826ae1c26d31f87d88efc920b271bec7eec2e17a": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8990. Move RemoteBlockReader to hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "31/08/15 1:54 PM",
      "commitName": "826ae1c26d31f87d88efc920b271bec7eec2e17a",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "31/08/15 11:48 AM",
      "commitNameOld": "caa04de149030691b7bc952b534c6128db217ed2",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.09,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) \n                               throws IOException {\n    \n    // This has to be set here, *before* the skip, since we can\n    // hit EOS during the skip, in the case that our entire read\n    // is smaller than the checksum chunk.\n    boolean eosBefore \u003d eos;\n\n    //for the first read, skip the extra bytes at the front.\n    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n      // Skip these bytes. But don\u0027t call this.skip()!\n      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n      if ( super.readAndDiscard(toSkip) !\u003d toSkip ) {\n        // should never happen\n        throw new IOException(\"Could not skip required number of bytes\");\n      }\n    }\n    \n    int nRead \u003d super.read(buf, off, len);\n\n    // if eos was set in the previous read, send a status code to the DN\n    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n      if (needChecksum()) {\n        sendReadResult(peer, Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(peer, Status.SUCCESS);\n      }\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java"
      }
    },
    "e89fc53a1d264fde407dd2c36defab5241cd0b52": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5574. Remove buffer copy in BlockReader.skip. Contributed by Binglin Chang.\n",
      "commitDate": "30/04/15 3:11 AM",
      "commitName": "e89fc53a1d264fde407dd2c36defab5241cd0b52",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "04/03/15 5:51 PM",
      "commitNameOld": "430b5371883e22abb65f37c3e3d4afc3f421fc89",
      "commitAuthorOld": "Dongming Liang",
      "daysBetweenCommits": 56.35,
      "commitsBetweenForRepo": 482,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,30 @@\n   public synchronized int read(byte[] buf, int off, int len) \n                                throws IOException {\n     \n     // This has to be set here, *before* the skip, since we can\n     // hit EOS during the skip, in the case that our entire read\n     // is smaller than the checksum chunk.\n     boolean eosBefore \u003d eos;\n \n     //for the first read, skip the extra bytes at the front.\n     if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n       // Skip these bytes. But don\u0027t call this.skip()!\n       int toSkip \u003d (int)(startOffset - firstChunkOffset);\n-      if ( skipBuf \u003d\u003d null ) {\n-        skipBuf \u003d new byte[bytesPerChecksum];\n-      }\n-      if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n+      if ( super.readAndDiscard(toSkip) !\u003d toSkip ) {\n         // should never happen\n         throw new IOException(\"Could not skip required number of bytes\");\n       }\n     }\n     \n     int nRead \u003d super.read(buf, off, len);\n \n     // if eos was set in the previous read, send a status code to the DN\n     if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n       if (needChecksum()) {\n         sendReadResult(peer, Status.CHECKSUM_OK);\n       } else {\n         sendReadResult(peer, Status.SUCCESS);\n       }\n     }\n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) \n                               throws IOException {\n    \n    // This has to be set here, *before* the skip, since we can\n    // hit EOS during the skip, in the case that our entire read\n    // is smaller than the checksum chunk.\n    boolean eosBefore \u003d eos;\n\n    //for the first read, skip the extra bytes at the front.\n    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n      // Skip these bytes. But don\u0027t call this.skip()!\n      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n      if ( super.readAndDiscard(toSkip) !\u003d toSkip ) {\n        // should never happen\n        throw new IOException(\"Could not skip required number of bytes\");\n      }\n    }\n    \n    int nRead \u003d super.read(buf, off, len);\n\n    // if eos was set in the previous read, send a status code to the DN\n    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n      if (needChecksum()) {\n        sendReadResult(peer, Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(peer, Status.SUCCESS);\n      }\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
      "extendedDetails": {}
    },
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:34 PM",
      "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "08/01/13 6:41 PM",
      "commitNameOld": "fab2cbc2c1fa7b592e27a186411dcc4a67ea2bc2",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public synchronized int read(byte[] buf, int off, int len) \n                                throws IOException {\n     \n     // This has to be set here, *before* the skip, since we can\n     // hit EOS during the skip, in the case that our entire read\n     // is smaller than the checksum chunk.\n     boolean eosBefore \u003d eos;\n \n     //for the first read, skip the extra bytes at the front.\n     if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n       // Skip these bytes. But don\u0027t call this.skip()!\n       int toSkip \u003d (int)(startOffset - firstChunkOffset);\n       if ( skipBuf \u003d\u003d null ) {\n         skipBuf \u003d new byte[bytesPerChecksum];\n       }\n       if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n         // should never happen\n         throw new IOException(\"Could not skip required number of bytes\");\n       }\n     }\n     \n     int nRead \u003d super.read(buf, off, len);\n \n     // if eos was set in the previous read, send a status code to the DN\n     if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n       if (needChecksum()) {\n-        sendReadResult(dnSock, Status.CHECKSUM_OK);\n+        sendReadResult(peer, Status.CHECKSUM_OK);\n       } else {\n-        sendReadResult(dnSock, Status.SUCCESS);\n+        sendReadResult(peer, Status.SUCCESS);\n       }\n     }\n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) \n                               throws IOException {\n    \n    // This has to be set here, *before* the skip, since we can\n    // hit EOS during the skip, in the case that our entire read\n    // is smaller than the checksum chunk.\n    boolean eosBefore \u003d eos;\n\n    //for the first read, skip the extra bytes at the front.\n    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n      // Skip these bytes. But don\u0027t call this.skip()!\n      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n      if ( skipBuf \u003d\u003d null ) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n        // should never happen\n        throw new IOException(\"Could not skip required number of bytes\");\n      }\n    }\n    \n    int nRead \u003d super.read(buf, off, len);\n\n    // if eos was set in the previous read, send a status code to the DN\n    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n      if (needChecksum()) {\n        sendReadResult(peer, Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(peer, Status.SUCCESS);\n      }\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
      "extendedDetails": {}
    },
    "837e17b2eac1471d93e2eff395272063b265fee7": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 6:39 PM",
      "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/01/13 12:44 PM",
      "commitNameOld": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.25,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public synchronized int read(byte[] buf, int off, int len) \n                                throws IOException {\n     \n     // This has to be set here, *before* the skip, since we can\n     // hit EOS during the skip, in the case that our entire read\n     // is smaller than the checksum chunk.\n     boolean eosBefore \u003d eos;\n \n     //for the first read, skip the extra bytes at the front.\n     if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n       // Skip these bytes. But don\u0027t call this.skip()!\n       int toSkip \u003d (int)(startOffset - firstChunkOffset);\n       if ( skipBuf \u003d\u003d null ) {\n         skipBuf \u003d new byte[bytesPerChecksum];\n       }\n       if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n         // should never happen\n         throw new IOException(\"Could not skip required number of bytes\");\n       }\n     }\n     \n     int nRead \u003d super.read(buf, off, len);\n \n     // if eos was set in the previous read, send a status code to the DN\n     if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n       if (needChecksum()) {\n-        sendReadResult(peer, Status.CHECKSUM_OK);\n+        sendReadResult(dnSock, Status.CHECKSUM_OK);\n       } else {\n-        sendReadResult(peer, Status.SUCCESS);\n+        sendReadResult(dnSock, Status.SUCCESS);\n       }\n     }\n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) \n                               throws IOException {\n    \n    // This has to be set here, *before* the skip, since we can\n    // hit EOS during the skip, in the case that our entire read\n    // is smaller than the checksum chunk.\n    boolean eosBefore \u003d eos;\n\n    //for the first read, skip the extra bytes at the front.\n    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n      // Skip these bytes. But don\u0027t call this.skip()!\n      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n      if ( skipBuf \u003d\u003d null ) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n        // should never happen\n        throw new IOException(\"Could not skip required number of bytes\");\n      }\n    }\n    \n    int nRead \u003d super.read(buf, off, len);\n\n    // if eos was set in the previous read, send a status code to the DN\n    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n      if (needChecksum()) {\n        sendReadResult(dnSock, Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(dnSock, Status.SUCCESS);\n      }\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
      "extendedDetails": {}
    },
    "239b2742d0e80d13c970fd062af4930e672fe903": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 12:44 PM",
      "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "03/01/13 10:59 PM",
      "commitNameOld": "32052a1e3a8007b5348dc42415861aeb859ebc5a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.57,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public synchronized int read(byte[] buf, int off, int len) \n                                throws IOException {\n     \n     // This has to be set here, *before* the skip, since we can\n     // hit EOS during the skip, in the case that our entire read\n     // is smaller than the checksum chunk.\n     boolean eosBefore \u003d eos;\n \n     //for the first read, skip the extra bytes at the front.\n     if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n       // Skip these bytes. But don\u0027t call this.skip()!\n       int toSkip \u003d (int)(startOffset - firstChunkOffset);\n       if ( skipBuf \u003d\u003d null ) {\n         skipBuf \u003d new byte[bytesPerChecksum];\n       }\n       if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n         // should never happen\n         throw new IOException(\"Could not skip required number of bytes\");\n       }\n     }\n     \n     int nRead \u003d super.read(buf, off, len);\n \n     // if eos was set in the previous read, send a status code to the DN\n     if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n       if (needChecksum()) {\n-        sendReadResult(dnSock, Status.CHECKSUM_OK);\n+        sendReadResult(peer, Status.CHECKSUM_OK);\n       } else {\n-        sendReadResult(dnSock, Status.SUCCESS);\n+        sendReadResult(peer, Status.SUCCESS);\n       }\n     }\n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) \n                               throws IOException {\n    \n    // This has to be set here, *before* the skip, since we can\n    // hit EOS during the skip, in the case that our entire read\n    // is smaller than the checksum chunk.\n    boolean eosBefore \u003d eos;\n\n    //for the first read, skip the extra bytes at the front.\n    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n      // Skip these bytes. But don\u0027t call this.skip()!\n      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n      if ( skipBuf \u003d\u003d null ) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n        // should never happen\n        throw new IOException(\"Could not skip required number of bytes\");\n      }\n    }\n    \n    int nRead \u003d super.read(buf, off, len);\n\n    // if eos was set in the previous read, send a status code to the DN\n    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n      if (needChecksum()) {\n        sendReadResult(peer, Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(peer, Status.SUCCESS);\n      }\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) \n                               throws IOException {\n    \n    // This has to be set here, *before* the skip, since we can\n    // hit EOS during the skip, in the case that our entire read\n    // is smaller than the checksum chunk.\n    boolean eosBefore \u003d eos;\n\n    //for the first read, skip the extra bytes at the front.\n    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n      // Skip these bytes. But don\u0027t call this.skip()!\n      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n      if ( skipBuf \u003d\u003d null ) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n        // should never happen\n        throw new IOException(\"Could not skip required number of bytes\");\n      }\n    }\n    \n    int nRead \u003d super.read(buf, off, len);\n\n    // if eos was set in the previous read, send a status code to the DN\n    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n      if (needChecksum()) {\n        sendReadResult(dnSock, Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(dnSock, Status.SUCCESS);\n      }\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) \n                               throws IOException {\n    \n    // This has to be set here, *before* the skip, since we can\n    // hit EOS during the skip, in the case that our entire read\n    // is smaller than the checksum chunk.\n    boolean eosBefore \u003d eos;\n\n    //for the first read, skip the extra bytes at the front.\n    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n      // Skip these bytes. But don\u0027t call this.skip()!\n      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n      if ( skipBuf \u003d\u003d null ) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n        // should never happen\n        throw new IOException(\"Could not skip required number of bytes\");\n      }\n    }\n    \n    int nRead \u003d super.read(buf, off, len);\n\n    // if eos was set in the previous read, send a status code to the DN\n    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n      if (needChecksum()) {\n        sendReadResult(dnSock, Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(dnSock, Status.SUCCESS);\n      }\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java"
      }
    },
    "dd86860633d2ed64705b669a75bf318442ed6225": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-2260. Refactor BlockReader into an interface and implementation. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159004 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/08/11 8:02 PM",
      "commitName": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "17/08/11 3:00 PM",
      "commitNameOld": "cc875f0124d1951a4aab0565442242dac3dd35c8",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.21,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) \n                               throws IOException {\n    \n    // This has to be set here, *before* the skip, since we can\n    // hit EOS during the skip, in the case that our entire read\n    // is smaller than the checksum chunk.\n    boolean eosBefore \u003d eos;\n\n    //for the first read, skip the extra bytes at the front.\n    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n      // Skip these bytes. But don\u0027t call this.skip()!\n      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n      if ( skipBuf \u003d\u003d null ) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n        // should never happen\n        throw new IOException(\"Could not skip required number of bytes\");\n      }\n    }\n    \n    int nRead \u003d super.read(buf, off, len);\n\n    // if eos was set in the previous read, send a status code to the DN\n    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n      if (needChecksum()) {\n        sendReadResult(dnSock, Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(dnSock, Status.SUCCESS);\n      }\n    }\n    return nRead;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/BlockReader.java",
        "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/RemoteBlockReader.java",
        "oldMethodName": "read",
        "newMethodName": "read"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,33 @@\n+  public synchronized int read(byte[] buf, int off, int len) \n+                               throws IOException {\n+    \n+    // This has to be set here, *before* the skip, since we can\n+    // hit EOS during the skip, in the case that our entire read\n+    // is smaller than the checksum chunk.\n+    boolean eosBefore \u003d eos;\n+\n+    //for the first read, skip the extra bytes at the front.\n+    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n+      // Skip these bytes. But don\u0027t call this.skip()!\n+      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n+      if ( skipBuf \u003d\u003d null ) {\n+        skipBuf \u003d new byte[bytesPerChecksum];\n+      }\n+      if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n+        // should never happen\n+        throw new IOException(\"Could not skip required number of bytes\");\n+      }\n+    }\n+    \n+    int nRead \u003d super.read(buf, off, len);\n+\n+    // if eos was set in the previous read, send a status code to the DN\n+    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n+      if (needChecksum()) {\n+        sendReadResult(dnSock, Status.CHECKSUM_OK);\n+      } else {\n+        sendReadResult(dnSock, Status.SUCCESS);\n+      }\n+    }\n+    return nRead;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) \n                               throws IOException {\n    \n    // This has to be set here, *before* the skip, since we can\n    // hit EOS during the skip, in the case that our entire read\n    // is smaller than the checksum chunk.\n    boolean eosBefore \u003d eos;\n\n    //for the first read, skip the extra bytes at the front.\n    if (lastChunkLen \u003c 0 \u0026\u0026 startOffset \u003e firstChunkOffset \u0026\u0026 len \u003e 0) {\n      // Skip these bytes. But don\u0027t call this.skip()!\n      int toSkip \u003d (int)(startOffset - firstChunkOffset);\n      if ( skipBuf \u003d\u003d null ) {\n        skipBuf \u003d new byte[bytesPerChecksum];\n      }\n      if ( super.read(skipBuf, 0, toSkip) !\u003d toSkip ) {\n        // should never happen\n        throw new IOException(\"Could not skip required number of bytes\");\n      }\n    }\n    \n    int nRead \u003d super.read(buf, off, len);\n\n    // if eos was set in the previous read, send a status code to the DN\n    if (eos \u0026\u0026 !eosBefore \u0026\u0026 nRead \u003e\u003d 0) {\n      if (needChecksum()) {\n        sendReadResult(dnSock, Status.CHECKSUM_OK);\n      } else {\n        sendReadResult(dnSock, Status.SUCCESS);\n      }\n    }\n    return nRead;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/BlockReader.java"
    }
  }
}