{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "AppLogAggregatorImpl.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
  "functionStartLine": 463,
  "functionEndLine": 486,
  "numCommitsSeen": 66,
  "timeTaken": 11466,
  "changeHistory": [
    "b22f56c4719e63bd4f6edc2a075e0bcdb9442255",
    "f59e36b4ce71d3019ab91b136b6d7646316954e7",
    "72fe54684198b7df5c5fb2114616dff6d17a4402",
    "81effb7dcde2b31423438d6f1b8b8204d4ca05b3",
    "25e2b02122c4ed760227ab33c49d3445c23b9276",
    "34cdcaad71cad76c0874a4e5266b4074009d2ffc",
    "e1fdf62123625e4ba399af02f8aad500637d29d1",
    "611294084103532fa599d97921339c281684681c",
    "ea17da82f7fc4b7fcc05bba82d141e27289fd7cb",
    "a75c4cf4e4400a2dcb3edc88df7f35a763f93c4e",
    "670fa24b48acb407c22fbfdde87ae3123dcbf449",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "b22f56c4719e63bd4f6edc2a075e0bcdb9442255": "Ybodychange",
    "f59e36b4ce71d3019ab91b136b6d7646316954e7": "Ybodychange",
    "72fe54684198b7df5c5fb2114616dff6d17a4402": "Ybodychange",
    "81effb7dcde2b31423438d6f1b8b8204d4ca05b3": "Ybodychange",
    "25e2b02122c4ed760227ab33c49d3445c23b9276": "Ybodychange",
    "34cdcaad71cad76c0874a4e5266b4074009d2ffc": "Ymultichange(Yreturntypechange,Yexceptionschange,Ybodychange)",
    "e1fdf62123625e4ba399af02f8aad500637d29d1": "Yfilerename",
    "611294084103532fa599d97921339c281684681c": "Ymultichange(Yreturntypechange,Yexceptionschange,Ybodychange)",
    "ea17da82f7fc4b7fcc05bba82d141e27289fd7cb": "Ybodychange",
    "a75c4cf4e4400a2dcb3edc88df7f35a763f93c4e": "Ybodychange",
    "670fa24b48acb407c22fbfdde87ae3123dcbf449": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymultichange(Ymovefromfile,Yexceptionschange,Ybodychange,Yparameterchange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b22f56c4719e63bd4f6edc2a075e0bcdb9442255": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8273. Log aggregation does not warn if HDFS quota in target directory is exceeded (grepas via rkanter)\n",
      "commitDate": "22/05/18 2:24 PM",
      "commitName": "b22f56c4719e63bd4f6edc2a075e0bcdb9442255",
      "commitAuthor": "Robert Kanter",
      "commitDateOld": "15/03/18 1:26 PM",
      "commitNameOld": "4bf622043f034835d65ff2a4785b9b06d0ef1fd2",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 68.04,
      "commitsBetweenForRepo": 1090,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,24 @@\n   public void run() {\n     try {\n       doAppLogAggregation();\n+    } catch (LogAggregationDFSException e) {\n+      // if the log aggregation could not be performed due to DFS issues\n+      // let\u0027s not clean up the log files, since that can result in\n+      // loss of logs\n+      LOG.error(\"Error occurred while aggregating the log for the application \"\n+          + appId, e);\n     } catch (Exception e) {\n-      // do post clean up of log directories on any exception\n+      // do post clean up of log directories on any other exception\n       LOG.error(\"Error occurred while aggregating the log for the application \"\n           + appId, e);\n       doAppLogAggregationPostCleanUp();\n     } finally {\n       if (!this.appAggregationFinished.get() \u0026\u0026 !this.aborted.get()) {\n         LOG.warn(\"Log aggregation did not complete for application \" + appId);\n         this.dispatcher.getEventHandler().handle(\n             new ApplicationEvent(this.appId,\n                 ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));\n       }\n       this.appAggregationFinished.set(true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      doAppLogAggregation();\n    } catch (LogAggregationDFSException e) {\n      // if the log aggregation could not be performed due to DFS issues\n      // let\u0027s not clean up the log files, since that can result in\n      // loss of logs\n      LOG.error(\"Error occurred while aggregating the log for the application \"\n          + appId, e);\n    } catch (Exception e) {\n      // do post clean up of log directories on any other exception\n      LOG.error(\"Error occurred while aggregating the log for the application \"\n          + appId, e);\n      doAppLogAggregationPostCleanUp();\n    } finally {\n      if (!this.appAggregationFinished.get() \u0026\u0026 !this.aborted.get()) {\n        LOG.warn(\"Log aggregation did not complete for application \" + appId);\n        this.dispatcher.getEventHandler().handle(\n            new ApplicationEvent(this.appId,\n                ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));\n      }\n      this.appAggregationFinished.set(true);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
      "extendedDetails": {}
    },
    "f59e36b4ce71d3019ab91b136b6d7646316954e7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6068. Log aggregation get failed when NM restart even with recovery (Junping Du via Varun Saxena)\n",
      "commitDate": "08/01/17 9:47 PM",
      "commitName": "f59e36b4ce71d3019ab91b136b6d7646316954e7",
      "commitAuthor": "Varun Saxena",
      "commitDateOld": "07/12/16 3:07 PM",
      "commitNameOld": "72fe54684198b7df5c5fb2114616dff6d17a4402",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 32.28,
      "commitsBetweenForRepo": 139,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public void run() {\n     try {\n       doAppLogAggregation();\n     } catch (Exception e) {\n       // do post clean up of log directories on any exception\n       LOG.error(\"Error occurred while aggregating the log for the application \"\n           + appId, e);\n       doAppLogAggregationPostCleanUp();\n     } finally {\n-      if (!this.appAggregationFinished.get()) {\n-        LOG.warn(\"Aggregation did not complete for application \" + appId);\n+      if (!this.appAggregationFinished.get() \u0026\u0026 !this.aborted.get()) {\n+        LOG.warn(\"Log aggregation did not complete for application \" + appId);\n         this.dispatcher.getEventHandler().handle(\n             new ApplicationEvent(this.appId,\n                 ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));\n       }\n       this.appAggregationFinished.set(true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      doAppLogAggregation();\n    } catch (Exception e) {\n      // do post clean up of log directories on any exception\n      LOG.error(\"Error occurred while aggregating the log for the application \"\n          + appId, e);\n      doAppLogAggregationPostCleanUp();\n    } finally {\n      if (!this.appAggregationFinished.get() \u0026\u0026 !this.aborted.get()) {\n        LOG.warn(\"Log aggregation did not complete for application \" + appId);\n        this.dispatcher.getEventHandler().handle(\n            new ApplicationEvent(this.appId,\n                ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));\n      }\n      this.appAggregationFinished.set(true);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
      "extendedDetails": {}
    },
    "72fe54684198b7df5c5fb2114616dff6d17a4402": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5963. Spelling errors in logging and exceptions for node manager, client, web-proxy, common, and app history code (gsohn via rkanter)\n",
      "commitDate": "07/12/16 3:07 PM",
      "commitName": "72fe54684198b7df5c5fb2114616dff6d17a4402",
      "commitAuthor": "Robert Kanter",
      "commitDateOld": "15/06/16 4:17 PM",
      "commitNameOld": "5dfc38ff57669cba9078146e91ed990a1d25a3f0",
      "commitAuthorOld": "Junping Du",
      "daysBetweenCommits": 174.99,
      "commitsBetweenForRepo": 1372,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public void run() {\n     try {\n       doAppLogAggregation();\n     } catch (Exception e) {\n       // do post clean up of log directories on any exception\n-      LOG.error(\"Error occured while aggregating the log for the application \"\n+      LOG.error(\"Error occurred while aggregating the log for the application \"\n           + appId, e);\n       doAppLogAggregationPostCleanUp();\n     } finally {\n       if (!this.appAggregationFinished.get()) {\n         LOG.warn(\"Aggregation did not complete for application \" + appId);\n         this.dispatcher.getEventHandler().handle(\n             new ApplicationEvent(this.appId,\n                 ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));\n       }\n       this.appAggregationFinished.set(true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      doAppLogAggregation();\n    } catch (Exception e) {\n      // do post clean up of log directories on any exception\n      LOG.error(\"Error occurred while aggregating the log for the application \"\n          + appId, e);\n      doAppLogAggregationPostCleanUp();\n    } finally {\n      if (!this.appAggregationFinished.get()) {\n        LOG.warn(\"Aggregation did not complete for application \" + appId);\n        this.dispatcher.getEventHandler().handle(\n            new ApplicationEvent(this.appId,\n                ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));\n      }\n      this.appAggregationFinished.set(true);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
      "extendedDetails": {}
    },
    "81effb7dcde2b31423438d6f1b8b8204d4ca05b3": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4325. Nodemanager log handlers fail to send finished/failed events in some cases. Contributed by Junping Du\n",
      "commitDate": "16/05/16 8:40 AM",
      "commitName": "81effb7dcde2b31423438d6f1b8b8204d4ca05b3",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "28/03/16 4:00 PM",
      "commitNameOld": "948b75807068c304ffe789e32f2b850c0d653e0a",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 48.69,
      "commitsBetweenForRepo": 301,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,18 @@\n   public void run() {\n     try {\n       doAppLogAggregation();\n     } catch (Exception e) {\n       // do post clean up of log directories on any exception\n       LOG.error(\"Error occured while aggregating the log for the application \"\n           + appId, e);\n       doAppLogAggregationPostCleanUp();\n     } finally {\n       if (!this.appAggregationFinished.get()) {\n         LOG.warn(\"Aggregation did not complete for application \" + appId);\n+        this.dispatcher.getEventHandler().handle(\n+            new ApplicationEvent(this.appId,\n+                ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));\n       }\n       this.appAggregationFinished.set(true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      doAppLogAggregation();\n    } catch (Exception e) {\n      // do post clean up of log directories on any exception\n      LOG.error(\"Error occured while aggregating the log for the application \"\n          + appId, e);\n      doAppLogAggregationPostCleanUp();\n    } finally {\n      if (!this.appAggregationFinished.get()) {\n        LOG.warn(\"Aggregation did not complete for application \" + appId);\n        this.dispatcher.getEventHandler().handle(\n            new ApplicationEvent(this.appId,\n                ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));\n      }\n      this.appAggregationFinished.set(true);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
      "extendedDetails": {}
    },
    "25e2b02122c4ed760227ab33c49d3445c23b9276": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3476. Nodemanager can fail to delete local logs if log aggregation fails. Contributed by Rohith\n",
      "commitDate": "08/05/15 3:45 PM",
      "commitName": "25e2b02122c4ed760227ab33c49d3445c23b9276",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "17/04/15 1:18 PM",
      "commitNameOld": "1db355a875c3ecc40a244045c6812e00c8d36ef1",
      "commitAuthorOld": "Junping Du",
      "daysBetweenCommits": 21.1,
      "commitsBetweenForRepo": 253,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,15 @@\n   public void run() {\n     try {\n       doAppLogAggregation();\n+    } catch (Exception e) {\n+      // do post clean up of log directories on any exception\n+      LOG.error(\"Error occured while aggregating the log for the application \"\n+          + appId, e);\n+      doAppLogAggregationPostCleanUp();\n     } finally {\n       if (!this.appAggregationFinished.get()) {\n         LOG.warn(\"Aggregation did not complete for application \" + appId);\n       }\n       this.appAggregationFinished.set(true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      doAppLogAggregation();\n    } catch (Exception e) {\n      // do post clean up of log directories on any exception\n      LOG.error(\"Error occured while aggregating the log for the application \"\n          + appId, e);\n      doAppLogAggregationPostCleanUp();\n    } finally {\n      if (!this.appAggregationFinished.get()) {\n        LOG.warn(\"Aggregation did not complete for application \" + appId);\n      }\n      this.appAggregationFinished.set(true);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
      "extendedDetails": {}
    },
    "34cdcaad71cad76c0874a4e5266b4074009d2ffc": {
      "type": "Ymultichange(Yreturntypechange,Yexceptionschange,Ybodychange)",
      "commitMessage": "YARN-2468. Enhanced NodeManager to support log handling APIs (YARN-2569) for use by long running services. Contributed by Xuan Gong.\n",
      "commitDate": "03/10/14 12:15 PM",
      "commitName": "34cdcaad71cad76c0874a4e5266b4074009d2ffc",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "YARN-2468. Enhanced NodeManager to support log handling APIs (YARN-2569) for use by long running services. Contributed by Xuan Gong.\n",
          "commitDate": "03/10/14 12:15 PM",
          "commitName": "34cdcaad71cad76c0874a4e5266b4074009d2ffc",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "12/08/14 3:56 AM",
          "commitNameOld": "c2febdcbaa12078db42403fe8fd74180fb58a84b",
          "commitAuthorOld": "Junping Du",
          "daysBetweenCommits": 52.35,
          "commitsBetweenForRepo": 563,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,5 +1,10 @@\n-        public Object run() throws Exception {\n-          FileSystem remoteFS \u003d FileSystem.get(conf);\n-          remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n-          return null;\n-        }\n\\ No newline at end of file\n+  public void run() {\n+    try {\n+      doAppLogAggregation();\n+    } finally {\n+      if (!this.appAggregationFinished.get()) {\n+        LOG.warn(\"Aggregation did not complete for application \" + appId);\n+      }\n+      this.appAggregationFinished.set(true);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public void run() {\n    try {\n      doAppLogAggregation();\n    } finally {\n      if (!this.appAggregationFinished.get()) {\n        LOG.warn(\"Aggregation did not complete for application \" + appId);\n      }\n      this.appAggregationFinished.set(true);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
          "extendedDetails": {
            "oldValue": "Object",
            "newValue": "void"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "YARN-2468. Enhanced NodeManager to support log handling APIs (YARN-2569) for use by long running services. Contributed by Xuan Gong.\n",
          "commitDate": "03/10/14 12:15 PM",
          "commitName": "34cdcaad71cad76c0874a4e5266b4074009d2ffc",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "12/08/14 3:56 AM",
          "commitNameOld": "c2febdcbaa12078db42403fe8fd74180fb58a84b",
          "commitAuthorOld": "Junping Du",
          "daysBetweenCommits": 52.35,
          "commitsBetweenForRepo": 563,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,5 +1,10 @@\n-        public Object run() throws Exception {\n-          FileSystem remoteFS \u003d FileSystem.get(conf);\n-          remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n-          return null;\n-        }\n\\ No newline at end of file\n+  public void run() {\n+    try {\n+      doAppLogAggregation();\n+    } finally {\n+      if (!this.appAggregationFinished.get()) {\n+        LOG.warn(\"Aggregation did not complete for application \" + appId);\n+      }\n+      this.appAggregationFinished.set(true);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public void run() {\n    try {\n      doAppLogAggregation();\n    } finally {\n      if (!this.appAggregationFinished.get()) {\n        LOG.warn(\"Aggregation did not complete for application \" + appId);\n      }\n      this.appAggregationFinished.set(true);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
          "extendedDetails": {
            "oldValue": "[Exception]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-2468. Enhanced NodeManager to support log handling APIs (YARN-2569) for use by long running services. Contributed by Xuan Gong.\n",
          "commitDate": "03/10/14 12:15 PM",
          "commitName": "34cdcaad71cad76c0874a4e5266b4074009d2ffc",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "12/08/14 3:56 AM",
          "commitNameOld": "c2febdcbaa12078db42403fe8fd74180fb58a84b",
          "commitAuthorOld": "Junping Du",
          "daysBetweenCommits": 52.35,
          "commitsBetweenForRepo": 563,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,5 +1,10 @@\n-        public Object run() throws Exception {\n-          FileSystem remoteFS \u003d FileSystem.get(conf);\n-          remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n-          return null;\n-        }\n\\ No newline at end of file\n+  public void run() {\n+    try {\n+      doAppLogAggregation();\n+    } finally {\n+      if (!this.appAggregationFinished.get()) {\n+        LOG.warn(\"Aggregation did not complete for application \" + appId);\n+      }\n+      this.appAggregationFinished.set(true);\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public void run() {\n    try {\n      doAppLogAggregation();\n    } finally {\n      if (!this.appAggregationFinished.get()) {\n        LOG.warn(\"Aggregation did not complete for application \" + appId);\n      }\n      this.appAggregationFinished.set(true);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "e1fdf62123625e4ba399af02f8aad500637d29d1": {
      "type": "Yfilerename",
      "commitMessage": "YARN-1. Promote YARN to be a sub-project of Apache Hadoop.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370666 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 10:22 PM",
      "commitName": "e1fdf62123625e4ba399af02f8aad500637d29d1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "07/08/12 7:53 PM",
      "commitNameOld": "34554d1e11ee1d5b564d7d9ed3e6d55931d72749",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "        public Object run() throws Exception {\n          FileSystem remoteFS \u003d FileSystem.get(conf);\n          remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n          return null;\n        }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java"
      }
    },
    "611294084103532fa599d97921339c281684681c": {
      "type": "Ymultichange(Yreturntypechange,Yexceptionschange,Ybodychange)",
      "commitMessage": "MAPREDUCE 3738. NM can hang during shutdown if AppLogAggregatorImpl thread dies unexpectedly. (Contributed by Jason Lowe) \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1293060 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/02/12 6:12 PM",
      "commitName": "611294084103532fa599d97921339c281684681c",
      "commitAuthor": "Siddharth Seth",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "MAPREDUCE 3738. NM can hang during shutdown if AppLogAggregatorImpl thread dies unexpectedly. (Contributed by Jason Lowe) \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1293060 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/02/12 6:12 PM",
          "commitName": "611294084103532fa599d97921339c281684681c",
          "commitAuthor": "Siddharth Seth",
          "commitDateOld": "14/12/11 11:55 AM",
          "commitNameOld": "50fa9b89f42bd3fe6aad5086b0df14a00dadb24b",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 71.26,
          "commitsBetweenForRepo": 374,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,58 +1,5 @@\n-  public void run() {    \n-    ContainerId containerId;\n-\n-    while (!this.appFinishing.get()) {\n-      try {\n-        containerId \u003d this.pendingContainers.poll();\n-        if (containerId \u003d\u003d null) {\n-          Thread.sleep(THREAD_SLEEP_TIME);\n-        } else {\n-          uploadLogsForContainer(containerId);\n-        }\n-      } catch (InterruptedException e) {\n-        LOG.warn(\"PendingContainers queue is interrupted\");\n-      }\n-    }\n-\n-    // Application is finished. Finish pending-containers\n-    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n-      uploadLogsForContainer(containerId);\n-    }\n-\n-    // Remove the local app-log-dirs\n-    List\u003cString\u003e rootLogDirs \u003d dirsHandler.getLogDirs();\n-    Path[] localAppLogDirs \u003d new Path[rootLogDirs.size()];\n-    int index \u003d 0;\n-    for (String rootLogDir : rootLogDirs) {\n-      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n-      index++;\n-    }\n-    this.delService.delete(this.userUgi.getShortUserName(), null,\n-        localAppLogDirs);\n-\n-    if (this.writer !\u003d null) {\n-      this.writer.closeWriter();\n-      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n-    }\n-\n-    try {\n-      userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n-        @Override\n         public Object run() throws Exception {\n           FileSystem remoteFS \u003d FileSystem.get(conf);\n           remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n           return null;\n-        }\n-      });\n-    } catch (Exception e) {\n-      LOG.error(\"Failed to move temporary log file to final location: [\"\n-          + remoteNodeTmpLogFileForApp + \"] to [\" + remoteNodeLogFileForApp\n-          + \"]\", e);\n-    }\n-    \n-    this.dispatcher.getEventHandler().handle(\n-        new ApplicationEvent(this.appId,\n-            ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED));\n-        \n-    this.appAggregationFinished.set(true);\n-  }\n\\ No newline at end of file\n+        }\n\\ No newline at end of file\n",
          "actualSource": "        public Object run() throws Exception {\n          FileSystem remoteFS \u003d FileSystem.get(conf);\n          remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n          return null;\n        }",
          "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "Object"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "MAPREDUCE 3738. NM can hang during shutdown if AppLogAggregatorImpl thread dies unexpectedly. (Contributed by Jason Lowe) \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1293060 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/02/12 6:12 PM",
          "commitName": "611294084103532fa599d97921339c281684681c",
          "commitAuthor": "Siddharth Seth",
          "commitDateOld": "14/12/11 11:55 AM",
          "commitNameOld": "50fa9b89f42bd3fe6aad5086b0df14a00dadb24b",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 71.26,
          "commitsBetweenForRepo": 374,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,58 +1,5 @@\n-  public void run() {    \n-    ContainerId containerId;\n-\n-    while (!this.appFinishing.get()) {\n-      try {\n-        containerId \u003d this.pendingContainers.poll();\n-        if (containerId \u003d\u003d null) {\n-          Thread.sleep(THREAD_SLEEP_TIME);\n-        } else {\n-          uploadLogsForContainer(containerId);\n-        }\n-      } catch (InterruptedException e) {\n-        LOG.warn(\"PendingContainers queue is interrupted\");\n-      }\n-    }\n-\n-    // Application is finished. Finish pending-containers\n-    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n-      uploadLogsForContainer(containerId);\n-    }\n-\n-    // Remove the local app-log-dirs\n-    List\u003cString\u003e rootLogDirs \u003d dirsHandler.getLogDirs();\n-    Path[] localAppLogDirs \u003d new Path[rootLogDirs.size()];\n-    int index \u003d 0;\n-    for (String rootLogDir : rootLogDirs) {\n-      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n-      index++;\n-    }\n-    this.delService.delete(this.userUgi.getShortUserName(), null,\n-        localAppLogDirs);\n-\n-    if (this.writer !\u003d null) {\n-      this.writer.closeWriter();\n-      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n-    }\n-\n-    try {\n-      userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n-        @Override\n         public Object run() throws Exception {\n           FileSystem remoteFS \u003d FileSystem.get(conf);\n           remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n           return null;\n-        }\n-      });\n-    } catch (Exception e) {\n-      LOG.error(\"Failed to move temporary log file to final location: [\"\n-          + remoteNodeTmpLogFileForApp + \"] to [\" + remoteNodeLogFileForApp\n-          + \"]\", e);\n-    }\n-    \n-    this.dispatcher.getEventHandler().handle(\n-        new ApplicationEvent(this.appId,\n-            ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED));\n-        \n-    this.appAggregationFinished.set(true);\n-  }\n\\ No newline at end of file\n+        }\n\\ No newline at end of file\n",
          "actualSource": "        public Object run() throws Exception {\n          FileSystem remoteFS \u003d FileSystem.get(conf);\n          remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n          return null;\n        }",
          "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[Exception]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE 3738. NM can hang during shutdown if AppLogAggregatorImpl thread dies unexpectedly. (Contributed by Jason Lowe) \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1293060 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/02/12 6:12 PM",
          "commitName": "611294084103532fa599d97921339c281684681c",
          "commitAuthor": "Siddharth Seth",
          "commitDateOld": "14/12/11 11:55 AM",
          "commitNameOld": "50fa9b89f42bd3fe6aad5086b0df14a00dadb24b",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 71.26,
          "commitsBetweenForRepo": 374,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,58 +1,5 @@\n-  public void run() {    \n-    ContainerId containerId;\n-\n-    while (!this.appFinishing.get()) {\n-      try {\n-        containerId \u003d this.pendingContainers.poll();\n-        if (containerId \u003d\u003d null) {\n-          Thread.sleep(THREAD_SLEEP_TIME);\n-        } else {\n-          uploadLogsForContainer(containerId);\n-        }\n-      } catch (InterruptedException e) {\n-        LOG.warn(\"PendingContainers queue is interrupted\");\n-      }\n-    }\n-\n-    // Application is finished. Finish pending-containers\n-    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n-      uploadLogsForContainer(containerId);\n-    }\n-\n-    // Remove the local app-log-dirs\n-    List\u003cString\u003e rootLogDirs \u003d dirsHandler.getLogDirs();\n-    Path[] localAppLogDirs \u003d new Path[rootLogDirs.size()];\n-    int index \u003d 0;\n-    for (String rootLogDir : rootLogDirs) {\n-      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n-      index++;\n-    }\n-    this.delService.delete(this.userUgi.getShortUserName(), null,\n-        localAppLogDirs);\n-\n-    if (this.writer !\u003d null) {\n-      this.writer.closeWriter();\n-      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n-    }\n-\n-    try {\n-      userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n-        @Override\n         public Object run() throws Exception {\n           FileSystem remoteFS \u003d FileSystem.get(conf);\n           remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n           return null;\n-        }\n-      });\n-    } catch (Exception e) {\n-      LOG.error(\"Failed to move temporary log file to final location: [\"\n-          + remoteNodeTmpLogFileForApp + \"] to [\" + remoteNodeLogFileForApp\n-          + \"]\", e);\n-    }\n-    \n-    this.dispatcher.getEventHandler().handle(\n-        new ApplicationEvent(this.appId,\n-            ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED));\n-        \n-    this.appAggregationFinished.set(true);\n-  }\n\\ No newline at end of file\n+        }\n\\ No newline at end of file\n",
          "actualSource": "        public Object run() throws Exception {\n          FileSystem remoteFS \u003d FileSystem.get(conf);\n          remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n          return null;\n        }",
          "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "ea17da82f7fc4b7fcc05bba82d141e27289fd7cb": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3121. NodeManager should handle disk-failures (Ravi Gummadi via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1208131 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/11/11 3:17 PM",
      "commitName": "ea17da82f7fc4b7fcc05bba82d141e27289fd7cb",
      "commitAuthor": "Mahadev Konar",
      "commitDateOld": "03/11/11 1:02 AM",
      "commitNameOld": "c27601fefebd0af887a12d684bfc6f90d9fc0321",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 26.64,
      "commitsBetweenForRepo": 114,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,57 @@\n   public void run() {    \n     ContainerId containerId;\n \n     while (!this.appFinishing.get()) {\n       try {\n         containerId \u003d this.pendingContainers.poll();\n         if (containerId \u003d\u003d null) {\n           Thread.sleep(THREAD_SLEEP_TIME);\n         } else {\n           uploadLogsForContainer(containerId);\n         }\n       } catch (InterruptedException e) {\n         LOG.warn(\"PendingContainers queue is interrupted\");\n       }\n     }\n \n     // Application is finished. Finish pending-containers\n     while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n       uploadLogsForContainer(containerId);\n     }\n \n     // Remove the local app-log-dirs\n-    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n+    List\u003cString\u003e rootLogDirs \u003d dirsHandler.getLogDirs();\n+    Path[] localAppLogDirs \u003d new Path[rootLogDirs.size()];\n     int index \u003d 0;\n-    for (String rootLogDir : this.rootLogDirs) {\n+    for (String rootLogDir : rootLogDirs) {\n       localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n       index++;\n     }\n     this.delService.delete(this.userUgi.getShortUserName(), null,\n         localAppLogDirs);\n \n     if (this.writer !\u003d null) {\n       this.writer.closeWriter();\n       LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n     }\n     try {\n       userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n         @Override\n         public Object run() throws Exception {\n           FileSystem remoteFS \u003d FileSystem.get(conf);\n           remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n           return null;\n         }\n       });\n     } catch (Exception e) {\n       LOG.error(\"Failed to move temporary log file to final location: [\"\n           + remoteNodeTmpLogFileForApp + \"] to [\" + remoteNodeLogFileForApp\n           + \"]\", e);\n     }\n     \n     this.dispatcher.getEventHandler().handle(\n         new ApplicationEvent(this.appId,\n             ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED));\n         \n     this.appAggregationFinished.set(true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {    \n    ContainerId containerId;\n\n    while (!this.appFinishing.get()) {\n      try {\n        containerId \u003d this.pendingContainers.poll();\n        if (containerId \u003d\u003d null) {\n          Thread.sleep(THREAD_SLEEP_TIME);\n        } else {\n          uploadLogsForContainer(containerId);\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(\"PendingContainers queue is interrupted\");\n      }\n    }\n\n    // Application is finished. Finish pending-containers\n    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n      uploadLogsForContainer(containerId);\n    }\n\n    // Remove the local app-log-dirs\n    List\u003cString\u003e rootLogDirs \u003d dirsHandler.getLogDirs();\n    Path[] localAppLogDirs \u003d new Path[rootLogDirs.size()];\n    int index \u003d 0;\n    for (String rootLogDir : rootLogDirs) {\n      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n      index++;\n    }\n    this.delService.delete(this.userUgi.getShortUserName(), null,\n        localAppLogDirs);\n\n    if (this.writer !\u003d null) {\n      this.writer.closeWriter();\n      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n    }\n    try {\n      userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n        @Override\n        public Object run() throws Exception {\n          FileSystem remoteFS \u003d FileSystem.get(conf);\n          remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n          return null;\n        }\n      });\n    } catch (Exception e) {\n      LOG.error(\"Failed to move temporary log file to final location: [\"\n          + remoteNodeTmpLogFileForApp + \"] to [\" + remoteNodeLogFileForApp\n          + \"]\", e);\n    }\n    \n    this.dispatcher.getEventHandler().handle(\n        new ApplicationEvent(this.appId,\n            ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED));\n        \n    this.appAggregationFinished.set(true);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
      "extendedDetails": {}
    },
    "a75c4cf4e4400a2dcb3edc88df7f35a763f93c4e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2696. Fixed NodeManager to cleanup logs in a thread when logs\u0027 aggregation is not enabled. Contributed by Siddharth Seth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195383 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/10/11 2:55 AM",
      "commitName": "a75c4cf4e4400a2dcb3edc88df7f35a763f93c4e",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "27/10/11 11:45 PM",
      "commitNameOld": "670fa24b48acb407c22fbfdde87ae3123dcbf449",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 3.13,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n   public void run() {    \n     ContainerId containerId;\n \n     while (!this.appFinishing.get()) {\n       try {\n         containerId \u003d this.pendingContainers.poll();\n         if (containerId \u003d\u003d null) {\n           Thread.sleep(THREAD_SLEEP_TIME);\n         } else {\n           uploadLogsForContainer(containerId);\n         }\n       } catch (InterruptedException e) {\n         LOG.warn(\"PendingContainers queue is interrupted\");\n       }\n     }\n \n     // Application is finished. Finish pending-containers\n     while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n       uploadLogsForContainer(containerId);\n     }\n \n     // Remove the local app-log-dirs\n     Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n     int index \u003d 0;\n     for (String rootLogDir : this.rootLogDirs) {\n       localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n       index++;\n     }\n     this.delService.delete(this.userUgi.getShortUserName(), null,\n         localAppLogDirs);\n \n     if (this.writer !\u003d null) {\n       this.writer.closeWriter();\n       LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n     }\n     try {\n       userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n         @Override\n         public Object run() throws Exception {\n           FileSystem remoteFS \u003d FileSystem.get(conf);\n           remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n           return null;\n         }\n       });\n     } catch (Exception e) {\n       LOG.error(\"Failed to move temporary log file to final location: [\"\n           + remoteNodeTmpLogFileForApp + \"] to [\" + remoteNodeLogFileForApp\n           + \"]\", e);\n     }\n     \n     this.dispatcher.getEventHandler().handle(\n         new ApplicationEvent(this.appId,\n-            ApplicationEventType.APPLICATION_LOG_AGGREGATION_FINISHED));\n+            ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED));\n         \n     this.appAggregationFinished.set(true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {    \n    ContainerId containerId;\n\n    while (!this.appFinishing.get()) {\n      try {\n        containerId \u003d this.pendingContainers.poll();\n        if (containerId \u003d\u003d null) {\n          Thread.sleep(THREAD_SLEEP_TIME);\n        } else {\n          uploadLogsForContainer(containerId);\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(\"PendingContainers queue is interrupted\");\n      }\n    }\n\n    // Application is finished. Finish pending-containers\n    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n      uploadLogsForContainer(containerId);\n    }\n\n    // Remove the local app-log-dirs\n    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n    int index \u003d 0;\n    for (String rootLogDir : this.rootLogDirs) {\n      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n      index++;\n    }\n    this.delService.delete(this.userUgi.getShortUserName(), null,\n        localAppLogDirs);\n\n    if (this.writer !\u003d null) {\n      this.writer.closeWriter();\n      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n    }\n    try {\n      userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n        @Override\n        public Object run() throws Exception {\n          FileSystem remoteFS \u003d FileSystem.get(conf);\n          remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n          return null;\n        }\n      });\n    } catch (Exception e) {\n      LOG.error(\"Failed to move temporary log file to final location: [\"\n          + remoteNodeTmpLogFileForApp + \"] to [\" + remoteNodeLogFileForApp\n          + \"]\", e);\n    }\n    \n    this.dispatcher.getEventHandler().handle(\n        new ApplicationEvent(this.appId,\n            ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED));\n        \n    this.appAggregationFinished.set(true);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
      "extendedDetails": {}
    },
    "670fa24b48acb407c22fbfdde87ae3123dcbf449": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2989. Modified JobHistory to link to task and AM logs from the JobHistoryServer. Contributed by Siddharth Seth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190174 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/10/11 11:45 PM",
      "commitName": "670fa24b48acb407c22fbfdde87ae3123dcbf449",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 64.27,
      "commitsBetweenForRepo": 481,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,56 @@\n-  public void run() {\n-\n+  public void run() {    \n     ContainerId containerId;\n \n     while (!this.appFinishing.get()) {\n       try {\n         containerId \u003d this.pendingContainers.poll();\n         if (containerId \u003d\u003d null) {\n           Thread.sleep(THREAD_SLEEP_TIME);\n         } else {\n           uploadLogsForContainer(containerId);\n         }\n       } catch (InterruptedException e) {\n         LOG.warn(\"PendingContainers queue is interrupted\");\n       }\n     }\n \n     // Application is finished. Finish pending-containers\n     while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n       uploadLogsForContainer(containerId);\n     }\n \n     // Remove the local app-log-dirs\n     Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n     int index \u003d 0;\n     for (String rootLogDir : this.rootLogDirs) {\n       localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n       index++;\n     }\n     this.delService.delete(this.userUgi.getShortUserName(), null,\n         localAppLogDirs);\n \n     if (this.writer !\u003d null) {\n       this.writer.closeWriter();\n       LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n     }\n-\n+    try {\n+      userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n+        @Override\n+        public Object run() throws Exception {\n+          FileSystem remoteFS \u003d FileSystem.get(conf);\n+          remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n+          return null;\n+        }\n+      });\n+    } catch (Exception e) {\n+      LOG.error(\"Failed to move temporary log file to final location: [\"\n+          + remoteNodeTmpLogFileForApp + \"] to [\" + remoteNodeLogFileForApp\n+          + \"]\", e);\n+    }\n+    \n+    this.dispatcher.getEventHandler().handle(\n+        new ApplicationEvent(this.appId,\n+            ApplicationEventType.APPLICATION_LOG_AGGREGATION_FINISHED));\n+        \n     this.appAggregationFinished.set(true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {    \n    ContainerId containerId;\n\n    while (!this.appFinishing.get()) {\n      try {\n        containerId \u003d this.pendingContainers.poll();\n        if (containerId \u003d\u003d null) {\n          Thread.sleep(THREAD_SLEEP_TIME);\n        } else {\n          uploadLogsForContainer(containerId);\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(\"PendingContainers queue is interrupted\");\n      }\n    }\n\n    // Application is finished. Finish pending-containers\n    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n      uploadLogsForContainer(containerId);\n    }\n\n    // Remove the local app-log-dirs\n    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n    int index \u003d 0;\n    for (String rootLogDir : this.rootLogDirs) {\n      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n      index++;\n    }\n    this.delService.delete(this.userUgi.getShortUserName(), null,\n        localAppLogDirs);\n\n    if (this.writer !\u003d null) {\n      this.writer.closeWriter();\n      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n    }\n    try {\n      userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n        @Override\n        public Object run() throws Exception {\n          FileSystem remoteFS \u003d FileSystem.get(conf);\n          remoteFS.rename(remoteNodeTmpLogFileForApp, remoteNodeLogFileForApp);\n          return null;\n        }\n      });\n    } catch (Exception e) {\n      LOG.error(\"Failed to move temporary log file to final location: [\"\n          + remoteNodeTmpLogFileForApp + \"] to [\" + remoteNodeLogFileForApp\n          + \"]\", e);\n    }\n    \n    this.dispatcher.getEventHandler().handle(\n        new ApplicationEvent(this.appId,\n            ApplicationEventType.APPLICATION_LOG_AGGREGATION_FINISHED));\n        \n    this.appAggregationFinished.set(true);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void run() {\n\n    ContainerId containerId;\n\n    while (!this.appFinishing.get()) {\n      try {\n        containerId \u003d this.pendingContainers.poll();\n        if (containerId \u003d\u003d null) {\n          Thread.sleep(THREAD_SLEEP_TIME);\n        } else {\n          uploadLogsForContainer(containerId);\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(\"PendingContainers queue is interrupted\");\n      }\n    }\n\n    // Application is finished. Finish pending-containers\n    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n      uploadLogsForContainer(containerId);\n    }\n\n    // Remove the local app-log-dirs\n    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n    int index \u003d 0;\n    for (String rootLogDir : this.rootLogDirs) {\n      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n      index++;\n    }\n    this.delService.delete(this.userUgi.getShortUserName(), null,\n        localAppLogDirs);\n\n    if (this.writer !\u003d null) {\n      this.writer.closeWriter();\n      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n    }\n\n    this.appAggregationFinished.set(true);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
        "newPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymultichange(Ymovefromfile,Yexceptionschange,Ybodychange,Yparameterchange)",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,39 @@\n-  public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)\n-    throws IOException, ClassNotFoundException, InterruptedException {\n-    this.umbilical \u003d umbilical;\n+  public void run() {\n \n-    if (isMapTask()) {\n-      // If there are no reducers then there won\u0027t be any sort. Hence the map \n-      // phase will govern the entire attempt\u0027s progress.\n-      if (conf.getNumReduceTasks() \u003d\u003d 0) {\n-        mapPhase \u003d getProgress().addPhase(\"map\", 1.0f);\n-      } else {\n-        // If there are reducers then the entire attempt\u0027s progress will be \n-        // split between the map phase (67%) and the sort phase (33%).\n-        mapPhase \u003d getProgress().addPhase(\"map\", 0.667f);\n-        sortPhase  \u003d getProgress().addPhase(\"sort\", 0.333f);\n+    ContainerId containerId;\n+\n+    while (!this.appFinishing.get()) {\n+      try {\n+        containerId \u003d this.pendingContainers.poll();\n+        if (containerId \u003d\u003d null) {\n+          Thread.sleep(THREAD_SLEEP_TIME);\n+        } else {\n+          uploadLogsForContainer(containerId);\n+        }\n+      } catch (InterruptedException e) {\n+        LOG.warn(\"PendingContainers queue is interrupted\");\n       }\n     }\n-    TaskReporter reporter \u003d startReporter(umbilical);\n- \n-    boolean useNewApi \u003d job.getUseNewMapper();\n-    initialize(job, getJobID(), reporter, useNewApi);\n \n-    // check if it is a cleanupJobTask\n-    if (jobCleanup) {\n-      runJobCleanupTask(umbilical, reporter);\n-      return;\n-    }\n-    if (jobSetup) {\n-      runJobSetupTask(umbilical, reporter);\n-      return;\n-    }\n-    if (taskCleanup) {\n-      runTaskCleanupTask(umbilical, reporter);\n-      return;\n+    // Application is finished. Finish pending-containers\n+    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n+      uploadLogsForContainer(containerId);\n     }\n \n-    if (useNewApi) {\n-      runNewMapper(job, splitMetaInfo, umbilical, reporter);\n-    } else {\n-      runOldMapper(job, splitMetaInfo, umbilical, reporter);\n+    // Remove the local app-log-dirs\n+    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n+    int index \u003d 0;\n+    for (String rootLogDir : this.rootLogDirs) {\n+      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n+      index++;\n     }\n-    done(umbilical, reporter);\n+    this.delService.delete(this.userUgi.getShortUserName(), null,\n+        localAppLogDirs);\n+\n+    if (this.writer !\u003d null) {\n+      this.writer.closeWriter();\n+      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n+    }\n+\n+    this.appAggregationFinished.set(true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void run() {\n\n    ContainerId containerId;\n\n    while (!this.appFinishing.get()) {\n      try {\n        containerId \u003d this.pendingContainers.poll();\n        if (containerId \u003d\u003d null) {\n          Thread.sleep(THREAD_SLEEP_TIME);\n        } else {\n          uploadLogsForContainer(containerId);\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(\"PendingContainers queue is interrupted\");\n      }\n    }\n\n    // Application is finished. Finish pending-containers\n    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n      uploadLogsForContainer(containerId);\n    }\n\n    // Remove the local app-log-dirs\n    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n    int index \u003d 0;\n    for (String rootLogDir : this.rootLogDirs) {\n      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n      index++;\n    }\n    this.delService.delete(this.userUgi.getShortUserName(), null,\n        localAppLogDirs);\n\n    if (this.writer !\u003d null) {\n      this.writer.closeWriter();\n      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n    }\n\n    this.appAggregationFinished.set(true);\n  }",
          "path": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
          "extendedDetails": {
            "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java",
            "newPath": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
            "oldMethodName": "run",
            "newMethodName": "run"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,39 @@\n-  public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)\n-    throws IOException, ClassNotFoundException, InterruptedException {\n-    this.umbilical \u003d umbilical;\n+  public void run() {\n \n-    if (isMapTask()) {\n-      // If there are no reducers then there won\u0027t be any sort. Hence the map \n-      // phase will govern the entire attempt\u0027s progress.\n-      if (conf.getNumReduceTasks() \u003d\u003d 0) {\n-        mapPhase \u003d getProgress().addPhase(\"map\", 1.0f);\n-      } else {\n-        // If there are reducers then the entire attempt\u0027s progress will be \n-        // split between the map phase (67%) and the sort phase (33%).\n-        mapPhase \u003d getProgress().addPhase(\"map\", 0.667f);\n-        sortPhase  \u003d getProgress().addPhase(\"sort\", 0.333f);\n+    ContainerId containerId;\n+\n+    while (!this.appFinishing.get()) {\n+      try {\n+        containerId \u003d this.pendingContainers.poll();\n+        if (containerId \u003d\u003d null) {\n+          Thread.sleep(THREAD_SLEEP_TIME);\n+        } else {\n+          uploadLogsForContainer(containerId);\n+        }\n+      } catch (InterruptedException e) {\n+        LOG.warn(\"PendingContainers queue is interrupted\");\n       }\n     }\n-    TaskReporter reporter \u003d startReporter(umbilical);\n- \n-    boolean useNewApi \u003d job.getUseNewMapper();\n-    initialize(job, getJobID(), reporter, useNewApi);\n \n-    // check if it is a cleanupJobTask\n-    if (jobCleanup) {\n-      runJobCleanupTask(umbilical, reporter);\n-      return;\n-    }\n-    if (jobSetup) {\n-      runJobSetupTask(umbilical, reporter);\n-      return;\n-    }\n-    if (taskCleanup) {\n-      runTaskCleanupTask(umbilical, reporter);\n-      return;\n+    // Application is finished. Finish pending-containers\n+    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n+      uploadLogsForContainer(containerId);\n     }\n \n-    if (useNewApi) {\n-      runNewMapper(job, splitMetaInfo, umbilical, reporter);\n-    } else {\n-      runOldMapper(job, splitMetaInfo, umbilical, reporter);\n+    // Remove the local app-log-dirs\n+    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n+    int index \u003d 0;\n+    for (String rootLogDir : this.rootLogDirs) {\n+      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n+      index++;\n     }\n-    done(umbilical, reporter);\n+    this.delService.delete(this.userUgi.getShortUserName(), null,\n+        localAppLogDirs);\n+\n+    if (this.writer !\u003d null) {\n+      this.writer.closeWriter();\n+      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n+    }\n+\n+    this.appAggregationFinished.set(true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void run() {\n\n    ContainerId containerId;\n\n    while (!this.appFinishing.get()) {\n      try {\n        containerId \u003d this.pendingContainers.poll();\n        if (containerId \u003d\u003d null) {\n          Thread.sleep(THREAD_SLEEP_TIME);\n        } else {\n          uploadLogsForContainer(containerId);\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(\"PendingContainers queue is interrupted\");\n      }\n    }\n\n    // Application is finished. Finish pending-containers\n    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n      uploadLogsForContainer(containerId);\n    }\n\n    // Remove the local app-log-dirs\n    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n    int index \u003d 0;\n    for (String rootLogDir : this.rootLogDirs) {\n      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n      index++;\n    }\n    this.delService.delete(this.userUgi.getShortUserName(), null,\n        localAppLogDirs);\n\n    if (this.writer !\u003d null) {\n      this.writer.closeWriter();\n      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n    }\n\n    this.appAggregationFinished.set(true);\n  }",
          "path": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
          "extendedDetails": {
            "oldValue": "[IOException, ClassNotFoundException, InterruptedException]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,39 @@\n-  public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)\n-    throws IOException, ClassNotFoundException, InterruptedException {\n-    this.umbilical \u003d umbilical;\n+  public void run() {\n \n-    if (isMapTask()) {\n-      // If there are no reducers then there won\u0027t be any sort. Hence the map \n-      // phase will govern the entire attempt\u0027s progress.\n-      if (conf.getNumReduceTasks() \u003d\u003d 0) {\n-        mapPhase \u003d getProgress().addPhase(\"map\", 1.0f);\n-      } else {\n-        // If there are reducers then the entire attempt\u0027s progress will be \n-        // split between the map phase (67%) and the sort phase (33%).\n-        mapPhase \u003d getProgress().addPhase(\"map\", 0.667f);\n-        sortPhase  \u003d getProgress().addPhase(\"sort\", 0.333f);\n+    ContainerId containerId;\n+\n+    while (!this.appFinishing.get()) {\n+      try {\n+        containerId \u003d this.pendingContainers.poll();\n+        if (containerId \u003d\u003d null) {\n+          Thread.sleep(THREAD_SLEEP_TIME);\n+        } else {\n+          uploadLogsForContainer(containerId);\n+        }\n+      } catch (InterruptedException e) {\n+        LOG.warn(\"PendingContainers queue is interrupted\");\n       }\n     }\n-    TaskReporter reporter \u003d startReporter(umbilical);\n- \n-    boolean useNewApi \u003d job.getUseNewMapper();\n-    initialize(job, getJobID(), reporter, useNewApi);\n \n-    // check if it is a cleanupJobTask\n-    if (jobCleanup) {\n-      runJobCleanupTask(umbilical, reporter);\n-      return;\n-    }\n-    if (jobSetup) {\n-      runJobSetupTask(umbilical, reporter);\n-      return;\n-    }\n-    if (taskCleanup) {\n-      runTaskCleanupTask(umbilical, reporter);\n-      return;\n+    // Application is finished. Finish pending-containers\n+    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n+      uploadLogsForContainer(containerId);\n     }\n \n-    if (useNewApi) {\n-      runNewMapper(job, splitMetaInfo, umbilical, reporter);\n-    } else {\n-      runOldMapper(job, splitMetaInfo, umbilical, reporter);\n+    // Remove the local app-log-dirs\n+    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n+    int index \u003d 0;\n+    for (String rootLogDir : this.rootLogDirs) {\n+      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n+      index++;\n     }\n-    done(umbilical, reporter);\n+    this.delService.delete(this.userUgi.getShortUserName(), null,\n+        localAppLogDirs);\n+\n+    if (this.writer !\u003d null) {\n+      this.writer.closeWriter();\n+      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n+    }\n+\n+    this.appAggregationFinished.set(true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void run() {\n\n    ContainerId containerId;\n\n    while (!this.appFinishing.get()) {\n      try {\n        containerId \u003d this.pendingContainers.poll();\n        if (containerId \u003d\u003d null) {\n          Thread.sleep(THREAD_SLEEP_TIME);\n        } else {\n          uploadLogsForContainer(containerId);\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(\"PendingContainers queue is interrupted\");\n      }\n    }\n\n    // Application is finished. Finish pending-containers\n    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n      uploadLogsForContainer(containerId);\n    }\n\n    // Remove the local app-log-dirs\n    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n    int index \u003d 0;\n    for (String rootLogDir : this.rootLogDirs) {\n      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n      index++;\n    }\n    this.delService.delete(this.userUgi.getShortUserName(), null,\n        localAppLogDirs);\n\n    if (this.writer !\u003d null) {\n      this.writer.closeWriter();\n      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n    }\n\n    this.appAggregationFinished.set(true);\n  }",
          "path": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,39 @@\n-  public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)\n-    throws IOException, ClassNotFoundException, InterruptedException {\n-    this.umbilical \u003d umbilical;\n+  public void run() {\n \n-    if (isMapTask()) {\n-      // If there are no reducers then there won\u0027t be any sort. Hence the map \n-      // phase will govern the entire attempt\u0027s progress.\n-      if (conf.getNumReduceTasks() \u003d\u003d 0) {\n-        mapPhase \u003d getProgress().addPhase(\"map\", 1.0f);\n-      } else {\n-        // If there are reducers then the entire attempt\u0027s progress will be \n-        // split between the map phase (67%) and the sort phase (33%).\n-        mapPhase \u003d getProgress().addPhase(\"map\", 0.667f);\n-        sortPhase  \u003d getProgress().addPhase(\"sort\", 0.333f);\n+    ContainerId containerId;\n+\n+    while (!this.appFinishing.get()) {\n+      try {\n+        containerId \u003d this.pendingContainers.poll();\n+        if (containerId \u003d\u003d null) {\n+          Thread.sleep(THREAD_SLEEP_TIME);\n+        } else {\n+          uploadLogsForContainer(containerId);\n+        }\n+      } catch (InterruptedException e) {\n+        LOG.warn(\"PendingContainers queue is interrupted\");\n       }\n     }\n-    TaskReporter reporter \u003d startReporter(umbilical);\n- \n-    boolean useNewApi \u003d job.getUseNewMapper();\n-    initialize(job, getJobID(), reporter, useNewApi);\n \n-    // check if it is a cleanupJobTask\n-    if (jobCleanup) {\n-      runJobCleanupTask(umbilical, reporter);\n-      return;\n-    }\n-    if (jobSetup) {\n-      runJobSetupTask(umbilical, reporter);\n-      return;\n-    }\n-    if (taskCleanup) {\n-      runTaskCleanupTask(umbilical, reporter);\n-      return;\n+    // Application is finished. Finish pending-containers\n+    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n+      uploadLogsForContainer(containerId);\n     }\n \n-    if (useNewApi) {\n-      runNewMapper(job, splitMetaInfo, umbilical, reporter);\n-    } else {\n-      runOldMapper(job, splitMetaInfo, umbilical, reporter);\n+    // Remove the local app-log-dirs\n+    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n+    int index \u003d 0;\n+    for (String rootLogDir : this.rootLogDirs) {\n+      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n+      index++;\n     }\n-    done(umbilical, reporter);\n+    this.delService.delete(this.userUgi.getShortUserName(), null,\n+        localAppLogDirs);\n+\n+    if (this.writer !\u003d null) {\n+      this.writer.closeWriter();\n+      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n+    }\n+\n+    this.appAggregationFinished.set(true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void run() {\n\n    ContainerId containerId;\n\n    while (!this.appFinishing.get()) {\n      try {\n        containerId \u003d this.pendingContainers.poll();\n        if (containerId \u003d\u003d null) {\n          Thread.sleep(THREAD_SLEEP_TIME);\n        } else {\n          uploadLogsForContainer(containerId);\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(\"PendingContainers queue is interrupted\");\n      }\n    }\n\n    // Application is finished. Finish pending-containers\n    while ((containerId \u003d this.pendingContainers.poll()) !\u003d null) {\n      uploadLogsForContainer(containerId);\n    }\n\n    // Remove the local app-log-dirs\n    Path[] localAppLogDirs \u003d new Path[this.rootLogDirs.length];\n    int index \u003d 0;\n    for (String rootLogDir : this.rootLogDirs) {\n      localAppLogDirs[index] \u003d new Path(rootLogDir, this.applicationId);\n      index++;\n    }\n    this.delService.delete(this.userUgi.getShortUserName(), null,\n        localAppLogDirs);\n\n    if (this.writer !\u003d null) {\n      this.writer.closeWriter();\n      LOG.info(\"Finished aggregate log-file for app \" + this.applicationId);\n    }\n\n    this.appAggregationFinished.set(true);\n  }",
          "path": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
          "extendedDetails": {
            "oldValue": "[job-JobConf(modifiers-final), umbilical-TaskUmbilicalProtocol(modifiers-final)]",
            "newValue": "[]"
          }
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,42 @@\n+  public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)\n+    throws IOException, ClassNotFoundException, InterruptedException {\n+    this.umbilical \u003d umbilical;\n+\n+    if (isMapTask()) {\n+      // If there are no reducers then there won\u0027t be any sort. Hence the map \n+      // phase will govern the entire attempt\u0027s progress.\n+      if (conf.getNumReduceTasks() \u003d\u003d 0) {\n+        mapPhase \u003d getProgress().addPhase(\"map\", 1.0f);\n+      } else {\n+        // If there are reducers then the entire attempt\u0027s progress will be \n+        // split between the map phase (67%) and the sort phase (33%).\n+        mapPhase \u003d getProgress().addPhase(\"map\", 0.667f);\n+        sortPhase  \u003d getProgress().addPhase(\"sort\", 0.333f);\n+      }\n+    }\n+    TaskReporter reporter \u003d startReporter(umbilical);\n+ \n+    boolean useNewApi \u003d job.getUseNewMapper();\n+    initialize(job, getJobID(), reporter, useNewApi);\n+\n+    // check if it is a cleanupJobTask\n+    if (jobCleanup) {\n+      runJobCleanupTask(umbilical, reporter);\n+      return;\n+    }\n+    if (jobSetup) {\n+      runJobSetupTask(umbilical, reporter);\n+      return;\n+    }\n+    if (taskCleanup) {\n+      runTaskCleanupTask(umbilical, reporter);\n+      return;\n+    }\n+\n+    if (useNewApi) {\n+      runNewMapper(job, splitMetaInfo, umbilical, reporter);\n+    } else {\n+      runOldMapper(job, splitMetaInfo, umbilical, reporter);\n+    }\n+    done(umbilical, reporter);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)\n    throws IOException, ClassNotFoundException, InterruptedException {\n    this.umbilical \u003d umbilical;\n\n    if (isMapTask()) {\n      // If there are no reducers then there won\u0027t be any sort. Hence the map \n      // phase will govern the entire attempt\u0027s progress.\n      if (conf.getNumReduceTasks() \u003d\u003d 0) {\n        mapPhase \u003d getProgress().addPhase(\"map\", 1.0f);\n      } else {\n        // If there are reducers then the entire attempt\u0027s progress will be \n        // split between the map phase (67%) and the sort phase (33%).\n        mapPhase \u003d getProgress().addPhase(\"map\", 0.667f);\n        sortPhase  \u003d getProgress().addPhase(\"sort\", 0.333f);\n      }\n    }\n    TaskReporter reporter \u003d startReporter(umbilical);\n \n    boolean useNewApi \u003d job.getUseNewMapper();\n    initialize(job, getJobID(), reporter, useNewApi);\n\n    // check if it is a cleanupJobTask\n    if (jobCleanup) {\n      runJobCleanupTask(umbilical, reporter);\n      return;\n    }\n    if (jobSetup) {\n      runJobSetupTask(umbilical, reporter);\n      return;\n    }\n    if (taskCleanup) {\n      runTaskCleanupTask(umbilical, reporter);\n      return;\n    }\n\n    if (useNewApi) {\n      runNewMapper(job, splitMetaInfo, umbilical, reporter);\n    } else {\n      runOldMapper(job, splitMetaInfo, umbilical, reporter);\n    }\n    done(umbilical, reporter);\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java"
    }
  }
}