{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "MapTask.java",
  "functionName": "mergeParts",
  "functionId": "mergeParts",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
  "functionStartLine": 1844,
  "functionEndLine": 1995,
  "numCommitsSeen": 36,
  "timeTaken": 10621,
  "changeHistory": [
    "683e0c71fe09600a24bdd7b707a613fe70ff1f6e",
    "9f192cc5ac4a6145e2eeaecba0a754d31e601898",
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
    "43553df4e36fa54e017481d3bcfd7251ecbd19bb",
    "03c1015d6ea92c782e9ad78794cf1e9640f71040",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "683e0c71fe09600a24bdd7b707a613fe70ff1f6e": "Ybodychange",
    "9f192cc5ac4a6145e2eeaecba0a754d31e601898": "Ybodychange",
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": "Ybodychange",
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4": "Ybodychange",
    "43553df4e36fa54e017481d3bcfd7251ecbd19bb": "Ybodychange",
    "03c1015d6ea92c782e9ad78794cf1e9640f71040": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymovefromfile",
    "ded6f225a55517deedc2bd502f2b68f1ca2ddee8": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "683e0c71fe09600a24bdd7b707a613fe70ff1f6e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6793. io.sort.factor code default and mapred-default.xml values inconsistent. Contributed by Gera Shegalov.\n",
      "commitDate": "21/11/16 8:40 AM",
      "commitName": "683e0c71fe09600a24bdd7b707a613fe70ff1f6e",
      "commitAuthor": "Rohith Sharma K S",
      "commitDateOld": "09/09/16 11:12 AM",
      "commitNameOld": "9f192cc5ac4a6145e2eeaecba0a754d31e601898",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 72.94,
      "commitsBetweenForRepo": 561,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,151 +1,152 @@\n     private void mergeParts() throws IOException, InterruptedException, \n                                      ClassNotFoundException {\n       // get the approximate size of the final output/index files\n       long finalOutFileSize \u003d 0;\n       long finalIndexFileSize \u003d 0;\n       final Path[] filename \u003d new Path[numSpills];\n       final TaskAttemptID mapId \u003d getTaskID();\n \n       for(int i \u003d 0; i \u003c numSpills; i++) {\n         filename[i] \u003d mapOutputFile.getSpillFile(i);\n         finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n       }\n       if (numSpills \u003d\u003d 1) { //the spill is the final output\n         sameVolRename(filename[0],\n             mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n         if (indexCacheList.size() \u003d\u003d 0) {\n           sameVolRename(mapOutputFile.getSpillIndexFile(0),\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n         } else {\n           indexCacheList.get(0).writeToFile(\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n         }\n         sortPhase.complete();\n         return;\n       }\n \n       // read in paged indices\n       for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n         Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n         indexCacheList.add(new SpillRecord(indexFileName, job));\n       }\n \n       //make correction in the length to include the sequence file header\n       //lengths for each partition\n       finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n       finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n       Path finalOutputFile \u003d\n           mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n       Path finalIndexFile \u003d\n           mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n \n       //The output stream for the final single output file\n       FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n       FSDataOutputStream finalPartitionOut \u003d null;\n \n       if (numSpills \u003d\u003d 0) {\n         //create dummy files\n         IndexRecord rec \u003d new IndexRecord();\n         SpillRecord sr \u003d new SpillRecord(partitions);\n         try {\n           for (int i \u003d 0; i \u003c partitions; i++) {\n             long segmentStart \u003d finalOut.getPos();\n             finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut,\n                 false);\n             Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec, null);\n             writer.close();\n             if (finalPartitionOut !\u003d finalOut) {\n               finalPartitionOut.close();\n               finalPartitionOut \u003d null;\n             }\n             rec.startOffset \u003d segmentStart;\n             rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n             rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n             sr.putIndex(rec, i);\n           }\n           sr.writeToFile(finalIndexFile, job);\n         } finally {\n           finalOut.close();\n           if (finalPartitionOut !\u003d null) {\n             finalPartitionOut.close();\n           }\n         }\n         sortPhase.complete();\n         return;\n       }\n       {\n         sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n         \n         IndexRecord rec \u003d new IndexRecord();\n         final SpillRecord spillRec \u003d new SpillRecord(partitions);\n         for (int parts \u003d 0; parts \u003c partitions; parts++) {\n           //create the segments to be merged\n           List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n             new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n           for(int i \u003d 0; i \u003c numSpills; i++) {\n             IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n \n             Segment\u003cK,V\u003e s \u003d\n               new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                                indexRecord.partLength, codec, true);\n             segmentList.add(i, s);\n \n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                   \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                   indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n             }\n           }\n \n-          int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n+          int mergeFactor \u003d job.getInt(MRJobConfig.IO_SORT_FACTOR,\n+              MRJobConfig.DEFAULT_IO_SORT_FACTOR);\n           // sort the segments only if there are intermediate merges\n           boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n           //merge\n           @SuppressWarnings(\"unchecked\")\n           RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                          keyClass, valClass, codec,\n                          segmentList, mergeFactor,\n                          new Path(mapId.toString()),\n                          job.getOutputKeyComparator(), reporter, sortSegments,\n                          null, spilledRecordsCounter, sortPhase.phase(),\n                          TaskType.MAP);\n \n           //write merged output to disk\n           long segmentStart \u003d finalOut.getPos();\n           finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut, false);\n           Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec,\n                                spilledRecordsCounter);\n           if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n             Merger.writeFile(kvIter, writer, reporter, job);\n           } else {\n             combineCollector.setWriter(writer);\n             combinerRunner.combine(kvIter, combineCollector);\n           }\n \n           //close\n           writer.close();\n           if (finalPartitionOut !\u003d finalOut) {\n             finalPartitionOut.close();\n             finalPartitionOut \u003d null;\n           }\n \n           sortPhase.startNextPhase();\n           \n           // record offsets\n           rec.startOffset \u003d segmentStart;\n           rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n           rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n           spillRec.putIndex(rec, parts);\n         }\n         spillRec.writeToFile(finalIndexFile, job);\n         finalOut.close();\n         if (finalPartitionOut !\u003d null) {\n           finalPartitionOut.close();\n         }\n         for(int i \u003d 0; i \u003c numSpills; i++) {\n           rfs.delete(filename[i],true);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize \u003d 0;\n      long finalIndexFileSize \u003d 0;\n      final Path[] filename \u003d new Path[numSpills];\n      final TaskAttemptID mapId \u003d getTaskID();\n\n      for(int i \u003d 0; i \u003c numSpills; i++) {\n        filename[i] \u003d mapOutputFile.getSpillFile(i);\n        finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills \u003d\u003d 1) { //the spill is the final output\n        sameVolRename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() \u003d\u003d 0) {\n          sameVolRename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        sortPhase.complete();\n        return;\n      }\n\n      // read in paged indices\n      for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n        Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile \u003d\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile \u003d\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n      FSDataOutputStream finalPartitionOut \u003d null;\n\n      if (numSpills \u003d\u003d 0) {\n        //create dummy files\n        IndexRecord rec \u003d new IndexRecord();\n        SpillRecord sr \u003d new SpillRecord(partitions);\n        try {\n          for (int i \u003d 0; i \u003c partitions; i++) {\n            long segmentStart \u003d finalOut.getPos();\n            finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut,\n                false);\n            Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec, null);\n            writer.close();\n            if (finalPartitionOut !\u003d finalOut) {\n              finalPartitionOut.close();\n              finalPartitionOut \u003d null;\n            }\n            rec.startOffset \u003d segmentStart;\n            rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n            rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n          if (finalPartitionOut !\u003d null) {\n            finalPartitionOut.close();\n          }\n        }\n        sortPhase.complete();\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        \n        IndexRecord rec \u003d new IndexRecord();\n        final SpillRecord spillRec \u003d new SpillRecord(partitions);\n        for (int parts \u003d 0; parts \u003c partitions; parts++) {\n          //create the segments to be merged\n          List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n            new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n          for(int i \u003d 0; i \u003c numSpills; i++) {\n            IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n\n            Segment\u003cK,V\u003e s \u003d\n              new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                  \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor \u003d job.getInt(MRJobConfig.IO_SORT_FACTOR,\n              MRJobConfig.DEFAULT_IO_SORT_FACTOR);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase(),\n                         TaskType.MAP);\n\n          //write merged output to disk\n          long segmentStart \u003d finalOut.getPos();\n          finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut, false);\n          Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n          if (finalPartitionOut !\u003d finalOut) {\n            finalPartitionOut.close();\n            finalPartitionOut \u003d null;\n          }\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset \u003d segmentStart;\n          rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n          rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        if (finalPartitionOut !\u003d null) {\n          finalPartitionOut.close();\n        }\n        for(int i \u003d 0; i \u003c numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "9f192cc5ac4a6145e2eeaecba0a754d31e601898": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6628. Potential memory leak in CryptoOutputStream. Contributed by Mariappan Asokan\n",
      "commitDate": "09/09/16 11:12 AM",
      "commitName": "9f192cc5ac4a6145e2eeaecba0a754d31e601898",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "26/04/15 8:31 PM",
      "commitNameOld": "618ba707f0f2ddc353414dbd0eee0ab9e83b8013",
      "commitAuthorOld": "Allen Wittenauer",
      "daysBetweenCommits": 501.61,
      "commitsBetweenForRepo": 3740,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,135 +1,151 @@\n     private void mergeParts() throws IOException, InterruptedException, \n                                      ClassNotFoundException {\n       // get the approximate size of the final output/index files\n       long finalOutFileSize \u003d 0;\n       long finalIndexFileSize \u003d 0;\n       final Path[] filename \u003d new Path[numSpills];\n       final TaskAttemptID mapId \u003d getTaskID();\n \n       for(int i \u003d 0; i \u003c numSpills; i++) {\n         filename[i] \u003d mapOutputFile.getSpillFile(i);\n         finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n       }\n       if (numSpills \u003d\u003d 1) { //the spill is the final output\n         sameVolRename(filename[0],\n             mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n         if (indexCacheList.size() \u003d\u003d 0) {\n           sameVolRename(mapOutputFile.getSpillIndexFile(0),\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n         } else {\n           indexCacheList.get(0).writeToFile(\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n         }\n         sortPhase.complete();\n         return;\n       }\n \n       // read in paged indices\n       for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n         Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n         indexCacheList.add(new SpillRecord(indexFileName, job));\n       }\n \n       //make correction in the length to include the sequence file header\n       //lengths for each partition\n       finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n       finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n       Path finalOutputFile \u003d\n           mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n       Path finalIndexFile \u003d\n           mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n \n       //The output stream for the final single output file\n       FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n+      FSDataOutputStream finalPartitionOut \u003d null;\n \n       if (numSpills \u003d\u003d 0) {\n         //create dummy files\n         IndexRecord rec \u003d new IndexRecord();\n         SpillRecord sr \u003d new SpillRecord(partitions);\n         try {\n           for (int i \u003d 0; i \u003c partitions; i++) {\n             long segmentStart \u003d finalOut.getPos();\n-            FSDataOutputStream finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut);\n+            finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut,\n+                false);\n             Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec, null);\n             writer.close();\n+            if (finalPartitionOut !\u003d finalOut) {\n+              finalPartitionOut.close();\n+              finalPartitionOut \u003d null;\n+            }\n             rec.startOffset \u003d segmentStart;\n             rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n             rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n             sr.putIndex(rec, i);\n           }\n           sr.writeToFile(finalIndexFile, job);\n         } finally {\n           finalOut.close();\n+          if (finalPartitionOut !\u003d null) {\n+            finalPartitionOut.close();\n+          }\n         }\n         sortPhase.complete();\n         return;\n       }\n       {\n         sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n         \n         IndexRecord rec \u003d new IndexRecord();\n         final SpillRecord spillRec \u003d new SpillRecord(partitions);\n         for (int parts \u003d 0; parts \u003c partitions; parts++) {\n           //create the segments to be merged\n           List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n             new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n           for(int i \u003d 0; i \u003c numSpills; i++) {\n             IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n \n             Segment\u003cK,V\u003e s \u003d\n               new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                                indexRecord.partLength, codec, true);\n             segmentList.add(i, s);\n \n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                   \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                   indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n             }\n           }\n \n           int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n           // sort the segments only if there are intermediate merges\n           boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n           //merge\n           @SuppressWarnings(\"unchecked\")\n           RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                          keyClass, valClass, codec,\n                          segmentList, mergeFactor,\n                          new Path(mapId.toString()),\n                          job.getOutputKeyComparator(), reporter, sortSegments,\n                          null, spilledRecordsCounter, sortPhase.phase(),\n                          TaskType.MAP);\n \n           //write merged output to disk\n           long segmentStart \u003d finalOut.getPos();\n-          FSDataOutputStream finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut);\n+          finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut, false);\n           Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec,\n                                spilledRecordsCounter);\n           if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n             Merger.writeFile(kvIter, writer, reporter, job);\n           } else {\n             combineCollector.setWriter(writer);\n             combinerRunner.combine(kvIter, combineCollector);\n           }\n \n           //close\n           writer.close();\n+          if (finalPartitionOut !\u003d finalOut) {\n+            finalPartitionOut.close();\n+            finalPartitionOut \u003d null;\n+          }\n \n           sortPhase.startNextPhase();\n           \n           // record offsets\n           rec.startOffset \u003d segmentStart;\n           rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n           rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n           spillRec.putIndex(rec, parts);\n         }\n         spillRec.writeToFile(finalIndexFile, job);\n         finalOut.close();\n+        if (finalPartitionOut !\u003d null) {\n+          finalPartitionOut.close();\n+        }\n         for(int i \u003d 0; i \u003c numSpills; i++) {\n           rfs.delete(filename[i],true);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize \u003d 0;\n      long finalIndexFileSize \u003d 0;\n      final Path[] filename \u003d new Path[numSpills];\n      final TaskAttemptID mapId \u003d getTaskID();\n\n      for(int i \u003d 0; i \u003c numSpills; i++) {\n        filename[i] \u003d mapOutputFile.getSpillFile(i);\n        finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills \u003d\u003d 1) { //the spill is the final output\n        sameVolRename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() \u003d\u003d 0) {\n          sameVolRename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        sortPhase.complete();\n        return;\n      }\n\n      // read in paged indices\n      for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n        Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile \u003d\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile \u003d\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n      FSDataOutputStream finalPartitionOut \u003d null;\n\n      if (numSpills \u003d\u003d 0) {\n        //create dummy files\n        IndexRecord rec \u003d new IndexRecord();\n        SpillRecord sr \u003d new SpillRecord(partitions);\n        try {\n          for (int i \u003d 0; i \u003c partitions; i++) {\n            long segmentStart \u003d finalOut.getPos();\n            finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut,\n                false);\n            Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec, null);\n            writer.close();\n            if (finalPartitionOut !\u003d finalOut) {\n              finalPartitionOut.close();\n              finalPartitionOut \u003d null;\n            }\n            rec.startOffset \u003d segmentStart;\n            rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n            rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n          if (finalPartitionOut !\u003d null) {\n            finalPartitionOut.close();\n          }\n        }\n        sortPhase.complete();\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        \n        IndexRecord rec \u003d new IndexRecord();\n        final SpillRecord spillRec \u003d new SpillRecord(partitions);\n        for (int parts \u003d 0; parts \u003c partitions; parts++) {\n          //create the segments to be merged\n          List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n            new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n          for(int i \u003d 0; i \u003c numSpills; i++) {\n            IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n\n            Segment\u003cK,V\u003e s \u003d\n              new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                  \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase(),\n                         TaskType.MAP);\n\n          //write merged output to disk\n          long segmentStart \u003d finalOut.getPos();\n          finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut, false);\n          Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n          if (finalPartitionOut !\u003d finalOut) {\n            finalPartitionOut.close();\n            finalPartitionOut \u003d null;\n          }\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset \u003d segmentStart;\n          rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n          rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        if (finalPartitionOut !\u003d null) {\n          finalPartitionOut.close();\n        }\n        for(int i \u003d 0; i \u003c numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5890. Support for encrypting Intermediate data and spills in local filesystem. (asuresh via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1609597 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/07/14 5:43 PM",
      "commitName": "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "10/03/14 6:29 PM",
      "commitNameOld": "98ecd4ffef333cb6703e922de3f1d8512cacefed",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 121.97,
      "commitsBetweenForRepo": 769,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,133 +1,135 @@\n     private void mergeParts() throws IOException, InterruptedException, \n                                      ClassNotFoundException {\n       // get the approximate size of the final output/index files\n       long finalOutFileSize \u003d 0;\n       long finalIndexFileSize \u003d 0;\n       final Path[] filename \u003d new Path[numSpills];\n       final TaskAttemptID mapId \u003d getTaskID();\n \n       for(int i \u003d 0; i \u003c numSpills; i++) {\n         filename[i] \u003d mapOutputFile.getSpillFile(i);\n         finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n       }\n       if (numSpills \u003d\u003d 1) { //the spill is the final output\n         sameVolRename(filename[0],\n             mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n         if (indexCacheList.size() \u003d\u003d 0) {\n           sameVolRename(mapOutputFile.getSpillIndexFile(0),\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n         } else {\n           indexCacheList.get(0).writeToFile(\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n         }\n         sortPhase.complete();\n         return;\n       }\n \n       // read in paged indices\n       for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n         Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n         indexCacheList.add(new SpillRecord(indexFileName, job));\n       }\n \n       //make correction in the length to include the sequence file header\n       //lengths for each partition\n       finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n       finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n       Path finalOutputFile \u003d\n           mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n       Path finalIndexFile \u003d\n           mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n \n       //The output stream for the final single output file\n       FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n \n       if (numSpills \u003d\u003d 0) {\n         //create dummy files\n         IndexRecord rec \u003d new IndexRecord();\n         SpillRecord sr \u003d new SpillRecord(partitions);\n         try {\n           for (int i \u003d 0; i \u003c partitions; i++) {\n             long segmentStart \u003d finalOut.getPos();\n+            FSDataOutputStream finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut);\n             Writer\u003cK, V\u003e writer \u003d\n-              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n+              new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec, null);\n             writer.close();\n             rec.startOffset \u003d segmentStart;\n-            rec.rawLength \u003d writer.getRawLength();\n-            rec.partLength \u003d writer.getCompressedLength();\n+            rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n+            rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n             sr.putIndex(rec, i);\n           }\n           sr.writeToFile(finalIndexFile, job);\n         } finally {\n           finalOut.close();\n         }\n         sortPhase.complete();\n         return;\n       }\n       {\n         sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n         \n         IndexRecord rec \u003d new IndexRecord();\n         final SpillRecord spillRec \u003d new SpillRecord(partitions);\n         for (int parts \u003d 0; parts \u003c partitions; parts++) {\n           //create the segments to be merged\n           List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n             new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n           for(int i \u003d 0; i \u003c numSpills; i++) {\n             IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n \n             Segment\u003cK,V\u003e s \u003d\n               new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                                indexRecord.partLength, codec, true);\n             segmentList.add(i, s);\n \n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                   \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                   indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n             }\n           }\n \n           int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n           // sort the segments only if there are intermediate merges\n           boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n           //merge\n           @SuppressWarnings(\"unchecked\")\n           RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                          keyClass, valClass, codec,\n                          segmentList, mergeFactor,\n                          new Path(mapId.toString()),\n                          job.getOutputKeyComparator(), reporter, sortSegments,\n                          null, spilledRecordsCounter, sortPhase.phase(),\n                          TaskType.MAP);\n \n           //write merged output to disk\n           long segmentStart \u003d finalOut.getPos();\n+          FSDataOutputStream finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut);\n           Writer\u003cK, V\u003e writer \u003d\n-              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n+              new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec,\n                                spilledRecordsCounter);\n           if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n             Merger.writeFile(kvIter, writer, reporter, job);\n           } else {\n             combineCollector.setWriter(writer);\n             combinerRunner.combine(kvIter, combineCollector);\n           }\n \n           //close\n           writer.close();\n \n           sortPhase.startNextPhase();\n           \n           // record offsets\n           rec.startOffset \u003d segmentStart;\n-          rec.rawLength \u003d writer.getRawLength();\n-          rec.partLength \u003d writer.getCompressedLength();\n+          rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n+          rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n           spillRec.putIndex(rec, parts);\n         }\n         spillRec.writeToFile(finalIndexFile, job);\n         finalOut.close();\n         for(int i \u003d 0; i \u003c numSpills; i++) {\n           rfs.delete(filename[i],true);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize \u003d 0;\n      long finalIndexFileSize \u003d 0;\n      final Path[] filename \u003d new Path[numSpills];\n      final TaskAttemptID mapId \u003d getTaskID();\n\n      for(int i \u003d 0; i \u003c numSpills; i++) {\n        filename[i] \u003d mapOutputFile.getSpillFile(i);\n        finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills \u003d\u003d 1) { //the spill is the final output\n        sameVolRename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() \u003d\u003d 0) {\n          sameVolRename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        sortPhase.complete();\n        return;\n      }\n\n      // read in paged indices\n      for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n        Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile \u003d\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile \u003d\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n\n      if (numSpills \u003d\u003d 0) {\n        //create dummy files\n        IndexRecord rec \u003d new IndexRecord();\n        SpillRecord sr \u003d new SpillRecord(partitions);\n        try {\n          for (int i \u003d 0; i \u003c partitions; i++) {\n            long segmentStart \u003d finalOut.getPos();\n            FSDataOutputStream finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut);\n            Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec, null);\n            writer.close();\n            rec.startOffset \u003d segmentStart;\n            rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n            rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n        }\n        sortPhase.complete();\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        \n        IndexRecord rec \u003d new IndexRecord();\n        final SpillRecord spillRec \u003d new SpillRecord(partitions);\n        for (int parts \u003d 0; parts \u003c partitions; parts++) {\n          //create the segments to be merged\n          List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n            new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n          for(int i \u003d 0; i \u003c numSpills; i++) {\n            IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n\n            Segment\u003cK,V\u003e s \u003d\n              new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                  \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase(),\n                         TaskType.MAP);\n\n          //write merged output to disk\n          long segmentStart \u003d finalOut.getPos();\n          FSDataOutputStream finalPartitionOut \u003d CryptoUtils.wrapIfNecessary(job, finalOut);\n          Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalPartitionOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset \u003d segmentStart;\n          rec.rawLength \u003d writer.getRawLength() + CryptoUtils.cryptoPadding(job);\n          rec.partLength \u003d writer.getCompressedLength() + CryptoUtils.cryptoPadding(job);\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        for(int i \u003d 0; i \u003c numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-434. LocalJobRunner limited to single reducer (Sandy Ryza and Aaron Kimball via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1510866 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/08/13 11:36 PM",
      "commitName": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
      "commitAuthor": "Sanford Ryza",
      "commitDateOld": "24/04/13 10:38 AM",
      "commitNameOld": "40e78c2ca23bcc56e7ceadd30421c05dbad17a1e",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 103.54,
      "commitsBetweenForRepo": 654,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,133 +1,133 @@\n     private void mergeParts() throws IOException, InterruptedException, \n                                      ClassNotFoundException {\n       // get the approximate size of the final output/index files\n       long finalOutFileSize \u003d 0;\n       long finalIndexFileSize \u003d 0;\n       final Path[] filename \u003d new Path[numSpills];\n       final TaskAttemptID mapId \u003d getTaskID();\n \n       for(int i \u003d 0; i \u003c numSpills; i++) {\n         filename[i] \u003d mapOutputFile.getSpillFile(i);\n         finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n       }\n       if (numSpills \u003d\u003d 1) { //the spill is the final output\n         sameVolRename(filename[0],\n             mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n         if (indexCacheList.size() \u003d\u003d 0) {\n           sameVolRename(mapOutputFile.getSpillIndexFile(0),\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n         } else {\n           indexCacheList.get(0).writeToFile(\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n         }\n         sortPhase.complete();\n         return;\n       }\n \n       // read in paged indices\n       for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n         Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n         indexCacheList.add(new SpillRecord(indexFileName, job));\n       }\n \n       //make correction in the length to include the sequence file header\n       //lengths for each partition\n       finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n       finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n       Path finalOutputFile \u003d\n           mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n       Path finalIndexFile \u003d\n           mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n \n       //The output stream for the final single output file\n       FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n \n       if (numSpills \u003d\u003d 0) {\n         //create dummy files\n         IndexRecord rec \u003d new IndexRecord();\n         SpillRecord sr \u003d new SpillRecord(partitions);\n         try {\n           for (int i \u003d 0; i \u003c partitions; i++) {\n             long segmentStart \u003d finalOut.getPos();\n             Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n             writer.close();\n             rec.startOffset \u003d segmentStart;\n             rec.rawLength \u003d writer.getRawLength();\n             rec.partLength \u003d writer.getCompressedLength();\n             sr.putIndex(rec, i);\n           }\n           sr.writeToFile(finalIndexFile, job);\n         } finally {\n           finalOut.close();\n         }\n         sortPhase.complete();\n         return;\n       }\n       {\n         sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n-        Merger.considerFinalMergeForProgress();\n         \n         IndexRecord rec \u003d new IndexRecord();\n         final SpillRecord spillRec \u003d new SpillRecord(partitions);\n         for (int parts \u003d 0; parts \u003c partitions; parts++) {\n           //create the segments to be merged\n           List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n             new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n           for(int i \u003d 0; i \u003c numSpills; i++) {\n             IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n \n             Segment\u003cK,V\u003e s \u003d\n               new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                                indexRecord.partLength, codec, true);\n             segmentList.add(i, s);\n \n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                   \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                   indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n             }\n           }\n \n           int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n           // sort the segments only if there are intermediate merges\n           boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n           //merge\n           @SuppressWarnings(\"unchecked\")\n           RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                          keyClass, valClass, codec,\n                          segmentList, mergeFactor,\n                          new Path(mapId.toString()),\n                          job.getOutputKeyComparator(), reporter, sortSegments,\n-                         null, spilledRecordsCounter, sortPhase.phase());\n+                         null, spilledRecordsCounter, sortPhase.phase(),\n+                         TaskType.MAP);\n \n           //write merged output to disk\n           long segmentStart \u003d finalOut.getPos();\n           Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n                                spilledRecordsCounter);\n           if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n             Merger.writeFile(kvIter, writer, reporter, job);\n           } else {\n             combineCollector.setWriter(writer);\n             combinerRunner.combine(kvIter, combineCollector);\n           }\n \n           //close\n           writer.close();\n \n           sortPhase.startNextPhase();\n           \n           // record offsets\n           rec.startOffset \u003d segmentStart;\n           rec.rawLength \u003d writer.getRawLength();\n           rec.partLength \u003d writer.getCompressedLength();\n           spillRec.putIndex(rec, parts);\n         }\n         spillRec.writeToFile(finalIndexFile, job);\n         finalOut.close();\n         for(int i \u003d 0; i \u003c numSpills; i++) {\n           rfs.delete(filename[i],true);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize \u003d 0;\n      long finalIndexFileSize \u003d 0;\n      final Path[] filename \u003d new Path[numSpills];\n      final TaskAttemptID mapId \u003d getTaskID();\n\n      for(int i \u003d 0; i \u003c numSpills; i++) {\n        filename[i] \u003d mapOutputFile.getSpillFile(i);\n        finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills \u003d\u003d 1) { //the spill is the final output\n        sameVolRename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() \u003d\u003d 0) {\n          sameVolRename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        sortPhase.complete();\n        return;\n      }\n\n      // read in paged indices\n      for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n        Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile \u003d\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile \u003d\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n\n      if (numSpills \u003d\u003d 0) {\n        //create dummy files\n        IndexRecord rec \u003d new IndexRecord();\n        SpillRecord sr \u003d new SpillRecord(partitions);\n        try {\n          for (int i \u003d 0; i \u003c partitions; i++) {\n            long segmentStart \u003d finalOut.getPos();\n            Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n            writer.close();\n            rec.startOffset \u003d segmentStart;\n            rec.rawLength \u003d writer.getRawLength();\n            rec.partLength \u003d writer.getCompressedLength();\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n        }\n        sortPhase.complete();\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        \n        IndexRecord rec \u003d new IndexRecord();\n        final SpillRecord spillRec \u003d new SpillRecord(partitions);\n        for (int parts \u003d 0; parts \u003c partitions; parts++) {\n          //create the segments to be merged\n          List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n            new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n          for(int i \u003d 0; i \u003c numSpills; i++) {\n            IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n\n            Segment\u003cK,V\u003e s \u003d\n              new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                  \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase(),\n                         TaskType.MAP);\n\n          //write merged output to disk\n          long segmentStart \u003d finalOut.getPos();\n          Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset \u003d segmentStart;\n          rec.rawLength \u003d writer.getRawLength();\n          rec.partLength \u003d writer.getCompressedLength();\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        for(int i \u003d 0; i \u003c numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "43553df4e36fa54e017481d3bcfd7251ecbd19bb": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3252. Fix map tasks to not rewrite data an extra time when map output fits in spill buffer. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1188424 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/10/11 3:33 PM",
      "commitName": "43553df4e36fa54e017481d3bcfd7251ecbd19bb",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "06/09/11 4:36 PM",
      "commitNameOld": "03c1015d6ea92c782e9ad78794cf1e9640f71040",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 47.96,
      "commitsBetweenForRepo": 353,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,133 +1,133 @@\n     private void mergeParts() throws IOException, InterruptedException, \n                                      ClassNotFoundException {\n       // get the approximate size of the final output/index files\n       long finalOutFileSize \u003d 0;\n       long finalIndexFileSize \u003d 0;\n       final Path[] filename \u003d new Path[numSpills];\n       final TaskAttemptID mapId \u003d getTaskID();\n \n       for(int i \u003d 0; i \u003c numSpills; i++) {\n         filename[i] \u003d mapOutputFile.getSpillFile(i);\n         finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n       }\n       if (numSpills \u003d\u003d 1) { //the spill is the final output\n-        rfs.rename(filename[0],\n+        sameVolRename(filename[0],\n             mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n         if (indexCacheList.size() \u003d\u003d 0) {\n-          rfs.rename(mapOutputFile.getSpillIndexFile(0),\n+          sameVolRename(mapOutputFile.getSpillIndexFile(0),\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n         } else {\n           indexCacheList.get(0).writeToFile(\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n         }\n         sortPhase.complete();\n         return;\n       }\n \n       // read in paged indices\n       for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n         Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n         indexCacheList.add(new SpillRecord(indexFileName, job));\n       }\n \n       //make correction in the length to include the sequence file header\n       //lengths for each partition\n       finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n       finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n       Path finalOutputFile \u003d\n           mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n       Path finalIndexFile \u003d\n           mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n \n       //The output stream for the final single output file\n       FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n \n       if (numSpills \u003d\u003d 0) {\n         //create dummy files\n         IndexRecord rec \u003d new IndexRecord();\n         SpillRecord sr \u003d new SpillRecord(partitions);\n         try {\n           for (int i \u003d 0; i \u003c partitions; i++) {\n             long segmentStart \u003d finalOut.getPos();\n             Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n             writer.close();\n             rec.startOffset \u003d segmentStart;\n             rec.rawLength \u003d writer.getRawLength();\n             rec.partLength \u003d writer.getCompressedLength();\n             sr.putIndex(rec, i);\n           }\n           sr.writeToFile(finalIndexFile, job);\n         } finally {\n           finalOut.close();\n         }\n         sortPhase.complete();\n         return;\n       }\n       {\n         sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n         Merger.considerFinalMergeForProgress();\n         \n         IndexRecord rec \u003d new IndexRecord();\n         final SpillRecord spillRec \u003d new SpillRecord(partitions);\n         for (int parts \u003d 0; parts \u003c partitions; parts++) {\n           //create the segments to be merged\n           List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n             new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n           for(int i \u003d 0; i \u003c numSpills; i++) {\n             IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n \n             Segment\u003cK,V\u003e s \u003d\n               new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                                indexRecord.partLength, codec, true);\n             segmentList.add(i, s);\n \n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                   \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                   indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n             }\n           }\n \n           int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n           // sort the segments only if there are intermediate merges\n           boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n           //merge\n           @SuppressWarnings(\"unchecked\")\n           RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                          keyClass, valClass, codec,\n                          segmentList, mergeFactor,\n                          new Path(mapId.toString()),\n                          job.getOutputKeyComparator(), reporter, sortSegments,\n                          null, spilledRecordsCounter, sortPhase.phase());\n \n           //write merged output to disk\n           long segmentStart \u003d finalOut.getPos();\n           Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n                                spilledRecordsCounter);\n           if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n             Merger.writeFile(kvIter, writer, reporter, job);\n           } else {\n             combineCollector.setWriter(writer);\n             combinerRunner.combine(kvIter, combineCollector);\n           }\n \n           //close\n           writer.close();\n \n           sortPhase.startNextPhase();\n           \n           // record offsets\n           rec.startOffset \u003d segmentStart;\n           rec.rawLength \u003d writer.getRawLength();\n           rec.partLength \u003d writer.getCompressedLength();\n           spillRec.putIndex(rec, parts);\n         }\n         spillRec.writeToFile(finalIndexFile, job);\n         finalOut.close();\n         for(int i \u003d 0; i \u003c numSpills; i++) {\n           rfs.delete(filename[i],true);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize \u003d 0;\n      long finalIndexFileSize \u003d 0;\n      final Path[] filename \u003d new Path[numSpills];\n      final TaskAttemptID mapId \u003d getTaskID();\n\n      for(int i \u003d 0; i \u003c numSpills; i++) {\n        filename[i] \u003d mapOutputFile.getSpillFile(i);\n        finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills \u003d\u003d 1) { //the spill is the final output\n        sameVolRename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() \u003d\u003d 0) {\n          sameVolRename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        sortPhase.complete();\n        return;\n      }\n\n      // read in paged indices\n      for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n        Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile \u003d\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile \u003d\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n\n      if (numSpills \u003d\u003d 0) {\n        //create dummy files\n        IndexRecord rec \u003d new IndexRecord();\n        SpillRecord sr \u003d new SpillRecord(partitions);\n        try {\n          for (int i \u003d 0; i \u003c partitions; i++) {\n            long segmentStart \u003d finalOut.getPos();\n            Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n            writer.close();\n            rec.startOffset \u003d segmentStart;\n            rec.rawLength \u003d writer.getRawLength();\n            rec.partLength \u003d writer.getCompressedLength();\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n        }\n        sortPhase.complete();\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        Merger.considerFinalMergeForProgress();\n        \n        IndexRecord rec \u003d new IndexRecord();\n        final SpillRecord spillRec \u003d new SpillRecord(partitions);\n        for (int parts \u003d 0; parts \u003c partitions; parts++) {\n          //create the segments to be merged\n          List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n            new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n          for(int i \u003d 0; i \u003c numSpills; i++) {\n            IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n\n            Segment\u003cK,V\u003e s \u003d\n              new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                  \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase());\n\n          //write merged output to disk\n          long segmentStart \u003d finalOut.getPos();\n          Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset \u003d segmentStart;\n          rec.rawLength \u003d writer.getRawLength();\n          rec.partLength \u003d writer.getCompressedLength();\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        for(int i \u003d 0; i \u003c numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "03c1015d6ea92c782e9ad78794cf1e9640f71040": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2800. Set final progress for tasks to ensure all task information is correctly logged to JobHistory. Contributed by Siddharth Seth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165930 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/09/11 4:36 PM",
      "commitName": "03c1015d6ea92c782e9ad78794cf1e9640f71040",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 12.97,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,131 +1,133 @@\n     private void mergeParts() throws IOException, InterruptedException, \n                                      ClassNotFoundException {\n       // get the approximate size of the final output/index files\n       long finalOutFileSize \u003d 0;\n       long finalIndexFileSize \u003d 0;\n       final Path[] filename \u003d new Path[numSpills];\n       final TaskAttemptID mapId \u003d getTaskID();\n \n       for(int i \u003d 0; i \u003c numSpills; i++) {\n         filename[i] \u003d mapOutputFile.getSpillFile(i);\n         finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n       }\n       if (numSpills \u003d\u003d 1) { //the spill is the final output\n         rfs.rename(filename[0],\n             mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n         if (indexCacheList.size() \u003d\u003d 0) {\n           rfs.rename(mapOutputFile.getSpillIndexFile(0),\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n         } else {\n           indexCacheList.get(0).writeToFile(\n             mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n         }\n+        sortPhase.complete();\n         return;\n       }\n \n       // read in paged indices\n       for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n         Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n         indexCacheList.add(new SpillRecord(indexFileName, job));\n       }\n \n       //make correction in the length to include the sequence file header\n       //lengths for each partition\n       finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n       finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n       Path finalOutputFile \u003d\n           mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n       Path finalIndexFile \u003d\n           mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n \n       //The output stream for the final single output file\n       FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n \n       if (numSpills \u003d\u003d 0) {\n         //create dummy files\n         IndexRecord rec \u003d new IndexRecord();\n         SpillRecord sr \u003d new SpillRecord(partitions);\n         try {\n           for (int i \u003d 0; i \u003c partitions; i++) {\n             long segmentStart \u003d finalOut.getPos();\n             Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n             writer.close();\n             rec.startOffset \u003d segmentStart;\n             rec.rawLength \u003d writer.getRawLength();\n             rec.partLength \u003d writer.getCompressedLength();\n             sr.putIndex(rec, i);\n           }\n           sr.writeToFile(finalIndexFile, job);\n         } finally {\n           finalOut.close();\n         }\n+        sortPhase.complete();\n         return;\n       }\n       {\n         sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n         Merger.considerFinalMergeForProgress();\n         \n         IndexRecord rec \u003d new IndexRecord();\n         final SpillRecord spillRec \u003d new SpillRecord(partitions);\n         for (int parts \u003d 0; parts \u003c partitions; parts++) {\n           //create the segments to be merged\n           List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n             new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n           for(int i \u003d 0; i \u003c numSpills; i++) {\n             IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n \n             Segment\u003cK,V\u003e s \u003d\n               new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                                indexRecord.partLength, codec, true);\n             segmentList.add(i, s);\n \n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                   \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                   indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n             }\n           }\n \n           int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n           // sort the segments only if there are intermediate merges\n           boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n           //merge\n           @SuppressWarnings(\"unchecked\")\n           RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                          keyClass, valClass, codec,\n                          segmentList, mergeFactor,\n                          new Path(mapId.toString()),\n                          job.getOutputKeyComparator(), reporter, sortSegments,\n                          null, spilledRecordsCounter, sortPhase.phase());\n \n           //write merged output to disk\n           long segmentStart \u003d finalOut.getPos();\n           Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n                                spilledRecordsCounter);\n           if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n             Merger.writeFile(kvIter, writer, reporter, job);\n           } else {\n             combineCollector.setWriter(writer);\n             combinerRunner.combine(kvIter, combineCollector);\n           }\n \n           //close\n           writer.close();\n \n           sortPhase.startNextPhase();\n           \n           // record offsets\n           rec.startOffset \u003d segmentStart;\n           rec.rawLength \u003d writer.getRawLength();\n           rec.partLength \u003d writer.getCompressedLength();\n           spillRec.putIndex(rec, parts);\n         }\n         spillRec.writeToFile(finalIndexFile, job);\n         finalOut.close();\n         for(int i \u003d 0; i \u003c numSpills; i++) {\n           rfs.delete(filename[i],true);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize \u003d 0;\n      long finalIndexFileSize \u003d 0;\n      final Path[] filename \u003d new Path[numSpills];\n      final TaskAttemptID mapId \u003d getTaskID();\n\n      for(int i \u003d 0; i \u003c numSpills; i++) {\n        filename[i] \u003d mapOutputFile.getSpillFile(i);\n        finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills \u003d\u003d 1) { //the spill is the final output\n        rfs.rename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() \u003d\u003d 0) {\n          rfs.rename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        sortPhase.complete();\n        return;\n      }\n\n      // read in paged indices\n      for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n        Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile \u003d\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile \u003d\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n\n      if (numSpills \u003d\u003d 0) {\n        //create dummy files\n        IndexRecord rec \u003d new IndexRecord();\n        SpillRecord sr \u003d new SpillRecord(partitions);\n        try {\n          for (int i \u003d 0; i \u003c partitions; i++) {\n            long segmentStart \u003d finalOut.getPos();\n            Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n            writer.close();\n            rec.startOffset \u003d segmentStart;\n            rec.rawLength \u003d writer.getRawLength();\n            rec.partLength \u003d writer.getCompressedLength();\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n        }\n        sortPhase.complete();\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        Merger.considerFinalMergeForProgress();\n        \n        IndexRecord rec \u003d new IndexRecord();\n        final SpillRecord spillRec \u003d new SpillRecord(partitions);\n        for (int parts \u003d 0; parts \u003c partitions; parts++) {\n          //create the segments to be merged\n          List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n            new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n          for(int i \u003d 0; i \u003c numSpills; i++) {\n            IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n\n            Segment\u003cK,V\u003e s \u003d\n              new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                  \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase());\n\n          //write merged output to disk\n          long segmentStart \u003d finalOut.getPos();\n          Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset \u003d segmentStart;\n          rec.rawLength \u003d writer.getRawLength();\n          rec.partLength \u003d writer.getCompressedLength();\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        for(int i \u003d 0; i \u003c numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize \u003d 0;\n      long finalIndexFileSize \u003d 0;\n      final Path[] filename \u003d new Path[numSpills];\n      final TaskAttemptID mapId \u003d getTaskID();\n\n      for(int i \u003d 0; i \u003c numSpills; i++) {\n        filename[i] \u003d mapOutputFile.getSpillFile(i);\n        finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills \u003d\u003d 1) { //the spill is the final output\n        rfs.rename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() \u003d\u003d 0) {\n          rfs.rename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        return;\n      }\n\n      // read in paged indices\n      for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n        Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile \u003d\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile \u003d\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n\n      if (numSpills \u003d\u003d 0) {\n        //create dummy files\n        IndexRecord rec \u003d new IndexRecord();\n        SpillRecord sr \u003d new SpillRecord(partitions);\n        try {\n          for (int i \u003d 0; i \u003c partitions; i++) {\n            long segmentStart \u003d finalOut.getPos();\n            Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n            writer.close();\n            rec.startOffset \u003d segmentStart;\n            rec.rawLength \u003d writer.getRawLength();\n            rec.partLength \u003d writer.getCompressedLength();\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n        }\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        Merger.considerFinalMergeForProgress();\n        \n        IndexRecord rec \u003d new IndexRecord();\n        final SpillRecord spillRec \u003d new SpillRecord(partitions);\n        for (int parts \u003d 0; parts \u003c partitions; parts++) {\n          //create the segments to be merged\n          List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n            new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n          for(int i \u003d 0; i \u003c numSpills; i++) {\n            IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n\n            Segment\u003cK,V\u003e s \u003d\n              new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                  \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase());\n\n          //write merged output to disk\n          long segmentStart \u003d finalOut.getPos();\n          Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset \u003d segmentStart;\n          rec.rawLength \u003d writer.getRawLength();\n          rec.partLength \u003d writer.getCompressedLength();\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        for(int i \u003d 0; i \u003c numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize \u003d 0;\n      long finalIndexFileSize \u003d 0;\n      final Path[] filename \u003d new Path[numSpills];\n      final TaskAttemptID mapId \u003d getTaskID();\n\n      for(int i \u003d 0; i \u003c numSpills; i++) {\n        filename[i] \u003d mapOutputFile.getSpillFile(i);\n        finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills \u003d\u003d 1) { //the spill is the final output\n        rfs.rename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() \u003d\u003d 0) {\n          rfs.rename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        return;\n      }\n\n      // read in paged indices\n      for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n        Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile \u003d\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile \u003d\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n\n      if (numSpills \u003d\u003d 0) {\n        //create dummy files\n        IndexRecord rec \u003d new IndexRecord();\n        SpillRecord sr \u003d new SpillRecord(partitions);\n        try {\n          for (int i \u003d 0; i \u003c partitions; i++) {\n            long segmentStart \u003d finalOut.getPos();\n            Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n            writer.close();\n            rec.startOffset \u003d segmentStart;\n            rec.rawLength \u003d writer.getRawLength();\n            rec.partLength \u003d writer.getCompressedLength();\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n        }\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        Merger.considerFinalMergeForProgress();\n        \n        IndexRecord rec \u003d new IndexRecord();\n        final SpillRecord spillRec \u003d new SpillRecord(partitions);\n        for (int parts \u003d 0; parts \u003c partitions; parts++) {\n          //create the segments to be merged\n          List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n            new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n          for(int i \u003d 0; i \u003c numSpills; i++) {\n            IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n\n            Segment\u003cK,V\u003e s \u003d\n              new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                  \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase());\n\n          //write merged output to disk\n          long segmentStart \u003d finalOut.getPos();\n          Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset \u003d segmentStart;\n          rec.rawLength \u003d writer.getRawLength();\n          rec.partLength \u003d writer.getCompressedLength();\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        for(int i \u003d 0; i \u003c numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
        "oldMethodName": "mergeParts",
        "newMethodName": "mergeParts"
      }
    },
    "ded6f225a55517deedc2bd502f2b68f1ca2ddee8": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2837. Ported bug fixes from y-merge to prepare for MAPREDUCE-279 merge. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1157249 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/08/11 2:00 PM",
      "commitName": "ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "01/08/11 3:53 PM",
      "commitNameOld": "9bac807cedbcff34e1a144fb475eff267e5ed86d",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 10.92,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,131 +1,131 @@\n     private void mergeParts() throws IOException, InterruptedException, \n                                      ClassNotFoundException {\n       // get the approximate size of the final output/index files\n       long finalOutFileSize \u003d 0;\n       long finalIndexFileSize \u003d 0;\n       final Path[] filename \u003d new Path[numSpills];\n       final TaskAttemptID mapId \u003d getTaskID();\n \n       for(int i \u003d 0; i \u003c numSpills; i++) {\n         filename[i] \u003d mapOutputFile.getSpillFile(i);\n         finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n       }\n       if (numSpills \u003d\u003d 1) { //the spill is the final output\n         rfs.rename(filename[0],\n-            new Path(filename[0].getParent(), \"file.out\"));\n+            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n         if (indexCacheList.size() \u003d\u003d 0) {\n           rfs.rename(mapOutputFile.getSpillIndexFile(0),\n-              new Path(filename[0].getParent(),\"file.out.index\"));\n+            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n         } else {\n           indexCacheList.get(0).writeToFile(\n-                new Path(filename[0].getParent(),\"file.out.index\"), job);\n+            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n         }\n         return;\n       }\n \n       // read in paged indices\n       for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n         Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n         indexCacheList.add(new SpillRecord(indexFileName, job));\n       }\n \n       //make correction in the length to include the sequence file header\n       //lengths for each partition\n       finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n       finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n       Path finalOutputFile \u003d\n           mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n       Path finalIndexFile \u003d\n           mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n \n       //The output stream for the final single output file\n       FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n \n       if (numSpills \u003d\u003d 0) {\n         //create dummy files\n         IndexRecord rec \u003d new IndexRecord();\n         SpillRecord sr \u003d new SpillRecord(partitions);\n         try {\n           for (int i \u003d 0; i \u003c partitions; i++) {\n             long segmentStart \u003d finalOut.getPos();\n             Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n             writer.close();\n             rec.startOffset \u003d segmentStart;\n             rec.rawLength \u003d writer.getRawLength();\n             rec.partLength \u003d writer.getCompressedLength();\n             sr.putIndex(rec, i);\n           }\n           sr.writeToFile(finalIndexFile, job);\n         } finally {\n           finalOut.close();\n         }\n         return;\n       }\n       {\n         sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n         Merger.considerFinalMergeForProgress();\n         \n         IndexRecord rec \u003d new IndexRecord();\n         final SpillRecord spillRec \u003d new SpillRecord(partitions);\n         for (int parts \u003d 0; parts \u003c partitions; parts++) {\n           //create the segments to be merged\n           List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n             new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n           for(int i \u003d 0; i \u003c numSpills; i++) {\n             IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n \n             Segment\u003cK,V\u003e s \u003d\n               new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                                indexRecord.partLength, codec, true);\n             segmentList.add(i, s);\n \n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                   \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                   indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n             }\n           }\n \n           int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n           // sort the segments only if there are intermediate merges\n           boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n           //merge\n           @SuppressWarnings(\"unchecked\")\n           RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                          keyClass, valClass, codec,\n                          segmentList, mergeFactor,\n                          new Path(mapId.toString()),\n                          job.getOutputKeyComparator(), reporter, sortSegments,\n                          null, spilledRecordsCounter, sortPhase.phase());\n \n           //write merged output to disk\n           long segmentStart \u003d finalOut.getPos();\n           Writer\u003cK, V\u003e writer \u003d\n               new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n                                spilledRecordsCounter);\n           if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n             Merger.writeFile(kvIter, writer, reporter, job);\n           } else {\n             combineCollector.setWriter(writer);\n             combinerRunner.combine(kvIter, combineCollector);\n           }\n \n           //close\n           writer.close();\n \n           sortPhase.startNextPhase();\n           \n           // record offsets\n           rec.startOffset \u003d segmentStart;\n           rec.rawLength \u003d writer.getRawLength();\n           rec.partLength \u003d writer.getCompressedLength();\n           spillRec.putIndex(rec, parts);\n         }\n         spillRec.writeToFile(finalIndexFile, job);\n         finalOut.close();\n         for(int i \u003d 0; i \u003c numSpills; i++) {\n           rfs.delete(filename[i],true);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize \u003d 0;\n      long finalIndexFileSize \u003d 0;\n      final Path[] filename \u003d new Path[numSpills];\n      final TaskAttemptID mapId \u003d getTaskID();\n\n      for(int i \u003d 0; i \u003c numSpills; i++) {\n        filename[i] \u003d mapOutputFile.getSpillFile(i);\n        finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills \u003d\u003d 1) { //the spill is the final output\n        rfs.rename(filename[0],\n            mapOutputFile.getOutputFileForWriteInVolume(filename[0]));\n        if (indexCacheList.size() \u003d\u003d 0) {\n          rfs.rename(mapOutputFile.getSpillIndexFile(0),\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));\n        } else {\n          indexCacheList.get(0).writeToFile(\n            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);\n        }\n        return;\n      }\n\n      // read in paged indices\n      for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n        Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile \u003d\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile \u003d\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n\n      if (numSpills \u003d\u003d 0) {\n        //create dummy files\n        IndexRecord rec \u003d new IndexRecord();\n        SpillRecord sr \u003d new SpillRecord(partitions);\n        try {\n          for (int i \u003d 0; i \u003c partitions; i++) {\n            long segmentStart \u003d finalOut.getPos();\n            Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n            writer.close();\n            rec.startOffset \u003d segmentStart;\n            rec.rawLength \u003d writer.getRawLength();\n            rec.partLength \u003d writer.getCompressedLength();\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n        }\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        Merger.considerFinalMergeForProgress();\n        \n        IndexRecord rec \u003d new IndexRecord();\n        final SpillRecord spillRec \u003d new SpillRecord(partitions);\n        for (int parts \u003d 0; parts \u003c partitions; parts++) {\n          //create the segments to be merged\n          List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n            new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n          for(int i \u003d 0; i \u003c numSpills; i++) {\n            IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n\n            Segment\u003cK,V\u003e s \u003d\n              new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                  \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase());\n\n          //write merged output to disk\n          long segmentStart \u003d finalOut.getPos();\n          Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset \u003d segmentStart;\n          rec.rawLength \u003d writer.getRawLength();\n          rec.partLength \u003d writer.getCompressedLength();\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        for(int i \u003d 0; i \u003c numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,131 @@\n+    private void mergeParts() throws IOException, InterruptedException, \n+                                     ClassNotFoundException {\n+      // get the approximate size of the final output/index files\n+      long finalOutFileSize \u003d 0;\n+      long finalIndexFileSize \u003d 0;\n+      final Path[] filename \u003d new Path[numSpills];\n+      final TaskAttemptID mapId \u003d getTaskID();\n+\n+      for(int i \u003d 0; i \u003c numSpills; i++) {\n+        filename[i] \u003d mapOutputFile.getSpillFile(i);\n+        finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n+      }\n+      if (numSpills \u003d\u003d 1) { //the spill is the final output\n+        rfs.rename(filename[0],\n+            new Path(filename[0].getParent(), \"file.out\"));\n+        if (indexCacheList.size() \u003d\u003d 0) {\n+          rfs.rename(mapOutputFile.getSpillIndexFile(0),\n+              new Path(filename[0].getParent(),\"file.out.index\"));\n+        } else {\n+          indexCacheList.get(0).writeToFile(\n+                new Path(filename[0].getParent(),\"file.out.index\"), job);\n+        }\n+        return;\n+      }\n+\n+      // read in paged indices\n+      for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n+        Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n+        indexCacheList.add(new SpillRecord(indexFileName, job));\n+      }\n+\n+      //make correction in the length to include the sequence file header\n+      //lengths for each partition\n+      finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n+      finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n+      Path finalOutputFile \u003d\n+          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n+      Path finalIndexFile \u003d\n+          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n+\n+      //The output stream for the final single output file\n+      FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n+\n+      if (numSpills \u003d\u003d 0) {\n+        //create dummy files\n+        IndexRecord rec \u003d new IndexRecord();\n+        SpillRecord sr \u003d new SpillRecord(partitions);\n+        try {\n+          for (int i \u003d 0; i \u003c partitions; i++) {\n+            long segmentStart \u003d finalOut.getPos();\n+            Writer\u003cK, V\u003e writer \u003d\n+              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n+            writer.close();\n+            rec.startOffset \u003d segmentStart;\n+            rec.rawLength \u003d writer.getRawLength();\n+            rec.partLength \u003d writer.getCompressedLength();\n+            sr.putIndex(rec, i);\n+          }\n+          sr.writeToFile(finalIndexFile, job);\n+        } finally {\n+          finalOut.close();\n+        }\n+        return;\n+      }\n+      {\n+        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n+        Merger.considerFinalMergeForProgress();\n+        \n+        IndexRecord rec \u003d new IndexRecord();\n+        final SpillRecord spillRec \u003d new SpillRecord(partitions);\n+        for (int parts \u003d 0; parts \u003c partitions; parts++) {\n+          //create the segments to be merged\n+          List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n+            new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n+          for(int i \u003d 0; i \u003c numSpills; i++) {\n+            IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n+\n+            Segment\u003cK,V\u003e s \u003d\n+              new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n+                               indexRecord.partLength, codec, true);\n+            segmentList.add(i, s);\n+\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n+                  \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n+                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n+            }\n+          }\n+\n+          int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n+          // sort the segments only if there are intermediate merges\n+          boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n+          //merge\n+          @SuppressWarnings(\"unchecked\")\n+          RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n+                         keyClass, valClass, codec,\n+                         segmentList, mergeFactor,\n+                         new Path(mapId.toString()),\n+                         job.getOutputKeyComparator(), reporter, sortSegments,\n+                         null, spilledRecordsCounter, sortPhase.phase());\n+\n+          //write merged output to disk\n+          long segmentStart \u003d finalOut.getPos();\n+          Writer\u003cK, V\u003e writer \u003d\n+              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n+                               spilledRecordsCounter);\n+          if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n+            Merger.writeFile(kvIter, writer, reporter, job);\n+          } else {\n+            combineCollector.setWriter(writer);\n+            combinerRunner.combine(kvIter, combineCollector);\n+          }\n+\n+          //close\n+          writer.close();\n+\n+          sortPhase.startNextPhase();\n+          \n+          // record offsets\n+          rec.startOffset \u003d segmentStart;\n+          rec.rawLength \u003d writer.getRawLength();\n+          rec.partLength \u003d writer.getCompressedLength();\n+          spillRec.putIndex(rec, parts);\n+        }\n+        spillRec.writeToFile(finalIndexFile, job);\n+        finalOut.close();\n+        for(int i \u003d 0; i \u003c numSpills; i++) {\n+          rfs.delete(filename[i],true);\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void mergeParts() throws IOException, InterruptedException, \n                                     ClassNotFoundException {\n      // get the approximate size of the final output/index files\n      long finalOutFileSize \u003d 0;\n      long finalIndexFileSize \u003d 0;\n      final Path[] filename \u003d new Path[numSpills];\n      final TaskAttemptID mapId \u003d getTaskID();\n\n      for(int i \u003d 0; i \u003c numSpills; i++) {\n        filename[i] \u003d mapOutputFile.getSpillFile(i);\n        finalOutFileSize +\u003d rfs.getFileStatus(filename[i]).getLen();\n      }\n      if (numSpills \u003d\u003d 1) { //the spill is the final output\n        rfs.rename(filename[0],\n            new Path(filename[0].getParent(), \"file.out\"));\n        if (indexCacheList.size() \u003d\u003d 0) {\n          rfs.rename(mapOutputFile.getSpillIndexFile(0),\n              new Path(filename[0].getParent(),\"file.out.index\"));\n        } else {\n          indexCacheList.get(0).writeToFile(\n                new Path(filename[0].getParent(),\"file.out.index\"), job);\n        }\n        return;\n      }\n\n      // read in paged indices\n      for (int i \u003d indexCacheList.size(); i \u003c numSpills; ++i) {\n        Path indexFileName \u003d mapOutputFile.getSpillIndexFile(i);\n        indexCacheList.add(new SpillRecord(indexFileName, job));\n      }\n\n      //make correction in the length to include the sequence file header\n      //lengths for each partition\n      finalOutFileSize +\u003d partitions * APPROX_HEADER_LENGTH;\n      finalIndexFileSize \u003d partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;\n      Path finalOutputFile \u003d\n          mapOutputFile.getOutputFileForWrite(finalOutFileSize);\n      Path finalIndexFile \u003d\n          mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);\n\n      //The output stream for the final single output file\n      FSDataOutputStream finalOut \u003d rfs.create(finalOutputFile, true, 4096);\n\n      if (numSpills \u003d\u003d 0) {\n        //create dummy files\n        IndexRecord rec \u003d new IndexRecord();\n        SpillRecord sr \u003d new SpillRecord(partitions);\n        try {\n          for (int i \u003d 0; i \u003c partitions; i++) {\n            long segmentStart \u003d finalOut.getPos();\n            Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec, null);\n            writer.close();\n            rec.startOffset \u003d segmentStart;\n            rec.rawLength \u003d writer.getRawLength();\n            rec.partLength \u003d writer.getCompressedLength();\n            sr.putIndex(rec, i);\n          }\n          sr.writeToFile(finalIndexFile, job);\n        } finally {\n          finalOut.close();\n        }\n        return;\n      }\n      {\n        sortPhase.addPhases(partitions); // Divide sort phase into sub-phases\n        Merger.considerFinalMergeForProgress();\n        \n        IndexRecord rec \u003d new IndexRecord();\n        final SpillRecord spillRec \u003d new SpillRecord(partitions);\n        for (int parts \u003d 0; parts \u003c partitions; parts++) {\n          //create the segments to be merged\n          List\u003cSegment\u003cK,V\u003e\u003e segmentList \u003d\n            new ArrayList\u003cSegment\u003cK, V\u003e\u003e(numSpills);\n          for(int i \u003d 0; i \u003c numSpills; i++) {\n            IndexRecord indexRecord \u003d indexCacheList.get(i).getIndex(parts);\n\n            Segment\u003cK,V\u003e s \u003d\n              new Segment\u003cK,V\u003e(job, rfs, filename[i], indexRecord.startOffset,\n                               indexRecord.partLength, codec, true);\n            segmentList.add(i, s);\n\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"MapId\u003d\" + mapId + \" Reducer\u003d\" + parts +\n                  \"Spill \u003d\" + i + \"(\" + indexRecord.startOffset + \",\" +\n                  indexRecord.rawLength + \", \" + indexRecord.partLength + \")\");\n            }\n          }\n\n          int mergeFactor \u003d job.getInt(JobContext.IO_SORT_FACTOR, 100);\n          // sort the segments only if there are intermediate merges\n          boolean sortSegments \u003d segmentList.size() \u003e mergeFactor;\n          //merge\n          @SuppressWarnings(\"unchecked\")\n          RawKeyValueIterator kvIter \u003d Merger.merge(job, rfs,\n                         keyClass, valClass, codec,\n                         segmentList, mergeFactor,\n                         new Path(mapId.toString()),\n                         job.getOutputKeyComparator(), reporter, sortSegments,\n                         null, spilledRecordsCounter, sortPhase.phase());\n\n          //write merged output to disk\n          long segmentStart \u003d finalOut.getPos();\n          Writer\u003cK, V\u003e writer \u003d\n              new Writer\u003cK, V\u003e(job, finalOut, keyClass, valClass, codec,\n                               spilledRecordsCounter);\n          if (combinerRunner \u003d\u003d null || numSpills \u003c minSpillsForCombine) {\n            Merger.writeFile(kvIter, writer, reporter, job);\n          } else {\n            combineCollector.setWriter(writer);\n            combinerRunner.combine(kvIter, combineCollector);\n          }\n\n          //close\n          writer.close();\n\n          sortPhase.startNextPhase();\n          \n          // record offsets\n          rec.startOffset \u003d segmentStart;\n          rec.rawLength \u003d writer.getRawLength();\n          rec.partLength \u003d writer.getCompressedLength();\n          spillRec.putIndex(rec, parts);\n        }\n        spillRec.writeToFile(finalIndexFile, job);\n        finalOut.close();\n        for(int i \u003d 0; i \u003c numSpills; i++) {\n          rfs.delete(filename[i],true);\n        }\n      }\n    }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/MapTask.java"
    }
  }
}