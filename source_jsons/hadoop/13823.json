{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "scheduleReconstruction",
  "functionId": "scheduleReconstruction___block-BlockInfo__priority-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 2084,
  "functionEndLine": 2167,
  "numCommitsSeen": 792,
  "timeTaken": 16807,
  "changeHistory": [
    "2ffec347eb4303ad78643431cd2e517d54bc3282",
    "02009c3bb762393540cdf92cfd9c840807272903",
    "c99a12167ff9566012ef32104a3964887d62c899",
    "6a9dc5f44b0c7945e3e9a56248cd4ff80d5c8f0f",
    "a2f0cbd92f7e90909cf817c261a5fae13a9695b4",
    "900221f95ea9fe1936b4d5f277e6047ee8734eca",
    "a7f085d6bf499edf23e650a4f7211c53a442da0e",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "743a99f2dbc9a27e19f92ff3551937d90dba2e89",
    "e54cc2931262bf49682a8323da9811976218c03b",
    "972782d9568e0849484c027f27c1638ba50ec56e",
    "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
    "70d6f201260086a3f12beaa317fede2a99639fef",
    "5ba2b98d0fe29603e136fc43a14f853e820cf7e2"
  ],
  "changeHistoryShort": {
    "2ffec347eb4303ad78643431cd2e517d54bc3282": "Ybodychange",
    "02009c3bb762393540cdf92cfd9c840807272903": "Ybodychange",
    "c99a12167ff9566012ef32104a3964887d62c899": "Ybodychange",
    "6a9dc5f44b0c7945e3e9a56248cd4ff80d5c8f0f": "Ymultichange(Ymodifierchange,Ybodychange)",
    "a2f0cbd92f7e90909cf817c261a5fae13a9695b4": "Ymultichange(Ymodifierchange,Ybodychange)",
    "900221f95ea9fe1936b4d5f277e6047ee8734eca": "Ymultichange(Ymodifierchange,Ybodychange)",
    "a7f085d6bf499edf23e650a4f7211c53a442da0e": "Ybodychange",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": "Ybodychange",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": "Ybodychange",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ybodychange",
    "743a99f2dbc9a27e19f92ff3551937d90dba2e89": "Ybodychange",
    "e54cc2931262bf49682a8323da9811976218c03b": "Ybodychange",
    "972782d9568e0849484c027f27c1638ba50ec56e": "Ybodychange",
    "a0fb2eff9b71e2e2c0e53262773b34bed82585d4": "Ymultichange(Yrename,Yreturntypechange,Ybodychange)",
    "70d6f201260086a3f12beaa317fede2a99639fef": "Ybodychange",
    "5ba2b98d0fe29603e136fc43a14f853e820cf7e2": "Ybodychange"
  },
  "changeHistoryDetails": {
    "2ffec347eb4303ad78643431cd2e517d54bc3282": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14946. Erasure Coding: Block recovery failed during decommissioning. Contributed by Fei Hui.\n",
      "commitDate": "04/11/19 12:07 PM",
      "commitName": "2ffec347eb4303ad78643431cd2e517d54bc3282",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "01/11/19 9:45 AM",
      "commitNameOld": "02009c3bb762393540cdf92cfd9c840807272903",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 3.14,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,84 @@\n   BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     List\u003cByte\u003e liveBusyBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, liveBusyBlockIndices, priority);\n     short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n         numReplicas);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n       NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n       NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n       additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       // Violates placement policy. Needed on a new rack or domain etc.\n       BlockPlacementStatus placementStatus \u003d getBlockPlacementStatus(block);\n       additionalReplRequired \u003d placementStatus.getAdditionalReplicasRequired();\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n         NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() -\n           numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired -\n             numReplicas.decommissioning() -\n             numReplicas.liveEnteringMaintenanceReplicas();\n       }\n-      byte[] indices \u003d new byte[liveBlockIndices.size()];\n-      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n-        indices[i] \u003d liveBlockIndices.get(i);\n-      }\n+      final DatanodeDescriptor[] newSrcNodes \u003d\n+          new DatanodeDescriptor[srcNodes.length];\n+      byte[] newIndices \u003d new byte[liveBlockIndices.size()];\n+      adjustSrcNodesAndIndices((BlockInfoStriped)block,\n+          srcNodes, liveBlockIndices, newSrcNodes, newIndices);\n       byte[] busyIndices \u003d new byte[liveBusyBlockIndices.size()];\n       for (int i \u003d 0; i \u003c liveBusyBlockIndices.size(); i++) {\n         busyIndices[i] \u003d liveBusyBlockIndices.get(i);\n       }\n-      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n+      return new ErasureCodingWork(getBlockPoolId(), block, bc, newSrcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n-          priority, indices, busyIndices);\n+          priority, newIndices, busyIndices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    List\u003cByte\u003e liveBusyBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, liveBusyBlockIndices, priority);\n    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n        numReplicas);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      // Violates placement policy. Needed on a new rack or domain etc.\n      BlockPlacementStatus placementStatus \u003d getBlockPlacementStatus(block);\n      additionalReplRequired \u003d placementStatus.getAdditionalReplicasRequired();\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() -\n          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired -\n            numReplicas.decommissioning() -\n            numReplicas.liveEnteringMaintenanceReplicas();\n      }\n      final DatanodeDescriptor[] newSrcNodes \u003d\n          new DatanodeDescriptor[srcNodes.length];\n      byte[] newIndices \u003d new byte[liveBlockIndices.size()];\n      adjustSrcNodesAndIndices((BlockInfoStriped)block,\n          srcNodes, liveBlockIndices, newSrcNodes, newIndices);\n      byte[] busyIndices \u003d new byte[liveBusyBlockIndices.size()];\n      for (int i \u003d 0; i \u003c liveBusyBlockIndices.size(); i++) {\n        busyIndices[i] \u003d liveBusyBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, newSrcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, newIndices, busyIndices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "02009c3bb762393540cdf92cfd9c840807272903": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14768. EC : Busy DN replica should be consider in live replica check. Contributed by guojh.\n",
      "commitDate": "01/11/19 9:45 AM",
      "commitName": "02009c3bb762393540cdf92cfd9c840807272903",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "31/10/19 11:19 AM",
      "commitNameOld": "9d25ae7669eed1a047578b574f42bd121b445a3c",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 0.93,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,83 @@\n   BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n+    List\u003cByte\u003e liveBusyBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n-        liveBlockIndices, priority);\n+        liveBlockIndices, liveBusyBlockIndices, priority);\n     short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n         numReplicas);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n       NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n       NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n       additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       // Violates placement policy. Needed on a new rack or domain etc.\n       BlockPlacementStatus placementStatus \u003d getBlockPlacementStatus(block);\n       additionalReplRequired \u003d placementStatus.getAdditionalReplicasRequired();\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n         NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() -\n           numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired -\n             numReplicas.decommissioning() -\n             numReplicas.liveEnteringMaintenanceReplicas();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n+      byte[] busyIndices \u003d new byte[liveBusyBlockIndices.size()];\n+      for (int i \u003d 0; i \u003c liveBusyBlockIndices.size(); i++) {\n+        busyIndices[i] \u003d liveBusyBlockIndices.get(i);\n+      }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n-          priority, indices);\n+          priority, indices, busyIndices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    List\u003cByte\u003e liveBusyBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, liveBusyBlockIndices, priority);\n    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n        numReplicas);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      // Violates placement policy. Needed on a new rack or domain etc.\n      BlockPlacementStatus placementStatus \u003d getBlockPlacementStatus(block);\n      additionalReplRequired \u003d placementStatus.getAdditionalReplicasRequired();\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() -\n          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired -\n            numReplicas.decommissioning() -\n            numReplicas.liveEnteringMaintenanceReplicas();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      byte[] busyIndices \u003d new byte[liveBusyBlockIndices.size()];\n      for (int i \u003d 0; i \u003c liveBusyBlockIndices.size(); i++) {\n        busyIndices[i] \u003d liveBusyBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices, busyIndices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "c99a12167ff9566012ef32104a3964887d62c899": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14637. Namenode may not replicate blocks to meet the policy after enabling upgradeDomain. Contributed by Stephen O\u0027Donnell.\n\nReviewed-by: Ayush Saxena \u003cayushsaxena@apache.org\u003e\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "03/10/19 10:13 PM",
      "commitName": "c99a12167ff9566012ef32104a3964887d62c899",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "28/09/19 9:14 AM",
      "commitNameOld": "c4c8d5fd0e3c17ccdcf18ece8e005f510328b060",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 5.54,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,78 @@\n   BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n         numReplicas);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n       NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n       NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n       additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n-      additionalReplRequired \u003d 1; // Needed on a new rack\n+      // Violates placement policy. Needed on a new rack or domain etc.\n+      BlockPlacementStatus placementStatus \u003d getBlockPlacementStatus(block);\n+      additionalReplRequired \u003d placementStatus.getAdditionalReplicasRequired();\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n         NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() -\n           numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired -\n             numReplicas.decommissioning() -\n             numReplicas.liveEnteringMaintenanceReplicas();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n        numReplicas);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      // Violates placement policy. Needed on a new rack or domain etc.\n      BlockPlacementStatus placementStatus \u003d getBlockPlacementStatus(block);\n      additionalReplRequired \u003d placementStatus.getAdditionalReplicasRequired();\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() -\n          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired -\n            numReplicas.decommissioning() -\n            numReplicas.liveEnteringMaintenanceReplicas();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "6a9dc5f44b0c7945e3e9a56248cd4ff80d5c8f0f": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-12043. Add counters for block re-replication. Contributed by Chen Liang.\n",
      "commitDate": "30/06/17 10:20 AM",
      "commitName": "6a9dc5f44b0c7945e3e9a56248cd4ff80d5c8f0f",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-12043. Add counters for block re-replication. Contributed by Chen Liang.\n",
          "commitDate": "30/06/17 10:20 AM",
          "commitName": "6a9dc5f44b0c7945e3e9a56248cd4ff80d5c8f0f",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "30/06/17 10:19 AM",
          "commitNameOld": "a2f0cbd92f7e90909cf817c261a5fae13a9695b4",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,73 +1,76 @@\n-  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n+  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n         numReplicas);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n+      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n+      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n       additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n+        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() -\n           numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired -\n             numReplicas.decommissioning() -\n             numReplicas.liveEnteringMaintenanceReplicas();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n        numReplicas);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() -\n          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired -\n            numReplicas.decommissioning() -\n            numReplicas.liveEnteringMaintenanceReplicas();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-12043. Add counters for block re-replication. Contributed by Chen Liang.\n",
          "commitDate": "30/06/17 10:20 AM",
          "commitName": "6a9dc5f44b0c7945e3e9a56248cd4ff80d5c8f0f",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "30/06/17 10:19 AM",
          "commitNameOld": "a2f0cbd92f7e90909cf817c261a5fae13a9695b4",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,73 +1,76 @@\n-  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n+  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n         numReplicas);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n+      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n+      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n       additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n+        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() -\n           numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired -\n             numReplicas.decommissioning() -\n             numReplicas.liveEnteringMaintenanceReplicas();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n        numReplicas);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() -\n          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired -\n            numReplicas.decommissioning() -\n            numReplicas.liveEnteringMaintenanceReplicas();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "a2f0cbd92f7e90909cf817c261a5fae13a9695b4": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "Revert \"HDFS-12043. Add counters for block re-replication. Contributed by Chen Liang.\"\n\nAccidentally committed the wrong patch version, reverting to fix that.\n\nThis reverts commit 900221f95ea9fe1936b4d5f277e6047ee8734eca.\n",
      "commitDate": "30/06/17 10:19 AM",
      "commitName": "a2f0cbd92f7e90909cf817c261a5fae13a9695b4",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "Revert \"HDFS-12043. Add counters for block re-replication. Contributed by Chen Liang.\"\n\nAccidentally committed the wrong patch version, reverting to fix that.\n\nThis reverts commit 900221f95ea9fe1936b4d5f277e6047ee8734eca.\n",
          "commitDate": "30/06/17 10:19 AM",
          "commitName": "a2f0cbd92f7e90909cf817c261a5fae13a9695b4",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "29/06/17 5:15 PM",
          "commitNameOld": "900221f95ea9fe1936b4d5f277e6047ee8734eca",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.71,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,76 +1,73 @@\n-  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n+  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n         numReplicas);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n-      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n-      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n       additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n-        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() -\n           numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired -\n             numReplicas.decommissioning() -\n             numReplicas.liveEnteringMaintenanceReplicas();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n        numReplicas);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() -\n          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired -\n            numReplicas.decommissioning() -\n            numReplicas.liveEnteringMaintenanceReplicas();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "Revert \"HDFS-12043. Add counters for block re-replication. Contributed by Chen Liang.\"\n\nAccidentally committed the wrong patch version, reverting to fix that.\n\nThis reverts commit 900221f95ea9fe1936b4d5f277e6047ee8734eca.\n",
          "commitDate": "30/06/17 10:19 AM",
          "commitName": "a2f0cbd92f7e90909cf817c261a5fae13a9695b4",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "29/06/17 5:15 PM",
          "commitNameOld": "900221f95ea9fe1936b4d5f277e6047ee8734eca",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.71,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,76 +1,73 @@\n-  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n+  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n         numReplicas);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n-      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n-      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n       additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n-        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() -\n           numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired -\n             numReplicas.decommissioning() -\n             numReplicas.liveEnteringMaintenanceReplicas();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n        numReplicas);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() -\n          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired -\n            numReplicas.decommissioning() -\n            numReplicas.liveEnteringMaintenanceReplicas();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "900221f95ea9fe1936b4d5f277e6047ee8734eca": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-12043. Add counters for block re-replication. Contributed by Chen Liang.\n",
      "commitDate": "29/06/17 5:15 PM",
      "commitName": "900221f95ea9fe1936b4d5f277e6047ee8734eca",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-12043. Add counters for block re-replication. Contributed by Chen Liang.\n",
          "commitDate": "29/06/17 5:15 PM",
          "commitName": "900221f95ea9fe1936b4d5f277e6047ee8734eca",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "21/06/17 4:34 PM",
          "commitNameOld": "c22cf004425daa9c350df5e365b0db85b1628b40",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 8.03,
          "commitsBetweenForRepo": 50,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,73 +1,76 @@\n-  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n+  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n         numReplicas);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n+      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n+      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n       additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n+        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() -\n           numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired -\n             numReplicas.decommissioning() -\n             numReplicas.liveEnteringMaintenanceReplicas();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n        numReplicas);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() -\n          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired -\n            numReplicas.decommissioning() -\n            numReplicas.liveEnteringMaintenanceReplicas();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-12043. Add counters for block re-replication. Contributed by Chen Liang.\n",
          "commitDate": "29/06/17 5:15 PM",
          "commitName": "900221f95ea9fe1936b4d5f277e6047ee8734eca",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "21/06/17 4:34 PM",
          "commitNameOld": "c22cf004425daa9c350df5e365b0db85b1628b40",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 8.03,
          "commitsBetweenForRepo": 50,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,73 +1,76 @@\n-  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n+  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n         numReplicas);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n+      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n+      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n       additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n+        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() -\n           numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired -\n             numReplicas.decommissioning() -\n             numReplicas.liveEnteringMaintenanceReplicas();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n        numReplicas);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        NameNode.getNameNodeMetrics().incNumTimesReReplicationNotScheduled();\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() -\n          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired -\n            numReplicas.decommissioning() -\n            numReplicas.liveEnteringMaintenanceReplicas();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "a7f085d6bf499edf23e650a4f7211c53a442da0e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11832. Switch leftover logs to slf4j format in BlockManager.java. Contributed by Hui Xu and Chen Liang.\n",
      "commitDate": "29/05/17 1:30 AM",
      "commitName": "a7f085d6bf499edf23e650a4f7211c53a442da0e",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "25/05/17 7:35 AM",
      "commitNameOld": "2e41f8803dd46d1bab16c1b206c71be72ea260a1",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 3.75,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,73 @@\n   private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n         numReplicas);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n-      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n-          \"from any node\");\n+      LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n       additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() -\n           numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired -\n             numReplicas.decommissioning() -\n             numReplicas.liveEnteringMaintenanceReplicas();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n        numReplicas);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block {} cannot be reconstructed from any node\", block);\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() -\n          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired -\n            numReplicas.decommissioning() -\n            numReplicas.liveEnteringMaintenanceReplicas();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "14/10/16 6:13 PM",
      "commitNameOld": "391ce535a739dc92cb90017d759217265a4fd969",
      "commitAuthorOld": "Vinitha Reddy Gankidi",
      "daysBetweenCommits": 2.98,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,74 @@\n   private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n-    short requiredRedundancy \u003d getExpectedRedundancyNum(block);\n-\n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n+    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n+        numReplicas);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n-    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n-        requiredRedundancy)) {\n+    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n       additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n-      if (additionalReplRequired - numReplicas.decommissioning() \u003e 0) {\n-        additionalReplRequired \u003d additionalReplRequired\n-            - numReplicas.decommissioning();\n+      if (additionalReplRequired - numReplicas.decommissioning() -\n+          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n+        additionalReplRequired \u003d additionalReplRequired -\n+            numReplicas.decommissioning() -\n+            numReplicas.liveEnteringMaintenanceReplicas();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    short requiredRedundancy \u003d getExpectedLiveRedundancyNum(block,\n        numReplicas);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() -\n          numReplicas.liveEnteringMaintenanceReplicas() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired -\n            numReplicas.decommissioning() -\n            numReplicas.liveEnteringMaintenanceReplicas();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10236. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-3]. Contributed by Rakesh R.\n",
      "commitDate": "26/05/16 4:50 PM",
      "commitName": "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "28/04/16 10:44 AM",
      "commitNameOld": "6243eabb48390fffada2418ade5adf9e0766afbe",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 28.25,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,73 @@\n   private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n-    short requiredReplication \u003d getExpectedReplicaNum(block);\n+    short requiredRedundancy \u003d getExpectedRedundancyNum(block);\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n-        requiredReplication)) {\n+        requiredRedundancy)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     int additionalReplRequired;\n-    if (numReplicas.liveReplicas() \u003c requiredReplication) {\n-      additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n+    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n+      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired\n             - numReplicas.decommissioning();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    short requiredRedundancy \u003d getExpectedRedundancyNum(block);\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredRedundancy)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredRedundancy) {\n      additionalReplRequired \u003d requiredRedundancy - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired\n            - numReplicas.decommissioning();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9869. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-2]. Contributed by Rakesh R.\n",
      "commitDate": "25/04/16 10:01 PM",
      "commitName": "5865fe2bf01284993572ea60b3ec3bf8b4492818",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "17/04/16 6:28 PM",
      "commitNameOld": "67523ffcf491f4f2db5335899c00a174d0caaa9b",
      "commitAuthorOld": "Walter Su",
      "daysBetweenCommits": 8.15,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,73 @@\n   private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReconstruction\n       neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     short requiredReplication \u003d getExpectedReplicaNum(block);\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n-    int pendingNum \u003d pendingReplications.getNumReplicas(block);\n+    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReconstruction.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredReplication) {\n       additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired\n             - numReplicas.decommissioning();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    short requiredReplication \u003d getExpectedReplicaNum(block);\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredReplication) {\n      additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired\n            - numReplicas.decommissioning();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,73 @@\n   private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n-      // remove from neededReplications\n-      neededReplications.remove(block, priority);\n+      // remove from neededReconstruction\n+      neededReconstruction.remove(block, priority);\n       return null;\n     }\n \n     short requiredReplication \u003d getExpectedReplicaNum(block);\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n-      neededReplications.remove(block, priority);\n-      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n+      neededReconstruction.remove(block, priority);\n+      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredReplication) {\n       additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n         return null;\n       }\n \n       // should reconstruct all the internal blocks before scheduling\n       // replication task for decommissioning node(s).\n       if (additionalReplRequired - numReplicas.decommissioning() \u003e 0) {\n         additionalReplRequired \u003d additionalReplRequired\n             - numReplicas.decommissioning();\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReconstruction\n      neededReconstruction.remove(block, priority);\n      return null;\n    }\n\n    short requiredReplication \u003d getExpectedReplicaNum(block);\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReconstruction.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReconstruction as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredReplication) {\n      additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired\n            - numReplicas.decommissioning();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "743a99f2dbc9a27e19f92ff3551937d90dba2e89": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8786. Erasure coding: use simple replication for internal blocks on decommissioning datanodes. Contributed by Rakesh R.\n",
      "commitDate": "08/03/16 10:24 AM",
      "commitName": "743a99f2dbc9a27e19f92ff3551937d90dba2e89",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "07/03/16 12:19 PM",
      "commitNameOld": "724d2299cd2516d90c030f6e20d814cceb439228",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.92,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,73 @@\n   private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReplications\n       neededReplications.remove(block, priority);\n       return null;\n     }\n \n     short requiredReplication \u003d getExpectedReplicaNum(block);\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n-    final int additionalReplRequired;\n+    int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredReplication) {\n       additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n         return null;\n       }\n+\n+      // should reconstruct all the internal blocks before scheduling\n+      // replication task for decommissioning node(s).\n+      if (additionalReplRequired - numReplicas.decommissioning() \u003e 0) {\n+        additionalReplRequired \u003d additionalReplRequired\n+            - numReplicas.decommissioning();\n+      }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReplications\n      neededReplications.remove(block, priority);\n      return null;\n    }\n\n    short requiredReplication \u003d getExpectedReplicaNum(block);\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredReplication) {\n      additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n\n      // should reconstruct all the internal blocks before scheduling\n      // replication task for decommissioning node(s).\n      if (additionalReplRequired - numReplicas.decommissioning() \u003e 0) {\n        additionalReplRequired \u003d additionalReplRequired\n            - numReplicas.decommissioning();\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "e54cc2931262bf49682a8323da9811976218c03b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9818. Correctly handle EC reconstruction work caused by not enough racks. Contributed by Jing Zhao.\n",
      "commitDate": "19/02/16 7:02 PM",
      "commitName": "e54cc2931262bf49682a8323da9811976218c03b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/02/16 11:07 AM",
      "commitNameOld": "972782d9568e0849484c027f27c1638ba50ec56e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 7.33,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,66 @@\n   private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n     // skip abandoned block or block reopened for append\n     if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReplications\n       neededReplications.remove(block, priority);\n       return null;\n     }\n \n     short requiredReplication \u003d getExpectedReplicaNum(block);\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     final int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredReplication) {\n       additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n         return null;\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n-      return new ErasureCodingWork(block, bc, srcNodes,\n+      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReplications\n      neededReplications.remove(block, priority);\n      return null;\n    }\n\n    short requiredReplication \u003d getExpectedReplicaNum(block);\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    final int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredReplication) {\n      additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(getBlockPoolId(), block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "972782d9568e0849484c027f27c1638ba50ec56e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9754. Avoid unnecessary getBlockCollection calls in BlockManager. Contributed by Jing Zhao.\n",
      "commitDate": "12/02/16 11:07 AM",
      "commitName": "972782d9568e0849484c027f27c1638ba50ec56e",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "10/02/16 9:24 PM",
      "commitNameOld": "19adb2bc641999b83e25ff0e107ba8c6edbad399",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.57,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,66 @@\n   private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n       int priority) {\n-    // block should belong to a file\n-    BlockCollection bc \u003d getBlockCollection(block);\n-    // abandoned block or block reopened for append\n-    if (bc \u003d\u003d null\n-        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n+    // skip abandoned block or block reopened for append\n+    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n       // remove from neededReplications\n       neededReplications.remove(block, priority);\n       return null;\n     }\n \n     short requiredReplication \u003d getExpectedReplicaNum(block);\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be reconstructed from any node\n       LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     final int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredReplication) {\n       additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n+    final BlockCollection bc \u003d getBlockCollection(block);\n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous reconstruction to finish.\n         return null;\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // skip abandoned block or block reopened for append\n    if (block.isDeleted() || !block.isCompleteOrCommitted()) {\n      // remove from neededReplications\n      neededReplications.remove(block, priority);\n      return null;\n    }\n\n    short requiredReplication \u003d getExpectedReplicaNum(block);\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    final int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredReplication) {\n      additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    final BlockCollection bc \u003d getBlockCollection(block);\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "a0fb2eff9b71e2e2c0e53262773b34bed82585d4": {
      "type": "Ymultichange(Yrename,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
      "commitDate": "09/02/16 2:43 PM",
      "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
          "commitDate": "09/02/16 2:43 PM",
          "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "02/02/16 11:23 AM",
          "commitNameOld": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 7.14,
          "commitsBetweenForRepo": 51,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,67 +1,68 @@\n-  private BlockRecoveryWork scheduleRecovery(BlockInfo block, int priority) {\n+  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n+      int priority) {\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       // remove from neededReplications\n       neededReplications.remove(block, priority);\n       return null;\n     }\n \n     short requiredReplication \u003d getExpectedReplicaNum(block);\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n-      // block can not be recovered from any node\n-      LOG.debug(\"Block \" + block + \" cannot be recovered \" +\n+      // block can not be reconstructed from any node\n+      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     final int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredReplication) {\n       additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n-        // Wait the previous recovery to finish.\n+        // Wait the previous reconstruction to finish.\n         return null;\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      // remove from neededReplications\n      neededReplications.remove(block, priority);\n      return null;\n    }\n\n    short requiredReplication \u003d getExpectedReplicaNum(block);\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    final int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredReplication) {\n      additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "scheduleRecovery",
            "newValue": "scheduleReconstruction"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
          "commitDate": "09/02/16 2:43 PM",
          "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "02/02/16 11:23 AM",
          "commitNameOld": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 7.14,
          "commitsBetweenForRepo": 51,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,67 +1,68 @@\n-  private BlockRecoveryWork scheduleRecovery(BlockInfo block, int priority) {\n+  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n+      int priority) {\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       // remove from neededReplications\n       neededReplications.remove(block, priority);\n       return null;\n     }\n \n     short requiredReplication \u003d getExpectedReplicaNum(block);\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n-      // block can not be recovered from any node\n-      LOG.debug(\"Block \" + block + \" cannot be recovered \" +\n+      // block can not be reconstructed from any node\n+      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     final int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredReplication) {\n       additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n-        // Wait the previous recovery to finish.\n+        // Wait the previous reconstruction to finish.\n         return null;\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      // remove from neededReplications\n      neededReplications.remove(block, priority);\n      return null;\n    }\n\n    short requiredReplication \u003d getExpectedReplicaNum(block);\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    final int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredReplication) {\n      additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "BlockRecoveryWork",
            "newValue": "BlockReconstructionWork"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9775. Erasure Coding : Rename BlockRecoveryWork to BlockReconstructionWork. Contributed by Rakesh R.\n\nChange-Id: I6dfc8efd94fa2bbb4eec0e4730a5a4f92c8a5519\n",
          "commitDate": "09/02/16 2:43 PM",
          "commitName": "a0fb2eff9b71e2e2c0e53262773b34bed82585d4",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "02/02/16 11:23 AM",
          "commitNameOld": "dd9ebf6eedfd4ff8b3486eae2a446de6b0c7fa8a",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 7.14,
          "commitsBetweenForRepo": 51,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,67 +1,68 @@\n-  private BlockRecoveryWork scheduleRecovery(BlockInfo block, int priority) {\n+  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n+      int priority) {\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       // remove from neededReplications\n       neededReplications.remove(block, priority);\n       return null;\n     }\n \n     short requiredReplication \u003d getExpectedReplicaNum(block);\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n-      // block can not be recovered from any node\n-      LOG.debug(\"Block \" + block + \" cannot be recovered \" +\n+      // block can not be reconstructed from any node\n+      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     final int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredReplication) {\n       additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n-        // Wait the previous recovery to finish.\n+        // Wait the previous reconstruction to finish.\n         return null;\n       }\n       byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n      int priority) {\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      // remove from neededReplications\n      neededReplications.remove(block, priority);\n      return null;\n    }\n\n    short requiredReplication \u003d getExpectedReplicaNum(block);\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be reconstructed from any node\n      LOG.debug(\"Block \" + block + \" cannot be reconstructed \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    final int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredReplication) {\n      additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous reconstruction to finish.\n        return null;\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "70d6f201260086a3f12beaa317fede2a99639fef": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9575. Use byte array for internal block indices in a striped block.  Contributed by jing9\n",
      "commitDate": "21/12/15 10:47 PM",
      "commitName": "70d6f201260086a3f12beaa317fede2a99639fef",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "16/12/15 6:16 PM",
      "commitNameOld": "f741476146574550a1a208d58ef8be76639e5ddc",
      "commitAuthorOld": "Uma Mahesh",
      "daysBetweenCommits": 5.19,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,67 @@\n   private BlockRecoveryWork scheduleRecovery(BlockInfo block, int priority) {\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       // remove from neededReplications\n       neededReplications.remove(block, priority);\n       return null;\n     }\n \n     short requiredReplication \u003d getExpectedReplicaNum(block);\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n-    List\u003cShort\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n+    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be recovered from any node\n       LOG.debug(\"Block \" + block + \" cannot be recovered \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     final int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredReplication) {\n       additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     if (block.isStriped()) {\n       if (pendingNum \u003e 0) {\n         // Wait the previous recovery to finish.\n         return null;\n       }\n-      short[] indices \u003d new short[liveBlockIndices.size()];\n+      byte[] indices \u003d new byte[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockRecoveryWork scheduleRecovery(BlockInfo block, int priority) {\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      // remove from neededReplications\n      neededReplications.remove(block, priority);\n      return null;\n    }\n\n    short requiredReplication \u003d getExpectedReplicaNum(block);\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cByte\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be recovered from any node\n      LOG.debug(\"Block \" + block + \" cannot be recovered \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    final int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredReplication) {\n      additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous recovery to finish.\n        return null;\n      }\n      byte[] indices \u003d new byte[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "5ba2b98d0fe29603e136fc43a14f853e820cf7e2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9275. Wait previous ErasureCodingWork to finish before schedule another one. (Walter Su via yliu)\n",
      "commitDate": "02/11/15 5:14 PM",
      "commitName": "5ba2b98d0fe29603e136fc43a14f853e820cf7e2",
      "commitAuthor": "yliu",
      "commitDateOld": "27/10/15 11:37 AM",
      "commitNameOld": "fe93577faf49ceb2ee47a7762a61625313ea773b",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 6.28,
      "commitsBetweenForRepo": 66,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,67 @@\n   private BlockRecoveryWork scheduleRecovery(BlockInfo block, int priority) {\n     // block should belong to a file\n     BlockCollection bc \u003d getBlockCollection(block);\n     // abandoned block or block reopened for append\n     if (bc \u003d\u003d null\n         || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n       // remove from neededReplications\n       neededReplications.remove(block, priority);\n       return null;\n     }\n \n     short requiredReplication \u003d getExpectedReplicaNum(block);\n \n     // get a source data-node\n     List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n     List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n     NumberReplicas numReplicas \u003d new NumberReplicas();\n     List\u003cShort\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n     final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n         containingNodes, liveReplicaNodes, numReplicas,\n         liveBlockIndices, priority);\n     if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n       // block can not be recovered from any node\n       LOG.debug(\"Block \" + block + \" cannot be recovered \" +\n           \"from any node\");\n       return null;\n     }\n \n     // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n     // not included in the numReplicas.liveReplicas() count\n     assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n \n     int pendingNum \u003d pendingReplications.getNumReplicas(block);\n     if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n         requiredReplication)) {\n       neededReplications.remove(block, priority);\n       blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n           \" it has enough replicas\", block);\n       return null;\n     }\n \n     final int additionalReplRequired;\n     if (numReplicas.liveReplicas() \u003c requiredReplication) {\n       additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n           - pendingNum;\n     } else {\n       additionalReplRequired \u003d 1; // Needed on a new rack\n     }\n \n     if (block.isStriped()) {\n+      if (pendingNum \u003e 0) {\n+        // Wait the previous recovery to finish.\n+        return null;\n+      }\n       short[] indices \u003d new short[liveBlockIndices.size()];\n       for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n         indices[i] \u003d liveBlockIndices.get(i);\n       }\n       return new ErasureCodingWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority, indices);\n     } else {\n       return new ReplicationWork(block, bc, srcNodes,\n           containingNodes, liveReplicaNodes, additionalReplRequired,\n           priority);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockRecoveryWork scheduleRecovery(BlockInfo block, int priority) {\n    // block should belong to a file\n    BlockCollection bc \u003d getBlockCollection(block);\n    // abandoned block or block reopened for append\n    if (bc \u003d\u003d null\n        || (bc.isUnderConstruction() \u0026\u0026 block.equals(bc.getLastBlock()))) {\n      // remove from neededReplications\n      neededReplications.remove(block, priority);\n      return null;\n    }\n\n    short requiredReplication \u003d getExpectedReplicaNum(block);\n\n    // get a source data-node\n    List\u003cDatanodeDescriptor\u003e containingNodes \u003d new ArrayList\u003c\u003e();\n    List\u003cDatanodeStorageInfo\u003e liveReplicaNodes \u003d new ArrayList\u003c\u003e();\n    NumberReplicas numReplicas \u003d new NumberReplicas();\n    List\u003cShort\u003e liveBlockIndices \u003d new ArrayList\u003c\u003e();\n    final DatanodeDescriptor[] srcNodes \u003d chooseSourceDatanodes(block,\n        containingNodes, liveReplicaNodes, numReplicas,\n        liveBlockIndices, priority);\n    if(srcNodes \u003d\u003d null || srcNodes.length \u003d\u003d 0) {\n      // block can not be recovered from any node\n      LOG.debug(\"Block \" + block + \" cannot be recovered \" +\n          \"from any node\");\n      return null;\n    }\n\n    // liveReplicaNodes can include READ_ONLY_SHARED replicas which are\n    // not included in the numReplicas.liveReplicas() count\n    assert liveReplicaNodes.size() \u003e\u003d numReplicas.liveReplicas();\n\n    int pendingNum \u003d pendingReplications.getNumReplicas(block);\n    if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum,\n        requiredReplication)) {\n      neededReplications.remove(block, priority);\n      blockLog.debug(\"BLOCK* Removing {} from neededReplications as\" +\n          \" it has enough replicas\", block);\n      return null;\n    }\n\n    final int additionalReplRequired;\n    if (numReplicas.liveReplicas() \u003c requiredReplication) {\n      additionalReplRequired \u003d requiredReplication - numReplicas.liveReplicas()\n          - pendingNum;\n    } else {\n      additionalReplRequired \u003d 1; // Needed on a new rack\n    }\n\n    if (block.isStriped()) {\n      if (pendingNum \u003e 0) {\n        // Wait the previous recovery to finish.\n        return null;\n      }\n      short[] indices \u003d new short[liveBlockIndices.size()];\n      for (int i \u003d 0 ; i \u003c liveBlockIndices.size(); i++) {\n        indices[i] \u003d liveBlockIndices.get(i);\n      }\n      return new ErasureCodingWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority, indices);\n    } else {\n      return new ReplicationWork(block, bc, srcNodes,\n          containingNodes, liveReplicaNodes, additionalReplRequired,\n          priority);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    }
  }
}