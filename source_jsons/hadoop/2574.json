{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ClientNamenodeProtocolTranslatorPB.java",
  "functionName": "getAdditionalDatanode",
  "functionId": "getAdditionalDatanode___src-String__fileId-long__blk-ExtendedBlock__existings-DatanodeInfo[]__existingStorageIDs-String[]__excludes-DatanodeInfo[]__numAdditionalNodes-int__clientName-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java",
  "functionStartLine": 537,
  "functionEndLine": 558,
  "numCommitsSeen": 219,
  "timeTaken": 4157,
  "changeHistory": [
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "63d9f1596c92206cce3b72e3214d2fb5f6242b90",
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc"
  ],
  "changeHistoryShort": {
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Yexceptionschange",
    "63d9f1596c92206cce3b72e3214d2fb5f6242b90": "Yfilerename",
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ybodychange",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": "Ybodychange",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 1:39 AM",
      "commitNameOld": "8fd55202468b28422b0df888641c9b08906fe4a7",
      "commitAuthorOld": "",
      "daysBetweenCommits": 4.42,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,22 @@\n   public LocatedBlock getAdditionalDatanode(String src, long fileId,\n       ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs,\n-      DatanodeInfo[] excludes,\n-      int numAdditionalNodes, String clientName) throws AccessControlException,\n-      FileNotFoundException, SafeModeException, UnresolvedLinkException,\n-      IOException {\n+      DatanodeInfo[] excludes, int numAdditionalNodes, String clientName)\n+      throws IOException {\n     GetAdditionalDatanodeRequestProto req \u003d GetAdditionalDatanodeRequestProto\n         .newBuilder()\n         .setSrc(src)\n         .setFileId(fileId)\n         .setBlk(PBHelperClient.convert(blk))\n         .addAllExistings(PBHelperClient.convert(existings))\n         .addAllExistingStorageUuids(Arrays.asList(existingStorageIDs))\n         .addAllExcludes(PBHelperClient.convert(excludes))\n         .setNumAdditionalNodes(numAdditionalNodes)\n         .setClientName(clientName)\n         .build();\n     try {\n       return PBHelperClient.convertLocatedBlockProto(\n           rpcProxy.getAdditionalDatanode(null, req).getBlock());\n     } catch (ServiceException e) {\n       throw ProtobufHelper.getRemoteException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public LocatedBlock getAdditionalDatanode(String src, long fileId,\n      ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs,\n      DatanodeInfo[] excludes, int numAdditionalNodes, String clientName)\n      throws IOException {\n    GetAdditionalDatanodeRequestProto req \u003d GetAdditionalDatanodeRequestProto\n        .newBuilder()\n        .setSrc(src)\n        .setFileId(fileId)\n        .setBlk(PBHelperClient.convert(blk))\n        .addAllExistings(PBHelperClient.convert(existings))\n        .addAllExistingStorageUuids(Arrays.asList(existingStorageIDs))\n        .addAllExcludes(PBHelperClient.convert(excludes))\n        .setNumAdditionalNodes(numAdditionalNodes)\n        .setClientName(clientName)\n        .build();\n    try {\n      return PBHelperClient.convertLocatedBlockProto(\n          rpcProxy.getAdditionalDatanode(null, req).getBlock());\n    } catch (ServiceException e) {\n      throw ProtobufHelper.getRemoteException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java",
      "extendedDetails": {
        "oldValue": "[AccessControlException, FileNotFoundException, SafeModeException, UnresolvedLinkException, IOException]",
        "newValue": "[IOException]"
      }
    },
    "63d9f1596c92206cce3b72e3214d2fb5f6242b90": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-9039. Separate client and server side methods of o.a.h.hdfs.NameNodeProxies. Contributed by Mingliang Liu.\n",
      "commitDate": "22/09/15 8:52 PM",
      "commitName": "63d9f1596c92206cce3b72e3214d2fb5f6242b90",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "22/09/15 9:08 AM",
      "commitNameOld": "cc2b4739902df60254dce2ddb23ef8f6ff2a3495",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 0.49,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public LocatedBlock getAdditionalDatanode(String src, long fileId,\n      ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs,\n      DatanodeInfo[] excludes,\n      int numAdditionalNodes, String clientName) throws AccessControlException,\n      FileNotFoundException, SafeModeException, UnresolvedLinkException,\n      IOException {\n    GetAdditionalDatanodeRequestProto req \u003d GetAdditionalDatanodeRequestProto\n        .newBuilder()\n        .setSrc(src)\n        .setFileId(fileId)\n        .setBlk(PBHelperClient.convert(blk))\n        .addAllExistings(PBHelperClient.convert(existings))\n        .addAllExistingStorageUuids(Arrays.asList(existingStorageIDs))\n        .addAllExcludes(PBHelperClient.convert(excludes))\n        .setNumAdditionalNodes(numAdditionalNodes)\n        .setClientName(clientName)\n        .build();\n    try {\n      return PBHelperClient.convert(rpcProxy.getAdditionalDatanode(null, req)\n          .getBlock());\n    } catch (ServiceException e) {\n      throw ProtobufHelper.getRemoteException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java"
      }
    },
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "22/08/15 1:31 PM",
      "commitNameOld": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 30.22,
      "commitsBetweenForRepo": 176,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public LocatedBlock getAdditionalDatanode(String src, long fileId,\n       ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs,\n       DatanodeInfo[] excludes,\n       int numAdditionalNodes, String clientName) throws AccessControlException,\n       FileNotFoundException, SafeModeException, UnresolvedLinkException,\n       IOException {\n     GetAdditionalDatanodeRequestProto req \u003d GetAdditionalDatanodeRequestProto\n         .newBuilder()\n         .setSrc(src)\n         .setFileId(fileId)\n         .setBlk(PBHelperClient.convert(blk))\n         .addAllExistings(PBHelperClient.convert(existings))\n         .addAllExistingStorageUuids(Arrays.asList(existingStorageIDs))\n         .addAllExcludes(PBHelperClient.convert(excludes))\n         .setNumAdditionalNodes(numAdditionalNodes)\n         .setClientName(clientName)\n         .build();\n     try {\n-      return PBHelper.convert(rpcProxy.getAdditionalDatanode(null, req)\n+      return PBHelperClient.convert(rpcProxy.getAdditionalDatanode(null, req)\n           .getBlock());\n     } catch (ServiceException e) {\n       throw ProtobufHelper.getRemoteException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public LocatedBlock getAdditionalDatanode(String src, long fileId,\n      ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs,\n      DatanodeInfo[] excludes,\n      int numAdditionalNodes, String clientName) throws AccessControlException,\n      FileNotFoundException, SafeModeException, UnresolvedLinkException,\n      IOException {\n    GetAdditionalDatanodeRequestProto req \u003d GetAdditionalDatanodeRequestProto\n        .newBuilder()\n        .setSrc(src)\n        .setFileId(fileId)\n        .setBlk(PBHelperClient.convert(blk))\n        .addAllExistings(PBHelperClient.convert(existings))\n        .addAllExistingStorageUuids(Arrays.asList(existingStorageIDs))\n        .addAllExcludes(PBHelperClient.convert(excludes))\n        .setNumAdditionalNodes(numAdditionalNodes)\n        .setClientName(clientName)\n        .build();\n    try {\n      return PBHelperClient.convert(rpcProxy.getAdditionalDatanode(null, req)\n          .getBlock());\n    } catch (ServiceException e) {\n      throw ProtobufHelper.getRemoteException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java",
      "extendedDetails": {}
    },
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8934. Move ShortCircuitShm to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "22/08/15 1:31 PM",
      "commitName": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "05/08/15 10:40 PM",
      "commitNameOld": "cc71ad80e184fc6e5043729e8cfcf6a62ca3e71f",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 16.62,
      "commitsBetweenForRepo": 81,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public LocatedBlock getAdditionalDatanode(String src, long fileId,\n       ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs,\n       DatanodeInfo[] excludes,\n       int numAdditionalNodes, String clientName) throws AccessControlException,\n       FileNotFoundException, SafeModeException, UnresolvedLinkException,\n       IOException {\n     GetAdditionalDatanodeRequestProto req \u003d GetAdditionalDatanodeRequestProto\n         .newBuilder()\n         .setSrc(src)\n         .setFileId(fileId)\n-        .setBlk(PBHelper.convert(blk))\n-        .addAllExistings(PBHelper.convert(existings))\n+        .setBlk(PBHelperClient.convert(blk))\n+        .addAllExistings(PBHelperClient.convert(existings))\n         .addAllExistingStorageUuids(Arrays.asList(existingStorageIDs))\n-        .addAllExcludes(PBHelper.convert(excludes))\n+        .addAllExcludes(PBHelperClient.convert(excludes))\n         .setNumAdditionalNodes(numAdditionalNodes)\n         .setClientName(clientName)\n         .build();\n     try {\n       return PBHelper.convert(rpcProxy.getAdditionalDatanode(null, req)\n           .getBlock());\n     } catch (ServiceException e) {\n       throw ProtobufHelper.getRemoteException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public LocatedBlock getAdditionalDatanode(String src, long fileId,\n      ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs,\n      DatanodeInfo[] excludes,\n      int numAdditionalNodes, String clientName) throws AccessControlException,\n      FileNotFoundException, SafeModeException, UnresolvedLinkException,\n      IOException {\n    GetAdditionalDatanodeRequestProto req \u003d GetAdditionalDatanodeRequestProto\n        .newBuilder()\n        .setSrc(src)\n        .setFileId(fileId)\n        .setBlk(PBHelperClient.convert(blk))\n        .addAllExistings(PBHelperClient.convert(existings))\n        .addAllExistingStorageUuids(Arrays.asList(existingStorageIDs))\n        .addAllExcludes(PBHelperClient.convert(excludes))\n        .setNumAdditionalNodes(numAdditionalNodes)\n        .setClientName(clientName)\n        .build();\n    try {\n      return PBHelper.convert(rpcProxy.getAdditionalDatanode(null, req)\n          .getBlock());\n    } catch (ServiceException e) {\n      throw ProtobufHelper.getRemoteException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java",
      "extendedDetails": {}
    },
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1593634 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/14 3:36 PM",
      "commitName": "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1593634 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/05/14 3:36 PM",
          "commitName": "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "24/03/14 4:32 PM",
          "commitNameOld": "c2ef7e239eb0e81cf8a3e971378e9e696202de67",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 45.96,
          "commitsBetweenForRepo": 271,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,24 @@\n-  public LocatedBlock getAdditionalDatanode(String src, ExtendedBlock blk,\n-      DatanodeInfo[] existings, String[] existingStorageIDs,\n+  public LocatedBlock getAdditionalDatanode(String src, long fileId,\n+      ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs,\n       DatanodeInfo[] excludes,\n       int numAdditionalNodes, String clientName) throws AccessControlException,\n       FileNotFoundException, SafeModeException, UnresolvedLinkException,\n       IOException {\n     GetAdditionalDatanodeRequestProto req \u003d GetAdditionalDatanodeRequestProto\n         .newBuilder()\n         .setSrc(src)\n+        .setFileId(fileId)\n         .setBlk(PBHelper.convert(blk))\n         .addAllExistings(PBHelper.convert(existings))\n         .addAllExistingStorageUuids(Arrays.asList(existingStorageIDs))\n         .addAllExcludes(PBHelper.convert(excludes))\n         .setNumAdditionalNodes(numAdditionalNodes)\n         .setClientName(clientName)\n         .build();\n     try {\n       return PBHelper.convert(rpcProxy.getAdditionalDatanode(null, req)\n           .getBlock());\n     } catch (ServiceException e) {\n       throw ProtobufHelper.getRemoteException(e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public LocatedBlock getAdditionalDatanode(String src, long fileId,\n      ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs,\n      DatanodeInfo[] excludes,\n      int numAdditionalNodes, String clientName) throws AccessControlException,\n      FileNotFoundException, SafeModeException, UnresolvedLinkException,\n      IOException {\n    GetAdditionalDatanodeRequestProto req \u003d GetAdditionalDatanodeRequestProto\n        .newBuilder()\n        .setSrc(src)\n        .setFileId(fileId)\n        .setBlk(PBHelper.convert(blk))\n        .addAllExistings(PBHelper.convert(existings))\n        .addAllExistingStorageUuids(Arrays.asList(existingStorageIDs))\n        .addAllExcludes(PBHelper.convert(excludes))\n        .setNumAdditionalNodes(numAdditionalNodes)\n        .setClientName(clientName)\n        .build();\n    try {\n      return PBHelper.convert(rpcProxy.getAdditionalDatanode(null, req)\n          .getBlock());\n    } catch (ServiceException e) {\n      throw ProtobufHelper.getRemoteException(e);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java",
          "extendedDetails": {
            "oldValue": "[src-String, blk-ExtendedBlock, existings-DatanodeInfo[], existingStorageIDs-String[], excludes-DatanodeInfo[], numAdditionalNodes-int, clientName-String]",
            "newValue": "[src-String, fileId-long, blk-ExtendedBlock, existings-DatanodeInfo[], existingStorageIDs-String[], excludes-DatanodeInfo[], numAdditionalNodes-int, clientName-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1593634 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/05/14 3:36 PM",
          "commitName": "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "24/03/14 4:32 PM",
          "commitNameOld": "c2ef7e239eb0e81cf8a3e971378e9e696202de67",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 45.96,
          "commitsBetweenForRepo": 271,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,24 @@\n-  public LocatedBlock getAdditionalDatanode(String src, ExtendedBlock blk,\n-      DatanodeInfo[] existings, String[] existingStorageIDs,\n+  public LocatedBlock getAdditionalDatanode(String src, long fileId,\n+      ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs,\n       DatanodeInfo[] excludes,\n       int numAdditionalNodes, String clientName) throws AccessControlException,\n       FileNotFoundException, SafeModeException, UnresolvedLinkException,\n       IOException {\n     GetAdditionalDatanodeRequestProto req \u003d GetAdditionalDatanodeRequestProto\n         .newBuilder()\n         .setSrc(src)\n+        .setFileId(fileId)\n         .setBlk(PBHelper.convert(blk))\n         .addAllExistings(PBHelper.convert(existings))\n         .addAllExistingStorageUuids(Arrays.asList(existingStorageIDs))\n         .addAllExcludes(PBHelper.convert(excludes))\n         .setNumAdditionalNodes(numAdditionalNodes)\n         .setClientName(clientName)\n         .build();\n     try {\n       return PBHelper.convert(rpcProxy.getAdditionalDatanode(null, req)\n           .getBlock());\n     } catch (ServiceException e) {\n       throw ProtobufHelper.getRemoteException(e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public LocatedBlock getAdditionalDatanode(String src, long fileId,\n      ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs,\n      DatanodeInfo[] excludes,\n      int numAdditionalNodes, String clientName) throws AccessControlException,\n      FileNotFoundException, SafeModeException, UnresolvedLinkException,\n      IOException {\n    GetAdditionalDatanodeRequestProto req \u003d GetAdditionalDatanodeRequestProto\n        .newBuilder()\n        .setSrc(src)\n        .setFileId(fileId)\n        .setBlk(PBHelper.convert(blk))\n        .addAllExistings(PBHelper.convert(existings))\n        .addAllExistingStorageUuids(Arrays.asList(existingStorageIDs))\n        .addAllExcludes(PBHelper.convert(excludes))\n        .setNumAdditionalNodes(numAdditionalNodes)\n        .setClientName(clientName)\n        .build();\n    try {\n      return PBHelper.convert(rpcProxy.getAdditionalDatanode(null, req)\n          .getBlock());\n    } catch (ServiceException e) {\n      throw ProtobufHelper.getRemoteException(e);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}