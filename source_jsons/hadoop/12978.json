{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeDescriptor.java",
  "functionName": "updateHeartbeat",
  "functionId": "updateHeartbeat___reports-StorageReport[]__cacheCapacity-long__cacheUsed-long__xceiverCount-int__volFailures-int__volumeFailureSummary-VolumeFailureSummary",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
  "functionStartLine": 378,
  "functionEndLine": 384,
  "numCommitsSeen": 167,
  "timeTaken": 3608,
  "changeHistory": [
    "f3f5e7ad005a88afad6fa09602073eaa450e21ed",
    "9729b244de50322c2cc889c97c2ffb2b4675cf77",
    "41980c56d3c01d7a0ddc7deea2d89b7f28026722",
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b",
    "f8a9329f2b8e768fe6730fc05436e973344b9132"
  ],
  "changeHistoryShort": {
    "f3f5e7ad005a88afad6fa09602073eaa450e21ed": "Ymodifierchange",
    "9729b244de50322c2cc889c97c2ffb2b4675cf77": "Ymultichange(Yparameterchange,Ybodychange)",
    "41980c56d3c01d7a0ddc7deea2d89b7f28026722": "Ybodychange",
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b": "Ybodychange",
    "f8a9329f2b8e768fe6730fc05436e973344b9132": "Ybodychange"
  },
  "changeHistoryDetails": {
    "f3f5e7ad005a88afad6fa09602073eaa450e21ed": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-14042. Fix NPE when PROVIDED storage is missing. Contributed by Virajith Jalaparti.\n",
      "commitDate": "05/11/18 11:02 AM",
      "commitName": "f3f5e7ad005a88afad6fa09602073eaa450e21ed",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "3ac07b720b7839a7fe6c83f4ccfe319b6a892501",
      "commitAuthorOld": "Uma Maheswara Rao Gangumalla",
      "daysBetweenCommits": 85.37,
      "commitsBetweenForRepo": 763,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,7 +1,7 @@\n-  public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n+  void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n       long cacheUsed, int xceiverCount, int volFailures,\n       VolumeFailureSummary volumeFailureSummary) {\n     updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n         volFailures, volumeFailureSummary);\n     heartbeatedSinceRegistration \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n      long cacheUsed, int xceiverCount, int volFailures,\n      VolumeFailureSummary volumeFailureSummary) {\n    updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n        volFailures, volumeFailureSummary);\n    heartbeatedSinceRegistration \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {
        "oldValue": "[public]",
        "newValue": "[]"
      }
    },
    "9729b244de50322c2cc889c97c2ffb2b4675cf77": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7604. Track and display failed DataNode storage locations in NameNode. Contributed by Chris Nauroth.\n",
      "commitDate": "16/02/15 2:43 PM",
      "commitName": "9729b244de50322c2cc889c97c2ffb2b4675cf77",
      "commitAuthor": "cnauroth",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7604. Track and display failed DataNode storage locations in NameNode. Contributed by Chris Nauroth.\n",
          "commitDate": "16/02/15 2:43 PM",
          "commitName": "9729b244de50322c2cc889c97c2ffb2b4675cf77",
          "commitAuthor": "cnauroth",
          "commitDateOld": "08/02/15 11:51 AM",
          "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 8.12,
          "commitsBetweenForRepo": 110,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,6 +1,7 @@\n   public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n-      long cacheUsed, int xceiverCount, int volFailures) {\n+      long cacheUsed, int xceiverCount, int volFailures,\n+      VolumeFailureSummary volumeFailureSummary) {\n     updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n-        volFailures);\n+        volFailures, volumeFailureSummary);\n     heartbeatedSinceRegistration \u003d true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n      long cacheUsed, int xceiverCount, int volFailures,\n      VolumeFailureSummary volumeFailureSummary) {\n    updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n        volFailures, volumeFailureSummary);\n    heartbeatedSinceRegistration \u003d true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
          "extendedDetails": {
            "oldValue": "[reports-StorageReport[], cacheCapacity-long, cacheUsed-long, xceiverCount-int, volFailures-int]",
            "newValue": "[reports-StorageReport[], cacheCapacity-long, cacheUsed-long, xceiverCount-int, volFailures-int, volumeFailureSummary-VolumeFailureSummary]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7604. Track and display failed DataNode storage locations in NameNode. Contributed by Chris Nauroth.\n",
          "commitDate": "16/02/15 2:43 PM",
          "commitName": "9729b244de50322c2cc889c97c2ffb2b4675cf77",
          "commitAuthor": "cnauroth",
          "commitDateOld": "08/02/15 11:51 AM",
          "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 8.12,
          "commitsBetweenForRepo": 110,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,6 +1,7 @@\n   public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n-      long cacheUsed, int xceiverCount, int volFailures) {\n+      long cacheUsed, int xceiverCount, int volFailures,\n+      VolumeFailureSummary volumeFailureSummary) {\n     updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n-        volFailures);\n+        volFailures, volumeFailureSummary);\n     heartbeatedSinceRegistration \u003d true;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n      long cacheUsed, int xceiverCount, int volFailures,\n      VolumeFailureSummary volumeFailureSummary) {\n    updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n        volFailures, volumeFailureSummary);\n    heartbeatedSinceRegistration \u003d true;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
          "extendedDetails": {}
        }
      ]
    },
    "41980c56d3c01d7a0ddc7deea2d89b7f28026722": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7208. NN doesn\u0027t schedule replication when a DN storage fails.  Contributed by Ming Ma\n",
      "commitDate": "15/10/14 8:44 PM",
      "commitName": "41980c56d3c01d7a0ddc7deea2d89b7f28026722",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "07/10/14 3:01 PM",
      "commitNameOld": "9b8a35aff6d4bd7bb066ce01fa63a88fa49245ee",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 8.24,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,6 @@\n   public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n       long cacheUsed, int xceiverCount, int volFailures) {\n-    long totalCapacity \u003d 0;\n-    long totalRemaining \u003d 0;\n-    long totalBlockPoolUsed \u003d 0;\n-    long totalDfsUsed \u003d 0;\n-\n-    setCacheCapacity(cacheCapacity);\n-    setCacheUsed(cacheUsed);\n-    setXceiverCount(xceiverCount);\n-    setLastUpdate(Time.now());    \n-    this.volumeFailures \u003d volFailures;\n-    for (StorageReport report : reports) {\n-      DatanodeStorageInfo storage \u003d updateStorage(report.getStorage());\n-      storage.receivedHeartbeat(report);\n-      totalCapacity +\u003d report.getCapacity();\n-      totalRemaining +\u003d report.getRemaining();\n-      totalBlockPoolUsed +\u003d report.getBlockPoolUsed();\n-      totalDfsUsed +\u003d report.getDfsUsed();\n-    }\n-    rollBlocksScheduled(getLastUpdate());\n-\n-    // Update total metrics for the node.\n-    setCapacity(totalCapacity);\n-    setRemaining(totalRemaining);\n-    setBlockPoolUsed(totalBlockPoolUsed);\n-    setDfsUsed(totalDfsUsed);\n+    updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n+        volFailures);\n+    heartbeatedSinceRegistration \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n      long cacheUsed, int xceiverCount, int volFailures) {\n    updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n        volFailures);\n    heartbeatedSinceRegistration \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    },
    "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6094. The same block can be counted twice towards safe mode threshold. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1578478 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/03/14 10:37 AM",
      "commitName": "809e8bf5b7fdfdb18f719614d1e54ca4fb47fa2b",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "31/01/14 1:00 PM",
      "commitNameOld": "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 44.86,
      "commitsBetweenForRepo": 401,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,28 @@\n   public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n       long cacheUsed, int xceiverCount, int volFailures) {\n     long totalCapacity \u003d 0;\n     long totalRemaining \u003d 0;\n     long totalBlockPoolUsed \u003d 0;\n     long totalDfsUsed \u003d 0;\n \n     setCacheCapacity(cacheCapacity);\n     setCacheUsed(cacheUsed);\n     setXceiverCount(xceiverCount);\n     setLastUpdate(Time.now());    \n     this.volumeFailures \u003d volFailures;\n     for (StorageReport report : reports) {\n-      DatanodeStorageInfo storage \u003d storageMap.get(report.getStorage().getStorageID());\n-      if (storage \u003d\u003d null) {\n-        // This is seen during cluster initialization when the heartbeat\n-        // is received before the initial block reports from each storage.\n-        storage \u003d updateStorage(report.getStorage());\n-      }\n+      DatanodeStorageInfo storage \u003d updateStorage(report.getStorage());\n       storage.receivedHeartbeat(report);\n       totalCapacity +\u003d report.getCapacity();\n       totalRemaining +\u003d report.getRemaining();\n       totalBlockPoolUsed +\u003d report.getBlockPoolUsed();\n       totalDfsUsed +\u003d report.getDfsUsed();\n     }\n     rollBlocksScheduled(getLastUpdate());\n \n     // Update total metrics for the node.\n     setCapacity(totalCapacity);\n     setRemaining(totalRemaining);\n     setBlockPoolUsed(totalBlockPoolUsed);\n     setDfsUsed(totalDfsUsed);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n      long cacheUsed, int xceiverCount, int volFailures) {\n    long totalCapacity \u003d 0;\n    long totalRemaining \u003d 0;\n    long totalBlockPoolUsed \u003d 0;\n    long totalDfsUsed \u003d 0;\n\n    setCacheCapacity(cacheCapacity);\n    setCacheUsed(cacheUsed);\n    setXceiverCount(xceiverCount);\n    setLastUpdate(Time.now());    \n    this.volumeFailures \u003d volFailures;\n    for (StorageReport report : reports) {\n      DatanodeStorageInfo storage \u003d updateStorage(report.getStorage());\n      storage.receivedHeartbeat(report);\n      totalCapacity +\u003d report.getCapacity();\n      totalRemaining +\u003d report.getRemaining();\n      totalBlockPoolUsed +\u003d report.getBlockPoolUsed();\n      totalDfsUsed +\u003d report.getDfsUsed();\n    }\n    rollBlocksScheduled(getLastUpdate());\n\n    // Update total metrics for the node.\n    setCapacity(totalCapacity);\n    setRemaining(totalRemaining);\n    setBlockPoolUsed(totalBlockPoolUsed);\n    setDfsUsed(totalDfsUsed);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    },
    "f8a9329f2b8e768fe6730fc05436e973344b9132": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5667. Include DatanodeStorage in StorageReport. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1555929 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/01/14 9:28 AM",
      "commitName": "f8a9329f2b8e768fe6730fc05436e973344b9132",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/11/13 12:07 PM",
      "commitNameOld": "97acde2d33967f7f870f7dfe96c6b558e6fe324b",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 44.89,
      "commitsBetweenForRepo": 191,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n       long cacheUsed, int xceiverCount, int volFailures) {\n     long totalCapacity \u003d 0;\n     long totalRemaining \u003d 0;\n     long totalBlockPoolUsed \u003d 0;\n     long totalDfsUsed \u003d 0;\n \n     setCacheCapacity(cacheCapacity);\n     setCacheUsed(cacheUsed);\n     setXceiverCount(xceiverCount);\n     setLastUpdate(Time.now());    \n     this.volumeFailures \u003d volFailures;\n     for (StorageReport report : reports) {\n-      DatanodeStorageInfo storage \u003d storageMap.get(report.getStorageID());\n+      DatanodeStorageInfo storage \u003d storageMap.get(report.getStorage().getStorageID());\n       if (storage \u003d\u003d null) {\n         // This is seen during cluster initialization when the heartbeat\n         // is received before the initial block reports from each storage.\n-        storage \u003d updateStorage(new DatanodeStorage(report.getStorageID()));\n+        storage \u003d updateStorage(report.getStorage());\n       }\n       storage.receivedHeartbeat(report);\n       totalCapacity +\u003d report.getCapacity();\n       totalRemaining +\u003d report.getRemaining();\n       totalBlockPoolUsed +\u003d report.getBlockPoolUsed();\n       totalDfsUsed +\u003d report.getDfsUsed();\n     }\n     rollBlocksScheduled(getLastUpdate());\n \n     // Update total metrics for the node.\n     setCapacity(totalCapacity);\n     setRemaining(totalRemaining);\n     setBlockPoolUsed(totalBlockPoolUsed);\n     setDfsUsed(totalDfsUsed);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n      long cacheUsed, int xceiverCount, int volFailures) {\n    long totalCapacity \u003d 0;\n    long totalRemaining \u003d 0;\n    long totalBlockPoolUsed \u003d 0;\n    long totalDfsUsed \u003d 0;\n\n    setCacheCapacity(cacheCapacity);\n    setCacheUsed(cacheUsed);\n    setXceiverCount(xceiverCount);\n    setLastUpdate(Time.now());    \n    this.volumeFailures \u003d volFailures;\n    for (StorageReport report : reports) {\n      DatanodeStorageInfo storage \u003d storageMap.get(report.getStorage().getStorageID());\n      if (storage \u003d\u003d null) {\n        // This is seen during cluster initialization when the heartbeat\n        // is received before the initial block reports from each storage.\n        storage \u003d updateStorage(report.getStorage());\n      }\n      storage.receivedHeartbeat(report);\n      totalCapacity +\u003d report.getCapacity();\n      totalRemaining +\u003d report.getRemaining();\n      totalBlockPoolUsed +\u003d report.getBlockPoolUsed();\n      totalDfsUsed +\u003d report.getDfsUsed();\n    }\n    rollBlocksScheduled(getLastUpdate());\n\n    // Update total metrics for the node.\n    setCapacity(totalCapacity);\n    setRemaining(totalRemaining);\n    setBlockPoolUsed(totalBlockPoolUsed);\n    setDfsUsed(totalDfsUsed);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
      "extendedDetails": {}
    }
  }
}