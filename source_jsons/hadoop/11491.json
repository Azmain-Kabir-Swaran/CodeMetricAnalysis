{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsVolumeList.java",
  "functionName": "addVolumeFailureInfo",
  "functionId": "addVolumeFailureInfo___volumeFailureInfo-VolumeFailureInfo",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
  "functionStartLine": 374,
  "functionEndLine": 384,
  "numCommitsSeen": 42,
  "timeTaken": 2567,
  "changeHistory": [
    "6d356b6b4d8ccb32397cacfb5d0357b21f6035fc",
    "9729b244de50322c2cc889c97c2ffb2b4675cf77"
  ],
  "changeHistoryShort": {
    "6d356b6b4d8ccb32397cacfb5d0357b21f6035fc": "Ybodychange",
    "9729b244de50322c2cc889c97c2ffb2b4675cf77": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6d356b6b4d8ccb32397cacfb5d0357b21f6035fc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11340. DataNode reconfigure for disks doesn\u0027t remove the failed volumes. (Manoj Govindassamy via lei)\n",
      "commitDate": "10/03/17 2:37 PM",
      "commitName": "6d356b6b4d8ccb32397cacfb5d0357b21f6035fc",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "20/12/16 1:53 PM",
      "commitNameOld": "f678080dbd25a218e0406463a3c3a1fc03680702",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 80.03,
      "commitsBetweenForRepo": 411,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,4 +1,11 @@\n   void addVolumeFailureInfo(VolumeFailureInfo volumeFailureInfo) {\n-    volumeFailureInfos.put(volumeFailureInfo.getFailedStorageLocation(),\n-        volumeFailureInfo);\n+    // There could be redundant requests for adding the same failed\n+    // volume because of repeated DataNode reconfigure with same list\n+    // of volumes. Ignoring update on failed volume so as to preserve\n+    // old failed capacity details in the map.\n+    if (!volumeFailureInfos.containsKey(volumeFailureInfo\n+        .getFailedStorageLocation())) {\n+      volumeFailureInfos.put(volumeFailureInfo.getFailedStorageLocation(),\n+          volumeFailureInfo);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void addVolumeFailureInfo(VolumeFailureInfo volumeFailureInfo) {\n    // There could be redundant requests for adding the same failed\n    // volume because of repeated DataNode reconfigure with same list\n    // of volumes. Ignoring update on failed volume so as to preserve\n    // old failed capacity details in the map.\n    if (!volumeFailureInfos.containsKey(volumeFailureInfo\n        .getFailedStorageLocation())) {\n      volumeFailureInfos.put(volumeFailureInfo.getFailedStorageLocation(),\n          volumeFailureInfo);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
      "extendedDetails": {}
    },
    "9729b244de50322c2cc889c97c2ffb2b4675cf77": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7604. Track and display failed DataNode storage locations in NameNode. Contributed by Chris Nauroth.\n",
      "commitDate": "16/02/15 2:43 PM",
      "commitName": "9729b244de50322c2cc889c97c2ffb2b4675cf77",
      "commitAuthor": "cnauroth",
      "diff": "@@ -0,0 +1,4 @@\n+  void addVolumeFailureInfo(VolumeFailureInfo volumeFailureInfo) {\n+    volumeFailureInfos.put(volumeFailureInfo.getFailedStorageLocation(),\n+        volumeFailureInfo);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void addVolumeFailureInfo(VolumeFailureInfo volumeFailureInfo) {\n    volumeFailureInfos.put(volumeFailureInfo.getFailedStorageLocation(),\n        volumeFailureInfo);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java"
    }
  }
}