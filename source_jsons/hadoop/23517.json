{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HistoryFileManager.java",
  "functionName": "scanIntermediateDirectory",
  "functionId": "scanIntermediateDirectory___absPath-Path(modifiers-final)",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
  "functionStartLine": 950,
  "functionEndLine": 1008,
  "numCommitsSeen": 57,
  "timeTaken": 8423,
  "changeHistory": [
    "5ffb54694b52657f3b7de4560474ab740734e1b2",
    "d777a1e4ca8e7cf0ce8967f79dd475468906c733",
    "0928502029ef141759008997335ea2cd836a7154",
    "64e4fb983e022d8d3375a3e1b8facbf95f7ba403",
    "7d04a96027ad75877b41b7cd8f67455dd13159d7",
    "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517"
  ],
  "changeHistoryShort": {
    "5ffb54694b52657f3b7de4560474ab740734e1b2": "Ybodychange",
    "d777a1e4ca8e7cf0ce8967f79dd475468906c733": "Ybodychange",
    "0928502029ef141759008997335ea2cd836a7154": "Ybodychange",
    "64e4fb983e022d8d3375a3e1b8facbf95f7ba403": "Ybodychange",
    "7d04a96027ad75877b41b7cd8f67455dd13159d7": "Ybodychange",
    "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f": "Ymovefromfile",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yintroduced"
  },
  "changeHistoryDetails": {
    "5ffb54694b52657f3b7de4560474ab740734e1b2": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6684. High contention on scanning of user directory under immediate_done in Job History Server. Contributed by Haibo Chen\n",
      "commitDate": "10/05/16 9:03 AM",
      "commitName": "5ffb54694b52657f3b7de4560474ab740734e1b2",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "20/04/16 7:02 PM",
      "commitNameOld": "1e48eefe5800975ea0c4295c9911ae3f572ed37d",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 19.58,
      "commitsBetweenForRepo": 113,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,58 @@\n   private void scanIntermediateDirectory(final Path absPath) throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Scanning intermediate dir \" + absPath);\n     }\n     List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n         intermediateDoneDirFc);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Found \" + fileStatusList.size() + \" files\");\n     }\n     for (FileStatus fs : fileStatusList) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"scanning file: \"+ fs.getPath());\n       }\n       JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n           .getName());\n       String confFileName \u003d JobHistoryUtils\n           .getIntermediateConfFileName(jobIndexInfo.getJobId());\n       String summaryFileName \u003d JobHistoryUtils\n           .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n-      HistoryFileInfo fileInfo \u003d new HistoryFileInfo(fs.getPath(), new Path(fs\n+      HistoryFileInfo fileInfo \u003d createHistoryFileInfo(fs.getPath(), new Path(fs\n           .getPath().getParent(), confFileName), new Path(fs.getPath()\n           .getParent(), summaryFileName), jobIndexInfo, false);\n \n       final HistoryFileInfo old \u003d jobListCache.addIfAbsent(fileInfo);\n       if (old \u003d\u003d null || old.didMoveFail()) {\n         final HistoryFileInfo found \u003d (old \u003d\u003d null) ? fileInfo : old;\n         long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n         if(found.getJobIndexInfo().getFinishTime() \u003c\u003d cutoff) {\n           try {\n             found.delete();\n           } catch (IOException e) {\n             LOG.warn(\"Error cleaning up a HistoryFile that is out of date.\", e);\n           }\n         } else {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Scheduling move to done of \" +found);\n           }\n           moveToDoneExecutor.execute(new Runnable() {\n             @Override\n             public void run() {\n               try {\n                 found.moveToDone();\n               } catch (IOException e) {\n                 LOG.info(\"Failed to process fileInfo for job: \" + \n                     found.getJobId(), e);\n               }\n             }\n           });\n         }\n       } else if (!old.isMovePending()) {\n         //This is a duplicate so just delete it\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Duplicate: deleting\");\n         }\n         fileInfo.delete();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void scanIntermediateDirectory(final Path absPath) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Scanning intermediate dir \" + absPath);\n    }\n    List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n        intermediateDoneDirFc);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Found \" + fileStatusList.size() + \" files\");\n    }\n    for (FileStatus fs : fileStatusList) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"scanning file: \"+ fs.getPath());\n      }\n      JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n          .getName());\n      String confFileName \u003d JobHistoryUtils\n          .getIntermediateConfFileName(jobIndexInfo.getJobId());\n      String summaryFileName \u003d JobHistoryUtils\n          .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n      HistoryFileInfo fileInfo \u003d createHistoryFileInfo(fs.getPath(), new Path(fs\n          .getPath().getParent(), confFileName), new Path(fs.getPath()\n          .getParent(), summaryFileName), jobIndexInfo, false);\n\n      final HistoryFileInfo old \u003d jobListCache.addIfAbsent(fileInfo);\n      if (old \u003d\u003d null || old.didMoveFail()) {\n        final HistoryFileInfo found \u003d (old \u003d\u003d null) ? fileInfo : old;\n        long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n        if(found.getJobIndexInfo().getFinishTime() \u003c\u003d cutoff) {\n          try {\n            found.delete();\n          } catch (IOException e) {\n            LOG.warn(\"Error cleaning up a HistoryFile that is out of date.\", e);\n          }\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Scheduling move to done of \" +found);\n          }\n          moveToDoneExecutor.execute(new Runnable() {\n            @Override\n            public void run() {\n              try {\n                found.moveToDone();\n              } catch (IOException e) {\n                LOG.info(\"Failed to process fileInfo for job: \" + \n                    found.getJobId(), e);\n              }\n            }\n          });\n        }\n      } else if (!old.isMovePending()) {\n        //This is a duplicate so just delete it\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Duplicate: deleting\");\n        }\n        fileInfo.delete();\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {}
    },
    "d777a1e4ca8e7cf0ce8967f79dd475468906c733": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11369. Fix new findbugs warnings in hadoop-mapreduce-client, non-core directories. Contributed by Li Lu.\n",
      "commitDate": "09/12/14 10:46 AM",
      "commitName": "d777a1e4ca8e7cf0ce8967f79dd475468906c733",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "16/01/14 9:10 AM",
      "commitNameOld": "3a6f8b878501cc6961a8388813f33bbeb5ebae34",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 327.07,
      "commitsBetweenForRepo": 2670,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,58 @@\n   private void scanIntermediateDirectory(final Path absPath) throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Scanning intermediate dir \" + absPath);\n     }\n     List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n         intermediateDoneDirFc);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Found \" + fileStatusList.size() + \" files\");\n     }\n     for (FileStatus fs : fileStatusList) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"scanning file: \"+ fs.getPath());\n       }\n       JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n           .getName());\n       String confFileName \u003d JobHistoryUtils\n           .getIntermediateConfFileName(jobIndexInfo.getJobId());\n       String summaryFileName \u003d JobHistoryUtils\n           .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n       HistoryFileInfo fileInfo \u003d new HistoryFileInfo(fs.getPath(), new Path(fs\n           .getPath().getParent(), confFileName), new Path(fs.getPath()\n           .getParent(), summaryFileName), jobIndexInfo, false);\n \n       final HistoryFileInfo old \u003d jobListCache.addIfAbsent(fileInfo);\n       if (old \u003d\u003d null || old.didMoveFail()) {\n         final HistoryFileInfo found \u003d (old \u003d\u003d null) ? fileInfo : old;\n         long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n         if(found.getJobIndexInfo().getFinishTime() \u003c\u003d cutoff) {\n           try {\n             found.delete();\n           } catch (IOException e) {\n             LOG.warn(\"Error cleaning up a HistoryFile that is out of date.\", e);\n           }\n         } else {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Scheduling move to done of \" +found);\n           }\n           moveToDoneExecutor.execute(new Runnable() {\n             @Override\n             public void run() {\n               try {\n                 found.moveToDone();\n               } catch (IOException e) {\n                 LOG.info(\"Failed to process fileInfo for job: \" + \n                     found.getJobId(), e);\n               }\n             }\n           });\n         }\n-      } else if (old !\u003d null \u0026\u0026 !old.isMovePending()) {\n+      } else if (!old.isMovePending()) {\n         //This is a duplicate so just delete it\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Duplicate: deleting\");\n         }\n         fileInfo.delete();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void scanIntermediateDirectory(final Path absPath) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Scanning intermediate dir \" + absPath);\n    }\n    List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n        intermediateDoneDirFc);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Found \" + fileStatusList.size() + \" files\");\n    }\n    for (FileStatus fs : fileStatusList) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"scanning file: \"+ fs.getPath());\n      }\n      JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n          .getName());\n      String confFileName \u003d JobHistoryUtils\n          .getIntermediateConfFileName(jobIndexInfo.getJobId());\n      String summaryFileName \u003d JobHistoryUtils\n          .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n      HistoryFileInfo fileInfo \u003d new HistoryFileInfo(fs.getPath(), new Path(fs\n          .getPath().getParent(), confFileName), new Path(fs.getPath()\n          .getParent(), summaryFileName), jobIndexInfo, false);\n\n      final HistoryFileInfo old \u003d jobListCache.addIfAbsent(fileInfo);\n      if (old \u003d\u003d null || old.didMoveFail()) {\n        final HistoryFileInfo found \u003d (old \u003d\u003d null) ? fileInfo : old;\n        long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n        if(found.getJobIndexInfo().getFinishTime() \u003c\u003d cutoff) {\n          try {\n            found.delete();\n          } catch (IOException e) {\n            LOG.warn(\"Error cleaning up a HistoryFile that is out of date.\", e);\n          }\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Scheduling move to done of \" +found);\n          }\n          moveToDoneExecutor.execute(new Runnable() {\n            @Override\n            public void run() {\n              try {\n                found.moveToDone();\n              } catch (IOException e) {\n                LOG.info(\"Failed to process fileInfo for job: \" + \n                    found.getJobId(), e);\n              }\n            }\n          });\n        }\n      } else if (!old.isMovePending()) {\n        //This is a duplicate so just delete it\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Duplicate: deleting\");\n        }\n        fileInfo.delete();\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {}
    },
    "0928502029ef141759008997335ea2cd836a7154": {
      "type": "Ybodychange",
      "commitMessage": "YARN-530. Defined Service model strictly, implemented AbstractService for robust subclassing and migrated yarn-common services. Contributed by Steve Loughran.\nYARN-117. Migrated rest of YARN to the new service model. Contributed by Steve Louhran.\nMAPREDUCE-5298. Moved MapReduce services to YARN-530 stricter lifecycle. Contributed by Steve Loughran.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1492718 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/06/13 8:54 AM",
      "commitName": "0928502029ef141759008997335ea2cd836a7154",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "03/06/13 9:05 PM",
      "commitNameOld": "a83fb61ac07c0468cbc7a38526e92683883dd932",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 9.49,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,58 @@\n   private void scanIntermediateDirectory(final Path absPath) throws IOException {\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Scanning intermediate dir \" + absPath);\n+    }\n     List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n         intermediateDoneDirFc);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Found \" + fileStatusList.size() + \" files\");\n+    }\n     for (FileStatus fs : fileStatusList) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"scanning file: \"+ fs.getPath());\n+      }\n       JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n           .getName());\n       String confFileName \u003d JobHistoryUtils\n           .getIntermediateConfFileName(jobIndexInfo.getJobId());\n       String summaryFileName \u003d JobHistoryUtils\n           .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n       HistoryFileInfo fileInfo \u003d new HistoryFileInfo(fs.getPath(), new Path(fs\n           .getPath().getParent(), confFileName), new Path(fs.getPath()\n           .getParent(), summaryFileName), jobIndexInfo, false);\n \n       final HistoryFileInfo old \u003d jobListCache.addIfAbsent(fileInfo);\n       if (old \u003d\u003d null || old.didMoveFail()) {\n         final HistoryFileInfo found \u003d (old \u003d\u003d null) ? fileInfo : old;\n         long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n         if(found.getJobIndexInfo().getFinishTime() \u003c\u003d cutoff) {\n           try {\n             found.delete();\n           } catch (IOException e) {\n             LOG.warn(\"Error cleaning up a HistoryFile that is out of date.\", e);\n           }\n         } else {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Scheduling move to done of \" +found);\n+          }\n           moveToDoneExecutor.execute(new Runnable() {\n             @Override\n             public void run() {\n               try {\n                 found.moveToDone();\n               } catch (IOException e) {\n                 LOG.info(\"Failed to process fileInfo for job: \" + \n                     found.getJobId(), e);\n               }\n             }\n           });\n         }\n       } else if (old !\u003d null \u0026\u0026 !old.isMovePending()) {\n         //This is a duplicate so just delete it\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Duplicate: deleting\");\n+        }\n         fileInfo.delete();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void scanIntermediateDirectory(final Path absPath) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Scanning intermediate dir \" + absPath);\n    }\n    List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n        intermediateDoneDirFc);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Found \" + fileStatusList.size() + \" files\");\n    }\n    for (FileStatus fs : fileStatusList) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"scanning file: \"+ fs.getPath());\n      }\n      JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n          .getName());\n      String confFileName \u003d JobHistoryUtils\n          .getIntermediateConfFileName(jobIndexInfo.getJobId());\n      String summaryFileName \u003d JobHistoryUtils\n          .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n      HistoryFileInfo fileInfo \u003d new HistoryFileInfo(fs.getPath(), new Path(fs\n          .getPath().getParent(), confFileName), new Path(fs.getPath()\n          .getParent(), summaryFileName), jobIndexInfo, false);\n\n      final HistoryFileInfo old \u003d jobListCache.addIfAbsent(fileInfo);\n      if (old \u003d\u003d null || old.didMoveFail()) {\n        final HistoryFileInfo found \u003d (old \u003d\u003d null) ? fileInfo : old;\n        long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n        if(found.getJobIndexInfo().getFinishTime() \u003c\u003d cutoff) {\n          try {\n            found.delete();\n          } catch (IOException e) {\n            LOG.warn(\"Error cleaning up a HistoryFile that is out of date.\", e);\n          }\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Scheduling move to done of \" +found);\n          }\n          moveToDoneExecutor.execute(new Runnable() {\n            @Override\n            public void run() {\n              try {\n                found.moveToDone();\n              } catch (IOException e) {\n                LOG.info(\"Failed to process fileInfo for job: \" + \n                    found.getJobId(), e);\n              }\n            }\n          });\n        }\n      } else if (old !\u003d null \u0026\u0026 !old.isMovePending()) {\n        //This is a duplicate so just delete it\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Duplicate: deleting\");\n        }\n        fileInfo.delete();\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {}
    },
    "64e4fb983e022d8d3375a3e1b8facbf95f7ba403": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4819. AM can rerun job after reporting final job status to the client (bobby and Bikas Saha via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1429114 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/01/13 12:35 PM",
      "commitName": "64e4fb983e022d8d3375a3e1b8facbf95f7ba403",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "14/11/12 4:16 PM",
      "commitNameOld": "905b17876c44634545a68300ff2f2d73fb86d3b7",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 50.85,
      "commitsBetweenForRepo": 182,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,43 @@\n   private void scanIntermediateDirectory(final Path absPath) throws IOException {\n     List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n         intermediateDoneDirFc);\n     for (FileStatus fs : fileStatusList) {\n       JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n           .getName());\n       String confFileName \u003d JobHistoryUtils\n           .getIntermediateConfFileName(jobIndexInfo.getJobId());\n       String summaryFileName \u003d JobHistoryUtils\n           .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n       HistoryFileInfo fileInfo \u003d new HistoryFileInfo(fs.getPath(), new Path(fs\n           .getPath().getParent(), confFileName), new Path(fs.getPath()\n           .getParent(), summaryFileName), jobIndexInfo, false);\n \n       final HistoryFileInfo old \u003d jobListCache.addIfAbsent(fileInfo);\n       if (old \u003d\u003d null || old.didMoveFail()) {\n         final HistoryFileInfo found \u003d (old \u003d\u003d null) ? fileInfo : old;\n         long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n         if(found.getJobIndexInfo().getFinishTime() \u003c\u003d cutoff) {\n           try {\n             found.delete();\n           } catch (IOException e) {\n             LOG.warn(\"Error cleaning up a HistoryFile that is out of date.\", e);\n           }\n         } else {\n           moveToDoneExecutor.execute(new Runnable() {\n             @Override\n             public void run() {\n               try {\n                 found.moveToDone();\n               } catch (IOException e) {\n                 LOG.info(\"Failed to process fileInfo for job: \" + \n                     found.getJobId(), e);\n               }\n             }\n           });\n         }\n+      } else if (old !\u003d null \u0026\u0026 !old.isMovePending()) {\n+        //This is a duplicate so just delete it\n+        fileInfo.delete();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void scanIntermediateDirectory(final Path absPath) throws IOException {\n    List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n        intermediateDoneDirFc);\n    for (FileStatus fs : fileStatusList) {\n      JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n          .getName());\n      String confFileName \u003d JobHistoryUtils\n          .getIntermediateConfFileName(jobIndexInfo.getJobId());\n      String summaryFileName \u003d JobHistoryUtils\n          .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n      HistoryFileInfo fileInfo \u003d new HistoryFileInfo(fs.getPath(), new Path(fs\n          .getPath().getParent(), confFileName), new Path(fs.getPath()\n          .getParent(), summaryFileName), jobIndexInfo, false);\n\n      final HistoryFileInfo old \u003d jobListCache.addIfAbsent(fileInfo);\n      if (old \u003d\u003d null || old.didMoveFail()) {\n        final HistoryFileInfo found \u003d (old \u003d\u003d null) ? fileInfo : old;\n        long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n        if(found.getJobIndexInfo().getFinishTime() \u003c\u003d cutoff) {\n          try {\n            found.delete();\n          } catch (IOException e) {\n            LOG.warn(\"Error cleaning up a HistoryFile that is out of date.\", e);\n          }\n        } else {\n          moveToDoneExecutor.execute(new Runnable() {\n            @Override\n            public void run() {\n              try {\n                found.moveToDone();\n              } catch (IOException e) {\n                LOG.info(\"Failed to process fileInfo for job: \" + \n                    found.getJobId(), e);\n              }\n            }\n          });\n        }\n      } else if (old !\u003d null \u0026\u0026 !old.isMovePending()) {\n        //This is a duplicate so just delete it\n        fileInfo.delete();\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {}
    },
    "7d04a96027ad75877b41b7cd8f67455dd13159d7": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3972. Fix locking and exception issues in JobHistory server. (Contributed by Robert Joseph Evans)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1327354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/04/12 6:59 PM",
      "commitName": "7d04a96027ad75877b41b7cd8f67455dd13159d7",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "10/04/12 11:11 AM",
      "commitNameOld": "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 7.32,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,40 @@\n   private void scanIntermediateDirectory(final Path absPath) throws IOException {\n     List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n         intermediateDoneDirFc);\n     for (FileStatus fs : fileStatusList) {\n       JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n           .getName());\n       String confFileName \u003d JobHistoryUtils\n           .getIntermediateConfFileName(jobIndexInfo.getJobId());\n       String summaryFileName \u003d JobHistoryUtils\n           .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n-      MetaInfo metaInfo \u003d new MetaInfo(fs.getPath(), new Path(fs.getPath()\n-          .getParent(), confFileName), new Path(fs.getPath().getParent(),\n-          summaryFileName), jobIndexInfo);\n-      if (!intermediateListCache.containsKey(jobIndexInfo.getJobId())) {\n-        intermediateListCache.put(jobIndexInfo.getJobId(), metaInfo);\n+      HistoryFileInfo fileInfo \u003d new HistoryFileInfo(fs.getPath(), new Path(fs\n+          .getPath().getParent(), confFileName), new Path(fs.getPath()\n+          .getParent(), summaryFileName), jobIndexInfo, false);\n+\n+      final HistoryFileInfo old \u003d jobListCache.addIfAbsent(fileInfo);\n+      if (old \u003d\u003d null || old.didMoveFail()) {\n+        final HistoryFileInfo found \u003d (old \u003d\u003d null) ? fileInfo : old;\n+        long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n+        if(found.getJobIndexInfo().getFinishTime() \u003c\u003d cutoff) {\n+          try {\n+            found.delete();\n+          } catch (IOException e) {\n+            LOG.warn(\"Error cleaning up a HistoryFile that is out of date.\", e);\n+          }\n+        } else {\n+          moveToDoneExecutor.execute(new Runnable() {\n+            @Override\n+            public void run() {\n+              try {\n+                found.moveToDone();\n+              } catch (IOException e) {\n+                LOG.info(\"Failed to process fileInfo for job: \" + \n+                    found.getJobId(), e);\n+              }\n+            }\n+          });\n+        }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void scanIntermediateDirectory(final Path absPath) throws IOException {\n    List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n        intermediateDoneDirFc);\n    for (FileStatus fs : fileStatusList) {\n      JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n          .getName());\n      String confFileName \u003d JobHistoryUtils\n          .getIntermediateConfFileName(jobIndexInfo.getJobId());\n      String summaryFileName \u003d JobHistoryUtils\n          .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n      HistoryFileInfo fileInfo \u003d new HistoryFileInfo(fs.getPath(), new Path(fs\n          .getPath().getParent(), confFileName), new Path(fs.getPath()\n          .getParent(), summaryFileName), jobIndexInfo, false);\n\n      final HistoryFileInfo old \u003d jobListCache.addIfAbsent(fileInfo);\n      if (old \u003d\u003d null || old.didMoveFail()) {\n        final HistoryFileInfo found \u003d (old \u003d\u003d null) ? fileInfo : old;\n        long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n        if(found.getJobIndexInfo().getFinishTime() \u003c\u003d cutoff) {\n          try {\n            found.delete();\n          } catch (IOException e) {\n            LOG.warn(\"Error cleaning up a HistoryFile that is out of date.\", e);\n          }\n        } else {\n          moveToDoneExecutor.execute(new Runnable() {\n            @Override\n            public void run() {\n              try {\n                found.moveToDone();\n              } catch (IOException e) {\n                LOG.info(\"Failed to process fileInfo for job: \" + \n                    found.getJobId(), e);\n              }\n            }\n          });\n        }\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {}
    },
    "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-4059. The history server should have a separate pluggable storage/query interface. (Robert Evans via tgraves).\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1311896 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/04/12 11:11 AM",
      "commitName": "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "10/04/12 9:13 AM",
      "commitNameOld": "000859a534f4cc6a57524a676805d8af6ad199de",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,18 @@\n-  private void scanIntermediateDirectory(final Path absPath)\n-      throws IOException {\n+  private void scanIntermediateDirectory(final Path absPath) throws IOException {\n     List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n         intermediateDoneDirFc);\n     for (FileStatus fs : fileStatusList) {\n       JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n           .getName());\n       String confFileName \u003d JobHistoryUtils\n           .getIntermediateConfFileName(jobIndexInfo.getJobId());\n       String summaryFileName \u003d JobHistoryUtils\n           .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n       MetaInfo metaInfo \u003d new MetaInfo(fs.getPath(), new Path(fs.getPath()\n           .getParent(), confFileName), new Path(fs.getPath().getParent(),\n           summaryFileName), jobIndexInfo);\n       if (!intermediateListCache.containsKey(jobIndexInfo.getJobId())) {\n         intermediateListCache.put(jobIndexInfo.getJobId(), metaInfo);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void scanIntermediateDirectory(final Path absPath) throws IOException {\n    List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n        intermediateDoneDirFc);\n    for (FileStatus fs : fileStatusList) {\n      JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n          .getName());\n      String confFileName \u003d JobHistoryUtils\n          .getIntermediateConfFileName(jobIndexInfo.getJobId());\n      String summaryFileName \u003d JobHistoryUtils\n          .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n      MetaInfo metaInfo \u003d new MetaInfo(fs.getPath(), new Path(fs.getPath()\n          .getParent(), confFileName), new Path(fs.getPath().getParent(),\n          summaryFileName), jobIndexInfo);\n      if (!intermediateListCache.containsKey(jobIndexInfo.getJobId())) {\n        intermediateListCache.put(jobIndexInfo.getJobId(), metaInfo);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
        "oldMethodName": "scanIntermediateDirectory",
        "newMethodName": "scanIntermediateDirectory"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void scanIntermediateDirectory(final Path absPath)\n      throws IOException {\n    List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n        intermediateDoneDirFc);\n    for (FileStatus fs : fileStatusList) {\n      JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n          .getName());\n      String confFileName \u003d JobHistoryUtils\n          .getIntermediateConfFileName(jobIndexInfo.getJobId());\n      String summaryFileName \u003d JobHistoryUtils\n          .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n      MetaInfo metaInfo \u003d new MetaInfo(fs.getPath(), new Path(fs.getPath()\n          .getParent(), confFileName), new Path(fs.getPath().getParent(),\n          summaryFileName), jobIndexInfo);\n      if (!intermediateListCache.containsKey(jobIndexInfo.getJobId())) {\n        intermediateListCache.put(jobIndexInfo.getJobId(), metaInfo);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,19 @@\n+  private void scanIntermediateDirectory(final Path absPath)\n+      throws IOException {\n+    List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n+        intermediateDoneDirFc);\n+    for (FileStatus fs : fileStatusList) {\n+      JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n+          .getName());\n+      String confFileName \u003d JobHistoryUtils\n+          .getIntermediateConfFileName(jobIndexInfo.getJobId());\n+      String summaryFileName \u003d JobHistoryUtils\n+          .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n+      MetaInfo metaInfo \u003d new MetaInfo(fs.getPath(), new Path(fs.getPath()\n+          .getParent(), confFileName), new Path(fs.getPath().getParent(),\n+          summaryFileName), jobIndexInfo);\n+      if (!intermediateListCache.containsKey(jobIndexInfo.getJobId())) {\n+        intermediateListCache.put(jobIndexInfo.getJobId(), metaInfo);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void scanIntermediateDirectory(final Path absPath)\n      throws IOException {\n    List\u003cFileStatus\u003e fileStatusList \u003d scanDirectoryForHistoryFiles(absPath,\n        intermediateDoneDirFc);\n    for (FileStatus fs : fileStatusList) {\n      JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(fs.getPath()\n          .getName());\n      String confFileName \u003d JobHistoryUtils\n          .getIntermediateConfFileName(jobIndexInfo.getJobId());\n      String summaryFileName \u003d JobHistoryUtils\n          .getIntermediateSummaryFileName(jobIndexInfo.getJobId());\n      MetaInfo metaInfo \u003d new MetaInfo(fs.getPath(), new Path(fs.getPath()\n          .getParent(), confFileName), new Path(fs.getPath().getParent(),\n          summaryFileName), jobIndexInfo);\n      if (!intermediateListCache.containsKey(jobIndexInfo.getJobId())) {\n        intermediateListCache.put(jobIndexInfo.getJobId(), metaInfo);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java"
    }
  }
}