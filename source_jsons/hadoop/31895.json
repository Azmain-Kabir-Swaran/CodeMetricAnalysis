{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FlowScanner.java",
  "functionName": "nextInternal",
  "functionId": "nextInternal___cells-List__Cell____scannerContext-ScannerContext",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-1/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
  "functionStartLine": 185,
  "functionEndLine": 245,
  "numCommitsSeen": 35,
  "timeTaken": 6347,
  "changeHistory": [
    "55ba49dd071b66e72c47a1c41e88b9a5feddf53b",
    "10663b78c8596693322dc3636f173035195bf607",
    "5e37ca5bb49f945e27f49a413d08baab562dfa9c",
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2",
    "b01514f65bc6090a50a583f67d1ecb5d74b6d276",
    "1a227744ac0ceff178171fc4ddbf3d27275bdc4f",
    "7b8cfa5c2ff62005c8b78867fedd64b48e50383d",
    "69dc561b61bf694cfdf0d2059f4f3dcee30e0632",
    "9bdd455dced15c84430ea0a0a59410df924f02a7",
    "960af7d4717b8a8949d0b2e43949e7daab45aa88",
    "51254a6b5133c8abfec4b7d2ac9477d112b3ccfa",
    "b51d0fef56a59b15489f5b932025718b4e9613d2",
    "a68e3839218523403f42acd7bdd7ce1da59a5e60"
  ],
  "changeHistoryShort": {
    "55ba49dd071b66e72c47a1c41e88b9a5feddf53b": "Yfilerename",
    "10663b78c8596693322dc3636f173035195bf607": "Yfilerename",
    "5e37ca5bb49f945e27f49a413d08baab562dfa9c": "Yfilerename",
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2": "Yfilerename",
    "b01514f65bc6090a50a583f67d1ecb5d74b6d276": "Yfilerename",
    "1a227744ac0ceff178171fc4ddbf3d27275bdc4f": "Ymultichange(Yparameterchange,Ybodychange)",
    "7b8cfa5c2ff62005c8b78867fedd64b48e50383d": "Ybodychange",
    "69dc561b61bf694cfdf0d2059f4f3dcee30e0632": "Ybodychange",
    "9bdd455dced15c84430ea0a0a59410df924f02a7": "Ybodychange",
    "960af7d4717b8a8949d0b2e43949e7daab45aa88": "Ymultichange(Yparameterchange,Ybodychange)",
    "51254a6b5133c8abfec4b7d2ac9477d112b3ccfa": "Ybodychange",
    "b51d0fef56a59b15489f5b932025718b4e9613d2": "Ybodychange",
    "a68e3839218523403f42acd7bdd7ce1da59a5e60": "Yintroduced"
  },
  "changeHistoryDetails": {
    "55ba49dd071b66e72c47a1c41e88b9a5feddf53b": {
      "type": "Yfilerename",
      "commitMessage": "YARN-7346. Add a profile to allow optional compilation for ATSv2 with HBase-2.0. Contributed by Haibo Chen and Rohith.\n",
      "commitDate": "05/03/18 10:25 PM",
      "commitName": "55ba49dd071b66e72c47a1c41e88b9a5feddf53b",
      "commitAuthor": "Rohith Sharma K S",
      "commitDateOld": "05/03/18 6:15 PM",
      "commitNameOld": "745190ecdca8f7dfc5eebffdd1c1aa4f86229120",
      "commitAuthorOld": "Takanobu Asanuma",
      "daysBetweenCommits": 0.17,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, ScannerContext scannerContext)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] previousColumnQualifier \u003d Separator.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    long currentTimestamp \u003d System.currentTimeMillis();\n    ValueConverter converter \u003d null;\n    int limit \u003d batchSize;\n\n    while (limit \u003c\u003d 0 || addedCnt \u003c limit) {\n      cell \u003d peekAtNextCell(scannerContext);\n      if (cell \u003d\u003d null) {\n        break;\n      }\n      byte[] currentColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (previousColumnQualifier \u003d\u003d null) {\n        // first time in loop\n        previousColumnQualifier \u003d currentColumnQualifier;\n      }\n\n      converter \u003d getValueConverter(currentColumnQualifier);\n      if (comp.compare(previousColumnQualifier, currentColumnQualifier) !\u003d 0) {\n        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n            converter, currentTimestamp);\n        resetState(currentColumnCells, alreadySeenAggDim);\n        previousColumnQualifier \u003d currentColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(currentColumnQualifier);\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          converter, scannerContext);\n      nextCell(scannerContext);\n    }\n    if ((!currentColumnCells.isEmpty()) \u0026\u0026 ((limit \u003c\u003d 0 || addedCnt \u003c limit))) {\n      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp, converter,\n          currentTimestamp);\n      if (LOG.isDebugEnabled()) {\n        if (addedCnt \u003e 0) {\n          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n              + \" rowKey\u003d\"\n              + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));\n        } else {\n          LOG.debug(\"emitted no cells for \" + this.action);\n        }\n      }\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-1/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-1/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java"
      }
    },
    "10663b78c8596693322dc3636f173035195bf607": {
      "type": "Yfilerename",
      "commitMessage": "Revert \"yarn-7346.07.patch\"\n\nThis reverts commit 5e37ca5bb49f945e27f49a413d08baab562dfa9c.\n",
      "commitDate": "28/02/18 9:11 PM",
      "commitName": "10663b78c8596693322dc3636f173035195bf607",
      "commitAuthor": "Haibo Chen",
      "commitDateOld": "28/02/18 9:10 PM",
      "commitNameOld": "d1274c3b71549cb000868500c293cafd880b3713",
      "commitAuthorOld": "Haibo Chen",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, ScannerContext scannerContext)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] previousColumnQualifier \u003d Separator.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    long currentTimestamp \u003d System.currentTimeMillis();\n    ValueConverter converter \u003d null;\n    int limit \u003d batchSize;\n\n    while (limit \u003c\u003d 0 || addedCnt \u003c limit) {\n      cell \u003d peekAtNextCell(scannerContext);\n      if (cell \u003d\u003d null) {\n        break;\n      }\n      byte[] currentColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (previousColumnQualifier \u003d\u003d null) {\n        // first time in loop\n        previousColumnQualifier \u003d currentColumnQualifier;\n      }\n\n      converter \u003d getValueConverter(currentColumnQualifier);\n      if (comp.compare(previousColumnQualifier, currentColumnQualifier) !\u003d 0) {\n        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n            converter, currentTimestamp);\n        resetState(currentColumnCells, alreadySeenAggDim);\n        previousColumnQualifier \u003d currentColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(currentColumnQualifier);\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          converter, scannerContext);\n      nextCell(scannerContext);\n    }\n    if ((!currentColumnCells.isEmpty()) \u0026\u0026 ((limit \u003c\u003d 0 || addedCnt \u003c limit))) {\n      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp, converter,\n          currentTimestamp);\n      if (LOG.isDebugEnabled()) {\n        if (addedCnt \u003e 0) {\n          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n              + \" rowKey\u003d\"\n              + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));\n        } else {\n          LOG.debug(\"emitted no cells for \" + this.action);\n        }\n      }\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-1/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java"
      }
    },
    "5e37ca5bb49f945e27f49a413d08baab562dfa9c": {
      "type": "Yfilerename",
      "commitMessage": "yarn-7346.07.patch\n",
      "commitDate": "28/02/18 9:04 PM",
      "commitName": "5e37ca5bb49f945e27f49a413d08baab562dfa9c",
      "commitAuthor": "Haibo Chen",
      "commitDateOld": "28/02/18 6:18 PM",
      "commitNameOld": "6e6945cd78d76c6beaec85c963f27e28bf96c0f2",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 0.12,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, ScannerContext scannerContext)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] previousColumnQualifier \u003d Separator.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    long currentTimestamp \u003d System.currentTimeMillis();\n    ValueConverter converter \u003d null;\n    int limit \u003d batchSize;\n\n    while (limit \u003c\u003d 0 || addedCnt \u003c limit) {\n      cell \u003d peekAtNextCell(scannerContext);\n      if (cell \u003d\u003d null) {\n        break;\n      }\n      byte[] currentColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (previousColumnQualifier \u003d\u003d null) {\n        // first time in loop\n        previousColumnQualifier \u003d currentColumnQualifier;\n      }\n\n      converter \u003d getValueConverter(currentColumnQualifier);\n      if (comp.compare(previousColumnQualifier, currentColumnQualifier) !\u003d 0) {\n        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n            converter, currentTimestamp);\n        resetState(currentColumnCells, alreadySeenAggDim);\n        previousColumnQualifier \u003d currentColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(currentColumnQualifier);\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          converter, scannerContext);\n      nextCell(scannerContext);\n    }\n    if ((!currentColumnCells.isEmpty()) \u0026\u0026 ((limit \u003c\u003d 0 || addedCnt \u003c limit))) {\n      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp, converter,\n          currentTimestamp);\n      if (LOG.isDebugEnabled()) {\n        if (addedCnt \u003e 0) {\n          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n              + \" rowKey\u003d\"\n              + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));\n        } else {\n          LOG.debug(\"emitted no cells for \" + this.action);\n        }\n      }\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-1/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-1/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java"
      }
    },
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2": {
      "type": "Yfilerename",
      "commitMessage": "YARN-7919. Refactor timelineservice-hbase module into submodules. Contributed by Haibo Chen.\n",
      "commitDate": "17/02/18 7:00 AM",
      "commitName": "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2",
      "commitAuthor": "Rohith Sharma K S",
      "commitDateOld": "17/02/18 3:24 AM",
      "commitNameOld": "a1e56a62863d8d494af309ec5f476c4b7e4d5ef9",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 0.15,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, ScannerContext scannerContext)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] previousColumnQualifier \u003d Separator.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    long currentTimestamp \u003d System.currentTimeMillis();\n    ValueConverter converter \u003d null;\n    int limit \u003d batchSize;\n\n    while (limit \u003c\u003d 0 || addedCnt \u003c limit) {\n      cell \u003d peekAtNextCell(scannerContext);\n      if (cell \u003d\u003d null) {\n        break;\n      }\n      byte[] currentColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (previousColumnQualifier \u003d\u003d null) {\n        // first time in loop\n        previousColumnQualifier \u003d currentColumnQualifier;\n      }\n\n      converter \u003d getValueConverter(currentColumnQualifier);\n      if (comp.compare(previousColumnQualifier, currentColumnQualifier) !\u003d 0) {\n        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n            converter, currentTimestamp);\n        resetState(currentColumnCells, alreadySeenAggDim);\n        previousColumnQualifier \u003d currentColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(currentColumnQualifier);\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          converter, scannerContext);\n      nextCell(scannerContext);\n    }\n    if ((!currentColumnCells.isEmpty()) \u0026\u0026 ((limit \u003c\u003d 0 || addedCnt \u003c limit))) {\n      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp, converter,\n          currentTimestamp);\n      if (LOG.isDebugEnabled()) {\n        if (addedCnt \u003e 0) {\n          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n              + \" rowKey\u003d\"\n              + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));\n        } else {\n          LOG.debug(\"emitted no cells for \" + this.action);\n        }\n      }\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java"
      }
    },
    "b01514f65bc6090a50a583f67d1ecb5d74b6d276": {
      "type": "Yfilerename",
      "commitMessage": "YARN-5928. Move ATSv2 HBase backend code into a new module that is only dependent at runtime by yarn servers. Contributed by Haibo Chen.\n",
      "commitDate": "19/01/17 8:52 PM",
      "commitName": "b01514f65bc6090a50a583f67d1ecb5d74b6d276",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "19/01/17 5:32 PM",
      "commitNameOld": "60865c8ea08053f3d6ac23f81c3376a3de3ca996",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.14,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, ScannerContext scannerContext)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] previousColumnQualifier \u003d Separator.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    long currentTimestamp \u003d System.currentTimeMillis();\n    ValueConverter converter \u003d null;\n    int limit \u003d batchSize;\n\n    while (limit \u003c\u003d 0 || addedCnt \u003c limit) {\n      cell \u003d peekAtNextCell(scannerContext);\n      if (cell \u003d\u003d null) {\n        break;\n      }\n      byte[] currentColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (previousColumnQualifier \u003d\u003d null) {\n        // first time in loop\n        previousColumnQualifier \u003d currentColumnQualifier;\n      }\n\n      converter \u003d getValueConverter(currentColumnQualifier);\n      if (comp.compare(previousColumnQualifier, currentColumnQualifier) !\u003d 0) {\n        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n            converter, currentTimestamp);\n        resetState(currentColumnCells, alreadySeenAggDim);\n        previousColumnQualifier \u003d currentColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(currentColumnQualifier);\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          converter, scannerContext);\n      nextCell(scannerContext);\n    }\n    if ((!currentColumnCells.isEmpty()) \u0026\u0026 ((limit \u003c\u003d 0 || addedCnt \u003c limit))) {\n      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp, converter,\n          currentTimestamp);\n      if (LOG.isDebugEnabled()) {\n        if (addedCnt \u003e 0) {\n          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n              + \" rowKey\u003d\"\n              + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));\n        } else {\n          LOG.debug(\"emitted no cells for \" + this.action);\n        }\n      }\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java"
      }
    },
    "1a227744ac0ceff178171fc4ddbf3d27275bdc4f": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-5070. upgrade HBase version for first merge (Vrushali C via sjlee)\n",
      "commitDate": "10/07/16 8:46 AM",
      "commitName": "1a227744ac0ceff178171fc4ddbf3d27275bdc4f",
      "commitAuthor": "Sangjin Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5070. upgrade HBase version for first merge (Vrushali C via sjlee)\n",
          "commitDate": "10/07/16 8:46 AM",
          "commitName": "1a227744ac0ceff178171fc4ddbf3d27275bdc4f",
          "commitAuthor": "Sangjin Lee",
          "commitDateOld": "10/07/16 8:46 AM",
          "commitNameOld": "7b8cfa5c2ff62005c8b78867fedd64b48e50383d",
          "commitAuthorOld": "Sangjin Lee",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 10,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,62 +1,61 @@\n-  private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n+  private boolean nextInternal(List\u003cCell\u003e cells, ScannerContext scannerContext)\n       throws IOException {\n     Cell cell \u003d null;\n     startNext();\n     // Loop through all the cells in this row\n     // For min/max/metrics we do need to scan the entire set of cells to get the\n     // right one\n     // But with flush/compaction, the number of cells being scanned will go down\n     // cells are grouped per column qualifier then sorted by cell timestamp\n     // (latest to oldest) per column qualifier\n     // So all cells in one qualifier come one after the other before we see the\n     // next column qualifier\n     ByteArrayComparator comp \u003d new ByteArrayComparator();\n-    byte[] currentColumnQualifier \u003d Separator.EMPTY_BYTES;\n+    byte[] previousColumnQualifier \u003d Separator.EMPTY_BYTES;\n     AggregationOperation currentAggOp \u003d null;\n     SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n     Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n     int addedCnt \u003d 0;\n     long currentTimestamp \u003d System.currentTimeMillis();\n     ValueConverter converter \u003d null;\n+    int limit \u003d batchSize;\n \n-    while (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit) {\n-      cell \u003d peekAtNextCell(cellLimit);\n+    while (limit \u003c\u003d 0 || addedCnt \u003c limit) {\n+      cell \u003d peekAtNextCell(scannerContext);\n       if (cell \u003d\u003d null) {\n         break;\n       }\n-      byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n-      if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n-        if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n-          addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n-              converter, currentTimestamp);\n-        }\n-        resetState(currentColumnCells, alreadySeenAggDim);\n-        currentColumnQualifier \u003d newColumnQualifier;\n-        currentAggOp \u003d getCurrentAggOp(cell);\n-        converter \u003d getValueConverter(newColumnQualifier);\n+      byte[] currentColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n+      if (previousColumnQualifier \u003d\u003d null) {\n+        // first time in loop\n+        previousColumnQualifier \u003d currentColumnQualifier;\n       }\n-      // No operation needs to be performed on non numeric converters.\n-      if (!isNumericConverter(converter)) {\n-        currentColumnCells.add(cell);\n-        nextCell(cellLimit);\n-        continue;\n+\n+      converter \u003d getValueConverter(currentColumnQualifier);\n+      if (comp.compare(previousColumnQualifier, currentColumnQualifier) !\u003d 0) {\n+        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n+            converter, currentTimestamp);\n+        resetState(currentColumnCells, alreadySeenAggDim);\n+        previousColumnQualifier \u003d currentColumnQualifier;\n+        currentAggOp \u003d getCurrentAggOp(cell);\n+        converter \u003d getValueConverter(currentColumnQualifier);\n       }\n       collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n-          (NumericValueConverter)converter);\n-      nextCell(cellLimit);\n+          converter, scannerContext);\n+      nextCell(scannerContext);\n     }\n-    if (!currentColumnCells.isEmpty()) {\n-      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n-          converter, currentTimestamp);\n+    if ((!currentColumnCells.isEmpty()) \u0026\u0026 ((limit \u003c\u003d 0 || addedCnt \u003c limit))) {\n+      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp, converter,\n+          currentTimestamp);\n       if (LOG.isDebugEnabled()) {\n         if (addedCnt \u003e 0) {\n           LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n               + \" rowKey\u003d\"\n-              + FlowRunRowKey.parseRowKey(cells.get(0).getRow()).toString());\n+              + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));\n         } else {\n           LOG.debug(\"emitted no cells for \" + this.action);\n         }\n       }\n     }\n     return hasMore();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, ScannerContext scannerContext)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] previousColumnQualifier \u003d Separator.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    long currentTimestamp \u003d System.currentTimeMillis();\n    ValueConverter converter \u003d null;\n    int limit \u003d batchSize;\n\n    while (limit \u003c\u003d 0 || addedCnt \u003c limit) {\n      cell \u003d peekAtNextCell(scannerContext);\n      if (cell \u003d\u003d null) {\n        break;\n      }\n      byte[] currentColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (previousColumnQualifier \u003d\u003d null) {\n        // first time in loop\n        previousColumnQualifier \u003d currentColumnQualifier;\n      }\n\n      converter \u003d getValueConverter(currentColumnQualifier);\n      if (comp.compare(previousColumnQualifier, currentColumnQualifier) !\u003d 0) {\n        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n            converter, currentTimestamp);\n        resetState(currentColumnCells, alreadySeenAggDim);\n        previousColumnQualifier \u003d currentColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(currentColumnQualifier);\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          converter, scannerContext);\n      nextCell(scannerContext);\n    }\n    if ((!currentColumnCells.isEmpty()) \u0026\u0026 ((limit \u003c\u003d 0 || addedCnt \u003c limit))) {\n      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp, converter,\n          currentTimestamp);\n      if (LOG.isDebugEnabled()) {\n        if (addedCnt \u003e 0) {\n          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n              + \" rowKey\u003d\"\n              + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));\n        } else {\n          LOG.debug(\"emitted no cells for \" + this.action);\n        }\n      }\n    }\n    return hasMore();\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
          "extendedDetails": {
            "oldValue": "[cells-List\u003cCell\u003e, cellLimit-int]",
            "newValue": "[cells-List\u003cCell\u003e, scannerContext-ScannerContext]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5070. upgrade HBase version for first merge (Vrushali C via sjlee)\n",
          "commitDate": "10/07/16 8:46 AM",
          "commitName": "1a227744ac0ceff178171fc4ddbf3d27275bdc4f",
          "commitAuthor": "Sangjin Lee",
          "commitDateOld": "10/07/16 8:46 AM",
          "commitNameOld": "7b8cfa5c2ff62005c8b78867fedd64b48e50383d",
          "commitAuthorOld": "Sangjin Lee",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 10,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,62 +1,61 @@\n-  private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n+  private boolean nextInternal(List\u003cCell\u003e cells, ScannerContext scannerContext)\n       throws IOException {\n     Cell cell \u003d null;\n     startNext();\n     // Loop through all the cells in this row\n     // For min/max/metrics we do need to scan the entire set of cells to get the\n     // right one\n     // But with flush/compaction, the number of cells being scanned will go down\n     // cells are grouped per column qualifier then sorted by cell timestamp\n     // (latest to oldest) per column qualifier\n     // So all cells in one qualifier come one after the other before we see the\n     // next column qualifier\n     ByteArrayComparator comp \u003d new ByteArrayComparator();\n-    byte[] currentColumnQualifier \u003d Separator.EMPTY_BYTES;\n+    byte[] previousColumnQualifier \u003d Separator.EMPTY_BYTES;\n     AggregationOperation currentAggOp \u003d null;\n     SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n     Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n     int addedCnt \u003d 0;\n     long currentTimestamp \u003d System.currentTimeMillis();\n     ValueConverter converter \u003d null;\n+    int limit \u003d batchSize;\n \n-    while (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit) {\n-      cell \u003d peekAtNextCell(cellLimit);\n+    while (limit \u003c\u003d 0 || addedCnt \u003c limit) {\n+      cell \u003d peekAtNextCell(scannerContext);\n       if (cell \u003d\u003d null) {\n         break;\n       }\n-      byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n-      if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n-        if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n-          addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n-              converter, currentTimestamp);\n-        }\n-        resetState(currentColumnCells, alreadySeenAggDim);\n-        currentColumnQualifier \u003d newColumnQualifier;\n-        currentAggOp \u003d getCurrentAggOp(cell);\n-        converter \u003d getValueConverter(newColumnQualifier);\n+      byte[] currentColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n+      if (previousColumnQualifier \u003d\u003d null) {\n+        // first time in loop\n+        previousColumnQualifier \u003d currentColumnQualifier;\n       }\n-      // No operation needs to be performed on non numeric converters.\n-      if (!isNumericConverter(converter)) {\n-        currentColumnCells.add(cell);\n-        nextCell(cellLimit);\n-        continue;\n+\n+      converter \u003d getValueConverter(currentColumnQualifier);\n+      if (comp.compare(previousColumnQualifier, currentColumnQualifier) !\u003d 0) {\n+        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n+            converter, currentTimestamp);\n+        resetState(currentColumnCells, alreadySeenAggDim);\n+        previousColumnQualifier \u003d currentColumnQualifier;\n+        currentAggOp \u003d getCurrentAggOp(cell);\n+        converter \u003d getValueConverter(currentColumnQualifier);\n       }\n       collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n-          (NumericValueConverter)converter);\n-      nextCell(cellLimit);\n+          converter, scannerContext);\n+      nextCell(scannerContext);\n     }\n-    if (!currentColumnCells.isEmpty()) {\n-      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n-          converter, currentTimestamp);\n+    if ((!currentColumnCells.isEmpty()) \u0026\u0026 ((limit \u003c\u003d 0 || addedCnt \u003c limit))) {\n+      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp, converter,\n+          currentTimestamp);\n       if (LOG.isDebugEnabled()) {\n         if (addedCnt \u003e 0) {\n           LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n               + \" rowKey\u003d\"\n-              + FlowRunRowKey.parseRowKey(cells.get(0).getRow()).toString());\n+              + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));\n         } else {\n           LOG.debug(\"emitted no cells for \" + this.action);\n         }\n       }\n     }\n     return hasMore();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, ScannerContext scannerContext)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] previousColumnQualifier \u003d Separator.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    long currentTimestamp \u003d System.currentTimeMillis();\n    ValueConverter converter \u003d null;\n    int limit \u003d batchSize;\n\n    while (limit \u003c\u003d 0 || addedCnt \u003c limit) {\n      cell \u003d peekAtNextCell(scannerContext);\n      if (cell \u003d\u003d null) {\n        break;\n      }\n      byte[] currentColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (previousColumnQualifier \u003d\u003d null) {\n        // first time in loop\n        previousColumnQualifier \u003d currentColumnQualifier;\n      }\n\n      converter \u003d getValueConverter(currentColumnQualifier);\n      if (comp.compare(previousColumnQualifier, currentColumnQualifier) !\u003d 0) {\n        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n            converter, currentTimestamp);\n        resetState(currentColumnCells, alreadySeenAggDim);\n        previousColumnQualifier \u003d currentColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(currentColumnQualifier);\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          converter, scannerContext);\n      nextCell(scannerContext);\n    }\n    if ((!currentColumnCells.isEmpty()) \u0026\u0026 ((limit \u003c\u003d 0 || addedCnt \u003c limit))) {\n      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp, converter,\n          currentTimestamp);\n      if (LOG.isDebugEnabled()) {\n        if (addedCnt \u003e 0) {\n          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n              + \" rowKey\u003d\"\n              + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));\n        } else {\n          LOG.debug(\"emitted no cells for \" + this.action);\n        }\n      }\n    }\n    return hasMore();\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
          "extendedDetails": {}
        }
      ]
    },
    "7b8cfa5c2ff62005c8b78867fedd64b48e50383d": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5109. timestamps are stored unencoded causing parse errors (Varun Saxena via sjlee)\n",
      "commitDate": "10/07/16 8:46 AM",
      "commitName": "7b8cfa5c2ff62005c8b78867fedd64b48e50383d",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "69dc561b61bf694cfdf0d2059f4f3dcee30e0632",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,62 @@\n   private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n       throws IOException {\n     Cell cell \u003d null;\n     startNext();\n     // Loop through all the cells in this row\n     // For min/max/metrics we do need to scan the entire set of cells to get the\n     // right one\n     // But with flush/compaction, the number of cells being scanned will go down\n     // cells are grouped per column qualifier then sorted by cell timestamp\n     // (latest to oldest) per column qualifier\n     // So all cells in one qualifier come one after the other before we see the\n     // next column qualifier\n     ByteArrayComparator comp \u003d new ByteArrayComparator();\n-    byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n+    byte[] currentColumnQualifier \u003d Separator.EMPTY_BYTES;\n     AggregationOperation currentAggOp \u003d null;\n     SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n     Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n     int addedCnt \u003d 0;\n     long currentTimestamp \u003d System.currentTimeMillis();\n     ValueConverter converter \u003d null;\n \n     while (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit) {\n       cell \u003d peekAtNextCell(cellLimit);\n       if (cell \u003d\u003d null) {\n         break;\n       }\n       byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n       if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n         if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n           addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n               converter, currentTimestamp);\n         }\n         resetState(currentColumnCells, alreadySeenAggDim);\n         currentColumnQualifier \u003d newColumnQualifier;\n         currentAggOp \u003d getCurrentAggOp(cell);\n         converter \u003d getValueConverter(newColumnQualifier);\n       }\n       // No operation needs to be performed on non numeric converters.\n       if (!isNumericConverter(converter)) {\n         currentColumnCells.add(cell);\n         nextCell(cellLimit);\n         continue;\n       }\n       collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n           (NumericValueConverter)converter);\n       nextCell(cellLimit);\n     }\n     if (!currentColumnCells.isEmpty()) {\n       addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n           converter, currentTimestamp);\n       if (LOG.isDebugEnabled()) {\n         if (addedCnt \u003e 0) {\n           LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n               + \" rowKey\u003d\"\n               + FlowRunRowKey.parseRowKey(cells.get(0).getRow()).toString());\n         } else {\n           LOG.debug(\"emitted no cells for \" + this.action);\n         }\n       }\n     }\n     return hasMore();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] currentColumnQualifier \u003d Separator.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    long currentTimestamp \u003d System.currentTimeMillis();\n    ValueConverter converter \u003d null;\n\n    while (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit) {\n      cell \u003d peekAtNextCell(cellLimit);\n      if (cell \u003d\u003d null) {\n        break;\n      }\n      byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n        if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n          addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n              converter, currentTimestamp);\n        }\n        resetState(currentColumnCells, alreadySeenAggDim);\n        currentColumnQualifier \u003d newColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(newColumnQualifier);\n      }\n      // No operation needs to be performed on non numeric converters.\n      if (!isNumericConverter(converter)) {\n        currentColumnCells.add(cell);\n        nextCell(cellLimit);\n        continue;\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          (NumericValueConverter)converter);\n      nextCell(cellLimit);\n    }\n    if (!currentColumnCells.isEmpty()) {\n      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n          converter, currentTimestamp);\n      if (LOG.isDebugEnabled()) {\n        if (addedCnt \u003e 0) {\n          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n              + \" rowKey\u003d\"\n              + FlowRunRowKey.parseRowKey(cells.get(0).getRow()).toString());\n        } else {\n          LOG.debug(\"emitted no cells for \" + this.action);\n        }\n      }\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
      "extendedDetails": {}
    },
    "69dc561b61bf694cfdf0d2059f4f3dcee30e0632": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4986. Add a check in the coprocessor for table to operated on (Vrushali C via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "69dc561b61bf694cfdf0d2059f4f3dcee30e0632",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "c2efdc415a13496da43a9a8d13c73d88ca8565a1",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,62 @@\n   private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n       throws IOException {\n     Cell cell \u003d null;\n     startNext();\n     // Loop through all the cells in this row\n     // For min/max/metrics we do need to scan the entire set of cells to get the\n     // right one\n     // But with flush/compaction, the number of cells being scanned will go down\n     // cells are grouped per column qualifier then sorted by cell timestamp\n     // (latest to oldest) per column qualifier\n     // So all cells in one qualifier come one after the other before we see the\n     // next column qualifier\n     ByteArrayComparator comp \u003d new ByteArrayComparator();\n     byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n     AggregationOperation currentAggOp \u003d null;\n     SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n     Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n     int addedCnt \u003d 0;\n     long currentTimestamp \u003d System.currentTimeMillis();\n     ValueConverter converter \u003d null;\n \n     while (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit) {\n       cell \u003d peekAtNextCell(cellLimit);\n       if (cell \u003d\u003d null) {\n         break;\n       }\n       byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n       if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n         if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n           addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n-              (NumericValueConverter)converter, currentTimestamp);\n+              converter, currentTimestamp);\n         }\n         resetState(currentColumnCells, alreadySeenAggDim);\n         currentColumnQualifier \u003d newColumnQualifier;\n         currentAggOp \u003d getCurrentAggOp(cell);\n         converter \u003d getValueConverter(newColumnQualifier);\n       }\n       // No operation needs to be performed on non numeric converters.\n       if (!isNumericConverter(converter)) {\n+        currentColumnCells.add(cell);\n         nextCell(cellLimit);\n         continue;\n       }\n       collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n           (NumericValueConverter)converter);\n       nextCell(cellLimit);\n     }\n     if (!currentColumnCells.isEmpty()) {\n       addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n-          (NumericValueConverter)converter, currentTimestamp);\n+          converter, currentTimestamp);\n       if (LOG.isDebugEnabled()) {\n         if (addedCnt \u003e 0) {\n           LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n               + \" rowKey\u003d\"\n               + FlowRunRowKey.parseRowKey(cells.get(0).getRow()).toString());\n         } else {\n           LOG.debug(\"emitted no cells for \" + this.action);\n         }\n       }\n     }\n     return hasMore();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    long currentTimestamp \u003d System.currentTimeMillis();\n    ValueConverter converter \u003d null;\n\n    while (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit) {\n      cell \u003d peekAtNextCell(cellLimit);\n      if (cell \u003d\u003d null) {\n        break;\n      }\n      byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n        if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n          addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n              converter, currentTimestamp);\n        }\n        resetState(currentColumnCells, alreadySeenAggDim);\n        currentColumnQualifier \u003d newColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(newColumnQualifier);\n      }\n      // No operation needs to be performed on non numeric converters.\n      if (!isNumericConverter(converter)) {\n        currentColumnCells.add(cell);\n        nextCell(cellLimit);\n        continue;\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          (NumericValueConverter)converter);\n      nextCell(cellLimit);\n    }\n    if (!currentColumnCells.isEmpty()) {\n      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n          converter, currentTimestamp);\n      if (LOG.isDebugEnabled()) {\n        if (addedCnt \u003e 0) {\n          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n              + \" rowKey\u003d\"\n              + FlowRunRowKey.parseRowKey(cells.get(0).getRow()).toString());\n        } else {\n          LOG.debug(\"emitted no cells for \" + this.action);\n        }\n      }\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
      "extendedDetails": {}
    },
    "9bdd455dced15c84430ea0a0a59410df924f02a7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4062. Add the flush and compaction functionality via coprocessors and scanners for flow run table (Vrushali C via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "9bdd455dced15c84430ea0a0a59410df924f02a7",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "960af7d4717b8a8949d0b2e43949e7daab45aa88",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,60 @@\n   private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n       throws IOException {\n     Cell cell \u003d null;\n     startNext();\n     // Loop through all the cells in this row\n     // For min/max/metrics we do need to scan the entire set of cells to get the\n     // right one\n     // But with flush/compaction, the number of cells being scanned will go down\n     // cells are grouped per column qualifier then sorted by cell timestamp\n     // (latest to oldest) per column qualifier\n     // So all cells in one qualifier come one after the other before we see the\n     // next column qualifier\n     ByteArrayComparator comp \u003d new ByteArrayComparator();\n     byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n     AggregationOperation currentAggOp \u003d null;\n     SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n     Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n     int addedCnt \u003d 0;\n+    long currentTimestamp \u003d System.currentTimeMillis();\n     ValueConverter converter \u003d null;\n-    while (((cell \u003d peekAtNextCell(cellLimit)) !\u003d null)\n-        \u0026\u0026 (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit)) {\n+    while (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit) {\n+      cell \u003d peekAtNextCell(cellLimit);\n+      if (cell \u003d\u003d null) {\n+        break;\n+      }\n       byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n       if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n         if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n           addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n-              (NumericValueConverter)converter);\n+              (NumericValueConverter)converter, currentTimestamp);\n         }\n         resetState(currentColumnCells, alreadySeenAggDim);\n         currentColumnQualifier \u003d newColumnQualifier;\n         currentAggOp \u003d getCurrentAggOp(cell);\n         converter \u003d getValueConverter(newColumnQualifier);\n       }\n       // No operation needs to be performed on non numeric converters.\n       if (!isNumericConverter(converter)) {\n         nextCell(cellLimit);\n         continue;\n       }\n       collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n           (NumericValueConverter)converter);\n       nextCell(cellLimit);\n     }\n     if (!currentColumnCells.isEmpty()) {\n-      emitCells(cells, currentColumnCells, currentAggOp,\n-          (NumericValueConverter)converter);\n+      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n+          (NumericValueConverter)converter, currentTimestamp);\n+      if (LOG.isDebugEnabled()) {\n+        if (addedCnt \u003e 0) {\n+          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n+              + \" rowKey\u003d\"\n+              + FlowRunRowKey.parseRowKey(cells.get(0).getRow()).toString());\n+        } else {\n+          LOG.debug(\"emitted no cells for \" + this.action);\n+        }\n+      }\n     }\n     return hasMore();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    long currentTimestamp \u003d System.currentTimeMillis();\n    ValueConverter converter \u003d null;\n    while (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit) {\n      cell \u003d peekAtNextCell(cellLimit);\n      if (cell \u003d\u003d null) {\n        break;\n      }\n      byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n        if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n          addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n              (NumericValueConverter)converter, currentTimestamp);\n        }\n        resetState(currentColumnCells, alreadySeenAggDim);\n        currentColumnQualifier \u003d newColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(newColumnQualifier);\n      }\n      // No operation needs to be performed on non numeric converters.\n      if (!isNumericConverter(converter)) {\n        nextCell(cellLimit);\n        continue;\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          (NumericValueConverter)converter);\n      nextCell(cellLimit);\n    }\n    if (!currentColumnCells.isEmpty()) {\n      addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n          (NumericValueConverter)converter, currentTimestamp);\n      if (LOG.isDebugEnabled()) {\n        if (addedCnt \u003e 0) {\n          LOG.debug(\"emitted cells. \" + addedCnt + \" for \" + this.action\n              + \" rowKey\u003d\"\n              + FlowRunRowKey.parseRowKey(cells.get(0).getRow()).toString());\n        } else {\n          LOG.debug(\"emitted no cells for \" + this.action);\n        }\n      }\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
      "extendedDetails": {}
    },
    "960af7d4717b8a8949d0b2e43949e7daab45aa88": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-4409. Fix javadoc and checkstyle issues in timelineservice code (Varun Saxena via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "960af7d4717b8a8949d0b2e43949e7daab45aa88",
      "commitAuthor": "Sangjin Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-4409. Fix javadoc and checkstyle issues in timelineservice code (Varun Saxena via sjlee)\n",
          "commitDate": "10/07/16 8:45 AM",
          "commitName": "960af7d4717b8a8949d0b2e43949e7daab45aa88",
          "commitAuthor": "Sangjin Lee",
          "commitDateOld": "10/07/16 8:45 AM",
          "commitNameOld": "51254a6b5133c8abfec4b7d2ac9477d112b3ccfa",
          "commitAuthorOld": "Sangjin Lee",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 19,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,46 +1,47 @@\n-  private boolean nextInternal(List\u003cCell\u003e cells, int limit) throws IOException {\n+  private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n+      throws IOException {\n     Cell cell \u003d null;\n     startNext();\n     // Loop through all the cells in this row\n     // For min/max/metrics we do need to scan the entire set of cells to get the\n     // right one\n     // But with flush/compaction, the number of cells being scanned will go down\n     // cells are grouped per column qualifier then sorted by cell timestamp\n     // (latest to oldest) per column qualifier\n     // So all cells in one qualifier come one after the other before we see the\n     // next column qualifier\n     ByteArrayComparator comp \u003d new ByteArrayComparator();\n     byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n     AggregationOperation currentAggOp \u003d null;\n     SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n     Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n     int addedCnt \u003d 0;\n     ValueConverter converter \u003d null;\n-    while (((cell \u003d peekAtNextCell(limit)) !\u003d null)\n-        \u0026\u0026 (limit \u003c\u003d 0 || addedCnt \u003c limit)) {\n+    while (((cell \u003d peekAtNextCell(cellLimit)) !\u003d null)\n+        \u0026\u0026 (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit)) {\n       byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n       if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n         if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n           addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n               (NumericValueConverter)converter);\n         }\n         resetState(currentColumnCells, alreadySeenAggDim);\n         currentColumnQualifier \u003d newColumnQualifier;\n         currentAggOp \u003d getCurrentAggOp(cell);\n         converter \u003d getValueConverter(newColumnQualifier);\n       }\n       // No operation needs to be performed on non numeric converters.\n       if (!isNumericConverter(converter)) {\n-        nextCell(limit);\n+        nextCell(cellLimit);\n         continue;\n       }\n       collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n           (NumericValueConverter)converter);\n-      nextCell(limit);\n+      nextCell(cellLimit);\n     }\n     if (!currentColumnCells.isEmpty()) {\n       emitCells(cells, currentColumnCells, currentAggOp,\n           (NumericValueConverter)converter);\n     }\n     return hasMore();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    ValueConverter converter \u003d null;\n    while (((cell \u003d peekAtNextCell(cellLimit)) !\u003d null)\n        \u0026\u0026 (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit)) {\n      byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n        if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n          addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n              (NumericValueConverter)converter);\n        }\n        resetState(currentColumnCells, alreadySeenAggDim);\n        currentColumnQualifier \u003d newColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(newColumnQualifier);\n      }\n      // No operation needs to be performed on non numeric converters.\n      if (!isNumericConverter(converter)) {\n        nextCell(cellLimit);\n        continue;\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          (NumericValueConverter)converter);\n      nextCell(cellLimit);\n    }\n    if (!currentColumnCells.isEmpty()) {\n      emitCells(cells, currentColumnCells, currentAggOp,\n          (NumericValueConverter)converter);\n    }\n    return hasMore();\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
          "extendedDetails": {
            "oldValue": "[cells-List\u003cCell\u003e, limit-int]",
            "newValue": "[cells-List\u003cCell\u003e, cellLimit-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-4409. Fix javadoc and checkstyle issues in timelineservice code (Varun Saxena via sjlee)\n",
          "commitDate": "10/07/16 8:45 AM",
          "commitName": "960af7d4717b8a8949d0b2e43949e7daab45aa88",
          "commitAuthor": "Sangjin Lee",
          "commitDateOld": "10/07/16 8:45 AM",
          "commitNameOld": "51254a6b5133c8abfec4b7d2ac9477d112b3ccfa",
          "commitAuthorOld": "Sangjin Lee",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 19,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,46 +1,47 @@\n-  private boolean nextInternal(List\u003cCell\u003e cells, int limit) throws IOException {\n+  private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n+      throws IOException {\n     Cell cell \u003d null;\n     startNext();\n     // Loop through all the cells in this row\n     // For min/max/metrics we do need to scan the entire set of cells to get the\n     // right one\n     // But with flush/compaction, the number of cells being scanned will go down\n     // cells are grouped per column qualifier then sorted by cell timestamp\n     // (latest to oldest) per column qualifier\n     // So all cells in one qualifier come one after the other before we see the\n     // next column qualifier\n     ByteArrayComparator comp \u003d new ByteArrayComparator();\n     byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n     AggregationOperation currentAggOp \u003d null;\n     SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n     Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n     int addedCnt \u003d 0;\n     ValueConverter converter \u003d null;\n-    while (((cell \u003d peekAtNextCell(limit)) !\u003d null)\n-        \u0026\u0026 (limit \u003c\u003d 0 || addedCnt \u003c limit)) {\n+    while (((cell \u003d peekAtNextCell(cellLimit)) !\u003d null)\n+        \u0026\u0026 (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit)) {\n       byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n       if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n         if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n           addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n               (NumericValueConverter)converter);\n         }\n         resetState(currentColumnCells, alreadySeenAggDim);\n         currentColumnQualifier \u003d newColumnQualifier;\n         currentAggOp \u003d getCurrentAggOp(cell);\n         converter \u003d getValueConverter(newColumnQualifier);\n       }\n       // No operation needs to be performed on non numeric converters.\n       if (!isNumericConverter(converter)) {\n-        nextCell(limit);\n+        nextCell(cellLimit);\n         continue;\n       }\n       collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n           (NumericValueConverter)converter);\n-      nextCell(limit);\n+      nextCell(cellLimit);\n     }\n     if (!currentColumnCells.isEmpty()) {\n       emitCells(cells, currentColumnCells, currentAggOp,\n           (NumericValueConverter)converter);\n     }\n     return hasMore();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, int cellLimit)\n      throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    ValueConverter converter \u003d null;\n    while (((cell \u003d peekAtNextCell(cellLimit)) !\u003d null)\n        \u0026\u0026 (cellLimit \u003c\u003d 0 || addedCnt \u003c cellLimit)) {\n      byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n        if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n          addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n              (NumericValueConverter)converter);\n        }\n        resetState(currentColumnCells, alreadySeenAggDim);\n        currentColumnQualifier \u003d newColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(newColumnQualifier);\n      }\n      // No operation needs to be performed on non numeric converters.\n      if (!isNumericConverter(converter)) {\n        nextCell(cellLimit);\n        continue;\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          (NumericValueConverter)converter);\n      nextCell(cellLimit);\n    }\n    if (!currentColumnCells.isEmpty()) {\n      emitCells(cells, currentColumnCells, currentAggOp,\n          (NumericValueConverter)converter);\n    }\n    return hasMore();\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
          "extendedDetails": {}
        }
      ]
    },
    "51254a6b5133c8abfec4b7d2ac9477d112b3ccfa": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4053. Change the way metric values are stored in HBase Storage (Varun Saxena via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "51254a6b5133c8abfec4b7d2ac9477d112b3ccfa",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "b51d0fef56a59b15489f5b932025718b4e9613d2",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,46 @@\n   private boolean nextInternal(List\u003cCell\u003e cells, int limit) throws IOException {\n     Cell cell \u003d null;\n     startNext();\n     // Loop through all the cells in this row\n     // For min/max/metrics we do need to scan the entire set of cells to get the\n     // right one\n     // But with flush/compaction, the number of cells being scanned will go down\n     // cells are grouped per column qualifier then sorted by cell timestamp\n     // (latest to oldest) per column qualifier\n     // So all cells in one qualifier come one after the other before we see the\n     // next column qualifier\n     ByteArrayComparator comp \u003d new ByteArrayComparator();\n     byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n     AggregationOperation currentAggOp \u003d null;\n     SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n     Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n     int addedCnt \u003d 0;\n+    ValueConverter converter \u003d null;\n     while (((cell \u003d peekAtNextCell(limit)) !\u003d null)\n         \u0026\u0026 (limit \u003c\u003d 0 || addedCnt \u003c limit)) {\n       byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n       if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n-        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp);\n+        if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n+          addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n+              (NumericValueConverter)converter);\n+        }\n         resetState(currentColumnCells, alreadySeenAggDim);\n         currentColumnQualifier \u003d newColumnQualifier;\n         currentAggOp \u003d getCurrentAggOp(cell);\n+        converter \u003d getValueConverter(newColumnQualifier);\n       }\n-      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim);\n+      // No operation needs to be performed on non numeric converters.\n+      if (!isNumericConverter(converter)) {\n+        nextCell(limit);\n+        continue;\n+      }\n+      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n+          (NumericValueConverter)converter);\n       nextCell(limit);\n     }\n     if (!currentColumnCells.isEmpty()) {\n-      emitCells(cells, currentColumnCells, currentAggOp);\n+      emitCells(cells, currentColumnCells, currentAggOp,\n+          (NumericValueConverter)converter);\n     }\n     return hasMore();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, int limit) throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    ValueConverter converter \u003d null;\n    while (((cell \u003d peekAtNextCell(limit)) !\u003d null)\n        \u0026\u0026 (limit \u003c\u003d 0 || addedCnt \u003c limit)) {\n      byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n        if (converter !\u003d null \u0026\u0026 isNumericConverter(converter)) {\n          addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp,\n              (NumericValueConverter)converter);\n        }\n        resetState(currentColumnCells, alreadySeenAggDim);\n        currentColumnQualifier \u003d newColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n        converter \u003d getValueConverter(newColumnQualifier);\n      }\n      // No operation needs to be performed on non numeric converters.\n      if (!isNumericConverter(converter)) {\n        nextCell(limit);\n        continue;\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim,\n          (NumericValueConverter)converter);\n      nextCell(limit);\n    }\n    if (!currentColumnCells.isEmpty()) {\n      emitCells(cells, currentColumnCells, currentAggOp,\n          (NumericValueConverter)converter);\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
      "extendedDetails": {}
    },
    "b51d0fef56a59b15489f5b932025718b4e9613d2": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4178. [storage implementation] app id as string in row keys can cause incorrect ordering (Varun Saxena via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "b51d0fef56a59b15489f5b932025718b4e9613d2",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "10fa6da7d8a6013698767c6136ae20f0e04415e9",
      "commitAuthorOld": "Vrushali",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   private boolean nextInternal(List\u003cCell\u003e cells, int limit) throws IOException {\n     Cell cell \u003d null;\n     startNext();\n     // Loop through all the cells in this row\n     // For min/max/metrics we do need to scan the entire set of cells to get the\n     // right one\n     // But with flush/compaction, the number of cells being scanned will go down\n     // cells are grouped per column qualifier then sorted by cell timestamp\n     // (latest to oldest) per column qualifier\n     // So all cells in one qualifier come one after the other before we see the\n     // next column qualifier\n     ByteArrayComparator comp \u003d new ByteArrayComparator();\n-    byte[] currentColumnQualifier \u003d TimelineWriterUtils.EMPTY_BYTES;\n+    byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n     AggregationOperation currentAggOp \u003d null;\n     SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n     Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n     int addedCnt \u003d 0;\n     while (((cell \u003d peekAtNextCell(limit)) !\u003d null)\n         \u0026\u0026 (limit \u003c\u003d 0 || addedCnt \u003c limit)) {\n       byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n       if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n         addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp);\n         resetState(currentColumnCells, alreadySeenAggDim);\n         currentColumnQualifier \u003d newColumnQualifier;\n         currentAggOp \u003d getCurrentAggOp(cell);\n       }\n       collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim);\n       nextCell(limit);\n     }\n     if (!currentColumnCells.isEmpty()) {\n       emitCells(cells, currentColumnCells, currentAggOp);\n     }\n     return hasMore();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, int limit) throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] currentColumnQualifier \u003d TimelineStorageUtils.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    while (((cell \u003d peekAtNextCell(limit)) !\u003d null)\n        \u0026\u0026 (limit \u003c\u003d 0 || addedCnt \u003c limit)) {\n      byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp);\n        resetState(currentColumnCells, alreadySeenAggDim);\n        currentColumnQualifier \u003d newColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim);\n      nextCell(limit);\n    }\n    if (!currentColumnCells.isEmpty()) {\n      emitCells(cells, currentColumnCells, currentAggOp);\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java",
      "extendedDetails": {}
    },
    "a68e3839218523403f42acd7bdd7ce1da59a5e60": {
      "type": "Yintroduced",
      "commitMessage": "YARN-3901. Populate flow run data in the flow_run \u0026 flow activity tables (Vrushali C via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "a68e3839218523403f42acd7bdd7ce1da59a5e60",
      "commitAuthor": "Sangjin Lee",
      "diff": "@@ -0,0 +1,34 @@\n+  private boolean nextInternal(List\u003cCell\u003e cells, int limit) throws IOException {\n+    Cell cell \u003d null;\n+    startNext();\n+    // Loop through all the cells in this row\n+    // For min/max/metrics we do need to scan the entire set of cells to get the\n+    // right one\n+    // But with flush/compaction, the number of cells being scanned will go down\n+    // cells are grouped per column qualifier then sorted by cell timestamp\n+    // (latest to oldest) per column qualifier\n+    // So all cells in one qualifier come one after the other before we see the\n+    // next column qualifier\n+    ByteArrayComparator comp \u003d new ByteArrayComparator();\n+    byte[] currentColumnQualifier \u003d TimelineWriterUtils.EMPTY_BYTES;\n+    AggregationOperation currentAggOp \u003d null;\n+    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n+    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n+    int addedCnt \u003d 0;\n+    while (((cell \u003d peekAtNextCell(limit)) !\u003d null)\n+        \u0026\u0026 (limit \u003c\u003d 0 || addedCnt \u003c limit)) {\n+      byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n+      if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n+        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp);\n+        resetState(currentColumnCells, alreadySeenAggDim);\n+        currentColumnQualifier \u003d newColumnQualifier;\n+        currentAggOp \u003d getCurrentAggOp(cell);\n+      }\n+      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim);\n+      nextCell(limit);\n+    }\n+    if (!currentColumnCells.isEmpty()) {\n+      emitCells(cells, currentColumnCells, currentAggOp);\n+    }\n+    return hasMore();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean nextInternal(List\u003cCell\u003e cells, int limit) throws IOException {\n    Cell cell \u003d null;\n    startNext();\n    // Loop through all the cells in this row\n    // For min/max/metrics we do need to scan the entire set of cells to get the\n    // right one\n    // But with flush/compaction, the number of cells being scanned will go down\n    // cells are grouped per column qualifier then sorted by cell timestamp\n    // (latest to oldest) per column qualifier\n    // So all cells in one qualifier come one after the other before we see the\n    // next column qualifier\n    ByteArrayComparator comp \u003d new ByteArrayComparator();\n    byte[] currentColumnQualifier \u003d TimelineWriterUtils.EMPTY_BYTES;\n    AggregationOperation currentAggOp \u003d null;\n    SortedSet\u003cCell\u003e currentColumnCells \u003d new TreeSet\u003c\u003e(KeyValue.COMPARATOR);\n    Set\u003cString\u003e alreadySeenAggDim \u003d new HashSet\u003c\u003e();\n    int addedCnt \u003d 0;\n    while (((cell \u003d peekAtNextCell(limit)) !\u003d null)\n        \u0026\u0026 (limit \u003c\u003d 0 || addedCnt \u003c limit)) {\n      byte[] newColumnQualifier \u003d CellUtil.cloneQualifier(cell);\n      if (comp.compare(currentColumnQualifier, newColumnQualifier) !\u003d 0) {\n        addedCnt +\u003d emitCells(cells, currentColumnCells, currentAggOp);\n        resetState(currentColumnCells, alreadySeenAggDim);\n        currentColumnQualifier \u003d newColumnQualifier;\n        currentAggOp \u003d getCurrentAggOp(cell);\n      }\n      collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim);\n      nextCell(limit);\n    }\n    if (!currentColumnCells.isEmpty()) {\n      emitCells(cells, currentColumnCells, currentAggOp);\n    }\n    return hasMore();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java"
    }
  }
}