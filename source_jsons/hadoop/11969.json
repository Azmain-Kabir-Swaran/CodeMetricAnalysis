{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "getBlockInputStreamWithCheckingPmemCache",
  "functionId": "getBlockInputStreamWithCheckingPmemCache___info-ReplicaInfo__b-ExtendedBlock__seekOffset-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 833,
  "functionEndLine": 850,
  "numCommitsSeen": 197,
  "timeTaken": 3453,
  "changeHistory": [
    "7030722e5d9f376245a9ab0a6a883538b6c55f82",
    "d1aad444907e1fc5314e8e64529e57c51ed7561c",
    "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a"
  ],
  "changeHistoryShort": {
    "7030722e5d9f376245a9ab0a6a883538b6c55f82": "Ybodychange",
    "d1aad444907e1fc5314e8e64529e57c51ed7561c": "Ybodychange",
    "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7030722e5d9f376245a9ab0a6a883538b6c55f82": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15080. Fix the issue in reading persistent memory cached data with an offset. Contributed by Feilong He.\n",
      "commitDate": "08/01/20 12:55 AM",
      "commitName": "7030722e5d9f376245a9ab0a6a883538b6c55f82",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "01/01/20 10:14 PM",
      "commitNameOld": "d79cce20abbbf321f6dcce03f4087544124a7cd2",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 6.11,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   private InputStream getBlockInputStreamWithCheckingPmemCache(\n       ReplicaInfo info, ExtendedBlock b, long seekOffset) throws IOException {\n     String cachePath \u003d cacheManager.getReplicaCachePath(\n         b.getBlockPoolId(), b.getBlockId());\n     if (cachePath !\u003d null) {\n       long addr \u003d cacheManager.getCacheAddress(\n           b.getBlockPoolId(), b.getBlockId());\n       if (addr !\u003d -1) {\n         LOG.debug(\"Get InputStream by cache address.\");\n         return FsDatasetUtil.getDirectInputStream(\n-            addr, info.getBlockDataLength());\n+            addr + seekOffset, info.getBlockDataLength() - seekOffset);\n       }\n       LOG.debug(\"Get InputStream by cache file path.\");\n       return FsDatasetUtil.getInputStreamAndSeek(\n           new File(cachePath), seekOffset);\n     }\n     return info.getDataInputStream(seekOffset);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private InputStream getBlockInputStreamWithCheckingPmemCache(\n      ReplicaInfo info, ExtendedBlock b, long seekOffset) throws IOException {\n    String cachePath \u003d cacheManager.getReplicaCachePath(\n        b.getBlockPoolId(), b.getBlockId());\n    if (cachePath !\u003d null) {\n      long addr \u003d cacheManager.getCacheAddress(\n          b.getBlockPoolId(), b.getBlockId());\n      if (addr !\u003d -1) {\n        LOG.debug(\"Get InputStream by cache address.\");\n        return FsDatasetUtil.getDirectInputStream(\n            addr + seekOffset, info.getBlockDataLength() - seekOffset);\n      }\n      LOG.debug(\"Get InputStream by cache file path.\");\n      return FsDatasetUtil.getInputStreamAndSeek(\n          new File(cachePath), seekOffset);\n    }\n    return info.getDataInputStream(seekOffset);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "d1aad444907e1fc5314e8e64529e57c51ed7561c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14356. Implement HDFS cache on SCM with native PMDK libs. Contributed by Feilong He.\n",
      "commitDate": "05/06/19 6:33 AM",
      "commitName": "d1aad444907e1fc5314e8e64529e57c51ed7561c",
      "commitAuthor": "Sammi Chen",
      "commitDateOld": "08/05/19 4:50 AM",
      "commitNameOld": "9b0aace1e6c54f201784912c0b623707aa82b761",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 28.07,
      "commitsBetweenForRepo": 170,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,18 @@\n   private InputStream getBlockInputStreamWithCheckingPmemCache(\n       ReplicaInfo info, ExtendedBlock b, long seekOffset) throws IOException {\n     String cachePath \u003d cacheManager.getReplicaCachePath(\n         b.getBlockPoolId(), b.getBlockId());\n     if (cachePath !\u003d null) {\n+      long addr \u003d cacheManager.getCacheAddress(\n+          b.getBlockPoolId(), b.getBlockId());\n+      if (addr !\u003d -1) {\n+        LOG.debug(\"Get InputStream by cache address.\");\n+        return FsDatasetUtil.getDirectInputStream(\n+            addr, info.getBlockDataLength());\n+      }\n+      LOG.debug(\"Get InputStream by cache file path.\");\n       return FsDatasetUtil.getInputStreamAndSeek(\n           new File(cachePath), seekOffset);\n     }\n     return info.getDataInputStream(seekOffset);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private InputStream getBlockInputStreamWithCheckingPmemCache(\n      ReplicaInfo info, ExtendedBlock b, long seekOffset) throws IOException {\n    String cachePath \u003d cacheManager.getReplicaCachePath(\n        b.getBlockPoolId(), b.getBlockId());\n    if (cachePath !\u003d null) {\n      long addr \u003d cacheManager.getCacheAddress(\n          b.getBlockPoolId(), b.getBlockId());\n      if (addr !\u003d -1) {\n        LOG.debug(\"Get InputStream by cache address.\");\n        return FsDatasetUtil.getDirectInputStream(\n            addr, info.getBlockDataLength());\n      }\n      LOG.debug(\"Get InputStream by cache file path.\");\n      return FsDatasetUtil.getInputStreamAndSeek(\n          new File(cachePath), seekOffset);\n    }\n    return info.getDataInputStream(seekOffset);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-14355 : Implement HDFS cache on SCM by using pure java mapped byte buffer. Contributed by Feilong He.\n",
      "commitDate": "30/03/19 11:33 PM",
      "commitName": "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,10 @@\n+  private InputStream getBlockInputStreamWithCheckingPmemCache(\n+      ReplicaInfo info, ExtendedBlock b, long seekOffset) throws IOException {\n+    String cachePath \u003d cacheManager.getReplicaCachePath(\n+        b.getBlockPoolId(), b.getBlockId());\n+    if (cachePath !\u003d null) {\n+      return FsDatasetUtil.getInputStreamAndSeek(\n+          new File(cachePath), seekOffset);\n+    }\n+    return info.getDataInputStream(seekOffset);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private InputStream getBlockInputStreamWithCheckingPmemCache(\n      ReplicaInfo info, ExtendedBlock b, long seekOffset) throws IOException {\n    String cachePath \u003d cacheManager.getReplicaCachePath(\n        b.getBlockPoolId(), b.getBlockId());\n    if (cachePath !\u003d null) {\n      return FsDatasetUtil.getInputStreamAndSeek(\n          new File(cachePath), seekOffset);\n    }\n    return info.getDataInputStream(seekOffset);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java"
    }
  }
}