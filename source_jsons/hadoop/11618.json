{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockPoolSlice.java",
  "functionName": "loadDfsUsed",
  "functionId": "loadDfsUsed",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
  "functionStartLine": 291,
  "functionEndLine": 326,
  "numCommitsSeen": 58,
  "timeTaken": 2724,
  "changeHistory": [
    "c07f7fa8ff752436726239d938e0461236839acf",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
    "fa6e59891c6125ae83fd601dbbcf928685f5dbfd"
  ],
  "changeHistoryShort": {
    "c07f7fa8ff752436726239d938e0461236839acf": "Ybodychange",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": "Ybodychange",
    "fa6e59891c6125ae83fd601dbbcf928685f5dbfd": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c07f7fa8ff752436726239d938e0461236839acf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9624. DataNode start slowly due to the initial DU command operations. (Lin Yiqun via wang)\n",
      "commitDate": "15/01/16 11:28 AM",
      "commitName": "c07f7fa8ff752436726239d938e0461236839acf",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "29/09/15 1:20 AM",
      "commitNameOld": "d6fa34e014b0e2a61b24f05dd08ebe12354267fd",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 108.46,
      "commitsBetweenForRepo": 737,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   long loadDfsUsed() {\n     long cachedDfsUsed;\n     long mtime;\n     Scanner sc;\n \n     try {\n       sc \u003d new Scanner(new File(currentDir, DU_CACHE_FILE), \"UTF-8\");\n     } catch (FileNotFoundException fnfe) {\n       return -1;\n     }\n \n     try {\n       // Get the recorded dfsUsed from the file.\n       if (sc.hasNextLong()) {\n         cachedDfsUsed \u003d sc.nextLong();\n       } else {\n         return -1;\n       }\n       // Get the recorded mtime from the file.\n       if (sc.hasNextLong()) {\n         mtime \u003d sc.nextLong();\n       } else {\n         return -1;\n       }\n \n       // Return the cached value if mtime is okay.\n-      if (mtime \u003e 0 \u0026\u0026 (Time.now() - mtime \u003c 600000L)) {\n+      if (mtime \u003e 0 \u0026\u0026 (timer.now() - mtime \u003c cachedDfsUsedCheckTime)) {\n         FsDatasetImpl.LOG.info(\"Cached dfsUsed found for \" + currentDir + \": \" +\n             cachedDfsUsed);\n         return cachedDfsUsed;\n       }\n       return -1;\n     } finally {\n       sc.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long loadDfsUsed() {\n    long cachedDfsUsed;\n    long mtime;\n    Scanner sc;\n\n    try {\n      sc \u003d new Scanner(new File(currentDir, DU_CACHE_FILE), \"UTF-8\");\n    } catch (FileNotFoundException fnfe) {\n      return -1;\n    }\n\n    try {\n      // Get the recorded dfsUsed from the file.\n      if (sc.hasNextLong()) {\n        cachedDfsUsed \u003d sc.nextLong();\n      } else {\n        return -1;\n      }\n      // Get the recorded mtime from the file.\n      if (sc.hasNextLong()) {\n        mtime \u003d sc.nextLong();\n      } else {\n        return -1;\n      }\n\n      // Return the cached value if mtime is okay.\n      if (mtime \u003e 0 \u0026\u0026 (timer.now() - mtime \u003c cachedDfsUsedCheckTime)) {\n        FsDatasetImpl.LOG.info(\"Cached dfsUsed found for \" + currentDir + \": \" +\n            cachedDfsUsed);\n        return cachedDfsUsed;\n      }\n      return -1;\n    } finally {\n      sc.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.\n",
      "commitDate": "11/12/14 12:36 PM",
      "commitName": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/11/14 9:57 AM",
      "commitNameOld": "058af60c56207907f2bedf76df4284e86d923e0c",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 15.11,
      "commitsBetweenForRepo": 103,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   long loadDfsUsed() {\n     long cachedDfsUsed;\n     long mtime;\n     Scanner sc;\n \n     try {\n-      sc \u003d new Scanner(new File(currentDir, DU_CACHE_FILE));\n+      sc \u003d new Scanner(new File(currentDir, DU_CACHE_FILE), \"UTF-8\");\n     } catch (FileNotFoundException fnfe) {\n       return -1;\n     }\n \n     try {\n       // Get the recorded dfsUsed from the file.\n       if (sc.hasNextLong()) {\n         cachedDfsUsed \u003d sc.nextLong();\n       } else {\n         return -1;\n       }\n       // Get the recorded mtime from the file.\n       if (sc.hasNextLong()) {\n         mtime \u003d sc.nextLong();\n       } else {\n         return -1;\n       }\n \n       // Return the cached value if mtime is okay.\n       if (mtime \u003e 0 \u0026\u0026 (Time.now() - mtime \u003c 600000L)) {\n         FsDatasetImpl.LOG.info(\"Cached dfsUsed found for \" + currentDir + \": \" +\n             cachedDfsUsed);\n         return cachedDfsUsed;\n       }\n       return -1;\n     } finally {\n       sc.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long loadDfsUsed() {\n    long cachedDfsUsed;\n    long mtime;\n    Scanner sc;\n\n    try {\n      sc \u003d new Scanner(new File(currentDir, DU_CACHE_FILE), \"UTF-8\");\n    } catch (FileNotFoundException fnfe) {\n      return -1;\n    }\n\n    try {\n      // Get the recorded dfsUsed from the file.\n      if (sc.hasNextLong()) {\n        cachedDfsUsed \u003d sc.nextLong();\n      } else {\n        return -1;\n      }\n      // Get the recorded mtime from the file.\n      if (sc.hasNextLong()) {\n        mtime \u003d sc.nextLong();\n      } else {\n        return -1;\n      }\n\n      // Return the cached value if mtime is okay.\n      if (mtime \u003e 0 \u0026\u0026 (Time.now() - mtime \u003c 600000L)) {\n        FsDatasetImpl.LOG.info(\"Cached dfsUsed found for \" + currentDir + \": \" +\n            cachedDfsUsed);\n        return cachedDfsUsed;\n      }\n      return -1;\n    } finally {\n      sc.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "fa6e59891c6125ae83fd601dbbcf928685f5dbfd": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5498. Improve datanode startup time. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1571797 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/02/14 11:27 AM",
      "commitName": "fa6e59891c6125ae83fd601dbbcf928685f5dbfd",
      "commitAuthor": "Kihwal Lee",
      "diff": "@@ -0,0 +1,36 @@\n+  long loadDfsUsed() {\n+    long cachedDfsUsed;\n+    long mtime;\n+    Scanner sc;\n+\n+    try {\n+      sc \u003d new Scanner(new File(currentDir, DU_CACHE_FILE));\n+    } catch (FileNotFoundException fnfe) {\n+      return -1;\n+    }\n+\n+    try {\n+      // Get the recorded dfsUsed from the file.\n+      if (sc.hasNextLong()) {\n+        cachedDfsUsed \u003d sc.nextLong();\n+      } else {\n+        return -1;\n+      }\n+      // Get the recorded mtime from the file.\n+      if (sc.hasNextLong()) {\n+        mtime \u003d sc.nextLong();\n+      } else {\n+        return -1;\n+      }\n+\n+      // Return the cached value if mtime is okay.\n+      if (mtime \u003e 0 \u0026\u0026 (Time.now() - mtime \u003c 600000L)) {\n+        FsDatasetImpl.LOG.info(\"Cached dfsUsed found for \" + currentDir + \": \" +\n+            cachedDfsUsed);\n+        return cachedDfsUsed;\n+      }\n+      return -1;\n+    } finally {\n+      sc.close();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  long loadDfsUsed() {\n    long cachedDfsUsed;\n    long mtime;\n    Scanner sc;\n\n    try {\n      sc \u003d new Scanner(new File(currentDir, DU_CACHE_FILE));\n    } catch (FileNotFoundException fnfe) {\n      return -1;\n    }\n\n    try {\n      // Get the recorded dfsUsed from the file.\n      if (sc.hasNextLong()) {\n        cachedDfsUsed \u003d sc.nextLong();\n      } else {\n        return -1;\n      }\n      // Get the recorded mtime from the file.\n      if (sc.hasNextLong()) {\n        mtime \u003d sc.nextLong();\n      } else {\n        return -1;\n      }\n\n      // Return the cached value if mtime is okay.\n      if (mtime \u003e 0 \u0026\u0026 (Time.now() - mtime \u003c 600000L)) {\n        FsDatasetImpl.LOG.info(\"Cached dfsUsed found for \" + currentDir + \": \" +\n            cachedDfsUsed);\n        return cachedDfsUsed;\n      }\n      return -1;\n    } finally {\n      sc.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java"
    }
  }
}