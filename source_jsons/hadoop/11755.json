{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsVolumeImpl.java",
  "functionName": "initializeCacheExecutor",
  "functionId": "initializeCacheExecutor___parent-File",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java",
  "functionStartLine": 193,
  "functionEndLine": 218,
  "numCommitsSeen": 71,
  "timeTaken": 3619,
  "changeHistory": [
    "a43c177f1d4c2b6149a2680dd23d91103eca3be0",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c",
    "bb84f1fccb18c6c7373851e05d2451d55e908242",
    "e871955765a5a40707e866179945c5dc4fefd389"
  ],
  "changeHistoryShort": {
    "a43c177f1d4c2b6149a2680dd23d91103eca3be0": "Ybodychange",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": "Ybodychange",
    "bb84f1fccb18c6c7373851e05d2451d55e908242": "Ybodychange",
    "e871955765a5a40707e866179945c5dc4fefd389": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a43c177f1d4c2b6149a2680dd23d91103eca3be0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15072. HDFS MiniCluster fails to start when run in directory path with a %. (#1775)\n\n",
      "commitDate": "07/01/20 6:28 PM",
      "commitName": "a43c177f1d4c2b6149a2680dd23d91103eca3be0",
      "commitAuthor": "Masatake Iwasaki",
      "commitDateOld": "06/12/19 3:46 PM",
      "commitNameOld": "11cd5b6e39adbf159891852f3482aebdde5459fb",
      "commitAuthorOld": "Masatake Iwasaki",
      "daysBetweenCommits": 32.11,
      "commitsBetweenForRepo": 87,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,26 @@\n   protected ThreadPoolExecutor initializeCacheExecutor(File parent) {\n     if (storageType.isTransient()) {\n       return null;\n     }\n     if (dataset.datanode \u003d\u003d null) {\n       // FsVolumeImpl is used in test.\n       return null;\n     }\n \n     final int maxNumThreads \u003d dataset.datanode.getConf().getInt(\n         DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_KEY,\n         DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_DEFAULT);\n \n+    String escapedPath \u003d parent.toString().replaceAll(\"%\", \"%%\");\n     ThreadFactory workerFactory \u003d new ThreadFactoryBuilder()\n         .setDaemon(true)\n-        .setNameFormat(\"FsVolumeImplWorker-\" + parent.toString() + \"-%d\")\n+        .setNameFormat(\"FsVolumeImplWorker-\" + escapedPath + \"-%d\")\n         .build();\n     ThreadPoolExecutor executor \u003d new ThreadPoolExecutor(\n         1, maxNumThreads,\n         60, TimeUnit.SECONDS,\n         new LinkedBlockingQueue\u003cRunnable\u003e(),\n         workerFactory);\n     executor.allowCoreThreadTimeOut(true);\n     return executor;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected ThreadPoolExecutor initializeCacheExecutor(File parent) {\n    if (storageType.isTransient()) {\n      return null;\n    }\n    if (dataset.datanode \u003d\u003d null) {\n      // FsVolumeImpl is used in test.\n      return null;\n    }\n\n    final int maxNumThreads \u003d dataset.datanode.getConf().getInt(\n        DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_KEY,\n        DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_DEFAULT);\n\n    String escapedPath \u003d parent.toString().replaceAll(\"%\", \"%%\");\n    ThreadFactory workerFactory \u003d new ThreadFactoryBuilder()\n        .setDaemon(true)\n        .setNameFormat(\"FsVolumeImplWorker-\" + escapedPath + \"-%d\")\n        .build();\n    ThreadPoolExecutor executor \u003d new ThreadPoolExecutor(\n        1, maxNumThreads,\n        60, TimeUnit.SECONDS,\n        new LinkedBlockingQueue\u003cRunnable\u003e(),\n        workerFactory);\n    executor.allowCoreThreadTimeOut(true);\n    return executor;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java",
      "extendedDetails": {}
    },
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)\n",
      "commitDate": "20/01/15 7:05 PM",
      "commitName": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/11/14 9:57 AM",
      "commitNameOld": "058af60c56207907f2bedf76df4284e86d923e0c",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 55.38,
      "commitsBetweenForRepo": 315,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,25 @@\n   protected ThreadPoolExecutor initializeCacheExecutor(File parent) {\n     if (storageType.isTransient()) {\n       return null;\n     }\n+    if (dataset.datanode \u003d\u003d null) {\n+      // FsVolumeImpl is used in test.\n+      return null;\n+    }\n \n     final int maxNumThreads \u003d dataset.datanode.getConf().getInt(\n         DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_KEY,\n         DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_DEFAULT);\n \n     ThreadFactory workerFactory \u003d new ThreadFactoryBuilder()\n         .setDaemon(true)\n         .setNameFormat(\"FsVolumeImplWorker-\" + parent.toString() + \"-%d\")\n         .build();\n     ThreadPoolExecutor executor \u003d new ThreadPoolExecutor(\n         1, maxNumThreads,\n         60, TimeUnit.SECONDS,\n         new LinkedBlockingQueue\u003cRunnable\u003e(),\n         workerFactory);\n     executor.allowCoreThreadTimeOut(true);\n     return executor;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected ThreadPoolExecutor initializeCacheExecutor(File parent) {\n    if (storageType.isTransient()) {\n      return null;\n    }\n    if (dataset.datanode \u003d\u003d null) {\n      // FsVolumeImpl is used in test.\n      return null;\n    }\n\n    final int maxNumThreads \u003d dataset.datanode.getConf().getInt(\n        DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_KEY,\n        DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_DEFAULT);\n\n    ThreadFactory workerFactory \u003d new ThreadFactoryBuilder()\n        .setDaemon(true)\n        .setNameFormat(\"FsVolumeImplWorker-\" + parent.toString() + \"-%d\")\n        .build();\n    ThreadPoolExecutor executor \u003d new ThreadPoolExecutor(\n        1, maxNumThreads,\n        60, TimeUnit.SECONDS,\n        new LinkedBlockingQueue\u003cRunnable\u003e(),\n        workerFactory);\n    executor.allowCoreThreadTimeOut(true);\n    return executor;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java",
      "extendedDetails": {}
    },
    "bb84f1fccb18c6c7373851e05d2451d55e908242": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7159. Use block storage policy to set lazy persist preference. (Arpit Agarwal)\n",
      "commitDate": "29/09/14 10:27 PM",
      "commitName": "bb84f1fccb18c6c7373851e05d2451d55e908242",
      "commitAuthor": "arp",
      "commitDateOld": "20/09/14 4:13 PM",
      "commitNameOld": "e257b6dbb11fdb6404a759f69d5252bc46d49c00",
      "commitAuthorOld": "",
      "daysBetweenCommits": 9.26,
      "commitsBetweenForRepo": 102,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,21 @@\n   protected ThreadPoolExecutor initializeCacheExecutor(File parent) {\n+    if (storageType.isTransient()) {\n+      return null;\n+    }\n+\n     final int maxNumThreads \u003d dataset.datanode.getConf().getInt(\n         DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_KEY,\n         DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_DEFAULT);\n \n     ThreadFactory workerFactory \u003d new ThreadFactoryBuilder()\n         .setDaemon(true)\n         .setNameFormat(\"FsVolumeImplWorker-\" + parent.toString() + \"-%d\")\n         .build();\n     ThreadPoolExecutor executor \u003d new ThreadPoolExecutor(\n         1, maxNumThreads,\n         60, TimeUnit.SECONDS,\n         new LinkedBlockingQueue\u003cRunnable\u003e(),\n         workerFactory);\n     executor.allowCoreThreadTimeOut(true);\n     return executor;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected ThreadPoolExecutor initializeCacheExecutor(File parent) {\n    if (storageType.isTransient()) {\n      return null;\n    }\n\n    final int maxNumThreads \u003d dataset.datanode.getConf().getInt(\n        DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_KEY,\n        DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_DEFAULT);\n\n    ThreadFactory workerFactory \u003d new ThreadFactoryBuilder()\n        .setDaemon(true)\n        .setNameFormat(\"FsVolumeImplWorker-\" + parent.toString() + \"-%d\")\n        .build();\n    ThreadPoolExecutor executor \u003d new ThreadPoolExecutor(\n        1, maxNumThreads,\n        60, TimeUnit.SECONDS,\n        new LinkedBlockingQueue\u003cRunnable\u003e(),\n        workerFactory);\n    executor.allowCoreThreadTimeOut(true);\n    return executor;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java",
      "extendedDetails": {}
    },
    "e871955765a5a40707e866179945c5dc4fefd389": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-6899. Allow changing MiniDFSCluster volumes per DN and capacity per volume. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1619970 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/08/14 11:01 PM",
      "commitName": "e871955765a5a40707e866179945c5dc4fefd389",
      "commitAuthor": "Arpit Agarwal",
      "diff": "@@ -0,0 +1,17 @@\n+  protected ThreadPoolExecutor initializeCacheExecutor(File parent) {\n+    final int maxNumThreads \u003d dataset.datanode.getConf().getInt(\n+        DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_KEY,\n+        DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_DEFAULT);\n+\n+    ThreadFactory workerFactory \u003d new ThreadFactoryBuilder()\n+        .setDaemon(true)\n+        .setNameFormat(\"FsVolumeImplWorker-\" + parent.toString() + \"-%d\")\n+        .build();\n+    ThreadPoolExecutor executor \u003d new ThreadPoolExecutor(\n+        1, maxNumThreads,\n+        60, TimeUnit.SECONDS,\n+        new LinkedBlockingQueue\u003cRunnable\u003e(),\n+        workerFactory);\n+    executor.allowCoreThreadTimeOut(true);\n+    return executor;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected ThreadPoolExecutor initializeCacheExecutor(File parent) {\n    final int maxNumThreads \u003d dataset.datanode.getConf().getInt(\n        DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_KEY,\n        DFSConfigKeys.DFS_DATANODE_FSDATASETCACHE_MAX_THREADS_PER_VOLUME_DEFAULT);\n\n    ThreadFactory workerFactory \u003d new ThreadFactoryBuilder()\n        .setDaemon(true)\n        .setNameFormat(\"FsVolumeImplWorker-\" + parent.toString() + \"-%d\")\n        .build();\n    ThreadPoolExecutor executor \u003d new ThreadPoolExecutor(\n        1, maxNumThreads,\n        60, TimeUnit.SECONDS,\n        new LinkedBlockingQueue\u003cRunnable\u003e(),\n        workerFactory);\n    executor.allowCoreThreadTimeOut(true);\n    return executor;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java"
    }
  }
}