{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ProvidedVolumeImpl.java",
  "functionName": "nextBlock",
  "functionId": "nextBlock",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java",
  "functionStartLine": 426,
  "functionEndLine": 443,
  "numCommitsSeen": 17,
  "timeTaken": 1931,
  "changeHistory": [
    "9c35be86e17021202823bfd3c2067ff3b312ce5c",
    "b668eb91556b8c85c2b4925808ccb1f769031c20"
  ],
  "changeHistoryShort": {
    "9c35be86e17021202823bfd3c2067ff3b312ce5c": "Ybodychange",
    "b668eb91556b8c85c2b4925808ccb1f769031c20": "Yintroduced"
  },
  "changeHistoryDetails": {
    "9c35be86e17021202823bfd3c2067ff3b312ce5c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12713. [READ] Refactor FileRegion and BlockAliasMap to separate out HDFS metadata and PROVIDED storage metadata. Contributed by Ewan Higgs\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "9c35be86e17021202823bfd3c2067ff3b312ce5c",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "a027055dd2bf5009fe272e9ceb08305bd0a8cc31",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,18 @@\n     public ExtendedBlock nextBlock() throws IOException {\n       if (null \u003d\u003d blockIterator || !blockIterator.hasNext()) {\n         return null;\n       }\n       FileRegion nextRegion \u003d null;\n       while (null \u003d\u003d nextRegion \u0026\u0026 blockIterator.hasNext()) {\n         FileRegion temp \u003d blockIterator.next();\n         if (temp.getBlock().getBlockId() \u003c state.lastBlockId) {\n           continue;\n         }\n-        if (temp.getBlockPoolId().equals(bpid)) {\n-          nextRegion \u003d temp;\n-        }\n+        nextRegion \u003d temp;\n       }\n       if (null \u003d\u003d nextRegion) {\n         return null;\n       }\n       state.lastBlockId \u003d nextRegion.getBlock().getBlockId();\n       return new ExtendedBlock(bpid, nextRegion.getBlock());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public ExtendedBlock nextBlock() throws IOException {\n      if (null \u003d\u003d blockIterator || !blockIterator.hasNext()) {\n        return null;\n      }\n      FileRegion nextRegion \u003d null;\n      while (null \u003d\u003d nextRegion \u0026\u0026 blockIterator.hasNext()) {\n        FileRegion temp \u003d blockIterator.next();\n        if (temp.getBlock().getBlockId() \u003c state.lastBlockId) {\n          continue;\n        }\n        nextRegion \u003d temp;\n      }\n      if (null \u003d\u003d nextRegion) {\n        return null;\n      }\n      state.lastBlockId \u003d nextRegion.getBlock().getBlockId();\n      return new ExtendedBlock(bpid, nextRegion.getBlock());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java",
      "extendedDetails": {}
    },
    "b668eb91556b8c85c2b4925808ccb1f769031c20": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-10675. Datanode support to read from external stores.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "b668eb91556b8c85c2b4925808ccb1f769031c20",
      "commitAuthor": "Virajith Jalaparti",
      "diff": "@@ -0,0 +1,20 @@\n+    public ExtendedBlock nextBlock() throws IOException {\n+      if (null \u003d\u003d blockIterator || !blockIterator.hasNext()) {\n+        return null;\n+      }\n+      FileRegion nextRegion \u003d null;\n+      while (null \u003d\u003d nextRegion \u0026\u0026 blockIterator.hasNext()) {\n+        FileRegion temp \u003d blockIterator.next();\n+        if (temp.getBlock().getBlockId() \u003c state.lastBlockId) {\n+          continue;\n+        }\n+        if (temp.getBlockPoolId().equals(bpid)) {\n+          nextRegion \u003d temp;\n+        }\n+      }\n+      if (null \u003d\u003d nextRegion) {\n+        return null;\n+      }\n+      state.lastBlockId \u003d nextRegion.getBlock().getBlockId();\n+      return new ExtendedBlock(bpid, nextRegion.getBlock());\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public ExtendedBlock nextBlock() throws IOException {\n      if (null \u003d\u003d blockIterator || !blockIterator.hasNext()) {\n        return null;\n      }\n      FileRegion nextRegion \u003d null;\n      while (null \u003d\u003d nextRegion \u0026\u0026 blockIterator.hasNext()) {\n        FileRegion temp \u003d blockIterator.next();\n        if (temp.getBlock().getBlockId() \u003c state.lastBlockId) {\n          continue;\n        }\n        if (temp.getBlockPoolId().equals(bpid)) {\n          nextRegion \u003d temp;\n        }\n      }\n      if (null \u003d\u003d nextRegion) {\n        return null;\n      }\n      state.lastBlockId \u003d nextRegion.getBlock().getBlockId();\n      return new ExtendedBlock(bpid, nextRegion.getBlock());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java"
    }
  }
}