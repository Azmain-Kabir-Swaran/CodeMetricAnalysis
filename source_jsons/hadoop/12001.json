{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "getBlockReports",
  "functionId": "getBlockReports___bpid-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 1926,
  "functionEndLine": 1979,
  "numCommitsSeen": 197,
  "timeTaken": 10282,
  "changeHistory": [
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8",
    "62c9e7fa99da1b1c8af222436102b8dea02fcde8",
    "8239e3afb31d3c4485817d4b8b8b195b554acbe7",
    "b668eb91556b8c85c2b4925808ccb1f769031c20",
    "86c9862bec0248d671e657aa56094a2919b8ac14",
    "07650bc37a3c78ecc6566d813778d0954d0b06b0",
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
    "63ac2db59af2b50e74dc892cae1dbc4d2e061423",
    "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1",
    "d324164a51a43d72c02567248bd9f0f12b244a40",
    "3b173d95171d01ab55042b1162569d1cf14a8d43",
    "fba994ffe20d387e8ed875e727fc3d93f7097101",
    "a1aa1836fb6831c25efe326cdfdc014370cf5957"
  ],
  "changeHistoryShort": {
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8": "Ybodychange",
    "62c9e7fa99da1b1c8af222436102b8dea02fcde8": "Ybodychange",
    "8239e3afb31d3c4485817d4b8b8b195b554acbe7": "Ybodychange",
    "b668eb91556b8c85c2b4925808ccb1f769031c20": "Ybodychange",
    "86c9862bec0248d671e657aa56094a2919b8ac14": "Ybodychange",
    "07650bc37a3c78ecc6566d813778d0954d0b06b0": "Ybodychange",
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c": "Ybodychange",
    "63ac2db59af2b50e74dc892cae1dbc4d2e061423": "Ybodychange",
    "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1": "Ybodychange",
    "d324164a51a43d72c02567248bd9f0f12b244a40": "Ybodychange",
    "3b173d95171d01ab55042b1162569d1cf14a8d43": "Ybodychange",
    "fba994ffe20d387e8ed875e727fc3d93f7097101": "Ybodychange",
    "a1aa1836fb6831c25efe326cdfdc014370cf5957": "Ymultichange(Yreturntypechange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15150. Introduce read write lock to Datanode. Contributed Stephen O\u0027Donnell.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "11/02/20 8:00 AM",
      "commitName": "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "28/01/20 10:10 AM",
      "commitNameOld": "1839c467f60cbb8592d446694ec3d7710cda5142",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 13.91,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n         new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n \n     List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n-    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n+    try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n       curVolumes \u003d volumes.getVolumes();\n       for (FsVolumeSpi v : curVolumes) {\n         builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n       }\n \n       Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n         // skip PROVIDED replicas.\n         if (b.getVolume().getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           continue;\n         }\n         String volStorageID \u003d b.getVolume().getStorageID();\n         if (!builders.containsKey(volStorageID)) {\n           if (!missingVolumesReported.contains(volStorageID)) {\n             LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n                 + \" replica block: \" + b + \". Probably being removed!\");\n             missingVolumesReported.add(volStorageID);\n           }\n           continue;\n         }\n         switch(b.getState()) {\n         case FINALIZED:\n         case RBW:\n         case RWR:\n           builders.get(volStorageID).add(b);\n           break;\n         case RUR:\n           ReplicaInfo orig \u003d b.getOriginalReplica();\n           builders.get(volStorageID).add(orig);\n           break;\n         case TEMPORARY:\n           break;\n         default:\n           assert false : \"Illegal ReplicaInfo state.\";\n         }\n       }\n     }\n \n     for (FsVolumeImpl v : curVolumes) {\n       blockReportsMap.put(v.toDatanodeStorage(),\n                           builders.get(v.getStorageID()).build());\n     }\n \n     return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n        new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n    try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n      curVolumes \u003d volumes.getVolumes();\n      for (FsVolumeSpi v : curVolumes) {\n        builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n      }\n\n      Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        // skip PROVIDED replicas.\n        if (b.getVolume().getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          continue;\n        }\n        String volStorageID \u003d b.getVolume().getStorageID();\n        if (!builders.containsKey(volStorageID)) {\n          if (!missingVolumesReported.contains(volStorageID)) {\n            LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n                + \" replica block: \" + b + \". Probably being removed!\");\n            missingVolumesReported.add(volStorageID);\n          }\n          continue;\n        }\n        switch(b.getState()) {\n        case FINALIZED:\n        case RBW:\n        case RWR:\n          builders.get(volStorageID).add(b);\n          break;\n        case RUR:\n          ReplicaInfo orig \u003d b.getOriginalReplica();\n          builders.get(volStorageID).add(orig);\n          break;\n        case TEMPORARY:\n          break;\n        default:\n          assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : curVolumes) {\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          builders.get(v.getStorageID()).build());\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "62c9e7fa99da1b1c8af222436102b8dea02fcde8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13036. Reusing the volume storage ID obtained by replicaInfo. Contributed by liaoyuxiangqin.\n",
      "commitDate": "19/01/18 5:51 PM",
      "commitName": "62c9e7fa99da1b1c8af222436102b8dea02fcde8",
      "commitAuthor": "Chen Liang",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "8239e3afb31d3c4485817d4b8b8b195b554acbe7",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 35.0,
      "commitsBetweenForRepo": 142,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n         new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n \n     List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       curVolumes \u003d volumes.getVolumes();\n       for (FsVolumeSpi v : curVolumes) {\n         builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n       }\n \n       Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n         // skip PROVIDED replicas.\n         if (b.getVolume().getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           continue;\n         }\n         String volStorageID \u003d b.getVolume().getStorageID();\n         if (!builders.containsKey(volStorageID)) {\n           if (!missingVolumesReported.contains(volStorageID)) {\n             LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n                 + \" replica block: \" + b + \". Probably being removed!\");\n             missingVolumesReported.add(volStorageID);\n           }\n           continue;\n         }\n         switch(b.getState()) {\n         case FINALIZED:\n         case RBW:\n         case RWR:\n-          builders.get(b.getVolume().getStorageID()).add(b);\n+          builders.get(volStorageID).add(b);\n           break;\n         case RUR:\n           ReplicaInfo orig \u003d b.getOriginalReplica();\n-          builders.get(b.getVolume().getStorageID()).add(orig);\n+          builders.get(volStorageID).add(orig);\n           break;\n         case TEMPORARY:\n           break;\n         default:\n           assert false : \"Illegal ReplicaInfo state.\";\n         }\n       }\n     }\n \n     for (FsVolumeImpl v : curVolumes) {\n       blockReportsMap.put(v.toDatanodeStorage(),\n                           builders.get(v.getStorageID()).build());\n     }\n \n     return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n        new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      curVolumes \u003d volumes.getVolumes();\n      for (FsVolumeSpi v : curVolumes) {\n        builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n      }\n\n      Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        // skip PROVIDED replicas.\n        if (b.getVolume().getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          continue;\n        }\n        String volStorageID \u003d b.getVolume().getStorageID();\n        if (!builders.containsKey(volStorageID)) {\n          if (!missingVolumesReported.contains(volStorageID)) {\n            LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n                + \" replica block: \" + b + \". Probably being removed!\");\n            missingVolumesReported.add(volStorageID);\n          }\n          continue;\n        }\n        switch(b.getState()) {\n        case FINALIZED:\n        case RBW:\n        case RWR:\n          builders.get(volStorageID).add(b);\n          break;\n        case RUR:\n          ReplicaInfo orig \u003d b.getOriginalReplica();\n          builders.get(volStorageID).add(orig);\n          break;\n        case TEMPORARY:\n          break;\n        default:\n          assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : curVolumes) {\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          builders.get(v.getStorageID()).build());\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "8239e3afb31d3c4485817d4b8b8b195b554acbe7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12712. [9806] Code style cleanup\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "8239e3afb31d3c4485817d4b8b8b195b554acbe7",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "90d1b47a2a400e07e2b6b812c4bbd9c4f2877786",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n         new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n \n     List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       curVolumes \u003d volumes.getVolumes();\n       for (FsVolumeSpi v : curVolumes) {\n         builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n       }\n \n       Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n-        //skip blocks in PROVIDED storage\n+        // skip PROVIDED replicas.\n         if (b.getVolume().getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           continue;\n         }\n         String volStorageID \u003d b.getVolume().getStorageID();\n         if (!builders.containsKey(volStorageID)) {\n           if (!missingVolumesReported.contains(volStorageID)) {\n             LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n                 + \" replica block: \" + b + \". Probably being removed!\");\n             missingVolumesReported.add(volStorageID);\n           }\n           continue;\n         }\n         switch(b.getState()) {\n         case FINALIZED:\n         case RBW:\n         case RWR:\n           builders.get(b.getVolume().getStorageID()).add(b);\n           break;\n         case RUR:\n           ReplicaInfo orig \u003d b.getOriginalReplica();\n           builders.get(b.getVolume().getStorageID()).add(orig);\n           break;\n         case TEMPORARY:\n           break;\n         default:\n           assert false : \"Illegal ReplicaInfo state.\";\n         }\n       }\n     }\n \n     for (FsVolumeImpl v : curVolumes) {\n       blockReportsMap.put(v.toDatanodeStorage(),\n                           builders.get(v.getStorageID()).build());\n     }\n \n     return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n        new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      curVolumes \u003d volumes.getVolumes();\n      for (FsVolumeSpi v : curVolumes) {\n        builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n      }\n\n      Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        // skip PROVIDED replicas.\n        if (b.getVolume().getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          continue;\n        }\n        String volStorageID \u003d b.getVolume().getStorageID();\n        if (!builders.containsKey(volStorageID)) {\n          if (!missingVolumesReported.contains(volStorageID)) {\n            LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n                + \" replica block: \" + b + \". Probably being removed!\");\n            missingVolumesReported.add(volStorageID);\n          }\n          continue;\n        }\n        switch(b.getState()) {\n        case FINALIZED:\n        case RBW:\n        case RWR:\n          builders.get(b.getVolume().getStorageID()).add(b);\n          break;\n        case RUR:\n          ReplicaInfo orig \u003d b.getOriginalReplica();\n          builders.get(b.getVolume().getStorageID()).add(orig);\n          break;\n        case TEMPORARY:\n          break;\n        default:\n          assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : curVolumes) {\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          builders.get(v.getStorageID()).build());\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "b668eb91556b8c85c2b4925808ccb1f769031c20": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10675. Datanode support to read from external stores.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "b668eb91556b8c85c2b4925808ccb1f769031c20",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "14/12/17 7:45 AM",
      "commitNameOld": "80db744ee57c52a1dc306c576c663ccc76cced4c",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,54 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n         new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n \n     List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       curVolumes \u003d volumes.getVolumes();\n       for (FsVolumeSpi v : curVolumes) {\n         builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n       }\n \n       Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n+        //skip blocks in PROVIDED storage\n+        if (b.getVolume().getStorageType() \u003d\u003d StorageType.PROVIDED) {\n+          continue;\n+        }\n         String volStorageID \u003d b.getVolume().getStorageID();\n         if (!builders.containsKey(volStorageID)) {\n           if (!missingVolumesReported.contains(volStorageID)) {\n             LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n                 + \" replica block: \" + b + \". Probably being removed!\");\n             missingVolumesReported.add(volStorageID);\n           }\n           continue;\n         }\n         switch(b.getState()) {\n         case FINALIZED:\n         case RBW:\n         case RWR:\n           builders.get(b.getVolume().getStorageID()).add(b);\n           break;\n         case RUR:\n           ReplicaInfo orig \u003d b.getOriginalReplica();\n           builders.get(b.getVolume().getStorageID()).add(orig);\n           break;\n         case TEMPORARY:\n           break;\n         default:\n           assert false : \"Illegal ReplicaInfo state.\";\n         }\n       }\n     }\n \n     for (FsVolumeImpl v : curVolumes) {\n       blockReportsMap.put(v.toDatanodeStorage(),\n                           builders.get(v.getStorageID()).build());\n     }\n \n     return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n        new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      curVolumes \u003d volumes.getVolumes();\n      for (FsVolumeSpi v : curVolumes) {\n        builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n      }\n\n      Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        //skip blocks in PROVIDED storage\n        if (b.getVolume().getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          continue;\n        }\n        String volStorageID \u003d b.getVolume().getStorageID();\n        if (!builders.containsKey(volStorageID)) {\n          if (!missingVolumesReported.contains(volStorageID)) {\n            LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n                + \" replica block: \" + b + \". Probably being removed!\");\n            missingVolumesReported.add(volStorageID);\n          }\n          continue;\n        }\n        switch(b.getState()) {\n        case FINALIZED:\n        case RBW:\n        case RWR:\n          builders.get(b.getVolume().getStorageID()).add(b);\n          break;\n        case RUR:\n          ReplicaInfo orig \u003d b.getOriginalReplica();\n          builders.get(b.getVolume().getStorageID()).add(orig);\n          break;\n        case TEMPORARY:\n          break;\n        default:\n          assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : curVolumes) {\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          builders.get(v.getStorageID()).build());\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "86c9862bec0248d671e657aa56094a2919b8ac14": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "13/09/16 12:54 PM",
      "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "10/09/16 6:22 PM",
      "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.77,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,50 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n         new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n \n     List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       curVolumes \u003d volumes.getVolumes();\n       for (FsVolumeSpi v : curVolumes) {\n         builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n       }\n \n       Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n         String volStorageID \u003d b.getVolume().getStorageID();\n         if (!builders.containsKey(volStorageID)) {\n           if (!missingVolumesReported.contains(volStorageID)) {\n             LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n                 + \" replica block: \" + b + \". Probably being removed!\");\n             missingVolumesReported.add(volStorageID);\n           }\n           continue;\n         }\n         switch(b.getState()) {\n-          case FINALIZED:\n-          case RBW:\n-          case RWR:\n-            builders.get(b.getVolume().getStorageID()).add(b);\n-            break;\n-          case RUR:\n-            ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n-            builders.get(rur.getVolume().getStorageID())\n-                .add(rur.getOriginalReplica());\n-            break;\n-          case TEMPORARY:\n-            break;\n-          default:\n-            assert false : \"Illegal ReplicaInfo state.\";\n+        case FINALIZED:\n+        case RBW:\n+        case RWR:\n+          builders.get(b.getVolume().getStorageID()).add(b);\n+          break;\n+        case RUR:\n+          ReplicaInfo orig \u003d b.getOriginalReplica();\n+          builders.get(b.getVolume().getStorageID()).add(orig);\n+          break;\n+        case TEMPORARY:\n+          break;\n+        default:\n+          assert false : \"Illegal ReplicaInfo state.\";\n         }\n       }\n     }\n \n     for (FsVolumeImpl v : curVolumes) {\n       blockReportsMap.put(v.toDatanodeStorage(),\n                           builders.get(v.getStorageID()).build());\n     }\n \n     return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n        new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      curVolumes \u003d volumes.getVolumes();\n      for (FsVolumeSpi v : curVolumes) {\n        builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n      }\n\n      Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        String volStorageID \u003d b.getVolume().getStorageID();\n        if (!builders.containsKey(volStorageID)) {\n          if (!missingVolumesReported.contains(volStorageID)) {\n            LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n                + \" replica block: \" + b + \". Probably being removed!\");\n            missingVolumesReported.add(volStorageID);\n          }\n          continue;\n        }\n        switch(b.getState()) {\n        case FINALIZED:\n        case RBW:\n        case RWR:\n          builders.get(b.getVolume().getStorageID()).add(b);\n          break;\n        case RUR:\n          ReplicaInfo orig \u003d b.getOriginalReplica();\n          builders.get(b.getVolume().getStorageID()).add(orig);\n          break;\n        case TEMPORARY:\n          break;\n        default:\n          assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : curVolumes) {\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          builders.get(v.getStorageID()).build());\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "07650bc37a3c78ecc6566d813778d0954d0b06b0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9781. FsDatasetImpl#getBlockReports can occasionally throw NullPointerException. Contributed by Manoj Govindassamy.\n",
      "commitDate": "02/09/16 3:33 PM",
      "commitName": "07650bc37a3c78ecc6566d813778d0954d0b06b0",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "27/08/16 10:51 PM",
      "commitNameOld": "c25817159af17753b398956cfe6ff14984801b01",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 5.7,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,51 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n         new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n \n-    List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n-    for (FsVolumeSpi v : curVolumes) {\n-      builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n-    }\n-\n+    List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n+      curVolumes \u003d volumes.getVolumes();\n+      for (FsVolumeSpi v : curVolumes) {\n+        builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n+      }\n+\n+      Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n+        String volStorageID \u003d b.getVolume().getStorageID();\n+        if (!builders.containsKey(volStorageID)) {\n+          if (!missingVolumesReported.contains(volStorageID)) {\n+            LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n+                + \" replica block: \" + b + \". Probably being removed!\");\n+            missingVolumesReported.add(volStorageID);\n+          }\n+          continue;\n+        }\n         switch(b.getState()) {\n           case FINALIZED:\n           case RBW:\n           case RWR:\n             builders.get(b.getVolume().getStorageID()).add(b);\n             break;\n           case RUR:\n             ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n             builders.get(rur.getVolume().getStorageID())\n                 .add(rur.getOriginalReplica());\n             break;\n           case TEMPORARY:\n             break;\n           default:\n             assert false : \"Illegal ReplicaInfo state.\";\n         }\n       }\n     }\n \n     for (FsVolumeImpl v : curVolumes) {\n       blockReportsMap.put(v.toDatanodeStorage(),\n                           builders.get(v.getStorageID()).build());\n     }\n \n     return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n        new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d null;\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      curVolumes \u003d volumes.getVolumes();\n      for (FsVolumeSpi v : curVolumes) {\n        builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n      }\n\n      Set\u003cString\u003e missingVolumesReported \u003d new HashSet\u003c\u003e();\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        String volStorageID \u003d b.getVolume().getStorageID();\n        if (!builders.containsKey(volStorageID)) {\n          if (!missingVolumesReported.contains(volStorageID)) {\n            LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n                + \" replica block: \" + b + \". Probably being removed!\");\n            missingVolumesReported.add(volStorageID);\n          }\n          continue;\n        }\n        switch(b.getState()) {\n          case FINALIZED:\n          case RBW:\n          case RWR:\n            builders.get(b.getVolume().getStorageID()).add(b);\n            break;\n          case RUR:\n            ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n            builders.get(rur.getVolume().getStorageID())\n                .add(rur.getOriginalReplica());\n            break;\n          case TEMPORARY:\n            break;\n          default:\n            assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : curVolumes) {\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          builders.get(v.getStorageID()).build());\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10682. Replace FsDatasetImpl object lock with a separate lock object. (Chen Liang)\n",
      "commitDate": "08/08/16 12:02 PM",
      "commitName": "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "08/07/16 7:40 PM",
      "commitNameOld": "da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 30.68,
      "commitsBetweenForRepo": 320,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n         new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n \n     List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n     for (FsVolumeSpi v : curVolumes) {\n       builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n     }\n \n-    synchronized(this) {\n+    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n         switch(b.getState()) {\n           case FINALIZED:\n           case RBW:\n           case RWR:\n             builders.get(b.getVolume().getStorageID()).add(b);\n             break;\n           case RUR:\n             ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n             builders.get(rur.getVolume().getStorageID())\n                 .add(rur.getOriginalReplica());\n             break;\n           case TEMPORARY:\n             break;\n           default:\n             assert false : \"Illegal ReplicaInfo state.\";\n         }\n       }\n     }\n \n     for (FsVolumeImpl v : curVolumes) {\n       blockReportsMap.put(v.toDatanodeStorage(),\n                           builders.get(v.getStorageID()).build());\n     }\n \n     return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n        new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n    for (FsVolumeSpi v : curVolumes) {\n      builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n    }\n\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        switch(b.getState()) {\n          case FINALIZED:\n          case RBW:\n          case RWR:\n            builders.get(b.getVolume().getStorageID()).add(b);\n            break;\n          case RUR:\n            ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n            builders.get(rur.getVolume().getStorageID())\n                .add(rur.getOriginalReplica());\n            break;\n          case TEMPORARY:\n            break;\n          default:\n            assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : curVolumes) {\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          builders.get(v.getStorageID()).build());\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "63ac2db59af2b50e74dc892cae1dbc4d2e061423": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10312. Large block reports may fail to decode at NameNode due to 64 MB protobuf maximum length restriction. Contributed by Chris Nauroth.\n",
      "commitDate": "20/04/16 1:39 PM",
      "commitName": "63ac2db59af2b50e74dc892cae1dbc4d2e061423",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "13/04/16 4:39 PM",
      "commitNameOld": "314aa21a89134fac68ac3cb95efdeb56bd3d7b05",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 6.87,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n         new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n \n     List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n     for (FsVolumeSpi v : curVolumes) {\n-      builders.put(v.getStorageID(), BlockListAsLongs.builder());\n+      builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n     }\n \n     synchronized(this) {\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n         switch(b.getState()) {\n           case FINALIZED:\n           case RBW:\n           case RWR:\n             builders.get(b.getVolume().getStorageID()).add(b);\n             break;\n           case RUR:\n             ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n             builders.get(rur.getVolume().getStorageID())\n                 .add(rur.getOriginalReplica());\n             break;\n           case TEMPORARY:\n             break;\n           default:\n             assert false : \"Illegal ReplicaInfo state.\";\n         }\n       }\n     }\n \n     for (FsVolumeImpl v : curVolumes) {\n       blockReportsMap.put(v.toDatanodeStorage(),\n                           builders.get(v.getStorageID()).build());\n     }\n \n     return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n        new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n    for (FsVolumeSpi v : curVolumes) {\n      builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n    }\n\n    synchronized(this) {\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        switch(b.getState()) {\n          case FINALIZED:\n          case RBW:\n          case RWR:\n            builders.get(b.getVolume().getStorageID()).add(b);\n            break;\n          case RUR:\n            ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n            builders.get(rur.getVolume().getStorageID())\n                .add(rur.getOriginalReplica());\n            break;\n          case TEMPORARY:\n            break;\n          default:\n            assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : curVolumes) {\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          builders.get(v.getStorageID()).build());\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7758. Retire FsDatasetSpi#getVolumes() and use FsDatasetSpi#getVolumeRefs() instead (Lei (Eddy) Xu via Colin P. McCabe)\n",
      "commitDate": "05/05/15 11:08 AM",
      "commitName": "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.05,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n         new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n \n-    List\u003cFsVolumeImpl\u003e curVolumes \u003d getVolumes();\n+    List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n     for (FsVolumeSpi v : curVolumes) {\n       builders.put(v.getStorageID(), BlockListAsLongs.builder());\n     }\n \n     synchronized(this) {\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n         switch(b.getState()) {\n           case FINALIZED:\n           case RBW:\n           case RWR:\n             builders.get(b.getVolume().getStorageID()).add(b);\n             break;\n           case RUR:\n             ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n             builders.get(rur.getVolume().getStorageID())\n                 .add(rur.getOriginalReplica());\n             break;\n           case TEMPORARY:\n             break;\n           default:\n             assert false : \"Illegal ReplicaInfo state.\";\n         }\n       }\n     }\n \n     for (FsVolumeImpl v : curVolumes) {\n       blockReportsMap.put(v.toDatanodeStorage(),\n                           builders.get(v.getStorageID()).build());\n     }\n \n     return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n        new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d volumes.getVolumes();\n    for (FsVolumeSpi v : curVolumes) {\n      builders.put(v.getStorageID(), BlockListAsLongs.builder());\n    }\n\n    synchronized(this) {\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        switch(b.getState()) {\n          case FINALIZED:\n          case RBW:\n          case RWR:\n            builders.get(b.getVolume().getStorageID()).add(b);\n            break;\n          case RUR:\n            ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n            builders.get(rur.getVolume().getStorageID())\n                .add(rur.getOriginalReplica());\n            break;\n          case TEMPORARY:\n            break;\n          default:\n            assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : curVolumes) {\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          builders.get(v.getStorageID()).build());\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "d324164a51a43d72c02567248bd9f0f12b244a40": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7435. PB encoding of block reports is very inefficient. Contributed by Daryn Sharp.\n",
      "commitDate": "13/03/15 12:23 PM",
      "commitName": "d324164a51a43d72c02567248bd9f0f12b244a40",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "12/03/15 12:00 PM",
      "commitNameOld": "b49c3a1813aa8c5b05fe6c02a653286c573137ca",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.02,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,40 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n-    Map\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e finalized \u003d\n-        new HashMap\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e();\n-    Map\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e uc \u003d\n-        new HashMap\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e();\n+    Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n+        new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n \n     List\u003cFsVolumeImpl\u003e curVolumes \u003d getVolumes();\n     for (FsVolumeSpi v : curVolumes) {\n-      finalized.put(v.getStorageID(), new ArrayList\u003cReplicaInfo\u003e());\n-      uc.put(v.getStorageID(), new ArrayList\u003cReplicaInfo\u003e());\n+      builders.put(v.getStorageID(), BlockListAsLongs.builder());\n     }\n \n     synchronized(this) {\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n         switch(b.getState()) {\n           case FINALIZED:\n-            finalized.get(b.getVolume().getStorageID()).add(b);\n-            break;\n           case RBW:\n           case RWR:\n-            uc.get(b.getVolume().getStorageID()).add(b);\n+            builders.get(b.getVolume().getStorageID()).add(b);\n             break;\n           case RUR:\n             ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n-            uc.get(rur.getVolume().getStorageID()).add(rur.getOriginalReplica());\n+            builders.get(rur.getVolume().getStorageID())\n+                .add(rur.getOriginalReplica());\n             break;\n           case TEMPORARY:\n             break;\n           default:\n             assert false : \"Illegal ReplicaInfo state.\";\n         }\n       }\n     }\n \n     for (FsVolumeImpl v : curVolumes) {\n-      ArrayList\u003cReplicaInfo\u003e finalizedList \u003d finalized.get(v.getStorageID());\n-      ArrayList\u003cReplicaInfo\u003e ucList \u003d uc.get(v.getStorageID());\n       blockReportsMap.put(v.toDatanodeStorage(),\n-                          new BlockListAsLongs(finalizedList, ucList));\n+                          builders.get(v.getStorageID()).build());\n     }\n \n     return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, BlockListAsLongs.Builder\u003e builders \u003d\n        new HashMap\u003cString, BlockListAsLongs.Builder\u003e();\n\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d getVolumes();\n    for (FsVolumeSpi v : curVolumes) {\n      builders.put(v.getStorageID(), BlockListAsLongs.builder());\n    }\n\n    synchronized(this) {\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        switch(b.getState()) {\n          case FINALIZED:\n          case RBW:\n          case RWR:\n            builders.get(b.getVolume().getStorageID()).add(b);\n            break;\n          case RUR:\n            ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n            builders.get(rur.getVolume().getStorageID())\n                .add(rur.getOriginalReplica());\n            break;\n          case TEMPORARY:\n            break;\n          default:\n            assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : curVolumes) {\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          builders.get(v.getStorageID()).build());\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "3b173d95171d01ab55042b1162569d1cf14a8d43": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7531. Improve the concurrent access on FsVolumeList (Lei Xu via Colin P. McCabe)\n",
      "commitDate": "17/12/14 4:41 PM",
      "commitName": "3b173d95171d01ab55042b1162569d1cf14a8d43",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "11/12/14 12:36 PM",
      "commitNameOld": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.17,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,46 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n     Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     Map\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e finalized \u003d\n         new HashMap\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e();\n     Map\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e uc \u003d\n         new HashMap\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e();\n \n-    for (FsVolumeSpi v : volumes.volumes) {\n+    List\u003cFsVolumeImpl\u003e curVolumes \u003d getVolumes();\n+    for (FsVolumeSpi v : curVolumes) {\n       finalized.put(v.getStorageID(), new ArrayList\u003cReplicaInfo\u003e());\n       uc.put(v.getStorageID(), new ArrayList\u003cReplicaInfo\u003e());\n     }\n \n     synchronized(this) {\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n         switch(b.getState()) {\n           case FINALIZED:\n             finalized.get(b.getVolume().getStorageID()).add(b);\n             break;\n           case RBW:\n           case RWR:\n             uc.get(b.getVolume().getStorageID()).add(b);\n             break;\n           case RUR:\n             ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n             uc.get(rur.getVolume().getStorageID()).add(rur.getOriginalReplica());\n             break;\n           case TEMPORARY:\n             break;\n           default:\n             assert false : \"Illegal ReplicaInfo state.\";\n         }\n       }\n     }\n \n-    for (FsVolumeImpl v : volumes.volumes) {\n+    for (FsVolumeImpl v : curVolumes) {\n       ArrayList\u003cReplicaInfo\u003e finalizedList \u003d finalized.get(v.getStorageID());\n       ArrayList\u003cReplicaInfo\u003e ucList \u003d uc.get(v.getStorageID());\n       blockReportsMap.put(v.toDatanodeStorage(),\n                           new BlockListAsLongs(finalizedList, ucList));\n     }\n \n     return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e finalized \u003d\n        new HashMap\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e();\n    Map\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e uc \u003d\n        new HashMap\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e();\n\n    List\u003cFsVolumeImpl\u003e curVolumes \u003d getVolumes();\n    for (FsVolumeSpi v : curVolumes) {\n      finalized.put(v.getStorageID(), new ArrayList\u003cReplicaInfo\u003e());\n      uc.put(v.getStorageID(), new ArrayList\u003cReplicaInfo\u003e());\n    }\n\n    synchronized(this) {\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        switch(b.getState()) {\n          case FINALIZED:\n            finalized.get(b.getVolume().getStorageID()).add(b);\n            break;\n          case RBW:\n          case RWR:\n            uc.get(b.getVolume().getStorageID()).add(b);\n            break;\n          case RUR:\n            ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n            uc.get(rur.getVolume().getStorageID()).add(rur.getOriginalReplica());\n            break;\n          case TEMPORARY:\n            break;\n          default:\n            assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : curVolumes) {\n      ArrayList\u003cReplicaInfo\u003e finalizedList \u003d finalized.get(v.getStorageID());\n      ArrayList\u003cReplicaInfo\u003e ucList \u003d uc.get(v.getStorageID());\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          new BlockListAsLongs(finalizedList, ucList));\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "fba994ffe20d387e8ed875e727fc3d93f7097101": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5648. Get rid of FsDatasetImpl#perVolumeReplicaMap.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1550357 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/13 11:01 PM",
      "commitName": "fba994ffe20d387e8ed875e727fc3d93f7097101",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "03/12/13 8:30 AM",
      "commitNameOld": "a1aa1836fb6831c25efe326cdfdc014370cf5957",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 8.6,
      "commitsBetweenForRepo": 57,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,45 @@\n   public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n-    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportMap \u003d\n+    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n         new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n-    for (FsVolumeImpl v : getVolumes()) {\n-      ReplicaMap rMap \u003d perVolumeReplicaMap.get(v.getStorageID());\n-      BlockListAsLongs blockList \u003d getBlockReportWithReplicaMap(bpid, rMap);\n-      blockReportMap.put(v.toDatanodeStorage(), blockList);\n+    Map\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e finalized \u003d\n+        new HashMap\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e();\n+    Map\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e uc \u003d\n+        new HashMap\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e();\n+\n+    for (FsVolumeSpi v : volumes.volumes) {\n+      finalized.put(v.getStorageID(), new ArrayList\u003cReplicaInfo\u003e());\n+      uc.put(v.getStorageID(), new ArrayList\u003cReplicaInfo\u003e());\n     }\n \n-    return blockReportMap;\n+    synchronized(this) {\n+      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n+        switch(b.getState()) {\n+          case FINALIZED:\n+            finalized.get(b.getVolume().getStorageID()).add(b);\n+            break;\n+          case RBW:\n+          case RWR:\n+            uc.get(b.getVolume().getStorageID()).add(b);\n+            break;\n+          case RUR:\n+            ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n+            uc.get(rur.getVolume().getStorageID()).add(rur.getOriginalReplica());\n+            break;\n+          case TEMPORARY:\n+            break;\n+          default:\n+            assert false : \"Illegal ReplicaInfo state.\";\n+        }\n+      }\n+    }\n+\n+    for (FsVolumeImpl v : volumes.volumes) {\n+      ArrayList\u003cReplicaInfo\u003e finalizedList \u003d finalized.get(v.getStorageID());\n+      ArrayList\u003cReplicaInfo\u003e ucList \u003d uc.get(v.getStorageID());\n+      blockReportsMap.put(v.toDatanodeStorage(),\n+                          new BlockListAsLongs(finalizedList, ucList));\n+    }\n+\n+    return blockReportsMap;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportsMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    Map\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e finalized \u003d\n        new HashMap\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e();\n    Map\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e uc \u003d\n        new HashMap\u003cString, ArrayList\u003cReplicaInfo\u003e\u003e();\n\n    for (FsVolumeSpi v : volumes.volumes) {\n      finalized.put(v.getStorageID(), new ArrayList\u003cReplicaInfo\u003e());\n      uc.put(v.getStorageID(), new ArrayList\u003cReplicaInfo\u003e());\n    }\n\n    synchronized(this) {\n      for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n        switch(b.getState()) {\n          case FINALIZED:\n            finalized.get(b.getVolume().getStorageID()).add(b);\n            break;\n          case RBW:\n          case RWR:\n            uc.get(b.getVolume().getStorageID()).add(b);\n            break;\n          case RUR:\n            ReplicaUnderRecovery rur \u003d (ReplicaUnderRecovery)b;\n            uc.get(rur.getVolume().getStorageID()).add(rur.getOriginalReplica());\n            break;\n          case TEMPORARY:\n            break;\n          default:\n            assert false : \"Illegal ReplicaInfo state.\";\n        }\n      }\n    }\n\n    for (FsVolumeImpl v : volumes.volumes) {\n      ArrayList\u003cReplicaInfo\u003e finalizedList \u003d finalized.get(v.getStorageID());\n      ArrayList\u003cReplicaInfo\u003e ucList \u003d uc.get(v.getStorageID());\n      blockReportsMap.put(v.toDatanodeStorage(),\n                          new BlockListAsLongs(finalizedList, ucList));\n    }\n\n    return blockReportsMap;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "a1aa1836fb6831c25efe326cdfdc014370cf5957": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-5484. StorageType and State in DatanodeStorageInfo in NameNode is not accurate. (Contributed by Eric Sirianni)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1547462 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/12/13 8:30 AM",
      "commitName": "a1aa1836fb6831c25efe326cdfdc014370cf5957",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5484. StorageType and State in DatanodeStorageInfo in NameNode is not accurate. (Contributed by Eric Sirianni)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1547462 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "03/12/13 8:30 AM",
          "commitName": "a1aa1836fb6831c25efe326cdfdc014370cf5957",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "02/12/13 9:41 AM",
          "commitNameOld": "18159be495f96bde4bd4fa2cacb14aafb87e87bc",
          "commitAuthorOld": "",
          "daysBetweenCommits": 0.95,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,12 @@\n-  public Map\u003cString, BlockListAsLongs\u003e getBlockReports(String bpid) {\n-    Map\u003cString, BlockListAsLongs\u003e blockReportMap \u003d\n-        new HashMap\u003cString, BlockListAsLongs\u003e();\n+  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n+    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportMap \u003d\n+        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     for (FsVolumeImpl v : getVolumes()) {\n       ReplicaMap rMap \u003d perVolumeReplicaMap.get(v.getStorageID());\n       BlockListAsLongs blockList \u003d getBlockReportWithReplicaMap(bpid, rMap);\n-      blockReportMap.put(v.getStorageID(), blockList);\n+      blockReportMap.put(v.toDatanodeStorage(), blockList);\n     }\n \n     return blockReportMap;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    for (FsVolumeImpl v : getVolumes()) {\n      ReplicaMap rMap \u003d perVolumeReplicaMap.get(v.getStorageID());\n      BlockListAsLongs blockList \u003d getBlockReportWithReplicaMap(bpid, rMap);\n      blockReportMap.put(v.toDatanodeStorage(), blockList);\n    }\n\n    return blockReportMap;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "Map\u003cString,BlockListAsLongs\u003e",
            "newValue": "Map\u003cDatanodeStorage,BlockListAsLongs\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5484. StorageType and State in DatanodeStorageInfo in NameNode is not accurate. (Contributed by Eric Sirianni)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1547462 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "03/12/13 8:30 AM",
          "commitName": "a1aa1836fb6831c25efe326cdfdc014370cf5957",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "02/12/13 9:41 AM",
          "commitNameOld": "18159be495f96bde4bd4fa2cacb14aafb87e87bc",
          "commitAuthorOld": "",
          "daysBetweenCommits": 0.95,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,12 @@\n-  public Map\u003cString, BlockListAsLongs\u003e getBlockReports(String bpid) {\n-    Map\u003cString, BlockListAsLongs\u003e blockReportMap \u003d\n-        new HashMap\u003cString, BlockListAsLongs\u003e();\n+  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n+    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportMap \u003d\n+        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n \n     for (FsVolumeImpl v : getVolumes()) {\n       ReplicaMap rMap \u003d perVolumeReplicaMap.get(v.getStorageID());\n       BlockListAsLongs blockList \u003d getBlockReportWithReplicaMap(bpid, rMap);\n-      blockReportMap.put(v.getStorageID(), blockList);\n+      blockReportMap.put(v.toDatanodeStorage(), blockList);\n     }\n \n     return blockReportMap;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Map\u003cDatanodeStorage, BlockListAsLongs\u003e getBlockReports(String bpid) {\n    Map\u003cDatanodeStorage, BlockListAsLongs\u003e blockReportMap \u003d\n        new HashMap\u003cDatanodeStorage, BlockListAsLongs\u003e();\n\n    for (FsVolumeImpl v : getVolumes()) {\n      ReplicaMap rMap \u003d perVolumeReplicaMap.get(v.getStorageID());\n      BlockListAsLongs blockList \u003d getBlockReportWithReplicaMap(bpid, rMap);\n      blockReportMap.put(v.toDatanodeStorage(), blockList);\n    }\n\n    return blockReportMap;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}