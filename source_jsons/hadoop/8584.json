{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSImageFormatPBINode.java",
  "functionName": "loadINodeDirectorySection",
  "functionId": "loadINodeDirectorySection___in-InputStream",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
  "functionStartLine": 263,
  "functionEndLine": 308,
  "numCommitsSeen": 63,
  "timeTaken": 3050,
  "changeHistory": [
    "b67812ea2111fa11bdd76096b923c93e1bdf2923",
    "2624b20291629b4565ea45590b66f2c38f96df67",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305"
  ],
  "changeHistoryShort": {
    "b67812ea2111fa11bdd76096b923c93e1bdf2923": "Ybodychange",
    "2624b20291629b4565ea45590b66f2c38f96df67": "Ybodychange",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b67812ea2111fa11bdd76096b923c93e1bdf2923": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14617. Improve fsimage load time by writing sub-sections to the fsimage index (#1028). Contributed by  Stephen O\u0027Donnell.\n\nReviewed-by: He Xiaoqiao \u003chexiaoqiao@apache.org\u003e",
      "commitDate": "22/08/19 5:09 PM",
      "commitName": "b67812ea2111fa11bdd76096b923c93e1bdf2923",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "22/07/19 8:07 PM",
      "commitNameOld": "377f95bbe8d2d171b5d7b0bfa7559e67ca4aae46",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 30.88,
      "commitsBetweenForRepo": 299,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,46 @@\n     void loadINodeDirectorySection(InputStream in) throws IOException {\n       final List\u003cINodeReference\u003e refList \u003d parent.getLoaderContext()\n           .getRefList();\n+      ArrayList\u003cINode\u003e inodeList \u003d new ArrayList\u003c\u003e();\n       while (true) {\n         INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n             .parseDelimitedFrom(in);\n         // note that in is a LimitedInputStream\n         if (e \u003d\u003d null) {\n           break;\n         }\n         INodeDirectory p \u003d dir.getInode(e.getParent()).asDirectory();\n         for (long id : e.getChildrenList()) {\n           INode child \u003d dir.getInode(id);\n-          addToParent(p, child);\n+          if (addToParent(p, child)) {\n+            if (child.isFile()) {\n+              inodeList.add(child);\n+            }\n+            if (inodeList.size() \u003e\u003d DIRECTORY_ENTRY_BATCH_SIZE) {\n+              addToCacheAndBlockMap(inodeList);\n+              inodeList.clear();\n+            }\n+          } else {\n+            LOG.warn(\"Failed to add the inode {} to the directory {}\",\n+                child.getId(), p.getId());\n+          }\n         }\n+\n         for (int refId : e.getRefChildrenList()) {\n           INodeReference ref \u003d refList.get(refId);\n-          addToParent(p, ref);\n+          if (addToParent(p, ref)) {\n+            if (ref.isFile()) {\n+              inodeList.add(ref);\n+            }\n+            if (inodeList.size() \u003e\u003d DIRECTORY_ENTRY_BATCH_SIZE) {\n+              addToCacheAndBlockMap(inodeList);\n+              inodeList.clear();\n+            }\n+          } else {\n+            LOG.warn(\"Failed to add the inode reference {} to the directory {}\",\n+                ref.getId(), p.getId());\n+          }\n         }\n       }\n+      addToCacheAndBlockMap(inodeList);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void loadINodeDirectorySection(InputStream in) throws IOException {\n      final List\u003cINodeReference\u003e refList \u003d parent.getLoaderContext()\n          .getRefList();\n      ArrayList\u003cINode\u003e inodeList \u003d new ArrayList\u003c\u003e();\n      while (true) {\n        INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n            .parseDelimitedFrom(in);\n        // note that in is a LimitedInputStream\n        if (e \u003d\u003d null) {\n          break;\n        }\n        INodeDirectory p \u003d dir.getInode(e.getParent()).asDirectory();\n        for (long id : e.getChildrenList()) {\n          INode child \u003d dir.getInode(id);\n          if (addToParent(p, child)) {\n            if (child.isFile()) {\n              inodeList.add(child);\n            }\n            if (inodeList.size() \u003e\u003d DIRECTORY_ENTRY_BATCH_SIZE) {\n              addToCacheAndBlockMap(inodeList);\n              inodeList.clear();\n            }\n          } else {\n            LOG.warn(\"Failed to add the inode {} to the directory {}\",\n                child.getId(), p.getId());\n          }\n        }\n\n        for (int refId : e.getRefChildrenList()) {\n          INodeReference ref \u003d refList.get(refId);\n          if (addToParent(p, ref)) {\n            if (ref.isFile()) {\n              inodeList.add(ref);\n            }\n            if (inodeList.size() \u003e\u003d DIRECTORY_ENTRY_BATCH_SIZE) {\n              addToCacheAndBlockMap(inodeList);\n              inodeList.clear();\n            }\n          } else {\n            LOG.warn(\"Failed to add the inode reference {} to the directory {}\",\n                ref.getId(), p.getId());\n          }\n        }\n      }\n      addToCacheAndBlockMap(inodeList);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "2624b20291629b4565ea45590b66f2c38f96df67": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5847. Consolidate INodeReference into a separate section. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567812 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 4:00 PM",
      "commitName": "2624b20291629b4565ea45590b66f2c38f96df67",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "10/02/14 3:13 PM",
      "commitNameOld": "5c978a43c3052cc1466b23653c354399186b4e10",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 2.03,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,21 @@\n     void loadINodeDirectorySection(InputStream in) throws IOException {\n+      final List\u003cINodeReference\u003e refList \u003d parent.getLoaderContext()\n+          .getRefList();\n       while (true) {\n         INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n             .parseDelimitedFrom(in);\n         // note that in is a LimitedInputStream\n         if (e \u003d\u003d null) {\n           break;\n         }\n         INodeDirectory p \u003d dir.getInode(e.getParent()).asDirectory();\n         for (long id : e.getChildrenList()) {\n           INode child \u003d dir.getInode(id);\n           addToParent(p, child);\n         }\n-        for (int i \u003d 0; i \u003c e.getNumOfRef(); i++) {\n-          INodeReference ref \u003d loadINodeReference(in);\n+        for (int refId : e.getRefChildrenList()) {\n+          INodeReference ref \u003d refList.get(refId);\n           addToParent(p, ref);\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    void loadINodeDirectorySection(InputStream in) throws IOException {\n      final List\u003cINodeReference\u003e refList \u003d parent.getLoaderContext()\n          .getRefList();\n      while (true) {\n        INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n            .parseDelimitedFrom(in);\n        // note that in is a LimitedInputStream\n        if (e \u003d\u003d null) {\n          break;\n        }\n        INodeDirectory p \u003d dir.getInode(e.getParent()).asDirectory();\n        for (long id : e.getChildrenList()) {\n          INode child \u003d dir.getInode(id);\n          addToParent(p, child);\n        }\n        for (int refId : e.getRefChildrenList()) {\n          INodeReference ref \u003d refList.get(refId);\n          addToParent(p, ref);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,19 @@\n+    void loadINodeDirectorySection(InputStream in) throws IOException {\n+      while (true) {\n+        INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n+            .parseDelimitedFrom(in);\n+        // note that in is a LimitedInputStream\n+        if (e \u003d\u003d null) {\n+          break;\n+        }\n+        INodeDirectory p \u003d dir.getInode(e.getParent()).asDirectory();\n+        for (long id : e.getChildrenList()) {\n+          INode child \u003d dir.getInode(id);\n+          addToParent(p, child);\n+        }\n+        for (int i \u003d 0; i \u003c e.getNumOfRef(); i++) {\n+          INodeReference ref \u003d loadINodeReference(in);\n+          addToParent(p, ref);\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    void loadINodeDirectorySection(InputStream in) throws IOException {\n      while (true) {\n        INodeDirectorySection.DirEntry e \u003d INodeDirectorySection.DirEntry\n            .parseDelimitedFrom(in);\n        // note that in is a LimitedInputStream\n        if (e \u003d\u003d null) {\n          break;\n        }\n        INodeDirectory p \u003d dir.getInode(e.getParent()).asDirectory();\n        for (long id : e.getChildrenList()) {\n          INode child \u003d dir.getInode(id);\n          addToParent(p, child);\n        }\n        for (int i \u003d 0; i \u003c e.getNumOfRef(); i++) {\n          INodeReference ref \u003d loadINodeReference(in);\n          addToParent(p, ref);\n        }\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java"
    }
  }
}