{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetUtil.java",
  "functionName": "computeChecksum",
  "functionId": "computeChecksum___srcMeta-File__dstMeta-File__blockFile-File__smallBufferSize-int__conf-Configuration",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java",
  "functionStartLine": 201,
  "functionEndLine": 222,
  "numCommitsSeen": 13,
  "timeTaken": 1769,
  "changeHistory": [
    "7a3188d054481b9bd563e337901e93476303ce7f",
    "c5573e6a7599da17cad733cd274e7a9b75b22bb0"
  ],
  "changeHistoryShort": {
    "7a3188d054481b9bd563e337901e93476303ce7f": "Ybodychange",
    "c5573e6a7599da17cad733cd274e7a9b75b22bb0": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7a3188d054481b9bd563e337901e93476303ce7f": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16282. Avoid FileStream to improve performance. Contributed by Ayush Saxena.\n",
      "commitDate": "02/05/19 12:58 PM",
      "commitName": "7a3188d054481b9bd563e337901e93476303ce7f",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "30/03/19 11:33 PM",
      "commitNameOld": "35ff31dd9462cf4fb4ebf5556ee8ae6bcd7c5c3a",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 32.56,
      "commitsBetweenForRepo": 209,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   public static void computeChecksum(File srcMeta, File dstMeta,\n       File blockFile, int smallBufferSize, Configuration conf)\n           throws IOException {\n     Preconditions.checkNotNull(srcMeta);\n     Preconditions.checkNotNull(dstMeta);\n     Preconditions.checkNotNull(blockFile);\n     // Create a dummy ReplicaInfo object pointing to the blockFile.\n     ReplicaInfo wrapper \u003d new FinalizedReplica(0, 0, 0, null, null) {\n       @Override\n       public URI getMetadataURI() {\n         return srcMeta.toURI();\n       }\n \n       @Override\n       public InputStream getDataInputStream(long seekOffset)\n           throws IOException {\n-        return new FileInputStream(blockFile);\n+        return Files.newInputStream(blockFile.toPath());\n       }\n     };\n \n     FsDatasetImpl.computeChecksum(wrapper, dstMeta, smallBufferSize, conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static void computeChecksum(File srcMeta, File dstMeta,\n      File blockFile, int smallBufferSize, Configuration conf)\n          throws IOException {\n    Preconditions.checkNotNull(srcMeta);\n    Preconditions.checkNotNull(dstMeta);\n    Preconditions.checkNotNull(blockFile);\n    // Create a dummy ReplicaInfo object pointing to the blockFile.\n    ReplicaInfo wrapper \u003d new FinalizedReplica(0, 0, 0, null, null) {\n      @Override\n      public URI getMetadataURI() {\n        return srcMeta.toURI();\n      }\n\n      @Override\n      public InputStream getDataInputStream(long seekOffset)\n          throws IOException {\n        return Files.newInputStream(blockFile.toPath());\n      }\n    };\n\n    FsDatasetImpl.computeChecksum(wrapper, dstMeta, smallBufferSize, conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java",
      "extendedDetails": {}
    },
    "c5573e6a7599da17cad733cd274e7a9b75b22bb0": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11009. Add a tool to reconstruct block meta file from CLI.\n",
      "commitDate": "18/10/16 10:42 PM",
      "commitName": "c5573e6a7599da17cad733cd274e7a9b75b22bb0",
      "commitAuthor": "Xiao Chen",
      "diff": "@@ -0,0 +1,21 @@\n+  public static void computeChecksum(File srcMeta, File dstMeta, File blockFile,\n+      int smallBufferSize, Configuration conf) throws IOException {\n+    Preconditions.checkNotNull(srcMeta);\n+    Preconditions.checkNotNull(dstMeta);\n+    Preconditions.checkNotNull(blockFile);\n+    // Create a dummy ReplicaInfo object pointing to the blockFile.\n+    ReplicaInfo wrapper \u003d new FinalizedReplica(0, 0, 0, null, null) {\n+      @Override\n+      public URI getMetadataURI() {\n+        return srcMeta.toURI();\n+      }\n+\n+      @Override\n+      public InputStream getDataInputStream(long seekOffset)\n+          throws IOException {\n+        return new FileInputStream(blockFile);\n+      }\n+    };\n+\n+    FsDatasetImpl.computeChecksum(wrapper, dstMeta, smallBufferSize, conf);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static void computeChecksum(File srcMeta, File dstMeta, File blockFile,\n      int smallBufferSize, Configuration conf) throws IOException {\n    Preconditions.checkNotNull(srcMeta);\n    Preconditions.checkNotNull(dstMeta);\n    Preconditions.checkNotNull(blockFile);\n    // Create a dummy ReplicaInfo object pointing to the blockFile.\n    ReplicaInfo wrapper \u003d new FinalizedReplica(0, 0, 0, null, null) {\n      @Override\n      public URI getMetadataURI() {\n        return srcMeta.toURI();\n      }\n\n      @Override\n      public InputStream getDataInputStream(long seekOffset)\n          throws IOException {\n        return new FileInputStream(blockFile);\n      }\n    };\n\n    FsDatasetImpl.computeChecksum(wrapper, dstMeta, smallBufferSize, conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java"
    }
  }
}