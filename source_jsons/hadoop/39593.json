{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ParentQueue.java",
  "functionName": "allocateResource",
  "functionId": "allocateResource___clusterResource-Resource__resource-Resource__nodePartition-String",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
  "functionStartLine": 1242,
  "functionEndLine": 1293,
  "numCommitsSeen": 137,
  "timeTaken": 5404,
  "changeHistory": [
    "39b4a37e02e929a698fcf9e32f1f71bb6b977635",
    "5e798b1a0ddceeaf54703b94052501867156e979",
    "eac6b4c35c50e555c2f1b5f913bb2c4d839f1ff4",
    "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
    "ae14e5d07f1b6702a5160637438028bb03d9387e"
  ],
  "changeHistoryShort": {
    "39b4a37e02e929a698fcf9e32f1f71bb6b977635": "Ybodychange",
    "5e798b1a0ddceeaf54703b94052501867156e979": "Ybodychange",
    "eac6b4c35c50e555c2f1b5f913bb2c4d839f1ff4": "Ymultichange(Yparameterchange,Ybodychange)",
    "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa": "Ymultichange(Ymodifierchange,Ybodychange)",
    "ae14e5d07f1b6702a5160637438028bb03d9387e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "39b4a37e02e929a698fcf9e32f1f71bb6b977635": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9341.  Fixed enentrant lock usage in YARN project.\n            Contributed by Prabhu Joseph\n",
      "commitDate": "07/03/19 1:47 PM",
      "commitName": "39b4a37e02e929a698fcf9e32f1f71bb6b977635",
      "commitAuthor": "Eric Yang",
      "commitDateOld": "04/03/19 9:10 PM",
      "commitNameOld": "e40e2d6ad5cbe782c3a067229270738b501ed27e",
      "commitAuthorOld": "Prabhu Joseph",
      "daysBetweenCommits": 2.69,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,52 @@\n   void allocateResource(Resource clusterResource,\n       Resource resource, String nodePartition) {\n+    writeLock.lock();\n     try {\n-      writeLock.lock();\n       super.allocateResource(clusterResource, resource, nodePartition);\n \n       /**\n        * check if we need to kill (killable) containers if maximum resource violated.\n        * Doing this because we will deduct killable resource when going from root.\n        * For example:\n        * \u003cpre\u003e\n        *      Root\n        *      /   \\\n        *     a     b\n        *   /  \\\n        *  a1  a2\n        * \u003c/pre\u003e\n        *\n        * a: max\u003d10G, used\u003d10G, killable\u003d2G\n        * a1: used\u003d8G, killable\u003d2G\n        * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n        *\n        * When we get queue-a to allocate resource, even if queue-a\n        * reaches its max resource, we deduct its used by killable, so we can allocate\n        * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n        *\n        * If scheduler finds a 2G available resource in existing cluster, and assigns it\n        * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n        *\n        * When this happens, we have to preempt killable container (on same or different\n        * nodes) of parent queue to avoid violating parent\u0027s max resource.\n        */\n       if (!queueResourceQuotas.getEffectiveMaxResource(nodePartition)\n           .equals(Resources.none())) {\n         if (Resources.lessThan(resourceCalculator, clusterResource,\n             queueResourceQuotas.getEffectiveMaxResource(nodePartition),\n             queueUsage.getUsed(nodePartition))) {\n           killContainersToEnforceMaxQueueCapacity(nodePartition,\n               clusterResource);\n         }\n       } else {\n         if (getQueueCapacities()\n             .getAbsoluteMaximumCapacity(nodePartition) \u003c getQueueCapacities()\n                 .getAbsoluteUsedCapacity(nodePartition)) {\n           killContainersToEnforceMaxQueueCapacity(nodePartition,\n               clusterResource);\n         }\n       }\n     } finally {\n       writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void allocateResource(Resource clusterResource,\n      Resource resource, String nodePartition) {\n    writeLock.lock();\n    try {\n      super.allocateResource(clusterResource, resource, nodePartition);\n\n      /**\n       * check if we need to kill (killable) containers if maximum resource violated.\n       * Doing this because we will deduct killable resource when going from root.\n       * For example:\n       * \u003cpre\u003e\n       *      Root\n       *      /   \\\n       *     a     b\n       *   /  \\\n       *  a1  a2\n       * \u003c/pre\u003e\n       *\n       * a: max\u003d10G, used\u003d10G, killable\u003d2G\n       * a1: used\u003d8G, killable\u003d2G\n       * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n       *\n       * When we get queue-a to allocate resource, even if queue-a\n       * reaches its max resource, we deduct its used by killable, so we can allocate\n       * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n       *\n       * If scheduler finds a 2G available resource in existing cluster, and assigns it\n       * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n       *\n       * When this happens, we have to preempt killable container (on same or different\n       * nodes) of parent queue to avoid violating parent\u0027s max resource.\n       */\n      if (!queueResourceQuotas.getEffectiveMaxResource(nodePartition)\n          .equals(Resources.none())) {\n        if (Resources.lessThan(resourceCalculator, clusterResource,\n            queueResourceQuotas.getEffectiveMaxResource(nodePartition),\n            queueUsage.getUsed(nodePartition))) {\n          killContainersToEnforceMaxQueueCapacity(nodePartition,\n              clusterResource);\n        }\n      } else {\n        if (getQueueCapacities()\n            .getAbsoluteMaximumCapacity(nodePartition) \u003c getQueueCapacities()\n                .getAbsoluteUsedCapacity(nodePartition)) {\n          killContainersToEnforceMaxQueueCapacity(nodePartition,\n              clusterResource);\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
      "extendedDetails": {}
    },
    "5e798b1a0ddceeaf54703b94052501867156e979": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6471. Support to add min/max resource configuration for a queue. (Sunil G via wangda)\n\nChange-Id: I9213f5297a6841fab5c573e85ee4c4e5f4a0b7ff\n",
      "commitDate": "07/12/17 6:56 PM",
      "commitName": "5e798b1a0ddceeaf54703b94052501867156e979",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/11/17 11:25 AM",
      "commitNameOld": "0987a7b8cbbbb2c1e4c2095821d98a7db19644df",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 21.31,
      "commitsBetweenForRepo": 94,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,52 @@\n   void allocateResource(Resource clusterResource,\n       Resource resource, String nodePartition) {\n     try {\n       writeLock.lock();\n       super.allocateResource(clusterResource, resource, nodePartition);\n \n       /**\n        * check if we need to kill (killable) containers if maximum resource violated.\n        * Doing this because we will deduct killable resource when going from root.\n        * For example:\n        * \u003cpre\u003e\n        *      Root\n        *      /   \\\n        *     a     b\n        *   /  \\\n        *  a1  a2\n        * \u003c/pre\u003e\n        *\n        * a: max\u003d10G, used\u003d10G, killable\u003d2G\n        * a1: used\u003d8G, killable\u003d2G\n        * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n        *\n        * When we get queue-a to allocate resource, even if queue-a\n        * reaches its max resource, we deduct its used by killable, so we can allocate\n        * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n        *\n        * If scheduler finds a 2G available resource in existing cluster, and assigns it\n        * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n        *\n        * When this happens, we have to preempt killable container (on same or different\n        * nodes) of parent queue to avoid violating parent\u0027s max resource.\n        */\n-      if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n-          \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n-        killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n+      if (!queueResourceQuotas.getEffectiveMaxResource(nodePartition)\n+          .equals(Resources.none())) {\n+        if (Resources.lessThan(resourceCalculator, clusterResource,\n+            queueResourceQuotas.getEffectiveMaxResource(nodePartition),\n+            queueUsage.getUsed(nodePartition))) {\n+          killContainersToEnforceMaxQueueCapacity(nodePartition,\n+              clusterResource);\n+        }\n+      } else {\n+        if (getQueueCapacities()\n+            .getAbsoluteMaximumCapacity(nodePartition) \u003c getQueueCapacities()\n+                .getAbsoluteUsedCapacity(nodePartition)) {\n+          killContainersToEnforceMaxQueueCapacity(nodePartition,\n+              clusterResource);\n+        }\n       }\n     } finally {\n       writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void allocateResource(Resource clusterResource,\n      Resource resource, String nodePartition) {\n    try {\n      writeLock.lock();\n      super.allocateResource(clusterResource, resource, nodePartition);\n\n      /**\n       * check if we need to kill (killable) containers if maximum resource violated.\n       * Doing this because we will deduct killable resource when going from root.\n       * For example:\n       * \u003cpre\u003e\n       *      Root\n       *      /   \\\n       *     a     b\n       *   /  \\\n       *  a1  a2\n       * \u003c/pre\u003e\n       *\n       * a: max\u003d10G, used\u003d10G, killable\u003d2G\n       * a1: used\u003d8G, killable\u003d2G\n       * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n       *\n       * When we get queue-a to allocate resource, even if queue-a\n       * reaches its max resource, we deduct its used by killable, so we can allocate\n       * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n       *\n       * If scheduler finds a 2G available resource in existing cluster, and assigns it\n       * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n       *\n       * When this happens, we have to preempt killable container (on same or different\n       * nodes) of parent queue to avoid violating parent\u0027s max resource.\n       */\n      if (!queueResourceQuotas.getEffectiveMaxResource(nodePartition)\n          .equals(Resources.none())) {\n        if (Resources.lessThan(resourceCalculator, clusterResource,\n            queueResourceQuotas.getEffectiveMaxResource(nodePartition),\n            queueUsage.getUsed(nodePartition))) {\n          killContainersToEnforceMaxQueueCapacity(nodePartition,\n              clusterResource);\n        }\n      } else {\n        if (getQueueCapacities()\n            .getAbsoluteMaximumCapacity(nodePartition) \u003c getQueueCapacities()\n                .getAbsoluteUsedCapacity(nodePartition)) {\n          killContainersToEnforceMaxQueueCapacity(nodePartition,\n              clusterResource);\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
      "extendedDetails": {}
    },
    "eac6b4c35c50e555c2f1b5f913bb2c4d839f1ff4": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-6216. Unify Container Resizing code paths with Container Updates making it scheduler agnostic. (Arun Suresh via wangda)\n",
      "commitDate": "28/02/17 10:35 AM",
      "commitName": "eac6b4c35c50e555c2f1b5f913bb2c4d839f1ff4",
      "commitAuthor": "Wangda Tan",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-6216. Unify Container Resizing code paths with Container Updates making it scheduler agnostic. (Arun Suresh via wangda)\n",
          "commitDate": "28/02/17 10:35 AM",
          "commitName": "eac6b4c35c50e555c2f1b5f913bb2c4d839f1ff4",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "09/02/17 10:23 AM",
          "commitNameOld": "5fb723bb77722d41df6959eee23e1b0cfeb5584e",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 19.01,
          "commitsBetweenForRepo": 112,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,40 @@\n   void allocateResource(Resource clusterResource,\n-      Resource resource, String nodePartition, boolean changeContainerResource) {\n+      Resource resource, String nodePartition) {\n     try {\n       writeLock.lock();\n-      super.allocateResource(clusterResource, resource, nodePartition,\n-          changeContainerResource);\n+      super.allocateResource(clusterResource, resource, nodePartition);\n \n       /**\n        * check if we need to kill (killable) containers if maximum resource violated.\n        * Doing this because we will deduct killable resource when going from root.\n        * For example:\n        * \u003cpre\u003e\n        *      Root\n        *      /   \\\n        *     a     b\n        *   /  \\\n        *  a1  a2\n        * \u003c/pre\u003e\n        *\n        * a: max\u003d10G, used\u003d10G, killable\u003d2G\n        * a1: used\u003d8G, killable\u003d2G\n        * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n        *\n        * When we get queue-a to allocate resource, even if queue-a\n        * reaches its max resource, we deduct its used by killable, so we can allocate\n        * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n        *\n        * If scheduler finds a 2G available resource in existing cluster, and assigns it\n        * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n        *\n        * When this happens, we have to preempt killable container (on same or different\n        * nodes) of parent queue to avoid violating parent\u0027s max resource.\n        */\n       if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n           \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n         killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n       }\n     } finally {\n       writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void allocateResource(Resource clusterResource,\n      Resource resource, String nodePartition) {\n    try {\n      writeLock.lock();\n      super.allocateResource(clusterResource, resource, nodePartition);\n\n      /**\n       * check if we need to kill (killable) containers if maximum resource violated.\n       * Doing this because we will deduct killable resource when going from root.\n       * For example:\n       * \u003cpre\u003e\n       *      Root\n       *      /   \\\n       *     a     b\n       *   /  \\\n       *  a1  a2\n       * \u003c/pre\u003e\n       *\n       * a: max\u003d10G, used\u003d10G, killable\u003d2G\n       * a1: used\u003d8G, killable\u003d2G\n       * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n       *\n       * When we get queue-a to allocate resource, even if queue-a\n       * reaches its max resource, we deduct its used by killable, so we can allocate\n       * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n       *\n       * If scheduler finds a 2G available resource in existing cluster, and assigns it\n       * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n       *\n       * When this happens, we have to preempt killable container (on same or different\n       * nodes) of parent queue to avoid violating parent\u0027s max resource.\n       */\n      if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n          \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n        killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, resource-Resource, nodePartition-String, changeContainerResource-boolean]",
            "newValue": "[clusterResource-Resource, resource-Resource, nodePartition-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-6216. Unify Container Resizing code paths with Container Updates making it scheduler agnostic. (Arun Suresh via wangda)\n",
          "commitDate": "28/02/17 10:35 AM",
          "commitName": "eac6b4c35c50e555c2f1b5f913bb2c4d839f1ff4",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "09/02/17 10:23 AM",
          "commitNameOld": "5fb723bb77722d41df6959eee23e1b0cfeb5584e",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 19.01,
          "commitsBetweenForRepo": 112,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,40 @@\n   void allocateResource(Resource clusterResource,\n-      Resource resource, String nodePartition, boolean changeContainerResource) {\n+      Resource resource, String nodePartition) {\n     try {\n       writeLock.lock();\n-      super.allocateResource(clusterResource, resource, nodePartition,\n-          changeContainerResource);\n+      super.allocateResource(clusterResource, resource, nodePartition);\n \n       /**\n        * check if we need to kill (killable) containers if maximum resource violated.\n        * Doing this because we will deduct killable resource when going from root.\n        * For example:\n        * \u003cpre\u003e\n        *      Root\n        *      /   \\\n        *     a     b\n        *   /  \\\n        *  a1  a2\n        * \u003c/pre\u003e\n        *\n        * a: max\u003d10G, used\u003d10G, killable\u003d2G\n        * a1: used\u003d8G, killable\u003d2G\n        * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n        *\n        * When we get queue-a to allocate resource, even if queue-a\n        * reaches its max resource, we deduct its used by killable, so we can allocate\n        * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n        *\n        * If scheduler finds a 2G available resource in existing cluster, and assigns it\n        * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n        *\n        * When this happens, we have to preempt killable container (on same or different\n        * nodes) of parent queue to avoid violating parent\u0027s max resource.\n        */\n       if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n           \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n         killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n       }\n     } finally {\n       writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void allocateResource(Resource clusterResource,\n      Resource resource, String nodePartition) {\n    try {\n      writeLock.lock();\n      super.allocateResource(clusterResource, resource, nodePartition);\n\n      /**\n       * check if we need to kill (killable) containers if maximum resource violated.\n       * Doing this because we will deduct killable resource when going from root.\n       * For example:\n       * \u003cpre\u003e\n       *      Root\n       *      /   \\\n       *     a     b\n       *   /  \\\n       *  a1  a2\n       * \u003c/pre\u003e\n       *\n       * a: max\u003d10G, used\u003d10G, killable\u003d2G\n       * a1: used\u003d8G, killable\u003d2G\n       * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n       *\n       * When we get queue-a to allocate resource, even if queue-a\n       * reaches its max resource, we deduct its used by killable, so we can allocate\n       * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n       *\n       * If scheduler finds a 2G available resource in existing cluster, and assigns it\n       * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n       *\n       * When this happens, we have to preempt killable container (on same or different\n       * nodes) of parent queue to avoid violating parent\u0027s max resource.\n       */\n      if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n          \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n        killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "YARN-3140. Improve locks in AbstractCSQueue/LeafQueue/ParentQueue. Contributed by Wangda Tan\n",
      "commitDate": "20/09/16 12:03 AM",
      "commitName": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-3140. Improve locks in AbstractCSQueue/LeafQueue/ParentQueue. Contributed by Wangda Tan\n",
          "commitDate": "20/09/16 12:03 AM",
          "commitName": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthor": "Jian He",
          "commitDateOld": "16/09/16 10:05 PM",
          "commitNameOld": "4174b9756c8c7877797545c4356b1f40df603ec5",
          "commitAuthorOld": "Naganarasimha",
          "daysBetweenCommits": 3.08,
          "commitsBetweenForRepo": 11,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,41 @@\n-  synchronized void allocateResource(Resource clusterResource,\n+  void allocateResource(Resource clusterResource,\n       Resource resource, String nodePartition, boolean changeContainerResource) {\n-    super.allocateResource(clusterResource, resource, nodePartition,\n-        changeContainerResource);\n+    try {\n+      writeLock.lock();\n+      super.allocateResource(clusterResource, resource, nodePartition,\n+          changeContainerResource);\n \n-    /**\n-     * check if we need to kill (killable) containers if maximum resource violated.\n-     * Doing this because we will deduct killable resource when going from root.\n-     * For example:\n-     * \u003cpre\u003e\n-     *      Root\n-     *      /   \\\n-     *     a     b\n-     *   /  \\\n-     *  a1  a2\n-     * \u003c/pre\u003e\n-     *\n-     * a: max\u003d10G, used\u003d10G, killable\u003d2G\n-     * a1: used\u003d8G, killable\u003d2G\n-     * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n-     *\n-     * When we get queue-a to allocate resource, even if queue-a\n-     * reaches its max resource, we deduct its used by killable, so we can allocate\n-     * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n-     *\n-     * If scheduler finds a 2G available resource in existing cluster, and assigns it\n-     * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n-     *\n-     * When this happens, we have to preempt killable container (on same or different\n-     * nodes) of parent queue to avoid violating parent\u0027s max resource.\n-     */\n-    if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n-        \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n-      killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n+      /**\n+       * check if we need to kill (killable) containers if maximum resource violated.\n+       * Doing this because we will deduct killable resource when going from root.\n+       * For example:\n+       * \u003cpre\u003e\n+       *      Root\n+       *      /   \\\n+       *     a     b\n+       *   /  \\\n+       *  a1  a2\n+       * \u003c/pre\u003e\n+       *\n+       * a: max\u003d10G, used\u003d10G, killable\u003d2G\n+       * a1: used\u003d8G, killable\u003d2G\n+       * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n+       *\n+       * When we get queue-a to allocate resource, even if queue-a\n+       * reaches its max resource, we deduct its used by killable, so we can allocate\n+       * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n+       *\n+       * If scheduler finds a 2G available resource in existing cluster, and assigns it\n+       * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n+       *\n+       * When this happens, we have to preempt killable container (on same or different\n+       * nodes) of parent queue to avoid violating parent\u0027s max resource.\n+       */\n+      if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n+          \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n+        killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n+      }\n+    } finally {\n+      writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void allocateResource(Resource clusterResource,\n      Resource resource, String nodePartition, boolean changeContainerResource) {\n    try {\n      writeLock.lock();\n      super.allocateResource(clusterResource, resource, nodePartition,\n          changeContainerResource);\n\n      /**\n       * check if we need to kill (killable) containers if maximum resource violated.\n       * Doing this because we will deduct killable resource when going from root.\n       * For example:\n       * \u003cpre\u003e\n       *      Root\n       *      /   \\\n       *     a     b\n       *   /  \\\n       *  a1  a2\n       * \u003c/pre\u003e\n       *\n       * a: max\u003d10G, used\u003d10G, killable\u003d2G\n       * a1: used\u003d8G, killable\u003d2G\n       * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n       *\n       * When we get queue-a to allocate resource, even if queue-a\n       * reaches its max resource, we deduct its used by killable, so we can allocate\n       * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n       *\n       * If scheduler finds a 2G available resource in existing cluster, and assigns it\n       * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n       *\n       * When this happens, we have to preempt killable container (on same or different\n       * nodes) of parent queue to avoid violating parent\u0027s max resource.\n       */\n      if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n          \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n        killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
          "extendedDetails": {
            "oldValue": "[synchronized]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3140. Improve locks in AbstractCSQueue/LeafQueue/ParentQueue. Contributed by Wangda Tan\n",
          "commitDate": "20/09/16 12:03 AM",
          "commitName": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthor": "Jian He",
          "commitDateOld": "16/09/16 10:05 PM",
          "commitNameOld": "4174b9756c8c7877797545c4356b1f40df603ec5",
          "commitAuthorOld": "Naganarasimha",
          "daysBetweenCommits": 3.08,
          "commitsBetweenForRepo": 11,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,41 @@\n-  synchronized void allocateResource(Resource clusterResource,\n+  void allocateResource(Resource clusterResource,\n       Resource resource, String nodePartition, boolean changeContainerResource) {\n-    super.allocateResource(clusterResource, resource, nodePartition,\n-        changeContainerResource);\n+    try {\n+      writeLock.lock();\n+      super.allocateResource(clusterResource, resource, nodePartition,\n+          changeContainerResource);\n \n-    /**\n-     * check if we need to kill (killable) containers if maximum resource violated.\n-     * Doing this because we will deduct killable resource when going from root.\n-     * For example:\n-     * \u003cpre\u003e\n-     *      Root\n-     *      /   \\\n-     *     a     b\n-     *   /  \\\n-     *  a1  a2\n-     * \u003c/pre\u003e\n-     *\n-     * a: max\u003d10G, used\u003d10G, killable\u003d2G\n-     * a1: used\u003d8G, killable\u003d2G\n-     * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n-     *\n-     * When we get queue-a to allocate resource, even if queue-a\n-     * reaches its max resource, we deduct its used by killable, so we can allocate\n-     * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n-     *\n-     * If scheduler finds a 2G available resource in existing cluster, and assigns it\n-     * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n-     *\n-     * When this happens, we have to preempt killable container (on same or different\n-     * nodes) of parent queue to avoid violating parent\u0027s max resource.\n-     */\n-    if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n-        \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n-      killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n+      /**\n+       * check if we need to kill (killable) containers if maximum resource violated.\n+       * Doing this because we will deduct killable resource when going from root.\n+       * For example:\n+       * \u003cpre\u003e\n+       *      Root\n+       *      /   \\\n+       *     a     b\n+       *   /  \\\n+       *  a1  a2\n+       * \u003c/pre\u003e\n+       *\n+       * a: max\u003d10G, used\u003d10G, killable\u003d2G\n+       * a1: used\u003d8G, killable\u003d2G\n+       * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n+       *\n+       * When we get queue-a to allocate resource, even if queue-a\n+       * reaches its max resource, we deduct its used by killable, so we can allocate\n+       * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n+       *\n+       * If scheduler finds a 2G available resource in existing cluster, and assigns it\n+       * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n+       *\n+       * When this happens, we have to preempt killable container (on same or different\n+       * nodes) of parent queue to avoid violating parent\u0027s max resource.\n+       */\n+      if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n+          \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n+        killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n+      }\n+    } finally {\n+      writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void allocateResource(Resource clusterResource,\n      Resource resource, String nodePartition, boolean changeContainerResource) {\n    try {\n      writeLock.lock();\n      super.allocateResource(clusterResource, resource, nodePartition,\n          changeContainerResource);\n\n      /**\n       * check if we need to kill (killable) containers if maximum resource violated.\n       * Doing this because we will deduct killable resource when going from root.\n       * For example:\n       * \u003cpre\u003e\n       *      Root\n       *      /   \\\n       *     a     b\n       *   /  \\\n       *  a1  a2\n       * \u003c/pre\u003e\n       *\n       * a: max\u003d10G, used\u003d10G, killable\u003d2G\n       * a1: used\u003d8G, killable\u003d2G\n       * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n       *\n       * When we get queue-a to allocate resource, even if queue-a\n       * reaches its max resource, we deduct its used by killable, so we can allocate\n       * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n       *\n       * If scheduler finds a 2G available resource in existing cluster, and assigns it\n       * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n       *\n       * When this happens, we have to preempt killable container (on same or different\n       * nodes) of parent queue to avoid violating parent\u0027s max resource.\n       */\n      if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n          \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n        killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "ae14e5d07f1b6702a5160637438028bb03d9387e": {
      "type": "Yintroduced",
      "commitMessage": "YARN-4108. CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n\n(cherry picked from commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913)\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "ae14e5d07f1b6702a5160637438028bb03d9387e",
      "commitAuthor": "Wangda Tan",
      "diff": "@@ -0,0 +1,36 @@\n+  synchronized void allocateResource(Resource clusterResource,\n+      Resource resource, String nodePartition, boolean changeContainerResource) {\n+    super.allocateResource(clusterResource, resource, nodePartition,\n+        changeContainerResource);\n+\n+    /**\n+     * check if we need to kill (killable) containers if maximum resource violated.\n+     * Doing this because we will deduct killable resource when going from root.\n+     * For example:\n+     * \u003cpre\u003e\n+     *      Root\n+     *      /   \\\n+     *     a     b\n+     *   /  \\\n+     *  a1  a2\n+     * \u003c/pre\u003e\n+     *\n+     * a: max\u003d10G, used\u003d10G, killable\u003d2G\n+     * a1: used\u003d8G, killable\u003d2G\n+     * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n+     *\n+     * When we get queue-a to allocate resource, even if queue-a\n+     * reaches its max resource, we deduct its used by killable, so we can allocate\n+     * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n+     *\n+     * If scheduler finds a 2G available resource in existing cluster, and assigns it\n+     * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n+     *\n+     * When this happens, we have to preempt killable container (on same or different\n+     * nodes) of parent queue to avoid violating parent\u0027s max resource.\n+     */\n+    if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n+        \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n+      killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void allocateResource(Resource clusterResource,\n      Resource resource, String nodePartition, boolean changeContainerResource) {\n    super.allocateResource(clusterResource, resource, nodePartition,\n        changeContainerResource);\n\n    /**\n     * check if we need to kill (killable) containers if maximum resource violated.\n     * Doing this because we will deduct killable resource when going from root.\n     * For example:\n     * \u003cpre\u003e\n     *      Root\n     *      /   \\\n     *     a     b\n     *   /  \\\n     *  a1  a2\n     * \u003c/pre\u003e\n     *\n     * a: max\u003d10G, used\u003d10G, killable\u003d2G\n     * a1: used\u003d8G, killable\u003d2G\n     * a2: used\u003d2G, pending\u003d2G, killable\u003d0G\n     *\n     * When we get queue-a to allocate resource, even if queue-a\n     * reaches its max resource, we deduct its used by killable, so we can allocate\n     * at most 2G resources. ResourceLimits passed down to a2 has headroom set to 2G.\n     *\n     * If scheduler finds a 2G available resource in existing cluster, and assigns it\n     * to a2, now a2\u0027s used\u003d 2G + 2G \u003d 4G, and a\u0027s used \u003d 8G + 4G \u003d 12G \u003e 10G\n     *\n     * When this happens, we have to preempt killable container (on same or different\n     * nodes) of parent queue to avoid violating parent\u0027s max resource.\n     */\n    if (getQueueCapacities().getAbsoluteMaximumCapacity(nodePartition)\n        \u003c getQueueCapacities().getAbsoluteUsedCapacity(nodePartition)) {\n      killContainersToEnforceMaxQueueCapacity(nodePartition, clusterResource);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java"
    }
  }
}