{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "computeTransferReadTimeout",
  "functionId": "computeTransferReadTimeout",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 1429,
  "functionEndLine": 1435,
  "numCommitsSeen": 35,
  "timeTaken": 1343,
  "changeHistory": [
    "3ae652f82110a52bf239f3c1849b48981558eb19"
  ],
  "changeHistoryShort": {
    "3ae652f82110a52bf239f3c1849b48981558eb19": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3ae652f82110a52bf239f3c1849b48981558eb19": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-10609. Uncaught InvalidEncryptionKeyException during pipeline recovery may abort downstream applications. Contributed by Wei-Chiu Chuang.\n",
      "commitDate": "26/09/16 1:11 PM",
      "commitName": "3ae652f82110a52bf239f3c1849b48981558eb19",
      "commitAuthor": "Wei-Chiu Chuang",
      "diff": "@@ -0,0 +1,7 @@\n+  private long computeTransferReadTimeout() {\n+    // transfer timeout multiplier based on the transfer size\n+    // One per 200 packets \u003d 12.8MB. Minimum is 2.\n+    int multi \u003d 2\n+        + (int) (bytesSent / dfsClient.getConf().getWritePacketSize()) / 200;\n+    return dfsClient.getDatanodeReadTimeout(multi);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private long computeTransferReadTimeout() {\n    // transfer timeout multiplier based on the transfer size\n    // One per 200 packets \u003d 12.8MB. Minimum is 2.\n    int multi \u003d 2\n        + (int) (bytesSent / dfsClient.getConf().getWritePacketSize()) / 200;\n    return dfsClient.getDatanodeReadTimeout(multi);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    }
  }
}