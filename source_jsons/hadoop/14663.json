{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "JsonUtil.java",
  "functionName": "toJsonString",
  "functionId": "toJsonString___status-HdfsFileStatus(modifiers-final)__includeType-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
  "functionStartLine": 105,
  "functionEndLine": 117,
  "numCommitsSeen": 83,
  "timeTaken": 4856,
  "changeHistory": [
    "85bab5fb572194fda38854f1f21c670925058009",
    "1bcfab8e7fd8562f1829ac484d2f6c91f7afe3d6",
    "ab04ff9efe632b4eca6faca7407ac35e00e6a379",
    "e2262d3d18c6d5c2aa20f96920104dc07271b869",
    "e3803d002c660f18a5c2ecf32344fd6f3f491a5b",
    "9b250d74f029f8ccf3a613f9fb74f59838a66ec1",
    "65158e478f135ec051c1939bd5f371818365dffd",
    "6ecf78a99b4b10d4c569cc2b335060ab988b8001",
    "4525c4a25ba90163c9543116e2bd54239e0dd097",
    "94c631af1fc49f5ae5881fcd5f0e80b17308d15d",
    "6afe3e0d22caa2b0752d52ddf7794c25a66cc9c8",
    "676f488efffd50eb47e75cd750f9bc948b9e12fb",
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
    "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de",
    "61d0b7530c8978c095ab6f62d9d38e168bd829c6"
  ],
  "changeHistoryShort": {
    "85bab5fb572194fda38854f1f21c670925058009": "Ybodychange",
    "1bcfab8e7fd8562f1829ac484d2f6c91f7afe3d6": "Ybodychange",
    "ab04ff9efe632b4eca6faca7407ac35e00e6a379": "Ybodychange",
    "e2262d3d18c6d5c2aa20f96920104dc07271b869": "Ybodychange",
    "e3803d002c660f18a5c2ecf32344fd6f3f491a5b": "Ybodychange",
    "9b250d74f029f8ccf3a613f9fb74f59838a66ec1": "Ybodychange",
    "65158e478f135ec051c1939bd5f371818365dffd": "Ybodychange",
    "6ecf78a99b4b10d4c569cc2b335060ab988b8001": "Ybodychange",
    "4525c4a25ba90163c9543116e2bd54239e0dd097": "Ybodychange",
    "94c631af1fc49f5ae5881fcd5f0e80b17308d15d": "Ybodychange",
    "6afe3e0d22caa2b0752d52ddf7794c25a66cc9c8": "Ybodychange",
    "676f488efffd50eb47e75cd750f9bc948b9e12fb": "Ymultichange(Yparameterchange,Ybodychange)",
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57": "Ybodychange",
    "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de": "Ybodychange",
    "61d0b7530c8978c095ab6f62d9d38e168bd829c6": "Yintroduced"
  },
  "changeHistoryDetails": {
    "85bab5fb572194fda38854f1f21c670925058009": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10784. Implement WebHdfsFileSystem#listStatusIterator.\n",
      "commitDate": "31/08/16 2:29 PM",
      "commitName": "85bab5fb572194fda38854f1f21c670925058009",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "12/02/16 8:57 AM",
      "commitNameOld": "e6a7044b8530afded8f8e86ff309dd0e4d39238a",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 201.19,
      "commitsBetweenForRepo": 1432,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,13 @@\n   public static String toJsonString(final HdfsFileStatus status,\n       boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n     }\n-    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n-    m.put(\"pathSuffix\", status.getLocalName());\n-    m.put(\"type\", WebHdfsConstants.PathType.valueOf(status));\n-    if (status.isSymlink()) {\n-      m.put(\"symlink\", status.getSymlink());\n-    }\n-\n-    m.put(\"length\", status.getLen());\n-    m.put(\"owner\", status.getOwner());\n-    m.put(\"group\", status.getGroup());\n-    FsPermission perm \u003d status.getPermission();\n-    m.put(\"permission\", toString(perm));\n-    if (perm.getAclBit()) {\n-      m.put(\"aclBit\", true);\n-    }\n-    if (perm.getEncryptedBit()) {\n-      m.put(\"encBit\", true);\n-    }\n-    m.put(\"accessTime\", status.getAccessTime());\n-    m.put(\"modificationTime\", status.getModificationTime());\n-    m.put(\"blockSize\", status.getBlockSize());\n-    m.put(\"replication\", status.getReplication());\n-    m.put(\"fileId\", status.getFileId());\n-    m.put(\"childrenNum\", status.getChildrenNum());\n-    m.put(\"storagePolicy\", status.getStoragePolicy());\n+    final Map\u003cString, Object\u003e m \u003d toJsonMap(status);\n     try {\n       return includeType ?\n           toJsonString(FileStatus.class, m) : MAPPER.writeValueAsString(m);\n     } catch (IOException ignored) {\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d toJsonMap(status);\n    try {\n      return includeType ?\n          toJsonString(FileStatus.class, m) : MAPPER.writeValueAsString(m);\n    } catch (IOException ignored) {\n    }\n    return null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "1bcfab8e7fd8562f1829ac484d2f6c91f7afe3d6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9724. Degraded performance in WebHDFS listing as it does not reuse ObjectMapper. Contributed by Akira Ajisaka.\n",
      "commitDate": "04/02/16 11:34 AM",
      "commitName": "1bcfab8e7fd8562f1829ac484d2f6c91f7afe3d6",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "25/11/15 1:41 PM",
      "commitNameOld": "e3d673901b396cf5bbede5ed6f607ce68301ec0a",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 70.91,
      "commitsBetweenForRepo": 419,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,37 @@\n   public static String toJsonString(final HdfsFileStatus status,\n       boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n     }\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"pathSuffix\", status.getLocalName());\n     m.put(\"type\", WebHdfsConstants.PathType.valueOf(status));\n     if (status.isSymlink()) {\n       m.put(\"symlink\", status.getSymlink());\n     }\n \n     m.put(\"length\", status.getLen());\n     m.put(\"owner\", status.getOwner());\n     m.put(\"group\", status.getGroup());\n     FsPermission perm \u003d status.getPermission();\n     m.put(\"permission\", toString(perm));\n     if (perm.getAclBit()) {\n       m.put(\"aclBit\", true);\n     }\n     if (perm.getEncryptedBit()) {\n       m.put(\"encBit\", true);\n     }\n     m.put(\"accessTime\", status.getAccessTime());\n     m.put(\"modificationTime\", status.getModificationTime());\n     m.put(\"blockSize\", status.getBlockSize());\n     m.put(\"replication\", status.getReplication());\n     m.put(\"fileId\", status.getFileId());\n     m.put(\"childrenNum\", status.getChildrenNum());\n     m.put(\"storagePolicy\", status.getStoragePolicy());\n-    ObjectMapper mapper \u003d new ObjectMapper();\n     try {\n       return includeType ?\n-          toJsonString(FileStatus.class, m) : mapper.writeValueAsString(m);\n+          toJsonString(FileStatus.class, m) : MAPPER.writeValueAsString(m);\n     } catch (IOException ignored) {\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"pathSuffix\", status.getLocalName());\n    m.put(\"type\", WebHdfsConstants.PathType.valueOf(status));\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"length\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    FsPermission perm \u003d status.getPermission();\n    m.put(\"permission\", toString(perm));\n    if (perm.getAclBit()) {\n      m.put(\"aclBit\", true);\n    }\n    if (perm.getEncryptedBit()) {\n      m.put(\"encBit\", true);\n    }\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    m.put(\"fileId\", status.getFileId());\n    m.put(\"childrenNum\", status.getChildrenNum());\n    m.put(\"storagePolicy\", status.getStoragePolicy());\n    try {\n      return includeType ?\n          toJsonString(FileStatus.class, m) : MAPPER.writeValueAsString(m);\n    } catch (IOException ignored) {\n    }\n    return null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "ab04ff9efe632b4eca6faca7407ac35e00e6a379": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8080. Separate JSON related routines used by WebHdfsFileSystem to a package local class. Contributed by Haohui Mai.\n",
      "commitDate": "07/04/15 9:30 PM",
      "commitName": "ab04ff9efe632b4eca6faca7407ac35e00e6a379",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/03/15 10:24 AM",
      "commitNameOld": "3d0708bdb0a75af3d87bbac9f6c4ffbcabab98ca",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 12.46,
      "commitsBetweenForRepo": 98,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   public static String toJsonString(final HdfsFileStatus status,\n       boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n     }\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"pathSuffix\", status.getLocalName());\n-    m.put(\"type\", PathType.valueOf(status));\n+    m.put(\"type\", WebHdfsConstants.PathType.valueOf(status));\n     if (status.isSymlink()) {\n       m.put(\"symlink\", status.getSymlink());\n     }\n \n     m.put(\"length\", status.getLen());\n     m.put(\"owner\", status.getOwner());\n     m.put(\"group\", status.getGroup());\n     FsPermission perm \u003d status.getPermission();\n     m.put(\"permission\", toString(perm));\n     if (perm.getAclBit()) {\n       m.put(\"aclBit\", true);\n     }\n     if (perm.getEncryptedBit()) {\n       m.put(\"encBit\", true);\n     }\n     m.put(\"accessTime\", status.getAccessTime());\n     m.put(\"modificationTime\", status.getModificationTime());\n     m.put(\"blockSize\", status.getBlockSize());\n     m.put(\"replication\", status.getReplication());\n     m.put(\"fileId\", status.getFileId());\n     m.put(\"childrenNum\", status.getChildrenNum());\n     m.put(\"storagePolicy\", status.getStoragePolicy());\n     ObjectMapper mapper \u003d new ObjectMapper();\n     try {\n       return includeType ?\n           toJsonString(FileStatus.class, m) : mapper.writeValueAsString(m);\n     } catch (IOException ignored) {\n     }\n     return null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"pathSuffix\", status.getLocalName());\n    m.put(\"type\", WebHdfsConstants.PathType.valueOf(status));\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"length\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    FsPermission perm \u003d status.getPermission();\n    m.put(\"permission\", toString(perm));\n    if (perm.getAclBit()) {\n      m.put(\"aclBit\", true);\n    }\n    if (perm.getEncryptedBit()) {\n      m.put(\"encBit\", true);\n    }\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    m.put(\"fileId\", status.getFileId());\n    m.put(\"childrenNum\", status.getChildrenNum());\n    m.put(\"storagePolicy\", status.getStoragePolicy());\n    ObjectMapper mapper \u003d new ObjectMapper();\n    try {\n      return includeType ?\n          toJsonString(FileStatus.class, m) : mapper.writeValueAsString(m);\n    } catch (IOException ignored) {\n    }\n    return null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "e2262d3d18c6d5c2aa20f96920104dc07271b869": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6565. Use jackson instead jetty json in hdfs-client. Contributed by Akira AJISAKA.\n",
      "commitDate": "03/03/15 5:54 PM",
      "commitName": "e2262d3d18c6d5c2aa20f96920104dc07271b869",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "08/12/14 10:23 AM",
      "commitNameOld": "ffe942b82c1208bc7b22899da3a233944cb5ab52",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 85.31,
      "commitsBetweenForRepo": 663,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,38 @@\n   public static String toJsonString(final HdfsFileStatus status,\n       boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n     }\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"pathSuffix\", status.getLocalName());\n     m.put(\"type\", PathType.valueOf(status));\n     if (status.isSymlink()) {\n       m.put(\"symlink\", status.getSymlink());\n     }\n \n     m.put(\"length\", status.getLen());\n     m.put(\"owner\", status.getOwner());\n     m.put(\"group\", status.getGroup());\n     FsPermission perm \u003d status.getPermission();\n     m.put(\"permission\", toString(perm));\n     if (perm.getAclBit()) {\n       m.put(\"aclBit\", true);\n     }\n     if (perm.getEncryptedBit()) {\n       m.put(\"encBit\", true);\n     }\n     m.put(\"accessTime\", status.getAccessTime());\n     m.put(\"modificationTime\", status.getModificationTime());\n     m.put(\"blockSize\", status.getBlockSize());\n     m.put(\"replication\", status.getReplication());\n     m.put(\"fileId\", status.getFileId());\n     m.put(\"childrenNum\", status.getChildrenNum());\n     m.put(\"storagePolicy\", status.getStoragePolicy());\n-    return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n+    ObjectMapper mapper \u003d new ObjectMapper();\n+    try {\n+      return includeType ?\n+          toJsonString(FileStatus.class, m) : mapper.writeValueAsString(m);\n+    } catch (IOException ignored) {\n+    }\n+    return null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"pathSuffix\", status.getLocalName());\n    m.put(\"type\", PathType.valueOf(status));\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"length\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    FsPermission perm \u003d status.getPermission();\n    m.put(\"permission\", toString(perm));\n    if (perm.getAclBit()) {\n      m.put(\"aclBit\", true);\n    }\n    if (perm.getEncryptedBit()) {\n      m.put(\"encBit\", true);\n    }\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    m.put(\"fileId\", status.getFileId());\n    m.put(\"childrenNum\", status.getChildrenNum());\n    m.put(\"storagePolicy\", status.getStoragePolicy());\n    ObjectMapper mapper \u003d new ObjectMapper();\n    try {\n      return includeType ?\n          toJsonString(FileStatus.class, m) : mapper.writeValueAsString(m);\n    } catch (IOException ignored) {\n    }\n    return null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "e3803d002c660f18a5c2ecf32344fd6f3f491a5b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6843. Create FileStatus isEncrypted() method (clamb via cmccabe)\n",
      "commitDate": "17/09/14 12:55 PM",
      "commitName": "e3803d002c660f18a5c2ecf32344fd6f3f491a5b",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "27/06/14 1:43 PM",
      "commitNameOld": "2efea952139b30dd1c881eed0b443ffa72be6dce",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 81.97,
      "commitsBetweenForRepo": 683,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,31 @@\n   public static String toJsonString(final HdfsFileStatus status,\n       boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n     }\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"pathSuffix\", status.getLocalName());\n     m.put(\"type\", PathType.valueOf(status));\n     if (status.isSymlink()) {\n       m.put(\"symlink\", status.getSymlink());\n     }\n \n     m.put(\"length\", status.getLen());\n     m.put(\"owner\", status.getOwner());\n     m.put(\"group\", status.getGroup());\n     FsPermission perm \u003d status.getPermission();\n     m.put(\"permission\", toString(perm));\n     if (perm.getAclBit()) {\n       m.put(\"aclBit\", true);\n     }\n+    if (perm.getEncryptedBit()) {\n+      m.put(\"encBit\", true);\n+    }\n     m.put(\"accessTime\", status.getAccessTime());\n     m.put(\"modificationTime\", status.getModificationTime());\n     m.put(\"blockSize\", status.getBlockSize());\n     m.put(\"replication\", status.getReplication());\n     m.put(\"fileId\", status.getFileId());\n     m.put(\"childrenNum\", status.getChildrenNum());\n     return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"pathSuffix\", status.getLocalName());\n    m.put(\"type\", PathType.valueOf(status));\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"length\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    FsPermission perm \u003d status.getPermission();\n    m.put(\"permission\", toString(perm));\n    if (perm.getAclBit()) {\n      m.put(\"aclBit\", true);\n    }\n    if (perm.getEncryptedBit()) {\n      m.put(\"encBit\", true);\n    }\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    m.put(\"fileId\", status.getFileId());\n    m.put(\"childrenNum\", status.getChildrenNum());\n    return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "9b250d74f029f8ccf3a613f9fb74f59838a66ec1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6847. Support storage policy on directories and include storage policy in HdfsFileStatus.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-6584@1618416 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/08/14 1:58 PM",
      "commitName": "9b250d74f029f8ccf3a613f9fb74f59838a66ec1",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "05/06/14 8:48 PM",
      "commitNameOld": "6a4f6d6b3e41e660c214b77c9ed43d4c65200b6a",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 71.72,
      "commitsBetweenForRepo": 480,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,29 @@\n   public static String toJsonString(final HdfsFileStatus status,\n       boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n     }\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"pathSuffix\", status.getLocalName());\n     m.put(\"type\", PathType.valueOf(status));\n     if (status.isSymlink()) {\n       m.put(\"symlink\", status.getSymlink());\n     }\n \n     m.put(\"length\", status.getLen());\n     m.put(\"owner\", status.getOwner());\n     m.put(\"group\", status.getGroup());\n     FsPermission perm \u003d status.getPermission();\n     m.put(\"permission\", toString(perm));\n     if (perm.getAclBit()) {\n       m.put(\"aclBit\", true);\n     }\n     m.put(\"accessTime\", status.getAccessTime());\n     m.put(\"modificationTime\", status.getModificationTime());\n     m.put(\"blockSize\", status.getBlockSize());\n     m.put(\"replication\", status.getReplication());\n     m.put(\"fileId\", status.getFileId());\n     m.put(\"childrenNum\", status.getChildrenNum());\n+    m.put(\"storagePolicy\", status.getStoragePolicy());\n     return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"pathSuffix\", status.getLocalName());\n    m.put(\"type\", PathType.valueOf(status));\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"length\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    FsPermission perm \u003d status.getPermission();\n    m.put(\"permission\", toString(perm));\n    if (perm.getAclBit()) {\n      m.put(\"aclBit\", true);\n    }\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    m.put(\"fileId\", status.getFileId());\n    m.put(\"childrenNum\", status.getChildrenNum());\n    m.put(\"storagePolicy\", status.getStoragePolicy());\n    return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "65158e478f135ec051c1939bd5f371818365dffd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6326. WebHdfs ACL compatibility is broken. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594743 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/05/14 3:35 PM",
      "commitName": "65158e478f135ec051c1939bd5f371818365dffd",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "24/03/14 10:51 PM",
      "commitNameOld": "3a61d25457606b93f7e99a48fe8f66984f4084b0",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 50.7,
      "commitsBetweenForRepo": 307,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,28 @@\n   public static String toJsonString(final HdfsFileStatus status,\n       boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n     }\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"pathSuffix\", status.getLocalName());\n     m.put(\"type\", PathType.valueOf(status));\n     if (status.isSymlink()) {\n       m.put(\"symlink\", status.getSymlink());\n     }\n \n     m.put(\"length\", status.getLen());\n     m.put(\"owner\", status.getOwner());\n     m.put(\"group\", status.getGroup());\n-    m.put(\"permission\", toString(status.getPermission()));\n+    FsPermission perm \u003d status.getPermission();\n+    m.put(\"permission\", toString(perm));\n+    if (perm.getAclBit()) {\n+      m.put(\"aclBit\", true);\n+    }\n     m.put(\"accessTime\", status.getAccessTime());\n     m.put(\"modificationTime\", status.getModificationTime());\n     m.put(\"blockSize\", status.getBlockSize());\n     m.put(\"replication\", status.getReplication());\n     m.put(\"fileId\", status.getFileId());\n     m.put(\"childrenNum\", status.getChildrenNum());\n     return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"pathSuffix\", status.getLocalName());\n    m.put(\"type\", PathType.valueOf(status));\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"length\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    FsPermission perm \u003d status.getPermission();\n    m.put(\"permission\", toString(perm));\n    if (perm.getAclBit()) {\n      m.put(\"aclBit\", true);\n    }\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    m.put(\"fileId\", status.getFileId());\n    m.put(\"childrenNum\", status.getChildrenNum());\n    return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "6ecf78a99b4b10d4c569cc2b335060ab988b8001": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4772. Add number of children in HdfsFileStatus. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1495253 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/06/13 5:32 PM",
      "commitName": "6ecf78a99b4b10d4c569cc2b335060ab988b8001",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "20/02/13 7:19 PM",
      "commitNameOld": "567ab4335f8ded3c03bdb0ada59fef4da6a36289",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 119.88,
      "commitsBetweenForRepo": 774,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   public static String toJsonString(final HdfsFileStatus status,\n       boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n     }\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"pathSuffix\", status.getLocalName());\n     m.put(\"type\", PathType.valueOf(status));\n     if (status.isSymlink()) {\n       m.put(\"symlink\", status.getSymlink());\n     }\n \n     m.put(\"length\", status.getLen());\n     m.put(\"owner\", status.getOwner());\n     m.put(\"group\", status.getGroup());\n     m.put(\"permission\", toString(status.getPermission()));\n     m.put(\"accessTime\", status.getAccessTime());\n     m.put(\"modificationTime\", status.getModificationTime());\n     m.put(\"blockSize\", status.getBlockSize());\n     m.put(\"replication\", status.getReplication());\n     m.put(\"fileId\", status.getFileId());\n+    m.put(\"childrenNum\", status.getChildrenNum());\n     return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"pathSuffix\", status.getLocalName());\n    m.put(\"type\", PathType.valueOf(status));\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"length\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    m.put(\"permission\", toString(status.getPermission()));\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    m.put(\"fileId\", status.getFileId());\n    m.put(\"childrenNum\", status.getChildrenNum());\n    return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "4525c4a25ba90163c9543116e2bd54239e0dd097": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4340. Update addBlock() to inculde inode id as additional argument. Contributed Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1443169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/02/13 11:52 AM",
      "commitName": "4525c4a25ba90163c9543116e2bd54239e0dd097",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "21/08/12 2:44 AM",
      "commitNameOld": "f2dd818201402d0ca8a7049ba7abf77188443a64",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 169.42,
      "commitsBetweenForRepo": 885,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,23 @@\n   public static String toJsonString(final HdfsFileStatus status,\n       boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n     }\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"pathSuffix\", status.getLocalName());\n     m.put(\"type\", PathType.valueOf(status));\n     if (status.isSymlink()) {\n       m.put(\"symlink\", status.getSymlink());\n     }\n \n     m.put(\"length\", status.getLen());\n     m.put(\"owner\", status.getOwner());\n     m.put(\"group\", status.getGroup());\n     m.put(\"permission\", toString(status.getPermission()));\n     m.put(\"accessTime\", status.getAccessTime());\n     m.put(\"modificationTime\", status.getModificationTime());\n     m.put(\"blockSize\", status.getBlockSize());\n     m.put(\"replication\", status.getReplication());\n+    m.put(\"fileId\", status.getFileId());\n     return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"pathSuffix\", status.getLocalName());\n    m.put(\"type\", PathType.valueOf(status));\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"length\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    m.put(\"permission\", toString(status.getPermission()));\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    m.put(\"fileId\", status.getFileId());\n    return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "94c631af1fc49f5ae5881fcd5f0e80b17308d15d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2540. Webhdfs: change \"Expect: 100-continue\" to two-step write; change \"HdfsFileStatus\" and \"localName\" respectively to \"FileStatus\" and \"pathSuffix\" in JSON response.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1199396 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/11/11 11:25 AM",
      "commitName": "94c631af1fc49f5ae5881fcd5f0e80b17308d15d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "03/11/11 3:34 PM",
      "commitNameOld": "6afe3e0d22caa2b0752d52ddf7794c25a66cc9c8",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.87,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,22 @@\n   public static String toJsonString(final HdfsFileStatus status,\n       boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n     }\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n-    m.put(\"localName\", status.getLocalName());\n+    m.put(\"pathSuffix\", status.getLocalName());\n     m.put(\"type\", PathType.valueOf(status));\n     if (status.isSymlink()) {\n       m.put(\"symlink\", status.getSymlink());\n     }\n \n     m.put(\"length\", status.getLen());\n     m.put(\"owner\", status.getOwner());\n     m.put(\"group\", status.getGroup());\n     m.put(\"permission\", toString(status.getPermission()));\n     m.put(\"accessTime\", status.getAccessTime());\n     m.put(\"modificationTime\", status.getModificationTime());\n     m.put(\"blockSize\", status.getBlockSize());\n     m.put(\"replication\", status.getReplication());\n-    return includeType ? toJsonString(HdfsFileStatus.class, m) : \n-      JSON.toString(m);\n+    return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"pathSuffix\", status.getLocalName());\n    m.put(\"type\", PathType.valueOf(status));\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"length\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    m.put(\"permission\", toString(status.getPermission()));\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    return includeType ? toJsonString(FileStatus.class, m): JSON.toString(m);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "6afe3e0d22caa2b0752d52ddf7794c25a66cc9c8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2527. WebHdfs: remove the use of \"Range\" header in Open; use ugi username if renewer parameter is null in GetDelegationToken; response OK when setting replication for non-files; rename GETFILEBLOCKLOCATIONS to GET_BLOCK_LOCATIONS and state that it is a private unstable API; replace isDirectory and isSymlink with enum {FILE, DIRECTORY, SYMLINK} in HdfsFileStatus JSON object. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1197329 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/11/11 3:34 PM",
      "commitName": "6afe3e0d22caa2b0752d52ddf7794c25a66cc9c8",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "27/10/11 4:13 PM",
      "commitNameOld": "8cb0d4b380e0fd4437310c1dd6ef8b8995cc383d",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.97,
      "commitsBetweenForRepo": 91,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,23 @@\n   public static String toJsonString(final HdfsFileStatus status,\n       boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n     }\n     final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n     m.put(\"localName\", status.getLocalName());\n-    m.put(\"isDir\", status.isDir());\n-    m.put(\"isSymlink\", status.isSymlink());\n+    m.put(\"type\", PathType.valueOf(status));\n     if (status.isSymlink()) {\n       m.put(\"symlink\", status.getSymlink());\n     }\n \n-    m.put(\"len\", status.getLen());\n+    m.put(\"length\", status.getLen());\n     m.put(\"owner\", status.getOwner());\n     m.put(\"group\", status.getGroup());\n     m.put(\"permission\", toString(status.getPermission()));\n     m.put(\"accessTime\", status.getAccessTime());\n     m.put(\"modificationTime\", status.getModificationTime());\n     m.put(\"blockSize\", status.getBlockSize());\n     m.put(\"replication\", status.getReplication());\n     return includeType ? toJsonString(HdfsFileStatus.class, m) : \n       JSON.toString(m);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"localName\", status.getLocalName());\n    m.put(\"type\", PathType.valueOf(status));\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"length\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    m.put(\"permission\", toString(status.getPermission()));\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    return includeType ? toJsonString(HdfsFileStatus.class, m) : \n      JSON.toString(m);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "676f488efffd50eb47e75cd750f9bc948b9e12fb": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-2404. webhdfs liststatus json response is not correct. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1180757 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/10/11 6:49 PM",
      "commitName": "676f488efffd50eb47e75cd750f9bc948b9e12fb",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2404. webhdfs liststatus json response is not correct. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1180757 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/10/11 6:49 PM",
          "commitName": "676f488efffd50eb47e75cd750f9bc948b9e12fb",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "05/10/11 4:29 AM",
          "commitNameOld": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 4.6,
          "commitsBetweenForRepo": 35,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,24 @@\n-  public static String toJsonString(final HdfsFileStatus status) {\n+  public static String toJsonString(final HdfsFileStatus status,\n+      boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n-    } else {\n-      final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n-      m.put(\"localName\", status.getLocalName());\n-      m.put(\"isDir\", status.isDir());\n-      m.put(\"isSymlink\", status.isSymlink());\n-      if (status.isSymlink()) {\n-        m.put(\"symlink\", status.getSymlink());\n-      }\n-\n-      m.put(\"len\", status.getLen());\n-      m.put(\"owner\", status.getOwner());\n-      m.put(\"group\", status.getGroup());\n-      m.put(\"permission\", toString(status.getPermission()));\n-      m.put(\"accessTime\", status.getAccessTime());\n-      m.put(\"modificationTime\", status.getModificationTime());\n-      m.put(\"blockSize\", status.getBlockSize());\n-      m.put(\"replication\", status.getReplication());\n-      return toJsonString(HdfsFileStatus.class, m);\n     }\n+    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n+    m.put(\"localName\", status.getLocalName());\n+    m.put(\"isDir\", status.isDir());\n+    m.put(\"isSymlink\", status.isSymlink());\n+    if (status.isSymlink()) {\n+      m.put(\"symlink\", status.getSymlink());\n+    }\n+\n+    m.put(\"len\", status.getLen());\n+    m.put(\"owner\", status.getOwner());\n+    m.put(\"group\", status.getGroup());\n+    m.put(\"permission\", toString(status.getPermission()));\n+    m.put(\"accessTime\", status.getAccessTime());\n+    m.put(\"modificationTime\", status.getModificationTime());\n+    m.put(\"blockSize\", status.getBlockSize());\n+    m.put(\"replication\", status.getReplication());\n+    return includeType ? toJsonString(HdfsFileStatus.class, m) : \n+      JSON.toString(m);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"localName\", status.getLocalName());\n    m.put(\"isDir\", status.isDir());\n    m.put(\"isSymlink\", status.isSymlink());\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"len\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    m.put(\"permission\", toString(status.getPermission()));\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    return includeType ? toJsonString(HdfsFileStatus.class, m) : \n      JSON.toString(m);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {
            "oldValue": "[status-HdfsFileStatus(modifiers-final)]",
            "newValue": "[status-HdfsFileStatus(modifiers-final), includeType-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2404. webhdfs liststatus json response is not correct. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1180757 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/10/11 6:49 PM",
          "commitName": "676f488efffd50eb47e75cd750f9bc948b9e12fb",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "05/10/11 4:29 AM",
          "commitNameOld": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 4.6,
          "commitsBetweenForRepo": 35,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,24 @@\n-  public static String toJsonString(final HdfsFileStatus status) {\n+  public static String toJsonString(final HdfsFileStatus status,\n+      boolean includeType) {\n     if (status \u003d\u003d null) {\n       return null;\n-    } else {\n-      final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n-      m.put(\"localName\", status.getLocalName());\n-      m.put(\"isDir\", status.isDir());\n-      m.put(\"isSymlink\", status.isSymlink());\n-      if (status.isSymlink()) {\n-        m.put(\"symlink\", status.getSymlink());\n-      }\n-\n-      m.put(\"len\", status.getLen());\n-      m.put(\"owner\", status.getOwner());\n-      m.put(\"group\", status.getGroup());\n-      m.put(\"permission\", toString(status.getPermission()));\n-      m.put(\"accessTime\", status.getAccessTime());\n-      m.put(\"modificationTime\", status.getModificationTime());\n-      m.put(\"blockSize\", status.getBlockSize());\n-      m.put(\"replication\", status.getReplication());\n-      return toJsonString(HdfsFileStatus.class, m);\n     }\n+    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n+    m.put(\"localName\", status.getLocalName());\n+    m.put(\"isDir\", status.isDir());\n+    m.put(\"isSymlink\", status.isSymlink());\n+    if (status.isSymlink()) {\n+      m.put(\"symlink\", status.getSymlink());\n+    }\n+\n+    m.put(\"len\", status.getLen());\n+    m.put(\"owner\", status.getOwner());\n+    m.put(\"group\", status.getGroup());\n+    m.put(\"permission\", toString(status.getPermission()));\n+    m.put(\"accessTime\", status.getAccessTime());\n+    m.put(\"modificationTime\", status.getModificationTime());\n+    m.put(\"blockSize\", status.getBlockSize());\n+    m.put(\"replication\", status.getReplication());\n+    return includeType ? toJsonString(HdfsFileStatus.class, m) : \n+      JSON.toString(m);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static String toJsonString(final HdfsFileStatus status,\n      boolean includeType) {\n    if (status \u003d\u003d null) {\n      return null;\n    }\n    final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n    m.put(\"localName\", status.getLocalName());\n    m.put(\"isDir\", status.isDir());\n    m.put(\"isSymlink\", status.isSymlink());\n    if (status.isSymlink()) {\n      m.put(\"symlink\", status.getSymlink());\n    }\n\n    m.put(\"len\", status.getLen());\n    m.put(\"owner\", status.getOwner());\n    m.put(\"group\", status.getGroup());\n    m.put(\"permission\", toString(status.getPermission()));\n    m.put(\"accessTime\", status.getAccessTime());\n    m.put(\"modificationTime\", status.getModificationTime());\n    m.put(\"blockSize\", status.getBlockSize());\n    m.put(\"replication\", status.getReplication());\n    return includeType ? toJsonString(HdfsFileStatus.class, m) : \n      JSON.toString(m);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
          "extendedDetails": {}
        }
      ]
    },
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2395. Add a root element in the JSON responses of webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/11 4:29 AM",
      "commitName": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/09/11 9:49 PM",
      "commitNameOld": "dc8464f943b61b795df0cc8baec171bf07355763",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.28,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public static String toJsonString(final HdfsFileStatus status) {\n     if (status \u003d\u003d null) {\n       return null;\n     } else {\n-      final Map\u003cString, Object\u003e m \u003d jsonMap.get();\n+      final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n       m.put(\"localName\", status.getLocalName());\n       m.put(\"isDir\", status.isDir());\n       m.put(\"isSymlink\", status.isSymlink());\n       if (status.isSymlink()) {\n         m.put(\"symlink\", status.getSymlink());\n       }\n \n       m.put(\"len\", status.getLen());\n       m.put(\"owner\", status.getOwner());\n       m.put(\"group\", status.getGroup());\n       m.put(\"permission\", toString(status.getPermission()));\n       m.put(\"accessTime\", status.getAccessTime());\n       m.put(\"modificationTime\", status.getModificationTime());\n       m.put(\"blockSize\", status.getBlockSize());\n       m.put(\"replication\", status.getReplication());\n-      return JSON.toString(m);\n+      return toJsonString(HdfsFileStatus.class, m);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status) {\n    if (status \u003d\u003d null) {\n      return null;\n    } else {\n      final Map\u003cString, Object\u003e m \u003d new TreeMap\u003cString, Object\u003e();\n      m.put(\"localName\", status.getLocalName());\n      m.put(\"isDir\", status.isDir());\n      m.put(\"isSymlink\", status.isSymlink());\n      if (status.isSymlink()) {\n        m.put(\"symlink\", status.getSymlink());\n      }\n\n      m.put(\"len\", status.getLen());\n      m.put(\"owner\", status.getOwner());\n      m.put(\"group\", status.getGroup());\n      m.put(\"permission\", toString(status.getPermission()));\n      m.put(\"accessTime\", status.getAccessTime());\n      m.put(\"modificationTime\", status.getModificationTime());\n      m.put(\"blockSize\", status.getBlockSize());\n      m.put(\"replication\", status.getReplication());\n      return toJsonString(HdfsFileStatus.class, m);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2340. Support getFileBlockLocations and getDelegationToken in webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1173468 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/09/11 7:56 PM",
      "commitName": "4dc4e9e63f7385ddd1d64ae1345e0d32a4acb9de",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "10/09/11 6:41 PM",
      "commitNameOld": "61d0b7530c8978c095ab6f62d9d38e168bd829c6",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 10.05,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,23 @@\n   public static String toJsonString(final HdfsFileStatus status) {\n-    final Map\u003cString, Object\u003e m \u003d jsonMap.get();\n     if (status \u003d\u003d null) {\n-      m.put(\"isNull\", true);\n+      return null;\n     } else {\n-      m.put(\"isNull\", false);\n+      final Map\u003cString, Object\u003e m \u003d jsonMap.get();\n       m.put(\"localName\", status.getLocalName());\n       m.put(\"isDir\", status.isDir());\n       m.put(\"isSymlink\", status.isSymlink());\n       if (status.isSymlink()) {\n         m.put(\"symlink\", status.getSymlink());\n       }\n \n       m.put(\"len\", status.getLen());\n       m.put(\"owner\", status.getOwner());\n       m.put(\"group\", status.getGroup());\n       m.put(\"permission\", toString(status.getPermission()));\n       m.put(\"accessTime\", status.getAccessTime());\n       m.put(\"modificationTime\", status.getModificationTime());\n       m.put(\"blockSize\", status.getBlockSize());\n       m.put(\"replication\", status.getReplication());\n+      return JSON.toString(m);\n     }\n-    return JSON.toString(m);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status) {\n    if (status \u003d\u003d null) {\n      return null;\n    } else {\n      final Map\u003cString, Object\u003e m \u003d jsonMap.get();\n      m.put(\"localName\", status.getLocalName());\n      m.put(\"isDir\", status.isDir());\n      m.put(\"isSymlink\", status.isSymlink());\n      if (status.isSymlink()) {\n        m.put(\"symlink\", status.getSymlink());\n      }\n\n      m.put(\"len\", status.getLen());\n      m.put(\"owner\", status.getOwner());\n      m.put(\"group\", status.getGroup());\n      m.put(\"permission\", toString(status.getPermission()));\n      m.put(\"accessTime\", status.getAccessTime());\n      m.put(\"modificationTime\", status.getModificationTime());\n      m.put(\"blockSize\", status.getBlockSize());\n      m.put(\"replication\", status.getReplication());\n      return JSON.toString(m);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
      "extendedDetails": {}
    },
    "61d0b7530c8978c095ab6f62d9d38e168bd829c6": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2284. Add a new FileSystem, webhdfs://, for supporting write Http access to HDFS.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1167662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/09/11 6:41 PM",
      "commitName": "61d0b7530c8978c095ab6f62d9d38e168bd829c6",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,24 @@\n+  public static String toJsonString(final HdfsFileStatus status) {\n+    final Map\u003cString, Object\u003e m \u003d jsonMap.get();\n+    if (status \u003d\u003d null) {\n+      m.put(\"isNull\", true);\n+    } else {\n+      m.put(\"isNull\", false);\n+      m.put(\"localName\", status.getLocalName());\n+      m.put(\"isDir\", status.isDir());\n+      m.put(\"isSymlink\", status.isSymlink());\n+      if (status.isSymlink()) {\n+        m.put(\"symlink\", status.getSymlink());\n+      }\n+\n+      m.put(\"len\", status.getLen());\n+      m.put(\"owner\", status.getOwner());\n+      m.put(\"group\", status.getGroup());\n+      m.put(\"permission\", toString(status.getPermission()));\n+      m.put(\"accessTime\", status.getAccessTime());\n+      m.put(\"modificationTime\", status.getModificationTime());\n+      m.put(\"blockSize\", status.getBlockSize());\n+      m.put(\"replication\", status.getReplication());\n+    }\n+    return JSON.toString(m);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static String toJsonString(final HdfsFileStatus status) {\n    final Map\u003cString, Object\u003e m \u003d jsonMap.get();\n    if (status \u003d\u003d null) {\n      m.put(\"isNull\", true);\n    } else {\n      m.put(\"isNull\", false);\n      m.put(\"localName\", status.getLocalName());\n      m.put(\"isDir\", status.isDir());\n      m.put(\"isSymlink\", status.isSymlink());\n      if (status.isSymlink()) {\n        m.put(\"symlink\", status.getSymlink());\n      }\n\n      m.put(\"len\", status.getLen());\n      m.put(\"owner\", status.getOwner());\n      m.put(\"group\", status.getGroup());\n      m.put(\"permission\", toString(status.getPermission()));\n      m.put(\"accessTime\", status.getAccessTime());\n      m.put(\"modificationTime\", status.getModificationTime());\n      m.put(\"blockSize\", status.getBlockSize());\n      m.put(\"replication\", status.getReplication());\n    }\n    return JSON.toString(m);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java"
    }
  }
}