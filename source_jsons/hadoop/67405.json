{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HadoopArchives.java",
  "functionName": "writeTopLevelDirs",
  "functionId": "writeTopLevelDirs___srcWriter-SequenceFile.Writer__paths-List__Path____parentPath-Path",
  "sourceFilePath": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
  "functionStartLine": 353,
  "functionEndLine": 408,
  "numCommitsSeen": 18,
  "timeTaken": 4774,
  "changeHistory": [
    "79301e80d7510f055c01a06970bb409607a4197c",
    "0201be46c298e94176ec6297e9d9cdba3afc2bbd",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "79301e80d7510f055c01a06970bb409607a4197c": "Ybodychange",
    "0201be46c298e94176ec6297e9d9cdba3afc2bbd": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "79301e80d7510f055c01a06970bb409607a4197c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11201. Hadoop Archives should support globs resolving to files. Contributed by Gera Shegalov.\n",
      "commitDate": "18/11/14 5:05 PM",
      "commitName": "79301e80d7510f055c01a06970bb409607a4197c",
      "commitAuthor": "cnauroth",
      "commitDateOld": "29/08/14 2:44 PM",
      "commitNameOld": "ea1c6f31c2d2ea5b38ed57e2aa241d122103a721",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 81.14,
      "commitsBetweenForRepo": 813,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,56 @@\n   private void writeTopLevelDirs(SequenceFile.Writer srcWriter, \n       List\u003cPath\u003e paths, Path parentPath) throws IOException {\n-    //add all the directories \n-    List\u003cPath\u003e justDirs \u003d new ArrayList\u003cPath\u003e();\n+    // extract paths from absolute URI\u0027s\n+    List\u003cPath\u003e justPaths \u003d new ArrayList\u003cPath\u003e();\n     for (Path p: paths) {\n-      if (!p.getFileSystem(getConf()).isFile(p)) {\n-        justDirs.add(new Path(p.toUri().getPath()));\n-      }\n-      else {\n-        justDirs.add(new Path(p.getParent().toUri().getPath()));\n-      }\n+      justPaths.add(new Path(p.toUri().getPath()));\n     }\n     /* find all the common parents of paths that are valid archive\n      * paths. The below is done so that we do not add a common path\n      * twice and also we need to only add valid child of a path that\n      * are specified the user.\n      */\n     TreeMap\u003cString, HashSet\u003cString\u003e\u003e allpaths \u003d new TreeMap\u003cString, \n                                                 HashSet\u003cString\u003e\u003e();\n     /* the largest depth of paths. the max number of times\n      * we need to iterate\n      */\n     Path deepest \u003d largestDepth(paths);\n     Path root \u003d new Path(Path.SEPARATOR);\n     for (int i \u003d parentPath.depth(); i \u003c deepest.depth(); i++) {\n       List\u003cPath\u003e parents \u003d new ArrayList\u003cPath\u003e();\n-      for (Path p: justDirs) {\n+      for (Path p: justPaths) {\n         if (p.compareTo(root) \u003d\u003d 0){\n           //do nothing\n         }\n         else {\n           Path parent \u003d p.getParent();\n           if (null !\u003d parent) {\n             if (allpaths.containsKey(parent.toString())) {\n               HashSet\u003cString\u003e children \u003d allpaths.get(parent.toString());\n               children.add(p.getName());\n             } \n             else {\n               HashSet\u003cString\u003e children \u003d new HashSet\u003cString\u003e();\n               children.add(p.getName());\n               allpaths.put(parent.toString(), children);\n             }\n             parents.add(parent);\n           }\n         }\n       }\n-      justDirs \u003d parents;\n+      justPaths \u003d parents;\n     }\n     Set\u003cMap.Entry\u003cString, HashSet\u003cString\u003e\u003e\u003e keyVals \u003d allpaths.entrySet();\n     for (Map.Entry\u003cString, HashSet\u003cString\u003e\u003e entry : keyVals) {\n       final Path relPath \u003d relPathToRoot(new Path(entry.getKey()), parentPath);\n       if (relPath !\u003d null) {\n         final String[] children \u003d new String[entry.getValue().size()];\n         int i \u003d 0;\n         for(String child: entry.getValue()) {\n           children[i++] \u003d child;\n         }\n         append(srcWriter, 0L, relPath.toString(), children);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void writeTopLevelDirs(SequenceFile.Writer srcWriter, \n      List\u003cPath\u003e paths, Path parentPath) throws IOException {\n    // extract paths from absolute URI\u0027s\n    List\u003cPath\u003e justPaths \u003d new ArrayList\u003cPath\u003e();\n    for (Path p: paths) {\n      justPaths.add(new Path(p.toUri().getPath()));\n    }\n    /* find all the common parents of paths that are valid archive\n     * paths. The below is done so that we do not add a common path\n     * twice and also we need to only add valid child of a path that\n     * are specified the user.\n     */\n    TreeMap\u003cString, HashSet\u003cString\u003e\u003e allpaths \u003d new TreeMap\u003cString, \n                                                HashSet\u003cString\u003e\u003e();\n    /* the largest depth of paths. the max number of times\n     * we need to iterate\n     */\n    Path deepest \u003d largestDepth(paths);\n    Path root \u003d new Path(Path.SEPARATOR);\n    for (int i \u003d parentPath.depth(); i \u003c deepest.depth(); i++) {\n      List\u003cPath\u003e parents \u003d new ArrayList\u003cPath\u003e();\n      for (Path p: justPaths) {\n        if (p.compareTo(root) \u003d\u003d 0){\n          //do nothing\n        }\n        else {\n          Path parent \u003d p.getParent();\n          if (null !\u003d parent) {\n            if (allpaths.containsKey(parent.toString())) {\n              HashSet\u003cString\u003e children \u003d allpaths.get(parent.toString());\n              children.add(p.getName());\n            } \n            else {\n              HashSet\u003cString\u003e children \u003d new HashSet\u003cString\u003e();\n              children.add(p.getName());\n              allpaths.put(parent.toString(), children);\n            }\n            parents.add(parent);\n          }\n        }\n      }\n      justPaths \u003d parents;\n    }\n    Set\u003cMap.Entry\u003cString, HashSet\u003cString\u003e\u003e\u003e keyVals \u003d allpaths.entrySet();\n    for (Map.Entry\u003cString, HashSet\u003cString\u003e\u003e entry : keyVals) {\n      final Path relPath \u003d relPathToRoot(new Path(entry.getKey()), parentPath);\n      if (relPath !\u003d null) {\n        final String[] children \u003d new String[entry.getValue().size()];\n        int i \u003d 0;\n        for(String child: entry.getValue()) {\n          children[i++] \u003d child;\n        }\n        append(srcWriter, 0L, relPath.toString(), children);\n      }\n    }\n  }",
      "path": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {}
    },
    "0201be46c298e94176ec6297e9d9cdba3afc2bbd": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7810. move hadoop archive to core from tools. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213907 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 12:17 PM",
      "commitName": "0201be46c298e94176ec6297e9d9cdba3afc2bbd",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "13/12/11 10:07 AM",
      "commitNameOld": "f2f4e9341387199e04679ebc8de5e05c0fdbd437",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.09,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void writeTopLevelDirs(SequenceFile.Writer srcWriter, \n      List\u003cPath\u003e paths, Path parentPath) throws IOException {\n    //add all the directories \n    List\u003cPath\u003e justDirs \u003d new ArrayList\u003cPath\u003e();\n    for (Path p: paths) {\n      if (!p.getFileSystem(getConf()).isFile(p)) {\n        justDirs.add(new Path(p.toUri().getPath()));\n      }\n      else {\n        justDirs.add(new Path(p.getParent().toUri().getPath()));\n      }\n    }\n    /* find all the common parents of paths that are valid archive\n     * paths. The below is done so that we do not add a common path\n     * twice and also we need to only add valid child of a path that\n     * are specified the user.\n     */\n    TreeMap\u003cString, HashSet\u003cString\u003e\u003e allpaths \u003d new TreeMap\u003cString, \n                                                HashSet\u003cString\u003e\u003e();\n    /* the largest depth of paths. the max number of times\n     * we need to iterate\n     */\n    Path deepest \u003d largestDepth(paths);\n    Path root \u003d new Path(Path.SEPARATOR);\n    for (int i \u003d parentPath.depth(); i \u003c deepest.depth(); i++) {\n      List\u003cPath\u003e parents \u003d new ArrayList\u003cPath\u003e();\n      for (Path p: justDirs) {\n        if (p.compareTo(root) \u003d\u003d 0){\n          //do nothing\n        }\n        else {\n          Path parent \u003d p.getParent();\n          if (null !\u003d parent) {\n            if (allpaths.containsKey(parent.toString())) {\n              HashSet\u003cString\u003e children \u003d allpaths.get(parent.toString());\n              children.add(p.getName());\n            } \n            else {\n              HashSet\u003cString\u003e children \u003d new HashSet\u003cString\u003e();\n              children.add(p.getName());\n              allpaths.put(parent.toString(), children);\n            }\n            parents.add(parent);\n          }\n        }\n      }\n      justDirs \u003d parents;\n    }\n    Set\u003cMap.Entry\u003cString, HashSet\u003cString\u003e\u003e\u003e keyVals \u003d allpaths.entrySet();\n    for (Map.Entry\u003cString, HashSet\u003cString\u003e\u003e entry : keyVals) {\n      final Path relPath \u003d relPathToRoot(new Path(entry.getKey()), parentPath);\n      if (relPath !\u003d null) {\n        final String[] children \u003d new String[entry.getValue().size()];\n        int i \u003d 0;\n        for(String child: entry.getValue()) {\n          children[i++] \u003d child;\n        }\n        append(srcWriter, 0L, relPath.toString(), children);\n      }\n    }\n  }",
      "path": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
        "newPath": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void writeTopLevelDirs(SequenceFile.Writer srcWriter, \n      List\u003cPath\u003e paths, Path parentPath) throws IOException {\n    //add all the directories \n    List\u003cPath\u003e justDirs \u003d new ArrayList\u003cPath\u003e();\n    for (Path p: paths) {\n      if (!p.getFileSystem(getConf()).isFile(p)) {\n        justDirs.add(new Path(p.toUri().getPath()));\n      }\n      else {\n        justDirs.add(new Path(p.getParent().toUri().getPath()));\n      }\n    }\n    /* find all the common parents of paths that are valid archive\n     * paths. The below is done so that we do not add a common path\n     * twice and also we need to only add valid child of a path that\n     * are specified the user.\n     */\n    TreeMap\u003cString, HashSet\u003cString\u003e\u003e allpaths \u003d new TreeMap\u003cString, \n                                                HashSet\u003cString\u003e\u003e();\n    /* the largest depth of paths. the max number of times\n     * we need to iterate\n     */\n    Path deepest \u003d largestDepth(paths);\n    Path root \u003d new Path(Path.SEPARATOR);\n    for (int i \u003d parentPath.depth(); i \u003c deepest.depth(); i++) {\n      List\u003cPath\u003e parents \u003d new ArrayList\u003cPath\u003e();\n      for (Path p: justDirs) {\n        if (p.compareTo(root) \u003d\u003d 0){\n          //do nothing\n        }\n        else {\n          Path parent \u003d p.getParent();\n          if (null !\u003d parent) {\n            if (allpaths.containsKey(parent.toString())) {\n              HashSet\u003cString\u003e children \u003d allpaths.get(parent.toString());\n              children.add(p.getName());\n            } \n            else {\n              HashSet\u003cString\u003e children \u003d new HashSet\u003cString\u003e();\n              children.add(p.getName());\n              allpaths.put(parent.toString(), children);\n            }\n            parents.add(parent);\n          }\n        }\n      }\n      justDirs \u003d parents;\n    }\n    Set\u003cMap.Entry\u003cString, HashSet\u003cString\u003e\u003e\u003e keyVals \u003d allpaths.entrySet();\n    for (Map.Entry\u003cString, HashSet\u003cString\u003e\u003e entry : keyVals) {\n      final Path relPath \u003d relPathToRoot(new Path(entry.getKey()), parentPath);\n      if (relPath !\u003d null) {\n        final String[] children \u003d new String[entry.getValue().size()];\n        int i \u003d 0;\n        for(String child: entry.getValue()) {\n          children[i++] \u003d child;\n        }\n        append(srcWriter, 0L, relPath.toString(), children);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
        "newPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/HadoopArchives.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void writeTopLevelDirs(SequenceFile.Writer srcWriter, \n      List\u003cPath\u003e paths, Path parentPath) throws IOException {\n    //add all the directories \n    List\u003cPath\u003e justDirs \u003d new ArrayList\u003cPath\u003e();\n    for (Path p: paths) {\n      if (!p.getFileSystem(getConf()).isFile(p)) {\n        justDirs.add(new Path(p.toUri().getPath()));\n      }\n      else {\n        justDirs.add(new Path(p.getParent().toUri().getPath()));\n      }\n    }\n    /* find all the common parents of paths that are valid archive\n     * paths. The below is done so that we do not add a common path\n     * twice and also we need to only add valid child of a path that\n     * are specified the user.\n     */\n    TreeMap\u003cString, HashSet\u003cString\u003e\u003e allpaths \u003d new TreeMap\u003cString, \n                                                HashSet\u003cString\u003e\u003e();\n    /* the largest depth of paths. the max number of times\n     * we need to iterate\n     */\n    Path deepest \u003d largestDepth(paths);\n    Path root \u003d new Path(Path.SEPARATOR);\n    for (int i \u003d parentPath.depth(); i \u003c deepest.depth(); i++) {\n      List\u003cPath\u003e parents \u003d new ArrayList\u003cPath\u003e();\n      for (Path p: justDirs) {\n        if (p.compareTo(root) \u003d\u003d 0){\n          //do nothing\n        }\n        else {\n          Path parent \u003d p.getParent();\n          if (null !\u003d parent) {\n            if (allpaths.containsKey(parent.toString())) {\n              HashSet\u003cString\u003e children \u003d allpaths.get(parent.toString());\n              children.add(p.getName());\n            } \n            else {\n              HashSet\u003cString\u003e children \u003d new HashSet\u003cString\u003e();\n              children.add(p.getName());\n              allpaths.put(parent.toString(), children);\n            }\n            parents.add(parent);\n          }\n        }\n      }\n      justDirs \u003d parents;\n    }\n    Set\u003cMap.Entry\u003cString, HashSet\u003cString\u003e\u003e\u003e keyVals \u003d allpaths.entrySet();\n    for (Map.Entry\u003cString, HashSet\u003cString\u003e\u003e entry : keyVals) {\n      final Path relPath \u003d relPathToRoot(new Path(entry.getKey()), parentPath);\n      if (relPath !\u003d null) {\n        final String[] children \u003d new String[entry.getValue().size()];\n        int i \u003d 0;\n        for(String child: entry.getValue()) {\n          children[i++] \u003d child;\n        }\n        append(srcWriter, 0L, relPath.toString(), children);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
        "newPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,61 @@\n+  private void writeTopLevelDirs(SequenceFile.Writer srcWriter, \n+      List\u003cPath\u003e paths, Path parentPath) throws IOException {\n+    //add all the directories \n+    List\u003cPath\u003e justDirs \u003d new ArrayList\u003cPath\u003e();\n+    for (Path p: paths) {\n+      if (!p.getFileSystem(getConf()).isFile(p)) {\n+        justDirs.add(new Path(p.toUri().getPath()));\n+      }\n+      else {\n+        justDirs.add(new Path(p.getParent().toUri().getPath()));\n+      }\n+    }\n+    /* find all the common parents of paths that are valid archive\n+     * paths. The below is done so that we do not add a common path\n+     * twice and also we need to only add valid child of a path that\n+     * are specified the user.\n+     */\n+    TreeMap\u003cString, HashSet\u003cString\u003e\u003e allpaths \u003d new TreeMap\u003cString, \n+                                                HashSet\u003cString\u003e\u003e();\n+    /* the largest depth of paths. the max number of times\n+     * we need to iterate\n+     */\n+    Path deepest \u003d largestDepth(paths);\n+    Path root \u003d new Path(Path.SEPARATOR);\n+    for (int i \u003d parentPath.depth(); i \u003c deepest.depth(); i++) {\n+      List\u003cPath\u003e parents \u003d new ArrayList\u003cPath\u003e();\n+      for (Path p: justDirs) {\n+        if (p.compareTo(root) \u003d\u003d 0){\n+          //do nothing\n+        }\n+        else {\n+          Path parent \u003d p.getParent();\n+          if (null !\u003d parent) {\n+            if (allpaths.containsKey(parent.toString())) {\n+              HashSet\u003cString\u003e children \u003d allpaths.get(parent.toString());\n+              children.add(p.getName());\n+            } \n+            else {\n+              HashSet\u003cString\u003e children \u003d new HashSet\u003cString\u003e();\n+              children.add(p.getName());\n+              allpaths.put(parent.toString(), children);\n+            }\n+            parents.add(parent);\n+          }\n+        }\n+      }\n+      justDirs \u003d parents;\n+    }\n+    Set\u003cMap.Entry\u003cString, HashSet\u003cString\u003e\u003e\u003e keyVals \u003d allpaths.entrySet();\n+    for (Map.Entry\u003cString, HashSet\u003cString\u003e\u003e entry : keyVals) {\n+      final Path relPath \u003d relPathToRoot(new Path(entry.getKey()), parentPath);\n+      if (relPath !\u003d null) {\n+        final String[] children \u003d new String[entry.getValue().size()];\n+        int i \u003d 0;\n+        for(String child: entry.getValue()) {\n+          children[i++] \u003d child;\n+        }\n+        append(srcWriter, 0L, relPath.toString(), children);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void writeTopLevelDirs(SequenceFile.Writer srcWriter, \n      List\u003cPath\u003e paths, Path parentPath) throws IOException {\n    //add all the directories \n    List\u003cPath\u003e justDirs \u003d new ArrayList\u003cPath\u003e();\n    for (Path p: paths) {\n      if (!p.getFileSystem(getConf()).isFile(p)) {\n        justDirs.add(new Path(p.toUri().getPath()));\n      }\n      else {\n        justDirs.add(new Path(p.getParent().toUri().getPath()));\n      }\n    }\n    /* find all the common parents of paths that are valid archive\n     * paths. The below is done so that we do not add a common path\n     * twice and also we need to only add valid child of a path that\n     * are specified the user.\n     */\n    TreeMap\u003cString, HashSet\u003cString\u003e\u003e allpaths \u003d new TreeMap\u003cString, \n                                                HashSet\u003cString\u003e\u003e();\n    /* the largest depth of paths. the max number of times\n     * we need to iterate\n     */\n    Path deepest \u003d largestDepth(paths);\n    Path root \u003d new Path(Path.SEPARATOR);\n    for (int i \u003d parentPath.depth(); i \u003c deepest.depth(); i++) {\n      List\u003cPath\u003e parents \u003d new ArrayList\u003cPath\u003e();\n      for (Path p: justDirs) {\n        if (p.compareTo(root) \u003d\u003d 0){\n          //do nothing\n        }\n        else {\n          Path parent \u003d p.getParent();\n          if (null !\u003d parent) {\n            if (allpaths.containsKey(parent.toString())) {\n              HashSet\u003cString\u003e children \u003d allpaths.get(parent.toString());\n              children.add(p.getName());\n            } \n            else {\n              HashSet\u003cString\u003e children \u003d new HashSet\u003cString\u003e();\n              children.add(p.getName());\n              allpaths.put(parent.toString(), children);\n            }\n            parents.add(parent);\n          }\n        }\n      }\n      justDirs \u003d parents;\n    }\n    Set\u003cMap.Entry\u003cString, HashSet\u003cString\u003e\u003e\u003e keyVals \u003d allpaths.entrySet();\n    for (Map.Entry\u003cString, HashSet\u003cString\u003e\u003e entry : keyVals) {\n      final Path relPath \u003d relPathToRoot(new Path(entry.getKey()), parentPath);\n      if (relPath !\u003d null) {\n        final String[] children \u003d new String[entry.getValue().size()];\n        int i \u003d 0;\n        for(String child: entry.getValue()) {\n          children[i++] \u003d child;\n        }\n        append(srcWriter, 0L, relPath.toString(), children);\n      }\n    }\n  }",
      "path": "mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java"
    }
  }
}