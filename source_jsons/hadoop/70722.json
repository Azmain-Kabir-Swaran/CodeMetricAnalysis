{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "AzureNativeFileSystemStore.java",
  "functionName": "listInternal",
  "functionId": "listInternal___prefix-String__maxListingCount-int(modifiers-final)__maxListingDepth-int(modifiers-final)",
  "sourceFilePath": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
  "functionStartLine": 2298,
  "functionEndLine": 2418,
  "numCommitsSeen": 99,
  "timeTaken": 4890,
  "changeHistory": [
    "45d9568aaaf532a6da11bd7c1844ff81bf66bab1",
    "15dd1f3381069c5fdc6690e3ab1907a133ba14bf",
    "2ed58c40e5dcbf5c5303c00e85096085b1055f85",
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40",
    "81bc395deb3ba00567dc067d6ca71bacf9e3bc82"
  ],
  "changeHistoryShort": {
    "45d9568aaaf532a6da11bd7c1844ff81bf66bab1": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Ybodychange)",
    "15dd1f3381069c5fdc6690e3ab1907a133ba14bf": "Ybodychange",
    "2ed58c40e5dcbf5c5303c00e85096085b1055f85": "Ybodychange",
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40": "Ybodychange",
    "81bc395deb3ba00567dc067d6ca71bacf9e3bc82": "Yintroduced"
  },
  "changeHistoryDetails": {
    "45d9568aaaf532a6da11bd7c1844ff81bf66bab1": {
      "type": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HADOOP-15547/ WASB: improve listStatus performance.\nContributed by Thomas Marquardt.\n\n(cherry picked from commit 749fff577ed9afb4ef8a54b8948f74be083cc620)\n",
      "commitDate": "19/07/18 12:31 PM",
      "commitName": "45d9568aaaf532a6da11bd7c1844ff81bf66bab1",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HADOOP-15547/ WASB: improve listStatus performance.\nContributed by Thomas Marquardt.\n\n(cherry picked from commit 749fff577ed9afb4ef8a54b8948f74be083cc620)\n",
          "commitDate": "19/07/18 12:31 PM",
          "commitName": "45d9568aaaf532a6da11bd7c1844ff81bf66bab1",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "27/06/18 10:37 PM",
          "commitNameOld": "2b2399d623539ab68e71a38fa9fbfc9a405bddb8",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 21.58,
          "commitsBetweenForRepo": 124,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,111 +1,121 @@\n-  private PartialListing list(String prefix, String delimiter,\n-      final int maxListingCount, final int maxListingDepth, String priorLastKey)\n+  private FileMetadata[] listInternal(String prefix, final int maxListingCount,\n+      final int maxListingDepth)\n       throws IOException {\n     try {\n       checkContainer(ContainerAccessType.PureRead);\n \n       if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n         prefix +\u003d PATH_DELIMITER;\n       }\n \n       // Enable flat listing option only if depth is unbounded and config\n       // KEY_ENABLE_FLAT_LISTING is enabled.\n       boolean enableFlatListing \u003d false;\n       if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n         KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n         enableFlatListing \u003d true;\n       }\n \n       Iterable\u003cListBlobItem\u003e objects;\n       if (prefix.equals(\"/\")) {\n         objects \u003d listRootBlobs(true, enableFlatListing);\n       } else {\n         objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n       }\n \n-      ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n+      HashMap\u003cString, FileMetadata\u003e fileMetadata \u003d new HashMap\u003c\u003e(256);\n+\n       for (ListBlobItem blobItem : objects) {\n         // Check that the maximum listing count is not exhausted.\n         //\n         if (0 \u003c maxListingCount\n             \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n           break;\n         }\n \n         if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n           String blobKey \u003d null;\n           CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n           BlobProperties properties \u003d blob.getProperties();\n \n           // Determine format of the blob name depending on whether an absolute\n           // path is being used or not.\n           blobKey \u003d normalizeKey(blob);\n \n           FileMetadata metadata;\n           if (retrieveFolderAttribute(blob)) {\n-            metadata \u003d new FileMetadata(blobKey,\n-                properties.getLastModified().getTime(),\n-                getPermissionStatus(blob),\n-                BlobMaterialization.Explicit);\n+              metadata \u003d new FileMetadata(blobKey,\n+                  properties.getLastModified().getTime(),\n+                  getPermissionStatus(blob),\n+                  BlobMaterialization.Explicit,\n+                  hadoopBlockSize);\n           } else {\n-            metadata \u003d new FileMetadata(\n-                blobKey,\n-                getDataLength(blob, properties),\n-                properties.getLastModified().getTime(),\n-                getPermissionStatus(blob));\n+              metadata \u003d new FileMetadata(\n+                  blobKey,\n+                  getDataLength(blob, properties),\n+                  properties.getLastModified().getTime(),\n+                  getPermissionStatus(blob),\n+                  hadoopBlockSize);\n           }\n+          // Add the metadata but remove duplicates.  Note that the azure\n+          // storage java SDK returns two types of entries: CloudBlobWrappter\n+          // and CloudDirectoryWrapper.  In the case where WASB generated the\n+          // data, there will be an empty blob for each \"directory\", and we will\n+          // receive a CloudBlobWrapper.  If there are also files within this\n+          // \"directory\", we will also receive a CloudDirectoryWrapper.  To\n+          // complicate matters, the data may not be generated by WASB, in\n+          // which case we may not have an empty blob for each \"directory\".\n+          // So, sometimes we receive both a CloudBlobWrapper and a\n+          // CloudDirectoryWrapper for each directory, and sometimes we receive\n+          // one or the other but not both.  We remove duplicates, but\n+          // prefer CloudBlobWrapper over CloudDirectoryWrapper.\n+          // Furthermore, it is very unfortunate that the list results are not\n+          // ordered, and it is a partial list which uses continuation.  So\n+          // the HashMap is the best structure to remove the duplicates, despite\n+          // its potential large size.\n+          fileMetadata.put(blobKey, metadata);\n \n-          // Add the metadata to the list, but remove any existing duplicate\n-          // entries first that we may have added by finding nested files.\n-          FileMetadata existing \u003d getFileMetadataInList(fileMetadata, blobKey);\n-          if (existing !\u003d null) {\n-            fileMetadata.remove(existing);\n-          }\n-          fileMetadata.add(metadata);\n         } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n           CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n           // Determine format of directory name depending on whether an absolute\n           // path is being used or not.\n           //\n           String dirKey \u003d normalizeKey(directory);\n           // Strip the last /\n           if (dirKey.endsWith(PATH_DELIMITER)) {\n             dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n           }\n \n           // Reached the targeted listing depth. Return metadata for the\n           // directory using default permissions.\n           //\n           // Note: Something smarter should be done about permissions. Maybe\n           // inherit the permissions of the first non-directory blob.\n           // Also, getting a proper value for last-modified is tricky.\n           FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n-              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n+              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit,\n+              hadoopBlockSize);\n \n           // Add the directory metadata to the list only if it\u0027s not already\n-          // there.\n-          if (getFileMetadataInList(fileMetadata, dirKey) \u003d\u003d null) {\n-            fileMetadata.add(directoryMetadata);\n+          // there.  See earlier note, we prefer CloudBlobWrapper over\n+          // CloudDirectoryWrapper because it may have additional metadata (\n+          // properties and ACLs).\n+          if (!fileMetadata.containsKey(dirKey)) {\n+            fileMetadata.put(dirKey, directoryMetadata);\n           }\n \n           if (!enableFlatListing) {\n             // Currently at a depth of one, decrement the listing depth for\n             // sub-directories.\n             buildUpList(directory, fileMetadata, maxListingCount,\n                 maxListingDepth - 1);\n           }\n         }\n       }\n-      // Note: Original code indicated that this may be a hack.\n-      priorLastKey \u003d null;\n-      PartialListing listing \u003d new PartialListing(priorLastKey,\n-          fileMetadata.toArray(new FileMetadata[] {}),\n-          0 \u003d\u003d fileMetadata.size() ? new String[] {}\n-      : new String[] { prefix });\n-      return listing;\n+      return fileMetadata.values().toArray(new FileMetadata[fileMetadata.size()]);\n     } catch (Exception e) {\n       // Re-throw as an Azure storage exception.\n       //\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private FileMetadata[] listInternal(String prefix, final int maxListingCount,\n      final int maxListingDepth)\n      throws IOException {\n    try {\n      checkContainer(ContainerAccessType.PureRead);\n\n      if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n        prefix +\u003d PATH_DELIMITER;\n      }\n\n      // Enable flat listing option only if depth is unbounded and config\n      // KEY_ENABLE_FLAT_LISTING is enabled.\n      boolean enableFlatListing \u003d false;\n      if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n        KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n        enableFlatListing \u003d true;\n      }\n\n      Iterable\u003cListBlobItem\u003e objects;\n      if (prefix.equals(\"/\")) {\n        objects \u003d listRootBlobs(true, enableFlatListing);\n      } else {\n        objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n      }\n\n      HashMap\u003cString, FileMetadata\u003e fileMetadata \u003d new HashMap\u003c\u003e(256);\n\n      for (ListBlobItem blobItem : objects) {\n        // Check that the maximum listing count is not exhausted.\n        //\n        if (0 \u003c maxListingCount\n            \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n          break;\n        }\n\n        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n          String blobKey \u003d null;\n          CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n          BlobProperties properties \u003d blob.getProperties();\n\n          // Determine format of the blob name depending on whether an absolute\n          // path is being used or not.\n          blobKey \u003d normalizeKey(blob);\n\n          FileMetadata metadata;\n          if (retrieveFolderAttribute(blob)) {\n              metadata \u003d new FileMetadata(blobKey,\n                  properties.getLastModified().getTime(),\n                  getPermissionStatus(blob),\n                  BlobMaterialization.Explicit,\n                  hadoopBlockSize);\n          } else {\n              metadata \u003d new FileMetadata(\n                  blobKey,\n                  getDataLength(blob, properties),\n                  properties.getLastModified().getTime(),\n                  getPermissionStatus(blob),\n                  hadoopBlockSize);\n          }\n          // Add the metadata but remove duplicates.  Note that the azure\n          // storage java SDK returns two types of entries: CloudBlobWrappter\n          // and CloudDirectoryWrapper.  In the case where WASB generated the\n          // data, there will be an empty blob for each \"directory\", and we will\n          // receive a CloudBlobWrapper.  If there are also files within this\n          // \"directory\", we will also receive a CloudDirectoryWrapper.  To\n          // complicate matters, the data may not be generated by WASB, in\n          // which case we may not have an empty blob for each \"directory\".\n          // So, sometimes we receive both a CloudBlobWrapper and a\n          // CloudDirectoryWrapper for each directory, and sometimes we receive\n          // one or the other but not both.  We remove duplicates, but\n          // prefer CloudBlobWrapper over CloudDirectoryWrapper.\n          // Furthermore, it is very unfortunate that the list results are not\n          // ordered, and it is a partial list which uses continuation.  So\n          // the HashMap is the best structure to remove the duplicates, despite\n          // its potential large size.\n          fileMetadata.put(blobKey, metadata);\n\n        } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n          CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n          // Determine format of directory name depending on whether an absolute\n          // path is being used or not.\n          //\n          String dirKey \u003d normalizeKey(directory);\n          // Strip the last /\n          if (dirKey.endsWith(PATH_DELIMITER)) {\n            dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n          }\n\n          // Reached the targeted listing depth. Return metadata for the\n          // directory using default permissions.\n          //\n          // Note: Something smarter should be done about permissions. Maybe\n          // inherit the permissions of the first non-directory blob.\n          // Also, getting a proper value for last-modified is tricky.\n          FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit,\n              hadoopBlockSize);\n\n          // Add the directory metadata to the list only if it\u0027s not already\n          // there.  See earlier note, we prefer CloudBlobWrapper over\n          // CloudDirectoryWrapper because it may have additional metadata (\n          // properties and ACLs).\n          if (!fileMetadata.containsKey(dirKey)) {\n            fileMetadata.put(dirKey, directoryMetadata);\n          }\n\n          if (!enableFlatListing) {\n            // Currently at a depth of one, decrement the listing depth for\n            // sub-directories.\n            buildUpList(directory, fileMetadata, maxListingCount,\n                maxListingDepth - 1);\n          }\n        }\n      }\n      return fileMetadata.values().toArray(new FileMetadata[fileMetadata.size()]);\n    } catch (Exception e) {\n      // Re-throw as an Azure storage exception.\n      //\n      throw new AzureException(e);\n    }\n  }",
          "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
          "extendedDetails": {
            "oldValue": "list",
            "newValue": "listInternal"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-15547/ WASB: improve listStatus performance.\nContributed by Thomas Marquardt.\n\n(cherry picked from commit 749fff577ed9afb4ef8a54b8948f74be083cc620)\n",
          "commitDate": "19/07/18 12:31 PM",
          "commitName": "45d9568aaaf532a6da11bd7c1844ff81bf66bab1",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "27/06/18 10:37 PM",
          "commitNameOld": "2b2399d623539ab68e71a38fa9fbfc9a405bddb8",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 21.58,
          "commitsBetweenForRepo": 124,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,111 +1,121 @@\n-  private PartialListing list(String prefix, String delimiter,\n-      final int maxListingCount, final int maxListingDepth, String priorLastKey)\n+  private FileMetadata[] listInternal(String prefix, final int maxListingCount,\n+      final int maxListingDepth)\n       throws IOException {\n     try {\n       checkContainer(ContainerAccessType.PureRead);\n \n       if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n         prefix +\u003d PATH_DELIMITER;\n       }\n \n       // Enable flat listing option only if depth is unbounded and config\n       // KEY_ENABLE_FLAT_LISTING is enabled.\n       boolean enableFlatListing \u003d false;\n       if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n         KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n         enableFlatListing \u003d true;\n       }\n \n       Iterable\u003cListBlobItem\u003e objects;\n       if (prefix.equals(\"/\")) {\n         objects \u003d listRootBlobs(true, enableFlatListing);\n       } else {\n         objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n       }\n \n-      ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n+      HashMap\u003cString, FileMetadata\u003e fileMetadata \u003d new HashMap\u003c\u003e(256);\n+\n       for (ListBlobItem blobItem : objects) {\n         // Check that the maximum listing count is not exhausted.\n         //\n         if (0 \u003c maxListingCount\n             \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n           break;\n         }\n \n         if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n           String blobKey \u003d null;\n           CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n           BlobProperties properties \u003d blob.getProperties();\n \n           // Determine format of the blob name depending on whether an absolute\n           // path is being used or not.\n           blobKey \u003d normalizeKey(blob);\n \n           FileMetadata metadata;\n           if (retrieveFolderAttribute(blob)) {\n-            metadata \u003d new FileMetadata(blobKey,\n-                properties.getLastModified().getTime(),\n-                getPermissionStatus(blob),\n-                BlobMaterialization.Explicit);\n+              metadata \u003d new FileMetadata(blobKey,\n+                  properties.getLastModified().getTime(),\n+                  getPermissionStatus(blob),\n+                  BlobMaterialization.Explicit,\n+                  hadoopBlockSize);\n           } else {\n-            metadata \u003d new FileMetadata(\n-                blobKey,\n-                getDataLength(blob, properties),\n-                properties.getLastModified().getTime(),\n-                getPermissionStatus(blob));\n+              metadata \u003d new FileMetadata(\n+                  blobKey,\n+                  getDataLength(blob, properties),\n+                  properties.getLastModified().getTime(),\n+                  getPermissionStatus(blob),\n+                  hadoopBlockSize);\n           }\n+          // Add the metadata but remove duplicates.  Note that the azure\n+          // storage java SDK returns two types of entries: CloudBlobWrappter\n+          // and CloudDirectoryWrapper.  In the case where WASB generated the\n+          // data, there will be an empty blob for each \"directory\", and we will\n+          // receive a CloudBlobWrapper.  If there are also files within this\n+          // \"directory\", we will also receive a CloudDirectoryWrapper.  To\n+          // complicate matters, the data may not be generated by WASB, in\n+          // which case we may not have an empty blob for each \"directory\".\n+          // So, sometimes we receive both a CloudBlobWrapper and a\n+          // CloudDirectoryWrapper for each directory, and sometimes we receive\n+          // one or the other but not both.  We remove duplicates, but\n+          // prefer CloudBlobWrapper over CloudDirectoryWrapper.\n+          // Furthermore, it is very unfortunate that the list results are not\n+          // ordered, and it is a partial list which uses continuation.  So\n+          // the HashMap is the best structure to remove the duplicates, despite\n+          // its potential large size.\n+          fileMetadata.put(blobKey, metadata);\n \n-          // Add the metadata to the list, but remove any existing duplicate\n-          // entries first that we may have added by finding nested files.\n-          FileMetadata existing \u003d getFileMetadataInList(fileMetadata, blobKey);\n-          if (existing !\u003d null) {\n-            fileMetadata.remove(existing);\n-          }\n-          fileMetadata.add(metadata);\n         } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n           CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n           // Determine format of directory name depending on whether an absolute\n           // path is being used or not.\n           //\n           String dirKey \u003d normalizeKey(directory);\n           // Strip the last /\n           if (dirKey.endsWith(PATH_DELIMITER)) {\n             dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n           }\n \n           // Reached the targeted listing depth. Return metadata for the\n           // directory using default permissions.\n           //\n           // Note: Something smarter should be done about permissions. Maybe\n           // inherit the permissions of the first non-directory blob.\n           // Also, getting a proper value for last-modified is tricky.\n           FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n-              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n+              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit,\n+              hadoopBlockSize);\n \n           // Add the directory metadata to the list only if it\u0027s not already\n-          // there.\n-          if (getFileMetadataInList(fileMetadata, dirKey) \u003d\u003d null) {\n-            fileMetadata.add(directoryMetadata);\n+          // there.  See earlier note, we prefer CloudBlobWrapper over\n+          // CloudDirectoryWrapper because it may have additional metadata (\n+          // properties and ACLs).\n+          if (!fileMetadata.containsKey(dirKey)) {\n+            fileMetadata.put(dirKey, directoryMetadata);\n           }\n \n           if (!enableFlatListing) {\n             // Currently at a depth of one, decrement the listing depth for\n             // sub-directories.\n             buildUpList(directory, fileMetadata, maxListingCount,\n                 maxListingDepth - 1);\n           }\n         }\n       }\n-      // Note: Original code indicated that this may be a hack.\n-      priorLastKey \u003d null;\n-      PartialListing listing \u003d new PartialListing(priorLastKey,\n-          fileMetadata.toArray(new FileMetadata[] {}),\n-          0 \u003d\u003d fileMetadata.size() ? new String[] {}\n-      : new String[] { prefix });\n-      return listing;\n+      return fileMetadata.values().toArray(new FileMetadata[fileMetadata.size()]);\n     } catch (Exception e) {\n       // Re-throw as an Azure storage exception.\n       //\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private FileMetadata[] listInternal(String prefix, final int maxListingCount,\n      final int maxListingDepth)\n      throws IOException {\n    try {\n      checkContainer(ContainerAccessType.PureRead);\n\n      if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n        prefix +\u003d PATH_DELIMITER;\n      }\n\n      // Enable flat listing option only if depth is unbounded and config\n      // KEY_ENABLE_FLAT_LISTING is enabled.\n      boolean enableFlatListing \u003d false;\n      if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n        KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n        enableFlatListing \u003d true;\n      }\n\n      Iterable\u003cListBlobItem\u003e objects;\n      if (prefix.equals(\"/\")) {\n        objects \u003d listRootBlobs(true, enableFlatListing);\n      } else {\n        objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n      }\n\n      HashMap\u003cString, FileMetadata\u003e fileMetadata \u003d new HashMap\u003c\u003e(256);\n\n      for (ListBlobItem blobItem : objects) {\n        // Check that the maximum listing count is not exhausted.\n        //\n        if (0 \u003c maxListingCount\n            \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n          break;\n        }\n\n        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n          String blobKey \u003d null;\n          CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n          BlobProperties properties \u003d blob.getProperties();\n\n          // Determine format of the blob name depending on whether an absolute\n          // path is being used or not.\n          blobKey \u003d normalizeKey(blob);\n\n          FileMetadata metadata;\n          if (retrieveFolderAttribute(blob)) {\n              metadata \u003d new FileMetadata(blobKey,\n                  properties.getLastModified().getTime(),\n                  getPermissionStatus(blob),\n                  BlobMaterialization.Explicit,\n                  hadoopBlockSize);\n          } else {\n              metadata \u003d new FileMetadata(\n                  blobKey,\n                  getDataLength(blob, properties),\n                  properties.getLastModified().getTime(),\n                  getPermissionStatus(blob),\n                  hadoopBlockSize);\n          }\n          // Add the metadata but remove duplicates.  Note that the azure\n          // storage java SDK returns two types of entries: CloudBlobWrappter\n          // and CloudDirectoryWrapper.  In the case where WASB generated the\n          // data, there will be an empty blob for each \"directory\", and we will\n          // receive a CloudBlobWrapper.  If there are also files within this\n          // \"directory\", we will also receive a CloudDirectoryWrapper.  To\n          // complicate matters, the data may not be generated by WASB, in\n          // which case we may not have an empty blob for each \"directory\".\n          // So, sometimes we receive both a CloudBlobWrapper and a\n          // CloudDirectoryWrapper for each directory, and sometimes we receive\n          // one or the other but not both.  We remove duplicates, but\n          // prefer CloudBlobWrapper over CloudDirectoryWrapper.\n          // Furthermore, it is very unfortunate that the list results are not\n          // ordered, and it is a partial list which uses continuation.  So\n          // the HashMap is the best structure to remove the duplicates, despite\n          // its potential large size.\n          fileMetadata.put(blobKey, metadata);\n\n        } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n          CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n          // Determine format of directory name depending on whether an absolute\n          // path is being used or not.\n          //\n          String dirKey \u003d normalizeKey(directory);\n          // Strip the last /\n          if (dirKey.endsWith(PATH_DELIMITER)) {\n            dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n          }\n\n          // Reached the targeted listing depth. Return metadata for the\n          // directory using default permissions.\n          //\n          // Note: Something smarter should be done about permissions. Maybe\n          // inherit the permissions of the first non-directory blob.\n          // Also, getting a proper value for last-modified is tricky.\n          FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit,\n              hadoopBlockSize);\n\n          // Add the directory metadata to the list only if it\u0027s not already\n          // there.  See earlier note, we prefer CloudBlobWrapper over\n          // CloudDirectoryWrapper because it may have additional metadata (\n          // properties and ACLs).\n          if (!fileMetadata.containsKey(dirKey)) {\n            fileMetadata.put(dirKey, directoryMetadata);\n          }\n\n          if (!enableFlatListing) {\n            // Currently at a depth of one, decrement the listing depth for\n            // sub-directories.\n            buildUpList(directory, fileMetadata, maxListingCount,\n                maxListingDepth - 1);\n          }\n        }\n      }\n      return fileMetadata.values().toArray(new FileMetadata[fileMetadata.size()]);\n    } catch (Exception e) {\n      // Re-throw as an Azure storage exception.\n      //\n      throw new AzureException(e);\n    }\n  }",
          "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
          "extendedDetails": {
            "oldValue": "[prefix-String, delimiter-String, maxListingCount-int(modifiers-final), maxListingDepth-int(modifiers-final), priorLastKey-String]",
            "newValue": "[prefix-String, maxListingCount-int(modifiers-final), maxListingDepth-int(modifiers-final)]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HADOOP-15547/ WASB: improve listStatus performance.\nContributed by Thomas Marquardt.\n\n(cherry picked from commit 749fff577ed9afb4ef8a54b8948f74be083cc620)\n",
          "commitDate": "19/07/18 12:31 PM",
          "commitName": "45d9568aaaf532a6da11bd7c1844ff81bf66bab1",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "27/06/18 10:37 PM",
          "commitNameOld": "2b2399d623539ab68e71a38fa9fbfc9a405bddb8",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 21.58,
          "commitsBetweenForRepo": 124,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,111 +1,121 @@\n-  private PartialListing list(String prefix, String delimiter,\n-      final int maxListingCount, final int maxListingDepth, String priorLastKey)\n+  private FileMetadata[] listInternal(String prefix, final int maxListingCount,\n+      final int maxListingDepth)\n       throws IOException {\n     try {\n       checkContainer(ContainerAccessType.PureRead);\n \n       if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n         prefix +\u003d PATH_DELIMITER;\n       }\n \n       // Enable flat listing option only if depth is unbounded and config\n       // KEY_ENABLE_FLAT_LISTING is enabled.\n       boolean enableFlatListing \u003d false;\n       if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n         KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n         enableFlatListing \u003d true;\n       }\n \n       Iterable\u003cListBlobItem\u003e objects;\n       if (prefix.equals(\"/\")) {\n         objects \u003d listRootBlobs(true, enableFlatListing);\n       } else {\n         objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n       }\n \n-      ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n+      HashMap\u003cString, FileMetadata\u003e fileMetadata \u003d new HashMap\u003c\u003e(256);\n+\n       for (ListBlobItem blobItem : objects) {\n         // Check that the maximum listing count is not exhausted.\n         //\n         if (0 \u003c maxListingCount\n             \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n           break;\n         }\n \n         if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n           String blobKey \u003d null;\n           CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n           BlobProperties properties \u003d blob.getProperties();\n \n           // Determine format of the blob name depending on whether an absolute\n           // path is being used or not.\n           blobKey \u003d normalizeKey(blob);\n \n           FileMetadata metadata;\n           if (retrieveFolderAttribute(blob)) {\n-            metadata \u003d new FileMetadata(blobKey,\n-                properties.getLastModified().getTime(),\n-                getPermissionStatus(blob),\n-                BlobMaterialization.Explicit);\n+              metadata \u003d new FileMetadata(blobKey,\n+                  properties.getLastModified().getTime(),\n+                  getPermissionStatus(blob),\n+                  BlobMaterialization.Explicit,\n+                  hadoopBlockSize);\n           } else {\n-            metadata \u003d new FileMetadata(\n-                blobKey,\n-                getDataLength(blob, properties),\n-                properties.getLastModified().getTime(),\n-                getPermissionStatus(blob));\n+              metadata \u003d new FileMetadata(\n+                  blobKey,\n+                  getDataLength(blob, properties),\n+                  properties.getLastModified().getTime(),\n+                  getPermissionStatus(blob),\n+                  hadoopBlockSize);\n           }\n+          // Add the metadata but remove duplicates.  Note that the azure\n+          // storage java SDK returns two types of entries: CloudBlobWrappter\n+          // and CloudDirectoryWrapper.  In the case where WASB generated the\n+          // data, there will be an empty blob for each \"directory\", and we will\n+          // receive a CloudBlobWrapper.  If there are also files within this\n+          // \"directory\", we will also receive a CloudDirectoryWrapper.  To\n+          // complicate matters, the data may not be generated by WASB, in\n+          // which case we may not have an empty blob for each \"directory\".\n+          // So, sometimes we receive both a CloudBlobWrapper and a\n+          // CloudDirectoryWrapper for each directory, and sometimes we receive\n+          // one or the other but not both.  We remove duplicates, but\n+          // prefer CloudBlobWrapper over CloudDirectoryWrapper.\n+          // Furthermore, it is very unfortunate that the list results are not\n+          // ordered, and it is a partial list which uses continuation.  So\n+          // the HashMap is the best structure to remove the duplicates, despite\n+          // its potential large size.\n+          fileMetadata.put(blobKey, metadata);\n \n-          // Add the metadata to the list, but remove any existing duplicate\n-          // entries first that we may have added by finding nested files.\n-          FileMetadata existing \u003d getFileMetadataInList(fileMetadata, blobKey);\n-          if (existing !\u003d null) {\n-            fileMetadata.remove(existing);\n-          }\n-          fileMetadata.add(metadata);\n         } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n           CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n           // Determine format of directory name depending on whether an absolute\n           // path is being used or not.\n           //\n           String dirKey \u003d normalizeKey(directory);\n           // Strip the last /\n           if (dirKey.endsWith(PATH_DELIMITER)) {\n             dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n           }\n \n           // Reached the targeted listing depth. Return metadata for the\n           // directory using default permissions.\n           //\n           // Note: Something smarter should be done about permissions. Maybe\n           // inherit the permissions of the first non-directory blob.\n           // Also, getting a proper value for last-modified is tricky.\n           FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n-              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n+              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit,\n+              hadoopBlockSize);\n \n           // Add the directory metadata to the list only if it\u0027s not already\n-          // there.\n-          if (getFileMetadataInList(fileMetadata, dirKey) \u003d\u003d null) {\n-            fileMetadata.add(directoryMetadata);\n+          // there.  See earlier note, we prefer CloudBlobWrapper over\n+          // CloudDirectoryWrapper because it may have additional metadata (\n+          // properties and ACLs).\n+          if (!fileMetadata.containsKey(dirKey)) {\n+            fileMetadata.put(dirKey, directoryMetadata);\n           }\n \n           if (!enableFlatListing) {\n             // Currently at a depth of one, decrement the listing depth for\n             // sub-directories.\n             buildUpList(directory, fileMetadata, maxListingCount,\n                 maxListingDepth - 1);\n           }\n         }\n       }\n-      // Note: Original code indicated that this may be a hack.\n-      priorLastKey \u003d null;\n-      PartialListing listing \u003d new PartialListing(priorLastKey,\n-          fileMetadata.toArray(new FileMetadata[] {}),\n-          0 \u003d\u003d fileMetadata.size() ? new String[] {}\n-      : new String[] { prefix });\n-      return listing;\n+      return fileMetadata.values().toArray(new FileMetadata[fileMetadata.size()]);\n     } catch (Exception e) {\n       // Re-throw as an Azure storage exception.\n       //\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private FileMetadata[] listInternal(String prefix, final int maxListingCount,\n      final int maxListingDepth)\n      throws IOException {\n    try {\n      checkContainer(ContainerAccessType.PureRead);\n\n      if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n        prefix +\u003d PATH_DELIMITER;\n      }\n\n      // Enable flat listing option only if depth is unbounded and config\n      // KEY_ENABLE_FLAT_LISTING is enabled.\n      boolean enableFlatListing \u003d false;\n      if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n        KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n        enableFlatListing \u003d true;\n      }\n\n      Iterable\u003cListBlobItem\u003e objects;\n      if (prefix.equals(\"/\")) {\n        objects \u003d listRootBlobs(true, enableFlatListing);\n      } else {\n        objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n      }\n\n      HashMap\u003cString, FileMetadata\u003e fileMetadata \u003d new HashMap\u003c\u003e(256);\n\n      for (ListBlobItem blobItem : objects) {\n        // Check that the maximum listing count is not exhausted.\n        //\n        if (0 \u003c maxListingCount\n            \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n          break;\n        }\n\n        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n          String blobKey \u003d null;\n          CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n          BlobProperties properties \u003d blob.getProperties();\n\n          // Determine format of the blob name depending on whether an absolute\n          // path is being used or not.\n          blobKey \u003d normalizeKey(blob);\n\n          FileMetadata metadata;\n          if (retrieveFolderAttribute(blob)) {\n              metadata \u003d new FileMetadata(blobKey,\n                  properties.getLastModified().getTime(),\n                  getPermissionStatus(blob),\n                  BlobMaterialization.Explicit,\n                  hadoopBlockSize);\n          } else {\n              metadata \u003d new FileMetadata(\n                  blobKey,\n                  getDataLength(blob, properties),\n                  properties.getLastModified().getTime(),\n                  getPermissionStatus(blob),\n                  hadoopBlockSize);\n          }\n          // Add the metadata but remove duplicates.  Note that the azure\n          // storage java SDK returns two types of entries: CloudBlobWrappter\n          // and CloudDirectoryWrapper.  In the case where WASB generated the\n          // data, there will be an empty blob for each \"directory\", and we will\n          // receive a CloudBlobWrapper.  If there are also files within this\n          // \"directory\", we will also receive a CloudDirectoryWrapper.  To\n          // complicate matters, the data may not be generated by WASB, in\n          // which case we may not have an empty blob for each \"directory\".\n          // So, sometimes we receive both a CloudBlobWrapper and a\n          // CloudDirectoryWrapper for each directory, and sometimes we receive\n          // one or the other but not both.  We remove duplicates, but\n          // prefer CloudBlobWrapper over CloudDirectoryWrapper.\n          // Furthermore, it is very unfortunate that the list results are not\n          // ordered, and it is a partial list which uses continuation.  So\n          // the HashMap is the best structure to remove the duplicates, despite\n          // its potential large size.\n          fileMetadata.put(blobKey, metadata);\n\n        } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n          CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n          // Determine format of directory name depending on whether an absolute\n          // path is being used or not.\n          //\n          String dirKey \u003d normalizeKey(directory);\n          // Strip the last /\n          if (dirKey.endsWith(PATH_DELIMITER)) {\n            dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n          }\n\n          // Reached the targeted listing depth. Return metadata for the\n          // directory using default permissions.\n          //\n          // Note: Something smarter should be done about permissions. Maybe\n          // inherit the permissions of the first non-directory blob.\n          // Also, getting a proper value for last-modified is tricky.\n          FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit,\n              hadoopBlockSize);\n\n          // Add the directory metadata to the list only if it\u0027s not already\n          // there.  See earlier note, we prefer CloudBlobWrapper over\n          // CloudDirectoryWrapper because it may have additional metadata (\n          // properties and ACLs).\n          if (!fileMetadata.containsKey(dirKey)) {\n            fileMetadata.put(dirKey, directoryMetadata);\n          }\n\n          if (!enableFlatListing) {\n            // Currently at a depth of one, decrement the listing depth for\n            // sub-directories.\n            buildUpList(directory, fileMetadata, maxListingCount,\n                maxListingDepth - 1);\n          }\n        }\n      }\n      return fileMetadata.values().toArray(new FileMetadata[fileMetadata.size()]);\n    } catch (Exception e) {\n      // Re-throw as an Azure storage exception.\n      //\n      throw new AzureException(e);\n    }\n  }",
          "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
          "extendedDetails": {
            "oldValue": "PartialListing",
            "newValue": "FileMetadata[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-15547/ WASB: improve listStatus performance.\nContributed by Thomas Marquardt.\n\n(cherry picked from commit 749fff577ed9afb4ef8a54b8948f74be083cc620)\n",
          "commitDate": "19/07/18 12:31 PM",
          "commitName": "45d9568aaaf532a6da11bd7c1844ff81bf66bab1",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "27/06/18 10:37 PM",
          "commitNameOld": "2b2399d623539ab68e71a38fa9fbfc9a405bddb8",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 21.58,
          "commitsBetweenForRepo": 124,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,111 +1,121 @@\n-  private PartialListing list(String prefix, String delimiter,\n-      final int maxListingCount, final int maxListingDepth, String priorLastKey)\n+  private FileMetadata[] listInternal(String prefix, final int maxListingCount,\n+      final int maxListingDepth)\n       throws IOException {\n     try {\n       checkContainer(ContainerAccessType.PureRead);\n \n       if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n         prefix +\u003d PATH_DELIMITER;\n       }\n \n       // Enable flat listing option only if depth is unbounded and config\n       // KEY_ENABLE_FLAT_LISTING is enabled.\n       boolean enableFlatListing \u003d false;\n       if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n         KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n         enableFlatListing \u003d true;\n       }\n \n       Iterable\u003cListBlobItem\u003e objects;\n       if (prefix.equals(\"/\")) {\n         objects \u003d listRootBlobs(true, enableFlatListing);\n       } else {\n         objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n       }\n \n-      ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n+      HashMap\u003cString, FileMetadata\u003e fileMetadata \u003d new HashMap\u003c\u003e(256);\n+\n       for (ListBlobItem blobItem : objects) {\n         // Check that the maximum listing count is not exhausted.\n         //\n         if (0 \u003c maxListingCount\n             \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n           break;\n         }\n \n         if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n           String blobKey \u003d null;\n           CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n           BlobProperties properties \u003d blob.getProperties();\n \n           // Determine format of the blob name depending on whether an absolute\n           // path is being used or not.\n           blobKey \u003d normalizeKey(blob);\n \n           FileMetadata metadata;\n           if (retrieveFolderAttribute(blob)) {\n-            metadata \u003d new FileMetadata(blobKey,\n-                properties.getLastModified().getTime(),\n-                getPermissionStatus(blob),\n-                BlobMaterialization.Explicit);\n+              metadata \u003d new FileMetadata(blobKey,\n+                  properties.getLastModified().getTime(),\n+                  getPermissionStatus(blob),\n+                  BlobMaterialization.Explicit,\n+                  hadoopBlockSize);\n           } else {\n-            metadata \u003d new FileMetadata(\n-                blobKey,\n-                getDataLength(blob, properties),\n-                properties.getLastModified().getTime(),\n-                getPermissionStatus(blob));\n+              metadata \u003d new FileMetadata(\n+                  blobKey,\n+                  getDataLength(blob, properties),\n+                  properties.getLastModified().getTime(),\n+                  getPermissionStatus(blob),\n+                  hadoopBlockSize);\n           }\n+          // Add the metadata but remove duplicates.  Note that the azure\n+          // storage java SDK returns two types of entries: CloudBlobWrappter\n+          // and CloudDirectoryWrapper.  In the case where WASB generated the\n+          // data, there will be an empty blob for each \"directory\", and we will\n+          // receive a CloudBlobWrapper.  If there are also files within this\n+          // \"directory\", we will also receive a CloudDirectoryWrapper.  To\n+          // complicate matters, the data may not be generated by WASB, in\n+          // which case we may not have an empty blob for each \"directory\".\n+          // So, sometimes we receive both a CloudBlobWrapper and a\n+          // CloudDirectoryWrapper for each directory, and sometimes we receive\n+          // one or the other but not both.  We remove duplicates, but\n+          // prefer CloudBlobWrapper over CloudDirectoryWrapper.\n+          // Furthermore, it is very unfortunate that the list results are not\n+          // ordered, and it is a partial list which uses continuation.  So\n+          // the HashMap is the best structure to remove the duplicates, despite\n+          // its potential large size.\n+          fileMetadata.put(blobKey, metadata);\n \n-          // Add the metadata to the list, but remove any existing duplicate\n-          // entries first that we may have added by finding nested files.\n-          FileMetadata existing \u003d getFileMetadataInList(fileMetadata, blobKey);\n-          if (existing !\u003d null) {\n-            fileMetadata.remove(existing);\n-          }\n-          fileMetadata.add(metadata);\n         } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n           CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n           // Determine format of directory name depending on whether an absolute\n           // path is being used or not.\n           //\n           String dirKey \u003d normalizeKey(directory);\n           // Strip the last /\n           if (dirKey.endsWith(PATH_DELIMITER)) {\n             dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n           }\n \n           // Reached the targeted listing depth. Return metadata for the\n           // directory using default permissions.\n           //\n           // Note: Something smarter should be done about permissions. Maybe\n           // inherit the permissions of the first non-directory blob.\n           // Also, getting a proper value for last-modified is tricky.\n           FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n-              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n+              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit,\n+              hadoopBlockSize);\n \n           // Add the directory metadata to the list only if it\u0027s not already\n-          // there.\n-          if (getFileMetadataInList(fileMetadata, dirKey) \u003d\u003d null) {\n-            fileMetadata.add(directoryMetadata);\n+          // there.  See earlier note, we prefer CloudBlobWrapper over\n+          // CloudDirectoryWrapper because it may have additional metadata (\n+          // properties and ACLs).\n+          if (!fileMetadata.containsKey(dirKey)) {\n+            fileMetadata.put(dirKey, directoryMetadata);\n           }\n \n           if (!enableFlatListing) {\n             // Currently at a depth of one, decrement the listing depth for\n             // sub-directories.\n             buildUpList(directory, fileMetadata, maxListingCount,\n                 maxListingDepth - 1);\n           }\n         }\n       }\n-      // Note: Original code indicated that this may be a hack.\n-      priorLastKey \u003d null;\n-      PartialListing listing \u003d new PartialListing(priorLastKey,\n-          fileMetadata.toArray(new FileMetadata[] {}),\n-          0 \u003d\u003d fileMetadata.size() ? new String[] {}\n-      : new String[] { prefix });\n-      return listing;\n+      return fileMetadata.values().toArray(new FileMetadata[fileMetadata.size()]);\n     } catch (Exception e) {\n       // Re-throw as an Azure storage exception.\n       //\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private FileMetadata[] listInternal(String prefix, final int maxListingCount,\n      final int maxListingDepth)\n      throws IOException {\n    try {\n      checkContainer(ContainerAccessType.PureRead);\n\n      if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n        prefix +\u003d PATH_DELIMITER;\n      }\n\n      // Enable flat listing option only if depth is unbounded and config\n      // KEY_ENABLE_FLAT_LISTING is enabled.\n      boolean enableFlatListing \u003d false;\n      if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n        KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n        enableFlatListing \u003d true;\n      }\n\n      Iterable\u003cListBlobItem\u003e objects;\n      if (prefix.equals(\"/\")) {\n        objects \u003d listRootBlobs(true, enableFlatListing);\n      } else {\n        objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n      }\n\n      HashMap\u003cString, FileMetadata\u003e fileMetadata \u003d new HashMap\u003c\u003e(256);\n\n      for (ListBlobItem blobItem : objects) {\n        // Check that the maximum listing count is not exhausted.\n        //\n        if (0 \u003c maxListingCount\n            \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n          break;\n        }\n\n        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n          String blobKey \u003d null;\n          CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n          BlobProperties properties \u003d blob.getProperties();\n\n          // Determine format of the blob name depending on whether an absolute\n          // path is being used or not.\n          blobKey \u003d normalizeKey(blob);\n\n          FileMetadata metadata;\n          if (retrieveFolderAttribute(blob)) {\n              metadata \u003d new FileMetadata(blobKey,\n                  properties.getLastModified().getTime(),\n                  getPermissionStatus(blob),\n                  BlobMaterialization.Explicit,\n                  hadoopBlockSize);\n          } else {\n              metadata \u003d new FileMetadata(\n                  blobKey,\n                  getDataLength(blob, properties),\n                  properties.getLastModified().getTime(),\n                  getPermissionStatus(blob),\n                  hadoopBlockSize);\n          }\n          // Add the metadata but remove duplicates.  Note that the azure\n          // storage java SDK returns two types of entries: CloudBlobWrappter\n          // and CloudDirectoryWrapper.  In the case where WASB generated the\n          // data, there will be an empty blob for each \"directory\", and we will\n          // receive a CloudBlobWrapper.  If there are also files within this\n          // \"directory\", we will also receive a CloudDirectoryWrapper.  To\n          // complicate matters, the data may not be generated by WASB, in\n          // which case we may not have an empty blob for each \"directory\".\n          // So, sometimes we receive both a CloudBlobWrapper and a\n          // CloudDirectoryWrapper for each directory, and sometimes we receive\n          // one or the other but not both.  We remove duplicates, but\n          // prefer CloudBlobWrapper over CloudDirectoryWrapper.\n          // Furthermore, it is very unfortunate that the list results are not\n          // ordered, and it is a partial list which uses continuation.  So\n          // the HashMap is the best structure to remove the duplicates, despite\n          // its potential large size.\n          fileMetadata.put(blobKey, metadata);\n\n        } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n          CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n          // Determine format of directory name depending on whether an absolute\n          // path is being used or not.\n          //\n          String dirKey \u003d normalizeKey(directory);\n          // Strip the last /\n          if (dirKey.endsWith(PATH_DELIMITER)) {\n            dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n          }\n\n          // Reached the targeted listing depth. Return metadata for the\n          // directory using default permissions.\n          //\n          // Note: Something smarter should be done about permissions. Maybe\n          // inherit the permissions of the first non-directory blob.\n          // Also, getting a proper value for last-modified is tricky.\n          FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit,\n              hadoopBlockSize);\n\n          // Add the directory metadata to the list only if it\u0027s not already\n          // there.  See earlier note, we prefer CloudBlobWrapper over\n          // CloudDirectoryWrapper because it may have additional metadata (\n          // properties and ACLs).\n          if (!fileMetadata.containsKey(dirKey)) {\n            fileMetadata.put(dirKey, directoryMetadata);\n          }\n\n          if (!enableFlatListing) {\n            // Currently at a depth of one, decrement the listing depth for\n            // sub-directories.\n            buildUpList(directory, fileMetadata, maxListingCount,\n                maxListingDepth - 1);\n          }\n        }\n      }\n      return fileMetadata.values().toArray(new FileMetadata[fileMetadata.size()]);\n    } catch (Exception e) {\n      // Re-throw as an Azure storage exception.\n      //\n      throw new AzureException(e);\n    }\n  }",
          "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "15dd1f3381069c5fdc6690e3ab1907a133ba14bf": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13675. Bug in return value for delete() calls in WASB. Contributed by Dushyanth\n",
      "commitDate": "05/12/16 12:04 PM",
      "commitName": "15dd1f3381069c5fdc6690e3ab1907a133ba14bf",
      "commitAuthor": "Mingliang Liu",
      "commitDateOld": "27/10/16 4:09 PM",
      "commitNameOld": "5877f20f9c3f6f0afa505715e9a2ee312475af17",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 38.87,
      "commitsBetweenForRepo": 283,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,111 +1,111 @@\n   private PartialListing list(String prefix, String delimiter,\n       final int maxListingCount, final int maxListingDepth, String priorLastKey)\n       throws IOException {\n     try {\n       checkContainer(ContainerAccessType.PureRead);\n \n       if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n         prefix +\u003d PATH_DELIMITER;\n       }\n \n       // Enable flat listing option only if depth is unbounded and config\n       // KEY_ENABLE_FLAT_LISTING is enabled.\n       boolean enableFlatListing \u003d false;\n       if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n         KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n         enableFlatListing \u003d true;\n       }\n \n       Iterable\u003cListBlobItem\u003e objects;\n       if (prefix.equals(\"/\")) {\n         objects \u003d listRootBlobs(true, enableFlatListing);\n       } else {\n         objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n       }\n \n       ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n       for (ListBlobItem blobItem : objects) {\n         // Check that the maximum listing count is not exhausted.\n         //\n         if (0 \u003c maxListingCount\n             \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n           break;\n         }\n \n         if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n           String blobKey \u003d null;\n           CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n           BlobProperties properties \u003d blob.getProperties();\n \n           // Determine format of the blob name depending on whether an absolute\n           // path is being used or not.\n           blobKey \u003d normalizeKey(blob);\n \n           FileMetadata metadata;\n           if (retrieveFolderAttribute(blob)) {\n             metadata \u003d new FileMetadata(blobKey,\n                 properties.getLastModified().getTime(),\n                 getPermissionStatus(blob),\n                 BlobMaterialization.Explicit);\n           } else {\n             metadata \u003d new FileMetadata(\n                 blobKey,\n                 getDataLength(blob, properties),\n                 properties.getLastModified().getTime(),\n                 getPermissionStatus(blob));\n           }\n \n           // Add the metadata to the list, but remove any existing duplicate\n           // entries first that we may have added by finding nested files.\n-          FileMetadata existing \u003d getDirectoryInList(fileMetadata, blobKey);\n+          FileMetadata existing \u003d getFileMetadataInList(fileMetadata, blobKey);\n           if (existing !\u003d null) {\n             fileMetadata.remove(existing);\n           }\n           fileMetadata.add(metadata);\n         } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n           CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n           // Determine format of directory name depending on whether an absolute\n           // path is being used or not.\n           //\n           String dirKey \u003d normalizeKey(directory);\n           // Strip the last /\n           if (dirKey.endsWith(PATH_DELIMITER)) {\n             dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n           }\n \n           // Reached the targeted listing depth. Return metadata for the\n           // directory using default permissions.\n           //\n           // Note: Something smarter should be done about permissions. Maybe\n           // inherit the permissions of the first non-directory blob.\n           // Also, getting a proper value for last-modified is tricky.\n           FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n               defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n \n           // Add the directory metadata to the list only if it\u0027s not already\n           // there.\n-          if (getDirectoryInList(fileMetadata, dirKey) \u003d\u003d null) {\n+          if (getFileMetadataInList(fileMetadata, dirKey) \u003d\u003d null) {\n             fileMetadata.add(directoryMetadata);\n           }\n \n           if (!enableFlatListing) {\n             // Currently at a depth of one, decrement the listing depth for\n             // sub-directories.\n             buildUpList(directory, fileMetadata, maxListingCount,\n                 maxListingDepth - 1);\n           }\n         }\n       }\n       // Note: Original code indicated that this may be a hack.\n       priorLastKey \u003d null;\n       PartialListing listing \u003d new PartialListing(priorLastKey,\n           fileMetadata.toArray(new FileMetadata[] {}),\n           0 \u003d\u003d fileMetadata.size() ? new String[] {}\n       : new String[] { prefix });\n       return listing;\n     } catch (Exception e) {\n       // Re-throw as an Azure storage exception.\n       //\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private PartialListing list(String prefix, String delimiter,\n      final int maxListingCount, final int maxListingDepth, String priorLastKey)\n      throws IOException {\n    try {\n      checkContainer(ContainerAccessType.PureRead);\n\n      if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n        prefix +\u003d PATH_DELIMITER;\n      }\n\n      // Enable flat listing option only if depth is unbounded and config\n      // KEY_ENABLE_FLAT_LISTING is enabled.\n      boolean enableFlatListing \u003d false;\n      if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n        KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n        enableFlatListing \u003d true;\n      }\n\n      Iterable\u003cListBlobItem\u003e objects;\n      if (prefix.equals(\"/\")) {\n        objects \u003d listRootBlobs(true, enableFlatListing);\n      } else {\n        objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n      }\n\n      ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n      for (ListBlobItem blobItem : objects) {\n        // Check that the maximum listing count is not exhausted.\n        //\n        if (0 \u003c maxListingCount\n            \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n          break;\n        }\n\n        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n          String blobKey \u003d null;\n          CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n          BlobProperties properties \u003d blob.getProperties();\n\n          // Determine format of the blob name depending on whether an absolute\n          // path is being used or not.\n          blobKey \u003d normalizeKey(blob);\n\n          FileMetadata metadata;\n          if (retrieveFolderAttribute(blob)) {\n            metadata \u003d new FileMetadata(blobKey,\n                properties.getLastModified().getTime(),\n                getPermissionStatus(blob),\n                BlobMaterialization.Explicit);\n          } else {\n            metadata \u003d new FileMetadata(\n                blobKey,\n                getDataLength(blob, properties),\n                properties.getLastModified().getTime(),\n                getPermissionStatus(blob));\n          }\n\n          // Add the metadata to the list, but remove any existing duplicate\n          // entries first that we may have added by finding nested files.\n          FileMetadata existing \u003d getFileMetadataInList(fileMetadata, blobKey);\n          if (existing !\u003d null) {\n            fileMetadata.remove(existing);\n          }\n          fileMetadata.add(metadata);\n        } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n          CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n          // Determine format of directory name depending on whether an absolute\n          // path is being used or not.\n          //\n          String dirKey \u003d normalizeKey(directory);\n          // Strip the last /\n          if (dirKey.endsWith(PATH_DELIMITER)) {\n            dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n          }\n\n          // Reached the targeted listing depth. Return metadata for the\n          // directory using default permissions.\n          //\n          // Note: Something smarter should be done about permissions. Maybe\n          // inherit the permissions of the first non-directory blob.\n          // Also, getting a proper value for last-modified is tricky.\n          FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n\n          // Add the directory metadata to the list only if it\u0027s not already\n          // there.\n          if (getFileMetadataInList(fileMetadata, dirKey) \u003d\u003d null) {\n            fileMetadata.add(directoryMetadata);\n          }\n\n          if (!enableFlatListing) {\n            // Currently at a depth of one, decrement the listing depth for\n            // sub-directories.\n            buildUpList(directory, fileMetadata, maxListingCount,\n                maxListingDepth - 1);\n          }\n        }\n      }\n      // Note: Original code indicated that this may be a hack.\n      priorLastKey \u003d null;\n      PartialListing listing \u003d new PartialListing(priorLastKey,\n          fileMetadata.toArray(new FileMetadata[] {}),\n          0 \u003d\u003d fileMetadata.size() ? new String[] {}\n      : new String[] { prefix });\n      return listing;\n    } catch (Exception e) {\n      // Re-throw as an Azure storage exception.\n      //\n      throw new AzureException(e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
      "extendedDetails": {}
    },
    "2ed58c40e5dcbf5c5303c00e85096085b1055f85": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13403. AzureNativeFileSystem rename/delete performance improvements. Contributed by Subramanyam Pattipaka.\n",
      "commitDate": "08/08/16 12:28 PM",
      "commitName": "2ed58c40e5dcbf5c5303c00e85096085b1055f85",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "04/03/16 10:57 PM",
      "commitNameOld": "c50aad0f854b74ede9668e35db314b0a93be81b2",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 156.52,
      "commitsBetweenForRepo": 1122,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,111 @@\n   private PartialListing list(String prefix, String delimiter,\n       final int maxListingCount, final int maxListingDepth, String priorLastKey)\n       throws IOException {\n     try {\n       checkContainer(ContainerAccessType.PureRead);\n \n       if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n         prefix +\u003d PATH_DELIMITER;\n       }\n \n+      // Enable flat listing option only if depth is unbounded and config\n+      // KEY_ENABLE_FLAT_LISTING is enabled.\n+      boolean enableFlatListing \u003d false;\n+      if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n+        KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n+        enableFlatListing \u003d true;\n+      }\n+\n       Iterable\u003cListBlobItem\u003e objects;\n       if (prefix.equals(\"/\")) {\n-        objects \u003d listRootBlobs(true);\n+        objects \u003d listRootBlobs(true, enableFlatListing);\n       } else {\n-        objects \u003d listRootBlobs(prefix, true);\n+        objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n       }\n \n       ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n       for (ListBlobItem blobItem : objects) {\n         // Check that the maximum listing count is not exhausted.\n         //\n         if (0 \u003c maxListingCount\n             \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n           break;\n         }\n \n         if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n           String blobKey \u003d null;\n           CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n           BlobProperties properties \u003d blob.getProperties();\n \n           // Determine format of the blob name depending on whether an absolute\n           // path is being used or not.\n           blobKey \u003d normalizeKey(blob);\n \n           FileMetadata metadata;\n           if (retrieveFolderAttribute(blob)) {\n             metadata \u003d new FileMetadata(blobKey,\n                 properties.getLastModified().getTime(),\n                 getPermissionStatus(blob),\n                 BlobMaterialization.Explicit);\n           } else {\n             metadata \u003d new FileMetadata(\n                 blobKey,\n                 getDataLength(blob, properties),\n                 properties.getLastModified().getTime(),\n                 getPermissionStatus(blob));\n           }\n \n           // Add the metadata to the list, but remove any existing duplicate\n           // entries first that we may have added by finding nested files.\n           FileMetadata existing \u003d getDirectoryInList(fileMetadata, blobKey);\n           if (existing !\u003d null) {\n             fileMetadata.remove(existing);\n           }\n           fileMetadata.add(metadata);\n         } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n           CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n           // Determine format of directory name depending on whether an absolute\n           // path is being used or not.\n           //\n           String dirKey \u003d normalizeKey(directory);\n           // Strip the last /\n           if (dirKey.endsWith(PATH_DELIMITER)) {\n             dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n           }\n \n           // Reached the targeted listing depth. Return metadata for the\n           // directory using default permissions.\n           //\n           // Note: Something smarter should be done about permissions. Maybe\n           // inherit the permissions of the first non-directory blob.\n           // Also, getting a proper value for last-modified is tricky.\n           FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n               defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n \n           // Add the directory metadata to the list only if it\u0027s not already\n           // there.\n           if (getDirectoryInList(fileMetadata, dirKey) \u003d\u003d null) {\n             fileMetadata.add(directoryMetadata);\n           }\n \n-          // Currently at a depth of one, decrement the listing depth for\n-          // sub-directories.\n-          buildUpList(directory, fileMetadata, maxListingCount,\n-              maxListingDepth - 1);\n+          if (!enableFlatListing) {\n+            // Currently at a depth of one, decrement the listing depth for\n+            // sub-directories.\n+            buildUpList(directory, fileMetadata, maxListingCount,\n+                maxListingDepth - 1);\n+          }\n         }\n       }\n       // Note: Original code indicated that this may be a hack.\n       priorLastKey \u003d null;\n       PartialListing listing \u003d new PartialListing(priorLastKey,\n           fileMetadata.toArray(new FileMetadata[] {}),\n           0 \u003d\u003d fileMetadata.size() ? new String[] {}\n       : new String[] { prefix });\n       return listing;\n     } catch (Exception e) {\n       // Re-throw as an Azure storage exception.\n       //\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private PartialListing list(String prefix, String delimiter,\n      final int maxListingCount, final int maxListingDepth, String priorLastKey)\n      throws IOException {\n    try {\n      checkContainer(ContainerAccessType.PureRead);\n\n      if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n        prefix +\u003d PATH_DELIMITER;\n      }\n\n      // Enable flat listing option only if depth is unbounded and config\n      // KEY_ENABLE_FLAT_LISTING is enabled.\n      boolean enableFlatListing \u003d false;\n      if (maxListingDepth \u003c 0 \u0026\u0026 sessionConfiguration.getBoolean(\n        KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\n        enableFlatListing \u003d true;\n      }\n\n      Iterable\u003cListBlobItem\u003e objects;\n      if (prefix.equals(\"/\")) {\n        objects \u003d listRootBlobs(true, enableFlatListing);\n      } else {\n        objects \u003d listRootBlobs(prefix, true, enableFlatListing);\n      }\n\n      ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n      for (ListBlobItem blobItem : objects) {\n        // Check that the maximum listing count is not exhausted.\n        //\n        if (0 \u003c maxListingCount\n            \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n          break;\n        }\n\n        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n          String blobKey \u003d null;\n          CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n          BlobProperties properties \u003d blob.getProperties();\n\n          // Determine format of the blob name depending on whether an absolute\n          // path is being used or not.\n          blobKey \u003d normalizeKey(blob);\n\n          FileMetadata metadata;\n          if (retrieveFolderAttribute(blob)) {\n            metadata \u003d new FileMetadata(blobKey,\n                properties.getLastModified().getTime(),\n                getPermissionStatus(blob),\n                BlobMaterialization.Explicit);\n          } else {\n            metadata \u003d new FileMetadata(\n                blobKey,\n                getDataLength(blob, properties),\n                properties.getLastModified().getTime(),\n                getPermissionStatus(blob));\n          }\n\n          // Add the metadata to the list, but remove any existing duplicate\n          // entries first that we may have added by finding nested files.\n          FileMetadata existing \u003d getDirectoryInList(fileMetadata, blobKey);\n          if (existing !\u003d null) {\n            fileMetadata.remove(existing);\n          }\n          fileMetadata.add(metadata);\n        } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n          CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n          // Determine format of directory name depending on whether an absolute\n          // path is being used or not.\n          //\n          String dirKey \u003d normalizeKey(directory);\n          // Strip the last /\n          if (dirKey.endsWith(PATH_DELIMITER)) {\n            dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n          }\n\n          // Reached the targeted listing depth. Return metadata for the\n          // directory using default permissions.\n          //\n          // Note: Something smarter should be done about permissions. Maybe\n          // inherit the permissions of the first non-directory blob.\n          // Also, getting a proper value for last-modified is tricky.\n          FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n\n          // Add the directory metadata to the list only if it\u0027s not already\n          // there.\n          if (getDirectoryInList(fileMetadata, dirKey) \u003d\u003d null) {\n            fileMetadata.add(directoryMetadata);\n          }\n\n          if (!enableFlatListing) {\n            // Currently at a depth of one, decrement the listing depth for\n            // sub-directories.\n            buildUpList(directory, fileMetadata, maxListingCount,\n                maxListingDepth - 1);\n          }\n        }\n      }\n      // Note: Original code indicated that this may be a hack.\n      priorLastKey \u003d null;\n      PartialListing listing \u003d new PartialListing(priorLastKey,\n          fileMetadata.toArray(new FileMetadata[] {}),\n          0 \u003d\u003d fileMetadata.size() ? new String[] {}\n      : new String[] { prefix });\n      return listing;\n    } catch (Exception e) {\n      // Re-throw as an Azure storage exception.\n      //\n      throw new AzureException(e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
      "extendedDetails": {}
    },
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10809. hadoop-azure: page blob support. Contributed by Dexter Bradshaw, Mostafa Elhemali, Eric Hanson, and Mike Liddell.\n",
      "commitDate": "08/10/14 2:20 PM",
      "commitName": "2217e2f8ff418b88eac6ad36cafe3a9795a11f40",
      "commitAuthor": "cnauroth",
      "commitDateOld": "24/06/14 1:52 PM",
      "commitNameOld": "0d91576ec31f63402f2db6107a04155368e2632d",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 106.02,
      "commitsBetweenForRepo": 1005,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,95 +1,101 @@\n   private PartialListing list(String prefix, String delimiter,\n       final int maxListingCount, final int maxListingDepth, String priorLastKey)\n       throws IOException {\n     try {\n       checkContainer(ContainerAccessType.PureRead);\n \n       if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n         prefix +\u003d PATH_DELIMITER;\n       }\n \n       Iterable\u003cListBlobItem\u003e objects;\n       if (prefix.equals(\"/\")) {\n         objects \u003d listRootBlobs(true);\n       } else {\n         objects \u003d listRootBlobs(prefix, true);\n       }\n \n       ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n       for (ListBlobItem blobItem : objects) {\n         // Check that the maximum listing count is not exhausted.\n         //\n-        if (0 \u003c maxListingCount \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n+        if (0 \u003c maxListingCount\n+            \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n           break;\n         }\n \n-        if (blobItem instanceof CloudBlockBlobWrapper) {\n+        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n           String blobKey \u003d null;\n-          CloudBlockBlobWrapper blob \u003d (CloudBlockBlobWrapper) blobItem;\n+          CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n           BlobProperties properties \u003d blob.getProperties();\n \n           // Determine format of the blob name depending on whether an absolute\n           // path is being used or not.\n           blobKey \u003d normalizeKey(blob);\n \n           FileMetadata metadata;\n           if (retrieveFolderAttribute(blob)) {\n-            metadata \u003d new FileMetadata(blobKey, properties.getLastModified()\n-                .getTime(), getPermissionStatus(blob),\n+            metadata \u003d new FileMetadata(blobKey,\n+                properties.getLastModified().getTime(),\n+                getPermissionStatus(blob),\n                 BlobMaterialization.Explicit);\n           } else {\n-            metadata \u003d new FileMetadata(blobKey, properties.getLength(),\n+            metadata \u003d new FileMetadata(\n+                blobKey,\n+                getDataLength(blob, properties),\n                 properties.getLastModified().getTime(),\n                 getPermissionStatus(blob));\n           }\n \n           // Add the metadata to the list, but remove any existing duplicate\n           // entries first that we may have added by finding nested files.\n           FileMetadata existing \u003d getDirectoryInList(fileMetadata, blobKey);\n           if (existing !\u003d null) {\n             fileMetadata.remove(existing);\n           }\n           fileMetadata.add(metadata);\n         } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n           CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n           // Determine format of directory name depending on whether an absolute\n           // path is being used or not.\n           //\n           String dirKey \u003d normalizeKey(directory);\n           // Strip the last /\n           if (dirKey.endsWith(PATH_DELIMITER)) {\n             dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n           }\n \n           // Reached the targeted listing depth. Return metadata for the\n           // directory using default permissions.\n           //\n           // Note: Something smarter should be done about permissions. Maybe\n           // inherit the permissions of the first non-directory blob.\n           // Also, getting a proper value for last-modified is tricky.\n           FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n               defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n \n           // Add the directory metadata to the list only if it\u0027s not already\n           // there.\n           if (getDirectoryInList(fileMetadata, dirKey) \u003d\u003d null) {\n             fileMetadata.add(directoryMetadata);\n           }\n \n           // Currently at a depth of one, decrement the listing depth for\n           // sub-directories.\n           buildUpList(directory, fileMetadata, maxListingCount,\n               maxListingDepth - 1);\n         }\n       }\n       // Note: Original code indicated that this may be a hack.\n       priorLastKey \u003d null;\n-      return new PartialListing(priorLastKey,\n+      PartialListing listing \u003d new PartialListing(priorLastKey,\n           fileMetadata.toArray(new FileMetadata[] {}),\n-          0 \u003d\u003d fileMetadata.size() ? new String[] {} : new String[] { prefix });\n+          0 \u003d\u003d fileMetadata.size() ? new String[] {}\n+      : new String[] { prefix });\n+      return listing;\n     } catch (Exception e) {\n       // Re-throw as an Azure storage exception.\n       //\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private PartialListing list(String prefix, String delimiter,\n      final int maxListingCount, final int maxListingDepth, String priorLastKey)\n      throws IOException {\n    try {\n      checkContainer(ContainerAccessType.PureRead);\n\n      if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n        prefix +\u003d PATH_DELIMITER;\n      }\n\n      Iterable\u003cListBlobItem\u003e objects;\n      if (prefix.equals(\"/\")) {\n        objects \u003d listRootBlobs(true);\n      } else {\n        objects \u003d listRootBlobs(prefix, true);\n      }\n\n      ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n      for (ListBlobItem blobItem : objects) {\n        // Check that the maximum listing count is not exhausted.\n        //\n        if (0 \u003c maxListingCount\n            \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n          break;\n        }\n\n        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\n          String blobKey \u003d null;\n          CloudBlobWrapper blob \u003d (CloudBlobWrapper) blobItem;\n          BlobProperties properties \u003d blob.getProperties();\n\n          // Determine format of the blob name depending on whether an absolute\n          // path is being used or not.\n          blobKey \u003d normalizeKey(blob);\n\n          FileMetadata metadata;\n          if (retrieveFolderAttribute(blob)) {\n            metadata \u003d new FileMetadata(blobKey,\n                properties.getLastModified().getTime(),\n                getPermissionStatus(blob),\n                BlobMaterialization.Explicit);\n          } else {\n            metadata \u003d new FileMetadata(\n                blobKey,\n                getDataLength(blob, properties),\n                properties.getLastModified().getTime(),\n                getPermissionStatus(blob));\n          }\n\n          // Add the metadata to the list, but remove any existing duplicate\n          // entries first that we may have added by finding nested files.\n          FileMetadata existing \u003d getDirectoryInList(fileMetadata, blobKey);\n          if (existing !\u003d null) {\n            fileMetadata.remove(existing);\n          }\n          fileMetadata.add(metadata);\n        } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n          CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n          // Determine format of directory name depending on whether an absolute\n          // path is being used or not.\n          //\n          String dirKey \u003d normalizeKey(directory);\n          // Strip the last /\n          if (dirKey.endsWith(PATH_DELIMITER)) {\n            dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n          }\n\n          // Reached the targeted listing depth. Return metadata for the\n          // directory using default permissions.\n          //\n          // Note: Something smarter should be done about permissions. Maybe\n          // inherit the permissions of the first non-directory blob.\n          // Also, getting a proper value for last-modified is tricky.\n          FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n\n          // Add the directory metadata to the list only if it\u0027s not already\n          // there.\n          if (getDirectoryInList(fileMetadata, dirKey) \u003d\u003d null) {\n            fileMetadata.add(directoryMetadata);\n          }\n\n          // Currently at a depth of one, decrement the listing depth for\n          // sub-directories.\n          buildUpList(directory, fileMetadata, maxListingCount,\n              maxListingDepth - 1);\n        }\n      }\n      // Note: Original code indicated that this may be a hack.\n      priorLastKey \u003d null;\n      PartialListing listing \u003d new PartialListing(priorLastKey,\n          fileMetadata.toArray(new FileMetadata[] {}),\n          0 \u003d\u003d fileMetadata.size() ? new String[] {}\n      : new String[] { prefix });\n      return listing;\n    } catch (Exception e) {\n      // Re-throw as an Azure storage exception.\n      //\n      throw new AzureException(e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
      "extendedDetails": {}
    },
    "81bc395deb3ba00567dc067d6ca71bacf9e3bc82": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-9629. Support Windows Azure Storage - Blob as a file system in Hadoop. Contributed by Dexter Bradshaw, Mostafa Elhemali, Xi Fang, Johannes Klein, David Lao, Mike Liddell, Chuan Liu, Lengning Liu, Ivan Mitic, Michael Rys, Alexander Stojanovic, Brian Swan, and Min Wei.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601781 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/06/14 3:26 PM",
      "commitName": "81bc395deb3ba00567dc067d6ca71bacf9e3bc82",
      "commitAuthor": "Chris Nauroth",
      "diff": "@@ -0,0 +1,95 @@\n+  private PartialListing list(String prefix, String delimiter,\n+      final int maxListingCount, final int maxListingDepth, String priorLastKey)\n+      throws IOException {\n+    try {\n+      checkContainer(ContainerAccessType.PureRead);\n+\n+      if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n+        prefix +\u003d PATH_DELIMITER;\n+      }\n+\n+      Iterable\u003cListBlobItem\u003e objects;\n+      if (prefix.equals(\"/\")) {\n+        objects \u003d listRootBlobs(true);\n+      } else {\n+        objects \u003d listRootBlobs(prefix, true);\n+      }\n+\n+      ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n+      for (ListBlobItem blobItem : objects) {\n+        // Check that the maximum listing count is not exhausted.\n+        //\n+        if (0 \u003c maxListingCount \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n+          break;\n+        }\n+\n+        if (blobItem instanceof CloudBlockBlobWrapper) {\n+          String blobKey \u003d null;\n+          CloudBlockBlobWrapper blob \u003d (CloudBlockBlobWrapper) blobItem;\n+          BlobProperties properties \u003d blob.getProperties();\n+\n+          // Determine format of the blob name depending on whether an absolute\n+          // path is being used or not.\n+          blobKey \u003d normalizeKey(blob);\n+\n+          FileMetadata metadata;\n+          if (retrieveFolderAttribute(blob)) {\n+            metadata \u003d new FileMetadata(blobKey, properties.getLastModified()\n+                .getTime(), getPermissionStatus(blob),\n+                BlobMaterialization.Explicit);\n+          } else {\n+            metadata \u003d new FileMetadata(blobKey, properties.getLength(),\n+                properties.getLastModified().getTime(),\n+                getPermissionStatus(blob));\n+          }\n+\n+          // Add the metadata to the list, but remove any existing duplicate\n+          // entries first that we may have added by finding nested files.\n+          FileMetadata existing \u003d getDirectoryInList(fileMetadata, blobKey);\n+          if (existing !\u003d null) {\n+            fileMetadata.remove(existing);\n+          }\n+          fileMetadata.add(metadata);\n+        } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n+          CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n+          // Determine format of directory name depending on whether an absolute\n+          // path is being used or not.\n+          //\n+          String dirKey \u003d normalizeKey(directory);\n+          // Strip the last /\n+          if (dirKey.endsWith(PATH_DELIMITER)) {\n+            dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n+          }\n+\n+          // Reached the targeted listing depth. Return metadata for the\n+          // directory using default permissions.\n+          //\n+          // Note: Something smarter should be done about permissions. Maybe\n+          // inherit the permissions of the first non-directory blob.\n+          // Also, getting a proper value for last-modified is tricky.\n+          FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n+              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n+\n+          // Add the directory metadata to the list only if it\u0027s not already\n+          // there.\n+          if (getDirectoryInList(fileMetadata, dirKey) \u003d\u003d null) {\n+            fileMetadata.add(directoryMetadata);\n+          }\n+\n+          // Currently at a depth of one, decrement the listing depth for\n+          // sub-directories.\n+          buildUpList(directory, fileMetadata, maxListingCount,\n+              maxListingDepth - 1);\n+        }\n+      }\n+      // Note: Original code indicated that this may be a hack.\n+      priorLastKey \u003d null;\n+      return new PartialListing(priorLastKey,\n+          fileMetadata.toArray(new FileMetadata[] {}),\n+          0 \u003d\u003d fileMetadata.size() ? new String[] {} : new String[] { prefix });\n+    } catch (Exception e) {\n+      // Re-throw as an Azure storage exception.\n+      //\n+      throw new AzureException(e);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private PartialListing list(String prefix, String delimiter,\n      final int maxListingCount, final int maxListingDepth, String priorLastKey)\n      throws IOException {\n    try {\n      checkContainer(ContainerAccessType.PureRead);\n\n      if (0 \u003c prefix.length() \u0026\u0026 !prefix.endsWith(PATH_DELIMITER)) {\n        prefix +\u003d PATH_DELIMITER;\n      }\n\n      Iterable\u003cListBlobItem\u003e objects;\n      if (prefix.equals(\"/\")) {\n        objects \u003d listRootBlobs(true);\n      } else {\n        objects \u003d listRootBlobs(prefix, true);\n      }\n\n      ArrayList\u003cFileMetadata\u003e fileMetadata \u003d new ArrayList\u003cFileMetadata\u003e();\n      for (ListBlobItem blobItem : objects) {\n        // Check that the maximum listing count is not exhausted.\n        //\n        if (0 \u003c maxListingCount \u0026\u0026 fileMetadata.size() \u003e\u003d maxListingCount) {\n          break;\n        }\n\n        if (blobItem instanceof CloudBlockBlobWrapper) {\n          String blobKey \u003d null;\n          CloudBlockBlobWrapper blob \u003d (CloudBlockBlobWrapper) blobItem;\n          BlobProperties properties \u003d blob.getProperties();\n\n          // Determine format of the blob name depending on whether an absolute\n          // path is being used or not.\n          blobKey \u003d normalizeKey(blob);\n\n          FileMetadata metadata;\n          if (retrieveFolderAttribute(blob)) {\n            metadata \u003d new FileMetadata(blobKey, properties.getLastModified()\n                .getTime(), getPermissionStatus(blob),\n                BlobMaterialization.Explicit);\n          } else {\n            metadata \u003d new FileMetadata(blobKey, properties.getLength(),\n                properties.getLastModified().getTime(),\n                getPermissionStatus(blob));\n          }\n\n          // Add the metadata to the list, but remove any existing duplicate\n          // entries first that we may have added by finding nested files.\n          FileMetadata existing \u003d getDirectoryInList(fileMetadata, blobKey);\n          if (existing !\u003d null) {\n            fileMetadata.remove(existing);\n          }\n          fileMetadata.add(metadata);\n        } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\n          CloudBlobDirectoryWrapper directory \u003d (CloudBlobDirectoryWrapper) blobItem;\n          // Determine format of directory name depending on whether an absolute\n          // path is being used or not.\n          //\n          String dirKey \u003d normalizeKey(directory);\n          // Strip the last /\n          if (dirKey.endsWith(PATH_DELIMITER)) {\n            dirKey \u003d dirKey.substring(0, dirKey.length() - 1);\n          }\n\n          // Reached the targeted listing depth. Return metadata for the\n          // directory using default permissions.\n          //\n          // Note: Something smarter should be done about permissions. Maybe\n          // inherit the permissions of the first non-directory blob.\n          // Also, getting a proper value for last-modified is tricky.\n          FileMetadata directoryMetadata \u003d new FileMetadata(dirKey, 0,\n              defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);\n\n          // Add the directory metadata to the list only if it\u0027s not already\n          // there.\n          if (getDirectoryInList(fileMetadata, dirKey) \u003d\u003d null) {\n            fileMetadata.add(directoryMetadata);\n          }\n\n          // Currently at a depth of one, decrement the listing depth for\n          // sub-directories.\n          buildUpList(directory, fileMetadata, maxListingCount,\n              maxListingDepth - 1);\n        }\n      }\n      // Note: Original code indicated that this may be a hack.\n      priorLastKey \u003d null;\n      return new PartialListing(priorLastKey,\n          fileMetadata.toArray(new FileMetadata[] {}),\n          0 \u003d\u003d fileMetadata.size() ? new String[] {} : new String[] { prefix });\n    } catch (Exception e) {\n      // Re-throw as an Azure storage exception.\n      //\n      throw new AzureException(e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java"
    }
  }
}